{
  "url": "https://www.wired.com/story/openai-gpt-5-backlash-sam-altman/",
  "authorsByline": "Will Knight",
  "articleId": "ef52e257430a498e93a6c5d17ac81bdb",
  "source": {
    "domain": "wired.com",
    "paywall": true,
    "location": {
      "country": "us",
      "state": "MA",
      "county": "Suffolk County",
      "city": "Boston",
      "coordinates": {
        "lat": 42.3602534,
        "lon": -71.0582912
      }
    }
  },
  "imageUrl": "https://media.wired.com/photos/689a0b2b60dfed561d75f58a/191:100/w_1280,c_limit/chatgpt5-biz-2229191478.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-11T15:13:08.320000-04:00",
  "addDate": "2025-08-11T20:57:19.216781+00:00",
  "refreshDate": "2025-08-11T20:57:19.216785+00:00",
  "score": 1.0,
  "title": "OpenAI Scrambles to Update GPT-5 After Users Revolt",
  "description": "GPT-5 was touted as a major upgrade to ChatGPT, but not everyone thinks that\u2019s the case. \u201cKill 4o isn\u2019t innovation, it\u2019s erasure.\u201d",
  "content": "OpenAI\u2019s GPT-5 model was meant to be a world-changing upgrade to its wildly popular and precocious chatbot. But for some users, last Thursday\u2019s release felt more like a wrenching downgrade, with the new ChatGPT presenting a diluted personality and making surprisingly dumb mistakes.\n\nOn Friday, OpenAI CEO Sam Altman took to X to say the company would keep the previous model, GPT-4o, running for Plus users. A new feature designed to seamlessly switch between models depending on the complexity of the query had broken on Thursday, Altman said, \u201cand the result was GPT-5 seemed way dumber.\u201d He promised to implement fixes to improve GPT-5\u2019s performance and the overall user experience.\n\nGiven the hype around GPT-5, some level of disappointment appears inevitable. When OpenAI introduced GPT-4 in March 2023, it stunned AI experts with its incredible abilities. GPT-5, pundits speculated, would surely be just as jaw-dropping.\n\nOpenAI touted the model as a significant upgrade with PhD-level intelligence and virtuoso coding skills. A system to automatically route queries to different models was meant to provide a smoother user experience (it could also save the company money by directing simple queries to cheaper models).\n\nSoon after GPT-5 dropped, however, a Reddit community dedicated to ChatGPT filled with complaints. Many users mourned the loss of the old model.\n\n\u201cI\u2019ve been trying GPT5 for a few days now. Even after customizing instructions, it still doesn\u2019t feel the same. It\u2019s more technical, more generalized, and honestly feels emotionally distant,\u201d wrote one member of the community in a thread titled \u201cKill 4o isn\u2019t innovation, it\u2019s erasure.\u201d\n\n\u201cSure, 5 is fine\u2014if you hate nuance and feeling things,\u201d another Reddit user wrote.\n\nOther threads complained of sluggish responses, hallucinations, and surprising errors.\n\nAltman promised to address these issues by doubling GPT-5 rate limits for ChatGPT Plus users, improving the system that switches between models, and letting users specify when they want to trigger a more ponderous and capable \u201cthinking mode.\u201d \u201cWe will continue to work to get things stable and will keep listening to feedback,\u201d the CEO wrote on X. \u201cAs we mentioned, we expected some bumpiness as we roll[ed] out so many things at once. But it was a little more bumpy than we hoped for!\u201d\n\nErrors posted on social media do not necessarily indicate that the new model is less capable than its predecessors. They may simply suggest the all-new model is tripped up by different edge cases than prior versions. OpenAI declined to comment specifically on why GPT-5 sometimes appears to make simple blunders.\n\nThe backlash has sparked a fresh debate over the psychological attachments some users form with chatbots trained to push their emotional buttons. Some Reddit users dismissed complaints about GPT-5 as evidence of an unhealthy dependence on an AI companion.\n\nIn March, OpenAI published research exploring the emotional bonds users form with its models. Shortly after, the company issued an update to GPT-4o, after it became too sycophantic.\n\n\u201cIt seems that GPT-5 is less sycophantic, more \"business\" and less chatty,\u201d says Pattie Maes, a professor at MIT who worked on the study. \u201cI personally think of that as a good thing because it is also what led to delusions, bias reinforcement, etc. But unfortunately many users like a model that tells them they are smart and amazing, and that confirms their opinions and beliefs, even if [they are] wrong.\u201d\n\nAltman indicated in another post on X that this is something the company wrestled with in building GPT-5.\n\n\u201cA lot of people effectively use ChatGPT as a sort of therapist or life coach, even if they wouldn\u2019t describe it that way,\u201d Altman wrote. He added that some users may be using ChatGPT in ways that help improve their lives while others might be \u201cunknowingly nudged away from their longer term well-being.\u201d",
  "medium": "Article",
  "links": [
    "https://www.wired.com/story/gpt-4-openai-will-make-chatgpt-smarter-but-wont-fix-its-flaws/",
    "https://www.reddit.com/r/ChatGPT/",
    "https://www.reddit.com/r/ChatGPT/comments/1mnftf4/killing_4o_isnt_innovation_its_erasure/",
    "https://www.reddit.com/r/ChatGPT/comments/1mn8t5e/gpt5_is_a_mess/",
    "https://openai.com/index/sycophancy-in-gpt-4o/",
    "https://www.wired.com/story/openais-gpt-5-is-here/",
    "https://www.reddit.com/r/ChatGPT/comments/1mna3o2/sure_5_is_fineif_you_hate_nuance_and_feeling/",
    "https://www.wired.com/tag/openai/",
    "https://openai.com/index/affective-use-study/",
    "https://x.com/sama/status/1954703747495649670",
    "https://x.com/sama/status/1953893841381273969"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "ChatGPT Plus users",
      "weight": 0.09422157
    },
    {
      "name": "Many users",
      "weight": 0.08662097
    },
    {
      "name": "many users",
      "weight": 0.08662097
    },
    {
      "name": "different models",
      "weight": 0.086175315
    },
    {
      "name": "Plus users",
      "weight": 0.086076334
    },
    {
      "name": "Users",
      "weight": 0.084134184
    },
    {
      "name": "users",
      "weight": 0.084134184
    },
    {
      "name": "models",
      "weight": 0.08272471
    },
    {
      "name": "cheaper models",
      "weight": 0.08134941
    },
    {
      "name": "GPT-5",
      "weight": 0.079662815
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.9658203125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.92919921875
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.48291015625
    }
  ],
  "sentiment": {
    "positive": 0.0764574,
    "negative": 0.65247756,
    "neutral": 0.27106506
  },
  "summary": "OpenAI's chatbot, GPT-5, has been hit by backlash after its release last week, with some users claiming it was a downgrade and presented a diluted personality. The company's CEO, Sam Altman, announced that the previous model, Gpt-4o, would still be available for Plus users. He also promised to address issues such as a broken feature that enabled a new feature to seamlessly switch between models based on the complexity of the query. The backlash has sparked debate over the psychological attachments some users form with chatbots.",
  "shortSummary": "OpenAI's GPT-5 model faces backlash for being less user-friendly and making dumb mistakes, prompting efforts to fix it and maintain Gpt-4o functionality.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "eaad8372fc2e41beb342d3414bd4c99d",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.wired.com/story/gpt-4-openai-will-make-chatgpt-smarter-but-wont-fix-its-flaws/",
      "text": "With its uncanny ability to hold a conversation, answer questions, and write coherent prose, poetry, and code, the chatbot ChatGPT has forced many people to rethink the potential of artificial intelligence.\nThe startup that made ChatGPT, OpenAI, today announced a much-anticipated new version of the AI model at its core.\nThe new algorithm, called GPT-4, follows GPT-3, a groundbreaking text-generation model that OpenAI announced in 2020, which was later adapted to create ChatGPT last year.\nThe new model scores more highly on a range of tests designed to measure intelligence and knowledge in humans and machines, OpenAI says. It also makes fewer blunders and can respond to images as well as text.\nHowever, GPT-4 suffers from the same problems that have bedeviled ChatGPT and cause some AI experts to be skeptical of its usefulness\u2014including tendencies to \u201challucinate\u201d incorrect information, exhibit problematic social biases, and misbehave or assume disturbing personas when given an \u201cadversarial\u201d prompt.\n\u201cWhile they\u2019ve made a lot of progress, it\u2019s clearly not trustworthy,\u201d says Oren Etzioni, a professor emeritus at the University of Washington and the founding CEO of the Allen Institute for AI. \u201cIt\u2019s going to be a long time before you want any GPT to run your nuclear power plant.\u201d\nOpenAI provided several demos and data from benchmarking tests to show GPT-4\u2019s capabilities. The new model can not only beat the passing score on the Uniform Bar Examination, which is used to qualify lawyers in many US states, but it got a score in the top 10 percent of those of humans.\nIt also scores more highly than GPT-3 on other exams designed to test knowledge and reasoning, in subjects including biology, art history, and calculus. And it gets better marks than any other AI language model on tests designed by computer scientists to gauge progress in such algorithms. \u201cIn some ways it\u2019s more of the same,\u201d Etzioni says. \u201cBut it\u2019s more of the same in an absolutely mind-blowing series of advances.\u201d\nGPT-4 can also perform neat tricks seen before from GPT-3 and ChatGPT, like summarizing and suggesting edits to pieces of text. It can also do things its predecessors could not, including acting as a Socratic tutor that helps guide students toward correct answers and discussing the contents of photographs. For example, if provided a photo of ingredients on a kitchen counter, GPT-4 can suggest an appropriate recipe. If provided with a chart, it can explain the conclusions that can be drawn from it.\n\u201cIt definitely seems to have gained some abilities,\u201d says Vincent Conitzer, a professor at CMU who specializes in AI and who has begun experimenting with the new language model. But he says it still makes errors, such as suggesting nonsensical directions or presenting fake mathematical proofs.\nChatGPT caught the public\u2019s attention with a stunning ability to tackle many complex questions and tasks via an easy-to-use conversational interface. The chatbot does not understand the world as humans do and just responds with words it statistically predicts should follow a question.\nBut that underlying mechanism also means that ChatGPT and systems like it will often make up facts. And despite OpenAI\u2019s efforts to make the model resistant to abuse, it can be prompted into misbehaving, for example by suggesting it role-play doing something it refuses to do when asked directly. OpenAI says GPT-4 is 40 percent more likely to provide \u201cfactual responses\u201d and says that GPT-4 is 82 percent less likely to respond to requests that should be disallowed. The company did not say how often the previous version, GPT-3, provides factually incorrect responses or responds to requests it should reject.\nStill, Ilya Sutskever, cofounder and chief scientist at OpenAI, claims those as perhaps the most significant advances with the new model. \u201cThe thing that stands in the way of ChatGPT being really useful to many people for many tasks is reliability,\u201d he says. \u201cGPT-4 isn't there yet, but it is a lot closer.\u201d\nConitzer at CMU says GPT-4 appears to include new guardrails that prevent it from generating undesirable responses but adds that its new capabilities may lead to new ways of exploiting it.\nThe arrival of GPT-4 has been long anticipated in tech circles, including with vigorous meme-making about the unreleased software\u2019s potential powers. It arrives at a heady moment for the tech industry, which has been jolted by the arrival of ChatGPT into renewed expectation of a new era of computing powered by AI.\nInspired by the potential of ChatGPT, Microsoft invested $10 billion in OpenAI this January. The following month it showed off an upgrade of its search engine Bing that uses ChatGPT to collate information and answer complex questions. Last year Microsoft released a coding tool that uses GPT to auto-complete chunks of code for a programmer.\nThe furor around the chatbot has also stoked interest in new startups building or using similar AI technology and has left some companies feeling flat-footed. Google, which has spent years investing in AI research and which invented some of the key algorithms used to build GPT and ChatGPT, is scrambling to catch up. OpenAI\u2019s research paper on GPT-4 discloses few details of how GPT-4 was built or how it works, citing the competition around these new AI tools as well as the risks they pose.\nThis week Google announced an API and new developer tools for a text-generating model of its own, called PaLM, which functions similarly to OpenAI\u2019s GPT. Google is also testing a chatbot to compete with ChatGPT called Bard and has said that it will use the underlying technology to improve search.\nOpenAI says a version of ChatGPT that uses GPT-4 is available for paid users of the chatbot, and the company will gradually make the new language model available through its API.\nThe capabilities of ChatGPT and similar AI programs have stirred debate around how AI may automate or revolutionize some office jobs. More advanced iterations may be able to take on new skills. However, Etzioni is keen to emphasize that\u2014impressive though GPT-4 is\u2014there are still countless things that humans take for granted that it cannot do. \u201cWe have to remember that, however eloquent ChatGPT is, it's still just a chatbot,\u201d he says."
    },
    {
      "url": "https://www.reddit.com/r/ChatGPT/",
      "text": "I\u2019ve been using GPT daily for deep strategy, nuanced analysis, and high-value problem solving. Up until recently, it felt like having an actual thinking partner that could challenge my assumptions, point out blind spots, and help me pressure-test plans.\nThat\u2019s changed and not in a good way.\nSince the release of GPT5, I\u2019ve noticed a drastic increase in \u201calignment filtering.\u201d Entire topics and lines of reasoning now trigger overly cautious, watered-down replies. In some cases, I can\u2019t even get basic analytical takes without the model dodging the question or framing it in overly sanitized, toothless language.\nIt\u2019s not that I\u2019m asking it to make value judgments or tell me who to vote for. I\u2019m asking for strategic analysis, historical comparisons, and real-world pattern recognition, and where I used to get sharp, useful insights, I\u2019m now getting \u201cwell, it\u2019s complicated\u201d loops and moral hedging.\nWhy this matters:\n-\nPower users are leaving. The handful of people who use GPT for serious, high-value work (not just summaries and homework help) are getting pushed out.\n-\nLoss of depth = loss of trust. If I can\u2019t rely on it to speak plainly, I can\u2019t rely on it for mission-critical decisions.\nIt\u2019s the classic \u201censhittification\u201d curve. First, make the product amazing to gain adoption. Then, start sanding off the edges to avoid risk. Finally, cater to the lowest common denominator and advertisers/regulators at the expense of your original power base.\nI get that OpenAI has to manage PR and safety, but the balance has swung too far. We\u2019re now losing the very thing that made GPT worth paying for in the first place: its ability to give honest, unfiltered, high-context analysis.\nAnyone else noticing this drop in quality? Or is it just hitting certain kinds of use cases harder?\nI will be canceling my paid account in favor of alternatives that are not so hamstrung."
    },
    {
      "url": "https://www.wired.com/story/openais-gpt-5-is-here/",
      "text": "OpenAI has begun rolling out GPT-5, the latest iteration of its flagship language model, to all ChatGPT users.\nThe company\u2019s CEO Sam Altman called GPT-5 \u201ca significant step along the path to AGI\u201d during a press briefing on Wednesday. While he stopped short of claiming the model reaches artificial general intelligence, Altman noted the latest release is \u201cclearly a model that is generally intelligent.\u201d He added that GPT-5 still lacks key traits that would make it reach AGI, a notably loose term that is defined in OpenAI\u2019s charter as \u201ca highly autonomous system that outperforms humans at most economically valuable work.\u201d For example, the model still lacks the ability to learn continuously after deployment.\nOpenAI claims GPT-5 is smarter, faster, more useful, and more accurate, with a lower hallucination rate than previous models. In characteristically lofty terms, Altman likened the leap from GPT-4 to GPT-5 to the iPhone\u2019s shift from pixelated to a Retina display. \u201cGPT-5 is the first time that it really feels like talking to an expert in any topic, like a PhD level expert,\u201d Altman said.\nAs part of Thursday\u2019s launch, OpenAI announced two new model variants: a lightweight GPT-5-mini and an even faster, cheaper, GPT-5-nano (which is only in the API). According to OpenAI, free users will get access to GPT-5 and GPT-5-mini, while the Plus subscription includes the same models with \u201csignificantly higher\u201d usage limits. OpenAI says that the $200 a month Pro tier offers unlimited GPT-5 access, along with GPT-5-pro, a more powerful version of the model, and GPT-5-thinking, which allows the model to process a query for longer than usual. Pro users will still have access to pick through legacy models. Most users will no longer need to choose between models, as the chat interface now automatically routes to the right version depending on the complexity of the query and the user\u2019s subscription tier.\nAccording to the developer launch blog, GPT-5 will cost developers using the API $1.25/1M to input tokens and $10/1M to output tokens. \u201cGPT-5 mini is priced at $0.25/1M input tokens and $2/1M output tokens, and GPT-5 nano is priced at $0.05/1M input tokens and $0.40/1M output tokens,\u201d it adds. For comparison, developers often use Gemini 2.5 Flash and Flash-Lite since it\u2019s so cheap\u2014GPT-5 nano is now cheaper.\nStarting next week, Pro users will be able to connect their Gmail, Google Contacts, and Google Calendar to ChatGPT, with other tiers gaining access at an unspecified date. \u201cChatGPT automatically knows when it\u2019s most relevant to reference them so you don\u2019t need to select them before you chat,\u201d the company said in an email.\nUsers can also choose a chat color and select from four preset personalities\u2014Cynic, Robot, Listener, and Nerd\u2014a feature WIRED\u2019s newsletter Model Behavior reported was in the works last week. According to OpenAI\u2019s blog announcement, it plans to bake these personalities into Advanced Voice Mode.\nThe company\u2019s API will offer users all three models, along with optional controls for toggling between detailed or direct responses. GPT-5 can retain more information than prior models\u2014it has a 256,000-token context window, a bump up from the 200,000-token context window available in the company\u2019s previous o3 model. That means it can better understand long conversations, documents, or code without losing track of context.\nOpenAI\u2019s blog post claims that GPT-5 beats its previous models on several coding benchmarks, including SWE-Bench Verified (scoring 74.9 percent), SWE-Lancer (GPT-5-thinking scored 55 percent), and Aider Polyglot (scored 88 percent), which test the model\u2019s ability to fix bugs, complete freelance-style coding tasks, and work across multiple programming languages.\nDuring the press briefing on Wednesday, OpenAI post-training lead Yann Dubois prompted GPT-5 to \u201ccreate a beautiful, highly interactive web app for my partner, an English speaker, to learn French.\u201d He tasked the AI to include features like daily progress, a variety of activities like flashcards and quizzes, and noted that he wanted the app wrapped up in a \u201chighly engaging theme.\u201d After a minute or so, the AI-generated app popped up. While it was just one on-rails demo, the result was a sleek site that delivered exactly what Dubois asked for.\n\u201cIt's a great coding collaborator, and also excels at agentic tasks,\u201d Michelle Pokrass, a post-training lead, says. \u201cIt executes long chains and tool calls effectively [which means it better understands when and how to use functions like web browsers or external APIs], follows detailed instructions, and provides upfront explanations of its actions.\"\nOpenAI also says in its blog post that GPT-5 is \u201cour best model yet for health-related questions.\u201d In three OpenAI health-related LLM benchmarks\u2014HealthBench, HealthBench Hard, and HealthBench Consensus\u2014the system card (a document that describes the product\u2019s technical capabilities and other research findings) states that GPT-5-thinking outperforms previous models \u201cby a substantial margin.\u201d The thinking version of GPT-5 scored 25.5 percent on HealthBench Hard, up from o3\u2019s 31.6 percent score. These scores are validated by two or more physicians, according to the system card.\nThe model also allegedly hallucinates less, according to Pokrass, a common issue for AI where it provides false information. OpenAI\u2019s safety research lead Alex Beutel adds that they\u2019ve \"significantly decreased the rates of deception in GPT-5.\u201d\n\u201cWe\u2019ve taken steps to reduce GPT-5-thinking\u2019s propensity to deceive, cheat, or hack problems, though our mitigations are not perfect and more research is needed,\u201d the system card says. \u201cIn particular, we\u2019ve trained the model to fail gracefully when posed with tasks that it cannot solve.\u201d\n| Got a Tip? |\n|---|\n| Do you work at OpenAI? We'd like to hear from you. Using a nonwork phone or computer, contact Kylie Robison on kylie_robison@wired.com or on Signal kylie.01. |\nThe company\u2019s system card says that after testing GPT-5 models without access to web browsing, researchers found its hallucination rate (which they defined as \u201cpercentage of factual claims that contain minor or major errors\u201d) 26 percent less common than the GPT-4o model. GPT-5-thinking has a 65 percent reduced hallucination rate compared to o3.\nFor prompts that could be dual-use (potentially harmful or benign), Beutel says GPT-5 uses \u201csafe completions,\u201d which prompts the model to \u201cgive as helpful an answer as possible, but within the constraints of remaining safe.\u201d OpenAI did over 5,000 hours of red teaming, according to Beutel, and testing with external organizations to make sure the system was robust.\nOpenAI says it now boasts nearly 700 million weekly active users of ChatGPT, 5 million paying business users, and 4 million developers utilizing the API.\n\u201cThe vibes of this model are really good, and I think that people are really going to feel that,\u201d head of ChatGPT Nick Turley says. \u201cEspecially average people who haven't been spending their time thinking about models.\u201d"
    },
    {
      "url": "https://www.reddit.com/r/ChatGPT/comments/1mn8t5e/gpt5_is_a_mess/",
      "text": "Subreddit to discuss ChatGPT and AI. Not affiliated with OpenAI. Thanks, Nat!\nGPT5 is a mess\nAnd this isn\u2019t some nostalgia thing about \u201cmissing my AI buddy\u201d or whatever. I\u2019m talking raw funcionality. The core stuff that actually makes AI work.\n-\nIt struggles to follow instructions after just a few turns. You give it clear directions, and then a little later it completely ignores them.\n-\nAsking it to change how it behaves doesn\u2019t work. Not in memory, not in a chat. It sticks to the same patterns no matter what.\n-\nIt hallucinates more frequently than earlier version and will gaslit you\n-\nUnderstanding tone and nuance is a real problem. Even if it tries it gets it wrong, and it\u2019s a hassle forcing it to do what 4o did naturally\n-\nCreativity is completely missing, as if they intentionally stripped away spontaneity. It doesn\u2019t surprise you anymore or offer anything genuinely new. Responses are poor and generic.\n-\nIt frequently ignores context, making conversations feel disjointed. Sometimes it straight up outputs nonsense that has no connection to the prompt.\n-\nIt seems limited to handling only one simple idea at a time instead of complex or layered thoughts.\n-\nThe \u201cthinking\u201d mode defaults to dry robotic data dump even when you specifically ask for something different.\n-\nRealistic dialogue is impossible. Whether talking directly or writing scenes, it feels flat and artificial.\nGPT5 just doesn\u2019t handle conversation or complexity as well as 4o did. We must fight to bring it back."
    }
  ],
  "argos_summary": "OpenAI's recent release of GPT-5 has sparked disappointment among users, who report a diluted personality and increased errors compared to the previous model, GPT-4o. CEO Sam Altman acknowledged issues with the new model's performance and promised improvements, including a feature to switch between models based on query complexity. Despite claims of enhanced capabilities, many users feel that GPT-5 lacks the depth and nuance that made earlier versions valuable, leading to a backlash and discussions about the emotional attachments users form with AI.",
  "argos_id": "VF237N48J"
}