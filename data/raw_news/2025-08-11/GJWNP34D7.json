{
  "url": "https://www.techradar.com/ai-platforms-assistants/roblox-is-sharing-its-ai-tool-to-fight-toxic-game-chats-heres-why-that-matters-for-kids",
  "authorsByline": "Eric Hal Schwartz",
  "articleId": "b09ec70b104d4441885d8e1bbd61f228",
  "source": {
    "domain": "techradar.com",
    "paywall": false,
    "location": {
      "country": "in",
      "city": "New Delhi",
      "coordinates": {
        "lat": 28.6138954,
        "lon": 77.2090057
      }
    }
  },
  "imageUrl": "https://cdn.mos.cms.futurecdn.net/YnWLeAyMaNvQEyu7rtmsb-1280-80.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-11T21:00:00+00:00",
  "addDate": "2025-08-11T21:52:07.772426+00:00",
  "refreshDate": "2025-08-11T21:52:07.772429+00:00",
  "score": 1.0,
  "title": "Game chats are full of toxicity \u2013 Roblox\u2019s free AI could help fix that",
  "description": "Parents can hope AI spots problem conversations before they start",
  "content": "Online game chats are notorious for vulgar, offensive, and even criminal behavior. Even if only a tiny percentage, the many millions of hours of chat can accumulate a lot of toxic interactions in a way that's a problem for players and video game companies, especially when it involves kids. Roblox has a lot of experience dealing with that aspect of gaming and has used AI to create a whole system to enforce safety rules among its more than 100 million mostly young daily users, Sentinel. Now, it's open-sourcing Sentinel, offering the AI and its capacity for identifying grooming and other dangerous behavior in chat before it escalates for free to any platform.\n\nThis isn\u2019t just a profanity filter that gets triggered when someone types a curse word. Roblox has always had that. Sentinel is built to watch patterns over time. It can track how conversations evolve, looking for subtle signs that someone is trying to build trust with a kid in potentially problematic ways. For instance, it might flag a long conversation where an adult-sounding player is just a little too interested in a kid\u2019s personal life.\n\nSentinel helped Roblox moderators file about 1,200 reports to the National Center for Missing and Exploited Children in just the first half of this year. As someone who grew up in the Wild West of early internet chatrooms, where \u201cmoderation\u201d usually meant suspecting that people who used correct spelling and grammar were adults, I can\u2019t overstate how much of a leap forward that feels.\n\nOpen-sourcing Sentinel means any game or online platform, whether as big as Minecraft or as small as an underground indie hit, can adapt Sentinel and use it to make their own communities safer. It\u2019s an unusually generous move, albeit one with obvious public relations and potential long-term commercial benefits for the company.\n\nFor kids (and their adult guardians), the benefits are obvious. If more games start running Sentinel-style checks, the odds of predators slipping through the cracks go down. Parents get another invisible safety net they didn\u2019t have to set up themselves. And the kids get to focus on playing rather than navigating the online equivalent of a dark alley.\n\nFor video games as a whole, it\u2019s a chance to raise the baseline of safety. Imagine if every major game, from the biggest esports titles to the smallest cozy simulators, had access to the same kind of early-warning system. It wouldn\u2019t eliminate the problem, but it could make bad behavior a lot harder to hide.\n\nOf course, nothing with \u201cAI\u201d in the description is without its complications. The most obvious one is privacy. This kind of tool works by scanning what people are saying to each other, in real time, looking for red flags. Roblox says it uses one-minute snapshots of chat and keeps a human review process for anything flagged. But you can\u2019t really get around the fact that this is surveillance, even if it\u2019s well-intentioned. And when you open-source a tool like this, you\u2019re not just giving the good guys a copy; you also make it easier for bad actors to see how you're stopping them and come up with ways around the system.\n\nThen there\u2019s the problem of language itself. People change how they talk all the time, especially online. Slang shifts, in-jokes mutate, and new apps create new shorthand. A system trained to catch grooming attempts in 2024 might miss the ones happening in 2026. Roblox updates Sentinel regularly, both with AI training and human review, but smaller platforms might not have the resources to keep up with what's happening in their chats.\n\nAnd while no sane person is against stopping child predators or jerks deliberately trying to upset children, AI tools like this can be abused. If certain political talk, controversial opinions, or simply complaints about the game are added to the filter list, there's little players can do about it. Roblox and any companies using Sentinel will need to be transparent, not just with the code, but also with how it's being deployed and what the data it collects will be used for.\n\nIt's also important to consider the context of Roblox's decision. The company is facing lawsuits over what's happened with children using the platform. One lawsuit alleges a 13\u2011year\u2011old was trafficked after meeting a predator on the platform. Sentinel isn't perfect, and companies using it could still face legal problems. Ideally, it would serve as a component of online safety setups that include things like better user education and parental controls. AI can't replace all safety programs.\n\nDespite the very real problems of deploying AI to help with online safety, I think open-sourcing Sentinel is one of the rare cases where the upside of using AI is both immediate and tangible. I\u2019ve written enough about algorithms making people angry, confused, or broke to appreciate when one is actually pointed toward making people safer. And making it open-source can help make more online spaces safer.\n\nI don\u2019t think Sentinel will stop every predator, and I don\u2019t think it should be a replacement for good parenting, better human moderation, and educating kids about how to be safe when playing online. But as a subtle extra line of defense, Sentinel has a part to play in building better online experiences for kids.\n\nYou might also like",
  "medium": "Article",
  "links": [
    "https://www.techradar.com/ai-platforms-assistants/chatgpt/openai-pulls-chat-sharing-tool-after-google-search-privacy-scare",
    "https://www.techradar.com/ai-platforms-assistants/chatgpt-just-got-so-much-smarter-heres-how-to-talk-to-your-kids-about-ai-and-gpt-5",
    "https://www.techradar.com/computing/artificial-intelligence/would-you-buy-your-child-a-chatgpt-powered-barbie-im-queasy-at-the-prospect-of-a-real-life-small-soldiers-scenario",
    "https://www.desmoinesregister.com/story/news/crime-and-courts/2025/08/07/roblox-lawsuit-iowa-kidnapping-13-year-old-groomed/85513448007/",
    "https://www.techradar.com/pro/5-reasons-why-gemini-ai-is-bad-news-for-kids",
    "https://www.techradar.com/tag/esports",
    "https://www.techradar.com/computing/artificial-intelligence/ai-godfather-sounds-the-alarm-on-autonomous-ai",
    "https://www.techradar.com/computing/artificial-intelligence/character-ai-institutes-new-safety-measures-for-ai-chatbot-conversations",
    "https://www.techradar.com/tag/minecraft"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Online game chats",
      "weight": 0.084522635
    },
    {
      "name": "Game chats",
      "weight": 0.07467009
    },
    {
      "name": "online safety",
      "weight": 0.07460967
    },
    {
      "name": "online safety setups",
      "weight": 0.072119296
    },
    {
      "name": "Sentinel",
      "weight": 0.06930039
    },
    {
      "name": "video game companies",
      "weight": 0.06869255
    },
    {
      "name": "online platform",
      "weight": 0.06727281
    },
    {
      "name": "better online experiences",
      "weight": 0.06692893
    },
    {
      "name": "more games",
      "weight": 0.06676571
    },
    {
      "name": "video games",
      "weight": 0.06298469
    }
  ],
  "topics": [
    {
      "name": "Gaming"
    },
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Games/Computer & Video Games/Gaming Reference & Reviews",
      "score": 0.8662109375
    },
    {
      "name": "/Internet & Telecom/Mobile & Wireless/Mobile Apps & Add-Ons",
      "score": 0.5478515625
    },
    {
      "name": "/News/Technology News",
      "score": 0.51513671875
    },
    {
      "name": "/Games/Computer & Video Games/Action & Platform Games",
      "score": 0.35302734375
    },
    {
      "name": "/Games/Computer & Video Games/Adventure Games",
      "score": 0.304931640625
    }
  ],
  "sentiment": {
    "positive": 0.056555856,
    "negative": 0.67204577,
    "neutral": 0.27139848
  },
  "summary": "Roblox's free AI, Sentinel, has been open-sourced to any platform for free, allowing it to identify grooming and other dangerous behavior in chat. This move comes with potential benefits for players and game companies, particularly as it involves kids. Sentinel can track conversations over time, flagging subtle signs that someone is trying to build trust with a kid in potentially problematic ways. It has already helped Roblox moderators file about 1,200 reports to the National Center for Missing and Exploited Children in just the first half of this year. The company's open-source move could help reduce child predators and make it easier for bad actors to find ways to evade the system. However, it also presents challenges for privacy, as Sentinel scans what people are saying to each other in real time, scanning for red flags. Despite these challenges, Roblx and companies may still face legal issues like child pornography.",
  "shortSummary": "Roblox\u2019s free AI Sentinel can detect toxic chat interactions and prevent child exploitation, offering an enhanced safety and integration platform for players and companies.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "6c3b20bb72824dadbca8569703c95c82",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.techradar.com/ai-platforms-assistants/chatgpt-just-got-so-much-smarter-heres-how-to-talk-to-your-kids-about-ai-and-gpt-5",
      "text": "ChatGPT just got so much smarter \u2013 here's how to talk to your kids about AI and GPT-5\nGet ahead of this now\nChatGPT's GPT-5 brain trainslpant marks a turning point in the steady march toward General Artificial Intelligence. With GPT-5 models, ChatGPT, from free to Pro, gets more personal, reliable, accurate, capable, proactive, and productive. It's even more ready than ever to engage with you on health issues.\nFor those who've been following along for the three years since OpenAI launched ChatGPT, these updates might not suprise you but even those most familair with ChatGPT might feel a little bit like the AI train is getting away from us, speeding to an unknown destination, and in a way, those who will be most affected by the coming AI wave are the youngest among us, specifically, your children.\nMaybe you don't have kids, but I bet you know someone who does, or maybe you're a teacher dealing with children every day. Children's lives are already surrounded by AI, and it's likely some are using it at school, at home, for fun, and even as a surrogate friend.\nIt's time, though, you had The Talk. That's right, talk with your kids about AI and its place in the world and, especially, their lives.\nI suggest you frame it this way:\nAI is not alive\nGPT-5 seriously levels up ChatGPT's conversational capabilities in both text and live conversations. It can seem alive and human. The algorithm and models are more complex than ever, but they do not yet match the complexity of the human brain (even if GPT-5 is a big step on the road to Artificial General Intelligence), though they can sometimes outthink you.\nChildren, in particular, will probably love chatting with GPT-5's more customizable voices. That's why it's so crucial they understand what they're really talking to \u2013 a cloud-based system hosted on servers possibly thousands of miles away. There's no one on the other side chatting with them.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nAI is just a tool\nSince the dawn of the digital age, new technologies have often seemed like magic. We could do things we never did before, like create art on a screen, manage vast amounts of data in spreadsheets, and build programs that could create rich, open worlds in video games.\nAI is on that path, but far more powerful. However, its capabilities should not be viewed as the end product, but rather as the tools and skills that help your children achieve their goals, whether it's hearing a funny story or completing a class project.\nIt's also a tool in that it only works as well as you understand how to use it. ChatGPT's success, even running GPT-5, depends largely on the quality of your prompt.\nYou'll want to show your kids how to create the best prompts and then follow-ups that ensure GPT-5 delivers the right response.\nAI is powerful but not perfect\nEven though GPT-5 promises to cut down on hallucinations, that does not mean it's error-free. Explain to your children how they cannot take ChatGPT's \"facts\" as pure truth without double-checking them.\nThat might be a tough lesson for your kids (and maybe you) who will think the work is done and want to return to playtime. You need to walk them through the process of fact-checking ChatGPT (and other AIs).\nAI should be treated as a work partner, not a servant\nGPT-5 can now, based on a prompt, code and build entire apps and websites. That's attractive to adults and children who might be looking to level up their coding skills but don't want to spend the time learning how to code.\nIt's worth reminding your kids that ChatGPT with GPT-5 is best used as a programming, development, and design partner. You provide the idea and then work with the AI to shape the final outcome.\nExplain the concept of collaboration with your kids and why that's valuable. Otherwise, they might supply one prompt, get an OK result, and assume they now know how to code and that the result is the best they and ChatGPT can do.\nThe less your kids engage with the AI coding output, the less they'll understand about application development.\nAI isn't your doctor\nThere is a strong focus in GPT-5 on health, helping you figure out what a symptom might mean or even the right questions to ask your doctors. Kids will surely try asking ChatGPT some health-related questions about bumps, bruises, aches, pains, and even odd symptoms. However, they need to understand that the best place to start addressing these concerns is with their parents, who will likely take them to the doctor.\nLook, I'm sure ChatGPT, especially with the GPT-5 upgrade, can help parents understand medical test results, but as OpenAI wrote in its release on the update: \"Important note: ChatGPT does not replace a medical professional. \"\nThe message to your kids is that human professionals are no so easily replaced.\nAI isn't your friend\nThis is one of the greatest concerns when it comes to AI, and I'm glad that GPT-5 is adding more guardrails to ensure that conversations do not go to dangerous places, and the recent upgrades added to impose pauses in challenging (perhaps overly emotional) conversations.\nEven so, your kids are probably already talking to and sharing with ChatGPT and other AIs. There is an epidemic of loneliness, and some kids may see ChatGPT's live voice mode as a friend. It's your job to remind them it's not, and to insert yourself between them and AIs.\nAI together\nThis leads me to my final tip, which is to do AI with your children.\nThink of AI like any emerging technology, from PCs to CD-ROMs to the Internet to social media. None of these tasks should ever have been undertaken by kids alone. Parents shouldn't act as if AI is not of interest to them and, therefore, not part of their job as a parent.\nIf your kids are using AI, you should be sitting next to them, exploring and learning together. Be as comfortable and conversant as they will become with AI.\nFollow these steps, and you and your whole family will be ready for ChatGPT, even with the introduction of the powerful GPT-5 model.\nYou might also like\nA 38-year industry veteran and award-winning journalist, Lance has covered technology since PCs were the size of suitcases and \u201con line\u201d meant \u201cwaiting.\u201d He\u2019s a former Lifewire Editor-in-Chief, Mashable Editor-in-Chief, and, before that, Editor in Chief of PCMag.com and Senior Vice President of Content for Ziff Davis, Inc. He also wrote a popular, weekly tech column for Medium called The Upgrade.\nLance Ulanoff makes frequent appearances on national, international, and local news programs including Live with Kelly and Mark, the Today Show, Good Morning America, CNBC, CNN, and the BBC.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/ai-platforms-assistants/chatgpt/openai-pulls-chat-sharing-tool-after-google-search-privacy-scare",
      "text": "OpenAI pulls chat sharing tool after Google search privacy scare\nCrime confessions and trade secrets made unexpected appearances\n- OpenAI has removed the ChatGPT feature, allowing people to search through public conversations with a search engine\n- Many users learned too late that enabling the \u201cdiscoverable\u201d setting could make chats accessible to anyone online\n- The decision came after several people saw their sensitive and private information publicized\nOpenAI has abruptly shut down a feature in ChatGPT that allowed chats shared publicly to be searchable through Google. Users were unpleasantly surprised to discover that private information shared with ChatGPT was now publicly available as a Google search result.\nDane Stuckey, OpenAI\u2019s Chief Information Security Officer, announced the shutdown on Thursday, with the feature disabled on Friday morning. OpenAI has also begun scrubbing search engine indexes of the previously shared conversations.\nOpenAI designed the \u201cMake this link discoverable\u201d checkbox, which they had clicked on, to give more people access to potentially helpful conversations between ChatGPT and different users. In practice, it opened the door to a new kind of digital exposure, where criminal confessions, personal secrets, and corporate practices were just a few keywords away from turning up in a Google Search.\nThis was not some obscure opt-out setting hidden in a developer console. It was right there in the sharing menu of any ChatGPT conversation users chose to turn into a public link. However, while OpenAI believed the wording was clear about what users were doing, many users apparently did not understand. They thought the link would remain private or unfindable unless explicitly shared.\nFor the average person using ChatGPT to plan a resignation letter, troubleshoot a relationship, or work on a legal strategy, that\u2019s more than a technical hiccup. It\u2019s a gut check. If you\u2019ve ever shared a link to a conversation, there\u2019s a nonzero chance someone found it by a Google search.\nSome of the reported indexed conversations included internal job applicant evaluations, sensitive mental health disclosures, confessions of crime, and proprietary software code. People were talking to ChatGPT as if it were private, but for anyone who toggled that setting, it wasn\u2019t.\nWe just removed a feature from @ChatGPTapp that allowed users to make their conversations discoverable by search engines, such as Google. This was a short-lived experiment to help people discover useful conversations. This feature required users to opt-in, first by picking a chat\u2026 pic.twitter.com/mGI3lF05UaJuly 31, 2025\nSearchable secrets\nTo OpenAI\u2019s credit, the company pulled the feature quickly. But the fallout raises important questions not just about privacy, but about how much users can reasonably be expected to understand about the tools they\u2019re using. The \u201cdiscoverable\u201d checkbox didn\u2019t hide behind a wall, but it also didn\u2019t do a great job of communicating the scope of its consequences. It\u2019s one thing to share a link, but indexing it in global search engines indefinitely is something else.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nIt is fascinating what this says about how users behave with AI. They confide in it, test ideas, vent frustrations, and draft things they\u2019d never type into a search bar. If you\u2019ve ever shared a ChatGPT conversation, it\u2019s worth checking whether that link is still live.\nWhile OpenAI is working with Google and other search engines to purge previously indexed content, search crawlers have long memories. Some content may linger for a while, even if deleted. OpenAI disabled the feature quickly, but the damage may already be done for some users.\nHopefully, the lesson will stick with the public, much like the idea of not deleting embarrassing browser histories has transitioned from a common joke to something only the most clueless person would forget to do. The magic of tools like ChatGPT lies in how they create the illusion of a conversation. But if you forget that it is still an illusion, you might not notice risks like buttons that send your digital heart-to-heart straight to Google.\nYou might also like\nEric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI\u2019s ChatGPT, Anthropic\u2019s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/pro/5-reasons-why-gemini-ai-is-bad-news-for-kids",
      "text": "5 reasons why Gemini AI is bad news for kids\nChildren are more vulnerable than adults \u2014 especially when it comes to online interactions\nAs a family tech expert, I\u2019ve seen social media and tech companies do some pretty incredible things. But Google\u2019s plan to roll out Gemini, its AI chatbot, to users under 13, is wild. They gave notice to parents in an email, but it felt much more like a warning than a warm invitation.\nSo, why are they doing this? No one\u2019s really sure, although Google is simply just joining Instagram, Snapchat, and a whole host of other platforms in the race to bring AI to nearly every facet of our lives.\nChildren, though, are much more vulnerable than adults \u2014 especially when it comes to online interactions. Here are my top 5 concerns about Google\u2019s recent and reckless decision in opening up Gemini to kids 13 and under.\nChief Parent Officer, Bark Technologies.\n1. It\u2019s teaching kids to outsource thinking and creativity from a very early age\nYoung kids need to be practicing writing, drawing, and critically thinking with their own minds \u2014 not using scraped words and images dredged up from the depths of the internet.\nChat GPT is already proving to be a breeding ground for cheating and shortcuts in schools. Giving younger kids instant access to Gemini will only accelerate this introduction to cutting corners when it comes to learning and being creative.\n2. Misinformation is rife on the platform\nWhen an AI platform like Gemini provides completely wrong information, it\u2019s called a \u201challucination.\u201d That\u2019s a quaint way of saying \u201cmaking things up that are total nonsense.\u201d\nGoogle even says in its FAQs about Gemini in the Family Link app that \u201c[Hallucinations are] a strange term, but it basically just means that Gemini sometimes gets things wrong or says things that aren\u2019t true. [They] can sound real, and Gemini might even say them confidently.\u201d\nSign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed!\nFor adults, these types of errors may be easy to recognize and ignore, like saying that the capital of France is Cairo. But for kids, they may not know when to double-check a simple answer \u2014 let alone something complicated or nuanced. This sort of defeats the purpose of having Gemini help with homework for children.\n3. Inappropriate content can present dangers to kids\nGemini can also act as a chatbot \u201cfriend\u201d for kids, which presents multiple dangers. Other similar chatbots have been blamed for exposure to sexual content and even one child\u2019s suicide.\nOf course, Google has stated that the Gemini for kids will have safeguards, but there\u2019s never a guarantee that inappropriate things won\u2019t slip through the cracks \u2014 especially when these AI platforms regularly hallucinate.\nFortunately, apps like Bark can monitor your child\u2019s saved photos, videos, and even text messages for inappropriate content they may save or share from Gemini.\n4. Personal info that\u2019s shared can be hacked\nSharing personal information \u2014 from sensitive emotional states to home addresses to personal photos \u2014 with an AI platform is vulnerable because everything can be hacked. If someone were to gain access to your child\u2019s Gemini chats, it could be stressful and even dangerous.\nHate speech and bias can be conveyed in Gemini responses\nThe way AI platforms like Gemini work is by scraping the totality of the internet for similar information \u2014 information that was written by other humans.\nThat means that human biases and viewpoints can be presented by Gemini as truth, which we now know isn\u2019t even always true.\nBecause AI platforms provide answers based on information that humans created, it can mirror prejudices that exist in the data it\u2019s fed with. This can include harmful positions about marginalized groups.\nFinal word\nAt the end of the day, technology is just another tool that can make our lives easier, but it\u2019s just that \u2014 a tool, not a necessity.\nEven though calculators are used every day in advanced math, kids still learn how to count, add, subtract, multiply, and divide the old-school way in elementary school.\nThe same should go with AI platforms like Gemini when it comes to writing, thinking, and being creative.\nCheckout our comprehensive list of the best AI tools.\nThis article was produced as part of TechRadarPro's Expert Insights channel where we feature the best and brightest minds in the technology industry today. The views expressed here are those of the author and are not necessarily those of TechRadarPro or Future plc. If you are interested in contributing find out more here: https://www.techradar.com/news/submit-your-story-to-techradar-pro\nChief Parent Officer, Bark Technologies.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/tag/esports",
      "text": "eSports\nLatest about eSports\nThe 17 best free games to play in 2025\nBy Jake Green last updated\nFind the best free games you can play right now, whether its the latest RPG or a competitive multiplayer title.\nI used a 600Hz esports monitor, but it didn't make me any less terrible at Counter-Strike\nBy Christian Guyton published\nThe Zowie XL2586X+ is the official monitor of the IEM Katowice Counter-Strike 2 tournament - but can it make you better at shooters?\nCan an AI coach help your esports team clinch the championship?\nBy Eric Hal Schwartz published\nPlayVS and Omni bring AI esports coaches to high school students.\nBenQ Zowie EC2-CW review: no-nonsense esports performance\nBy Dashiell Wood published\nThanks to its superior comfort, the Zowie EC2-CW is an excellent choice for competitive FPS games.\nHow Crestron AV-over-IP solution is helping this eSports and hotel business thrive\nBy TechRadar Pro published\nAs Crestron rapidly approaches the shipment of one million DM NVX\u00ae AV-over-IP units since the product\u2019s introduction, here are two real-world examples of successful implementation.\nApex Legends tournament disrupted by rogue RCE exploit hack\nBy River Hart published\nApex hacks RCE hacks caused chaos during last weekend's ALGS regional finals. Wondering what happened and why? Here's everything you need to know.\nAsus ROG Harpe Ace Aim Lab Edition review: a lightweight mouse that's perfect for esports\nBy Dashiell Wood published\nIf you're willing to splash out, the Asus ROG Harpe Ace Aim Lab Edition is a great choice for esports thanks to its light weight and high-end specs.\nOne of the world\u2019s biggest esports events will be held in London next year\nBy Dashiell Wood published\nThe League of Legends World Championship Finals is set to be held at the O2 arena in London, England, next year.\nSign up for breaking news, reviews, opinion, top tech deals, and more."
    },
    {
      "url": "https://www.techradar.com/computing/artificial-intelligence/would-you-buy-your-child-a-chatgpt-powered-barbie-im-queasy-at-the-prospect-of-a-real-life-small-soldiers-scenario",
      "text": "I\u2019m a ChatGPT fan, but the deal between OpenAI and Mattel makes me nervous \u2013 AI and kids are a bad mix\nOpenAI's deal with Mattel leaves me a little uneasy\nMattel is partnering with OpenAI to build AI\u2011powered toys, which might lead to some amazing fun, but also sounds like the premise for a million stories of things going wrong.\nTo be clear, I don't think AI is going to end the world. I've used ChatGPT in a million ways, including as an aide for activities as a parent. AI has helped me brainstorm bedtime stories and design coloring books, among other things. But that's me using it, not opening it up to direct interaction with children.\nThe official announcement is very optimistic, of course. Mattel says it\u2019s bringing the \u201cmagic of AI\u201d to playtime, promising age\u2011appropriate, safe, and creative experiences for kids. OpenAI says it\u2019s thrilled to help power these toys with ChatGPT, and both companies seem intent on positioning this as a step forward for playtime and childhood development.\nBut I can\u2019t help thinking of how ChatGPT conversations can spiral into bizarre conspiracy theories, except suddenly it's a Barbie doll talking to an eight-year-old. Or a GI Joe veering from positive messages about \"knowing is half the battle,\" to pitching cryptocurrency mining because some six\u2011year\u2011old heard the word \u201cblockchain\u201d somewhere and thought it sounded like a cool weapon for the toy.\nAs you might have noted from the top image, the first thought I had was about the film Small Soldiers. The 1998 corny classic about an executive at a toy company deciding to save money by installing military-grade AI chips into action figures, leading to the toys staging guerrilla warfare in the suburbs? It was a satire, and not a bad one at that. But, as over-the-top as that outcome might be, it's hard not to see the glimmer of chaotic potential in installing generative AI in the toys children may spend a lot of time with.\nI do get the appeal of AI in a toy, I do. Barbie could be more than just a doll you dress up, she could be a curious, clever conversationalist who can explain space missions or play pretend in a dozen different roles. Or you could have a Hot Wheels car commenting on the track you built for it. I can even picture AI in Uno as a deckpad that actually teaches younger kids strategy and sportsmanship.\nBut I think generative AI models like ChatGPT shouldn't be used by kids. They may be pared down for safety's sake, but at a certain point, that stops being AI and just becomes a fairly robust set of pre-planned responses without the flexibility of AI. That means avoiding the weirdness, hallucinations, and moments of unintended inappropriateness from AI that adults can brush off but kids might absorb.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nToying with AI\nMattel has been at this a long time and knows what it is doing, in general, with its products. It's certainly not to their advantage to have their toys go even slightly haywire. The company said it will build safety and privacy into every AI interaction. They promise to focus on appropriate experiences. But \u201cappropriate\u201d is a very slippery word in AI, especially when it comes to language models trained on the internet.\nChatGPT isn\u2019t a closed-loop system that was built for toys, though. It wasn\u2019t designed specifically for young kids. And even when you train it with guidelines, filters, and special voice modules, it\u2019s still built on a model that learns and imitates. There\u2019s also the deeper question: what kind of relationship do we want kids to have with these toys?\nThere\u2019s a big difference between playing with a doll and imagining conversations with it, and forming a bond with a toy that independently responds. I don\u2019t expect a doll to go the full Chucky or M3gan, but when we blur the line between playmate and program, the outcomes can get hard to predict.\nI use ChatGPT with my son in the same way I use scissors or glue \u2013 a tool for his entertainment that I control. I\u2019m the gatekeeper, and AI built into a toy is hard to monitor that way. The doll talks. The car replies. The toy engages, and kids may not notice anything amiss because they don't have the experience.\nIf Barbie\u2019s AI has a glitch, if GI Joe suddenly slips into dark military metaphors, if a Hot Wheels car randomly says something bizarre, a parent might not even know until it\u2019s been said and absorbed. If we\u2019re not comfortable letting these toys run unsupervised, they\u2019re not ready.\nIt\u2019s not about banning AI from childhood. It\u2019s about knowing the difference between what\u2019s helpful and what\u2019s too risky. I want AI in the toy world to be very narrowly constrained, like how a TV show aimed at toddlers is carefully designed to be appropriate. Those shows won't (hardly ever) go off script, but AI's power is in writing its own script.\nI might sound too harsh about this, and goodness knows there have been other tech toy scares. Furbies were creepy. Talking Elmo had glitches. Talking Barbies once had sexist lines about math being hard. All issues that could be resolved, except maybe the Furbies. I do think AI in toys has potential, but I'll be skeptical until I see how well Mattel and OpenAI navigate that narrow path between not really using AI and giving the AI too much freedom to be a bad virtual friend to give your child.\nYou might also like\nEric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI\u2019s ChatGPT, Anthropic\u2019s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/computing/artificial-intelligence/ai-godfather-sounds-the-alarm-on-autonomous-ai",
      "text": "'AI Godfather' sounds the alarm on autonomous AI\nYoshua Bengio calls for restraint as companies push for AI dominance\n- 'AI godfather' Yoshua Bengio warns that the AI race prioritizes speed over safety\n- This risks unpredictable and dangerous consequences\n- He urges global cooperation to enforce AI regulations before autonomous systems become difficult to control\n'AI godfather' Yoshua Bengio helped create the foundations of the neural networks running all kinds of AI tools today, from chatbots mimicking cartoon characters to scientific research assistants. Now, he has an urgent warning for AI developers, as he explained in a Sky News interview. The race to develop ever-more-powerful AI systems is escalating at a pace that, in his view, is far too reckless.\nAnd it\u2019s not just about which company builds the best chatbot or who gets the most funding. Bengio believes that the rapid, unregulated push toward advanced AI could have catastrophic consequences if safety isn\u2019t treated as a top priority.\nBengio described watching developers racing against each other, getting sloppy, or taking dangerous shortcuts. Though speed can make the difference in breaking ground on a new kind of product worth billions and playing catch-up to a rival, it may not be worth it to society.\nThat pressure has only intensified for AI developers with the rise of Chinese AI firms like DeepSeek, whose advanced chatbot capabilities have caught the attention of Western companies and governments alike. Instead of slowing down and carefully considering the risks, major tech firms are accelerating their AI development in an all-out sprint for superiority. Bengio worries this will lead to rushed deployments, inadequate safety measures, and systems that behave in ways we don\u2019t yet fully understand.\nBengio explained that he has been warning about the need for stronger AI oversight, but recent events have made his message feel even more urgent. The current moment is a \"turning point,\" where we either implement meaningful regulations and safety protocols or risk letting AI development spiral into something unpredictable.\nAfter all, more and more AI systems don\u2019t just process information but can make autonomous decisions. These AI agents are capable of acting on their own rather than simply responding to user inputs. They're exactly what Bengio sees as the most dangerous path forward. With enough computing power, an AI that can strategize, adapt, and take independent actions could quickly become difficult to control should humans want to take back the reins.\nAI takeover\nThe problem isn\u2019t just theoretical. Already, AI models are making financial trades, managing logistics, and even writing and deploying software with minimal human oversight. Bengio warns that we\u2019re only a few steps away from much more complex, potentially unpredictable AI behavior. If a system like this is deployed without strict safeguards, the consequences could range from annoying hiccups in service to full-on security and economic crises.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nBengio isn\u2019t calling for a halt to AI development. He made clear that he's an optimist about AI's abilities when used responsibly for things like medical and environmental research. He just sees a need for a priority shift to more thoughtful and deliberate work on AI technology. His unique perspective may carry some weight when he calls for AI developers to put ethics and safety ahead of competing with rival companies. That's why he participates in policy discussions at events like the upcoming International AI Safety Summit in Paris,\nHe also thinks regulation needs to be bolstered by companies willing to take responsibility for their systems. They need to invest as much in safety research as they do in performance improvements, he claims, though that balance is hard to imagine appearing in today's AI melee. In an industry where speed equals dominance, no company wants to be the first to hit the brakes.\nThe global cooperation Bengio pitches might not appear immediately, but as the AI arms race continues, warnings from Bengio and others in similar positions of prestige grow more urgent. He hopes the industry will recognize the risks now rather than when a crisis forces the matter. The question is whether the world is ready to listen before it\u2019s too late.\nYou might also like...\nEric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI\u2019s ChatGPT, Anthropic\u2019s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.desmoinesregister.com/story/news/crime-and-courts/2025/08/07/roblox-lawsuit-iowa-kidnapping-13-year-old-groomed/85513448007/",
      "text": "13-year-old's family thought Roblox was safe. Now they're suing over her alleged abduction\n- The family of a 13-year-old girl from West Des Moines is suing Roblox, claiming the platform failed to protect her from online sexual predators.\n- The lawsuit alleges the girl was groomed and later kidnaped by a man she met on Roblox, leading to her sexual assault in Tennessee.\n- Roblox faces multiple lawsuits and criticism regarding child safety concerns on its platform.\nThe family of a West Des Moines girl allegedly taken and abused by an alleged sexual predator she met online while using Roblox is suing the video gaming company, saying it has failed to protect children from sexual predators online.\nThe lawsuit, filed July 29, involves a 13-year-old girl who authorities say was kidnapped from a family member's home in West Des Moines and driven to Jefferson County, Tennessee, east of Knoxville, where she was repeatedly sexually assaulted.\nThe suspect, Martin Amaya Sandoval, currently faces statutory rape charges in Tennessee. West Des Moines police said in May they expect to bring charges in Iowa as well, although none appear in online court records as of Aug. 4.\nIn their complaint, the girl's family says the teen first encountered Sandoval on Roblox, an online service that allows users to create and share their own games and interact with other users. The company advertises its platform as safe for children, with options for chat filters and other parental controls. Despite this, in recent years there have been widespread reports of children exposed to sexual content or otherwise exploited by other Roblox users.\nIn the case of the West Des Moines teen, the complaint alleges she began playing Roblox when she was 8 and that her parents allowed it because they believed the app was safe for children. In early 2025, when she was 13, she allegedly began communicating online with Sandoval, who although in his mid-30s was using a child's profile and claimed to be a minor. Over four months, beginning on Roblox and later on Snapchat, he allegedly claimed to love her, exchanged sexual messages and images with her and finally convinced her to meet him in person, leading to an alleged kidnaping and assault.\nThe complaint by her family, who are only identified in the suit with their initials, accuses Roblox of intentionally failing to implement age verification and other safety measures to boost user numbers, prioritizing investors over children's safety.\n\"Had (Roblox) designed the app with even the most basic screening or age and identity verification, as well as effective parental controls, (the 13-year-old) never would have interacted with this predator and never would have suffered the harm that she did. In short, (the 13-year-old's) life has been devastated as a direct result of Defendant\u2019s conduct,\" the complaint alleges.\nWidespread reports of sexual content on Roblox\nCalifornia-based Roblox, which reported more than $1 billion in revenue in its latest quarter, is one of the most valuable video game companies in the world. But the company has been under growing pressure in recent years to clean up its platform.\nAn October 2024 report by the investment firm Hindenburg Research described the service as an \"X-rated pedophile hellscape\" and said researchers found multiple groups on the service openly trading child pornography. News media investigations have found numerous other cases of children being groomed, solicited and abused by people they met on Roblox.\nThe company is facing numerous lawsuits from hundreds of users claiming to have been harmed due to lapses in child safety on its platform.\nThe West Des Moines kidnapping isn't the first such case reported in Iowa. Online court records show at least three other instances since 2021 in which investigators have sought search warrants for Roblox data. Each involves allegations of children, some as young as 7, being solicited or groomed by strangers encountered on Roblox. It's not clear how many of those investigations resulted in criminal charges.\nRoblox has not yet responded to the new complaint in court. In an emailed statement, the company declined to comment on pending litigation but said \"Roblox is deeply committed to the safety and well-being of our community with tens of millions having positive experiences every day. Protecting children is a top priority, and we invest significant resources in advanced safety technology. ... We are always evolving our safety practices to promote a secure and positive experience for everyone.\u201d\nWilliam Morris covers courts for the Des Moines Register. He can be contacted at wrmorris2@registermedia.com or 715-573-8166."
    }
  ],
  "argos_summary": "Roblox has developed an AI system called Sentinel to enhance safety in online gaming chats, particularly for its young users, by identifying grooming and other dangerous behaviors. The company is now open-sourcing Sentinel, allowing any platform to implement its safety measures, which could significantly reduce the risk of predators targeting children. However, concerns about privacy, the potential for misuse, and the challenges of keeping the AI updated with evolving language and behaviors remain. This initiative comes amid ongoing lawsuits against Roblox regarding child safety, highlighting the urgent need for effective online protection.",
  "argos_id": "GJWNP34D7"
}