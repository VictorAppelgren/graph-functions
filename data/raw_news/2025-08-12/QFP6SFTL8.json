{
  "url": "https://techcrunch.com/2025/08/01/tesla-partly-liable-in-florida-autopilot-trial-jury-awards-200m-in-damages/",
  "authorsByline": "Sean O'Kane",
  "articleId": "c19d8551ed3f498cb6a7d30b38854261",
  "source": {
    "domain": "techcrunch.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "CA",
      "county": "San Francisco",
      "city": "San Francisco",
      "coordinates": {
        "lat": 37.7790262,
        "lon": -122.419906
      }
    }
  },
  "imageUrl": "https://techcrunch.com/wp-content/uploads/2024/07/GettyImages-2159532501.jpg?resize=1200,801",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-01T18:24:42+00:00",
  "addDate": "2025-08-01T23:05:26.772157+00:00",
  "refreshDate": "2025-08-01T23:05:26.772160+00:00",
  "score": 1.0,
  "title": "Tesla partly liable in Florida Autopilot trial, jury awards $200M in damages",
  "description": "The jury verdict is one of the first major legal decisions about driver assistance technology that has gone against Tesla. Both Elon Musk and Tesla have spent years making claims about Autopilot's capabilities.",
  "content": "A jury in federal court in Miami has found Tesla partly to blame for a fatal 2019 crash that involved the use of the company\u2019s Autopilot driver assistance system. \n\n\n\nThe jury assessed punitive damages only against Tesla, CNBC reported. The punitive fines coupled with a compensatory damages puts the total payments to around $242.5 million.\n\nNeither the driver of the car nor the Autopilot system braked in time to avoid going through an intersection, where the car struck an SUV and killed a pedestrian. The jury assigned the driver two-thirds of the blame, and attributed one-third to Tesla. (The driver was sued separately.)\n\nThe verdict comes at the end of a three-week trial over the crash, which killed 20-year-old Naibel Benavides Leon and severely injured her boyfriend Dillon Angulo. The verdict is one of the first major legal decisions about driver assistance technology that has gone against Tesla. The company has previously settled lawsuits involving similar claims about Autopilot.\n\nBrett Schreiber, the lead attorney for the plaintiffs in the case, said in a statement to TechCrunch that Tesla designed Autopilot \u201conly for controlled access highways yet deliberately chose not to restrict drivers from using it elsewhere, alongside Elon Musk telling the world Autopilot drove better than humans.\u201d\n\n\u201cTesla\u2019s lies turned our roads into test tracks for their fundamentally flawed technology, putting everyday Americans like Naibel Benavides and Dillon Angulo in harm\u2019s way,\u201d said Schreiber. \u201cToday\u2019s verdict represents justice for Naibel\u2019s tragic death and Dillon\u2019s lifelong injuries, holding Tesla and Musk accountable for propping up the company\u2019s trillion-dollar valuation with self-driving hype at the expense of human lives.\u201d\n\nTesla, in a statement provided to TechCrunch, said it plans to appeal the verdict \u201cgiven the substantial errors of law and irregularities at trial.\u201d\n\n\u201cToday\u2019s verdict is wrong and only works to set back automotive safety and jeopardize Tesla\u2019s and the entire industry\u2019s efforts to develop and implement life-saving technology,\u201d the company wrote. \u201cTo be clear, no car in 2019, and none today, would have prevented this crash. This was never about Autopilot; it was a fiction concocted by plaintiffs\u2019 lawyers blaming the car when the driver \u2014 from day one \u2014 admitted and accepted responsibility.\u201d\n\nTesla and Musk spent years making claims about Autopilot\u2019s capabilities that have led to overconfidence in the driver assistance system, a reality that government officials \u2014 and Musk himself \u2014 have spoken about for years.\n\nThe National Transportation Safety Board (NTSB) came to this determination in 2020 after investigating a 2018 crash where the driver died after hitting a concrete barrier. That driver, Walter Huang, was playing a mobile game while using Autopilot. The NTSB made a number of recommendations following that investigation, which Tesla largely ignored, the safety board later claimed.\n\nOn a 2018 conference call, Musk said \u201ccomplacency\u201d with driver assistance systems like Autopilot is a problem.\n\n\u201cThey just get too used to it. That tends to be more of an issue. It\u2019s not a lack of understanding of what Autopilot can do. It\u2019s [drivers] thinking they know more about Autopilot than they do,\u201d Musk said at the time.\n\nThe trial took place at a time when Tesla is currently in the middle of rolling out the first versions of its long-promised Robotaxi network, starting in Austin, Texas. Those vehicles are using an enhanced version of Tesla\u2019s more capable driver assistance system, which it calls Full Self-Driving.\n\nUpdate: This story has been updated to include the amount of compensatory damages in the total.",
  "medium": "Article",
  "links": [
    "https://www.theverge.com/2018/5/2/17313324/tesla-autopilot-safety-statistics-elon-musk-q1-earnings",
    "https://www.bloomberg.com/news/articles/2021-10-25/tesla-criticized-for-not-responding-to-accident-investigators",
    "https://www.theverge.com/2020/2/25/21153320/tesla-autopilot-walter-huang-death-ntsb-probable-cause",
    "https://techcrunch.com/2025/06/22/tesla-launches-robotaxi-rides-in-austin-with-big-promises-and-unanswered-questions/",
    "https://www.bloomberg.com/news/articles/2025-07-17/tesla-settled-over-autopilot-crash-with-truck-crossing-highway",
    "https://www.cnbc.com/2025/08/01/tesla-must-pay-329-million-in-damages-in-fatal-autopilot-case.html"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "driver assistance systems",
      "weight": 0.1027625
    },
    {
      "name": "driver assistance technology",
      "weight": 0.09966008
    },
    {
      "name": "Tesla",
      "weight": 0.09027174
    },
    {
      "name": "drivers",
      "weight": 0.088343434
    },
    {
      "name": "Florida Autopilot trial",
      "weight": 0.08657917
    },
    {
      "name": "Autopilot",
      "weight": 0.08026397
    },
    {
      "name": "compensatory damages",
      "weight": 0.06463685
    },
    {
      "name": "Elon Musk",
      "weight": 0.064005174
    },
    {
      "name": "Musk",
      "weight": 0.06096892
    },
    {
      "name": "punitive damages",
      "weight": 0.060737684
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Tech"
    },
    {
      "name": "Auto"
    }
  ],
  "taxonomies": [
    {
      "name": "/Sensitive Subjects/Accidents & Disasters",
      "score": 0.7490234375
    },
    {
      "name": "/Sensitive Subjects/Death & Tragedy",
      "score": 0.62890625
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.5927734375
    },
    {
      "name": "/Law & Government/Legal/Other",
      "score": 0.39453125
    },
    {
      "name": "/Law & Government/Government/Courts & Judiciary",
      "score": 0.378173828125
    },
    {
      "name": "/Autos & Vehicles/Motor Vehicles (By Brand)/Tesla Motors",
      "score": 0.30322265625
    }
  ],
  "sentiment": {
    "positive": 0.050964355,
    "negative": 0.76416016,
    "neutral": 0.18469238
  },
  "summary": "A jury in Miami has found Tesla partly responsible for a 2019 crash involving the company's Autopilot driver assistance system that killed 20-year-old Naibel Benavides Leon and severely injured her boyfriend, Dillon Angulo. The jury awarded $200M in punitive damages, with the total payments totalling around $242.5 million. The driver was not directly responsible for the accident, and the Autopilet system was not able to brake in time to avoid an intersection. This verdict is one of the first major legal decisions against Tesla regarding driver assistance technology. The National Transportation Safety Board (NTSB) made recommendations following an investigation into the crash, which Tesla largely ignored. Tesla plans to appeal the verdict given its substantial errors of law and irregularities at trial.",
  "shortSummary": "A jury in Miami found Tesla partly responsible for a fatal 2019 crash involving Autopilot, awarding $200 million in damages.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": true,
  "reprintGroupId": "14c571115ed54018a68758d18afbb886",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.cnbc.com/2025/08/01/tesla-must-pay-329-million-in-damages-in-fatal-autopilot-case.html",
      "text": "A jury in Miami has determined that Tesla should be held partly liable for a fatal 2019 Autopilot crash, and must compensate the family of the deceased and an injured survivor a portion of $329 million in damages.\nTesla's payout is based on $129 million in compensatory damages, and $200 million in punitive damages against the company.\nThe jury determined Tesla should be held 33% responsible for the fatal crash. That means the automaker would be responsible for about $42.5 million in compensatory damages. In cases like these, punitive damages are typically capped at three times compensatory damages.\nThe plaintiffs' attorneys told CNBC on Friday that because punitive damages were only assessed against Tesla, they expect the automaker to pay the full $200 million, bringing total payments to around $242.5 million.\nTesla said it plans to appeal the decision.\nAttorneys for the plaintiffs had asked the jury to award damages based on $345 million in total damages. The trial in the Southern District of Florida started on July 14.\nThe suit centered around who shouldered the blame for the deadly crash in Key Largo, Florida. A Tesla owner named George McGee was driving his Model S electric sedan while using the company's Enhanced Autopilot, a partially automated driving system.\nWhile driving, McGee dropped his mobile phone that he was using and scrambled to pick it up. He said during the trial that he believed Enhanced Autopilot would brake if an obstacle was in the way. His Model S accelerated through an intersection at just over 60 miles per hour, hitting a nearby empty parked car and its owners, who were standing on the other side of their vehicle.\nNaibel Benavides, who was 22, died on the scene from injuries sustained in the crash. Her body was discovered about 75 feet away from the point of impact. Her boyfriend, Dillon Angulo, survived but suffered multiple broken bones, a traumatic brain injury and psychological effects.\n\"Tesla designed Autopilot only for controlled access highways yet deliberately chose not to restrict drivers from using it elsewhere, alongside Elon Musk telling the world Autopilot drove better than humans,\" Brett Schreiber, counsel for the plaintiffs, said in an e-mailed statement on Friday. \"Tesla's lies turned our roads into test tracks for their fundamentally flawed technology, putting everyday Americans like Naibel Benavides and Dillon Angulo in harm's way.\"\nFollowing the verdict, the plaintiffs' families hugged each other and their lawyers, and Angulo was \"visibly emotional\" as he embraced his mother, according to NBC.\nHere is Tesla's response to CNBC:\n\"Today's verdict is wrong and only works to set back automotive safety and jeopardize Tesla's and the entire industry's efforts to develop and implement life-saving technology. We plan to appeal given the substantial errors of law and irregularities at trial.\nEven though this jury found that the driver was overwhelmingly responsible for this tragic accident in 2019, the evidence has always shown that this driver was solely at fault because he was speeding, with his foot on the accelerator \u2013 which overrode Autopilot \u2013 as he rummaged for his dropped phone without his eyes on the road. To be clear, no car in 2019, and none today, would have prevented this crash.\nThis was never about Autopilot; it was a fiction concocted by plaintiffs' lawyers blaming the car when the driver \u2013 from day one \u2013 admitted and accepted responsibility.\"\nThe verdict comes as Musk, Tesla's CEO, is trying to persuade investors that his company can pivot into a leader in autonomous vehicles, and that its self-driving systems are safe enough to operate fleets of robotaxis on public roads in the U.S.\nTesla shares dipped 1.8% on Friday and are now down 25% for the year, the biggest drop among tech's megacap companies.\nThe verdict could set a precedent for Autopilot-related suits against Tesla. About a dozen active cases are underway focused on similar claims involving incidents where Autopilot or Tesla's FSD\u2014 Full Self-Driving (Supervised) \u2014 had been in use just before a fatal or injurious crash.\nThe National Highway Traffic Safety Administration initiated a probe in 2021 into possible safety defects in Tesla's Autopilot systems. During the course of that investigation, Tesla made changes, including a number of over-the-air software updates.\nThe agency then opened a second probe, which is ongoing, evaluating whether Tesla's \"recall remedy\" to resolve issues with the behavior of its Autopilot, especially around stationary first responder vehicles, had been effective.\nThe NHTSA has also warned Tesla that its social media posts may mislead drivers into thinking its cars are capable of functioning as robotaxis, even though owners manuals say the cars require hands-on steering and a driver attentive to steering and braking at all times.\nA site that tracks Tesla-involved collisions, TeslaDeaths.com, has reported at least 58 deaths resulting from incidents where Tesla drivers had Autopilot engaged just before impact.\nRead the jury's verdict below.\nCorrection: A prior version of this story and the headline incorrectly stated how much Tesla will be forced to pay as a result of the jury's decision.\nWATCH: Tesla's tough quarter"
    },
    {
      "url": "https://www.theverge.com/2020/2/25/21153320/tesla-autopilot-walter-huang-death-ntsb-probable-cause",
      "text": "The National Transportation Safety Board said Tuesday that Tesla\u2019s Autopilot driver assistance system was one of the probable causes of a fatal 2018 crash into a concrete barrier. In addition, the safety board said the driver was playing a mobile game while using Autopilot before the crash, and investigators also determined he was overly confident in Autopilot\u2019s capabilities.\nThe safety board arrived at those probable causes after a nearly two-year investigation into the crash. NTSB investigators also named a number of contributing factors, including that the crash attenuator in front of the barrier was damaged and had not been repaired by California\u2019s transportation department, Caltrans, in a timely manner. Had the crash attenuator been replaced, NTSB investigators said Tuesday that the driver, Walter Huang, likely would have survived.\nThe NTSB shared its findings at the end of a three-hour-long hearing on Tuesday. During the hearing, board members took issue with Tesla\u2019s approach to mitigating the misuse of Autopilot, the National Highway Traffic Safety Administration\u2019s lax approach to regulating partial automation technology, and Apple \u2014 Huang\u2019s employer \u2014 for not having a distracted driving policy. (Huang was playing the mobile game on a company-issued iPhone.)\n\u201cWe urge Tesla to continue to work on improving their Autopilot technology\u201d\n\u201cIn this crash we saw an over-reliance on technology, we saw distraction, we saw a lack of policy prohibiting cell phone use while driving, and we saw infrastructure failures, which, when combined, led to this tragic loss,\u201d NTSB chairman Robert Sumwalt said at the end of the hearing on Tuesday. \u201cWe urge Tesla to continue to work on improving their Autopilot technology and for NHTSA to fulfill its oversight responsibility to ensure that corrective action is taken where necessary. It\u2019s time to stop enabling drivers in any partially automated vehicle to pretend that they have driverless cars.\u201d\nThe investigators\u2019 findings\nOn March 23rd, 2018, Huang was traveling south on US-101 using Autopilot on his way to work in Mountain View, California. He eventually approached a section of the highway where State Route 85 begins by splitting off to the left of US-101. He was in the left-most HOV lane thanks to the clean air sticker that electric vehicle owners are eligible for.\nAs the left exit lane for State Route 85 started to split off to the left, the Autopilot system in Huang\u2019s Model X briefly lost sight of the lines marking his lane. (Investigators showed photos on Tuesday of the worn-down lane markers.) Autopilot started following the right-most lane marker of the exit lane, and Huang\u2019s Model X steered into the \u201cgore area\u201d that separates the exit lane from the rest of the highway. A second or so later, his Model X smashed into the damaged crash attenuator and the concrete barrier. He died a few hours later at a local hospital.\nInvestigators recapped the crash during Tuesday\u2019s hearing, presenting evidence that the NTSB made public last week. At the end of the presentation, and after a few hours of questions from the five members of the safety board, the team of investigators presented 23 findings, and made nine new safety recommendations, in addition to naming the probable causes.\nOne of the team\u2019s findings was that the crash was made possible, in part, because of the limits of Autopilot\u2019s vision-based processing system. Tesla CEO Elon Musk has long argued that autonomous cars don\u2019t need LIDAR (a laser sensor that can build a real-time 3D model of the world), and so Autopilot is designed around a system of cameras, as well as ultrasonic sensors and a forward-facing radar. That reliance on cameras has limits, investigators said Tuesday, and the way Huang\u2019s car drifted out of the HOV lane is an example of those limits. In fact, as the investigators found, Huang\u2019s car had done this same dangerous maneuver multiple times in the days and weeks before his crash.\nIn addition, the investigators said that Tesla\u2019s method of making sure drivers are paying attention while using Autopilot \u2014 using a torque sensor to measure force on the steering wheel \u2014 \u201cdid not provide an effective means of monitoring the driver\u2019s level of engagement with the driving task.\u201d\nThe NTSB says Tesla\u2019s driver monitoring is inadequate\nNTSB investigators also found that the Model X\u2019s forward collision warning system didn\u2019t alert Huang to the coming impact, nor did it slow the vehicle down at all; in fact, the Model X sped up before impact because the system thought it was free to resume the 75 mile per hour cruise control speed Huang had set. The NTSB said Tesla had not designed these emergency systems to handle a situation like this. It also placed some blame on the National Highway Traffic Safety Administration for not requiring companies like Tesla to make those systems work in a crash like this.\nIf Tesla doesn\u2019t add new safeguards that limit the use of the Autopilot outside of its advertised applications, investigators wrote, then the \u201crisk for future crashes will remain.\u201d\nThe investigators gave similar weight to distracted driving\u2019s role in Huang\u2019s death. They found that he was playing a mobile game prior to the crash, and said that was \u201clikely\u201d the reason why he didn\u2019t try to turn away from the barrier. The team said that countermeasures, like limiting distracting features or locking out smartphones entirely, would help lower the rate of crashes tied to distracted driving.\nApple does have a feature that turns off many features while driving, but one NTSB board member, Thomas Chapman, said he was \u201cfrankly unaware I had such an option on my phone.\u201d\n\u201cAnd that of course is the point, it makes more sense to turn on such a feature as the default setting to better ensure users are aware such an option exists,\u201d Chapman said.\nMaking things worse was Huang\u2019s apparent overconfidence in Autopilot\u2019s capabilities, evidenced by his willingness to play a mobile game while operating the car. Tesla itself has said in the past that overconfidence is a risk for drivers using Autopilot, and is one of the reasons the company constantly reminds owners its cars\u2019 manuals to pay close attention while using the driver assistance system.\nSumwalt was particularly stunned by Huang\u2019s apparent overconfidence in Autopilot, though, especially after learning that he had dealt with Autopilot failing in that same area before.\n\u201cThis kind of points out two things to me. These semi-autonomous vehicles can lead drivers to be complacent, highly complacent, about their systems. And it also points out that smartphones manipulating them can be so addictive that people aren\u2019t going to put them down,\u201d he said midway through Tuesday\u2019s hearing.\nNine new recommendations\nThe new recommendations were issued to multiple parties, but not to Tesla \u2014 perhaps because the company has still not officially responded to the safety board\u2019s recommendations from a 2017 investigation into a different Autopilot-related fatal crash, something Sumwalt criticized Tesla for on Tuesday.\nInstead, the NTSB appears happy to try to influence the decisions of Tesla and other automakers by asking other government agencies to step in and regulate.\nOne member said NHTSA\u2019s crash testing, which doesn\u2019t examine features like Autopilot, is \u201cpretty worthless\u201d\nThe first four recommendations were aimed at NHTSA. The NTSB asked NHTSA to start testing automakers\u2019 forward collision avoidance systems, and especially how they deal with \u201ccommon obstacles\u201d like \u201ctraffic safety hardware, crosstraffic vehicle profiles, and other applicable vehicle shapes or objects found in the highway operating environment.\u201d The current version of NHTSA\u2019s evaluation process, known as the New Car Assessment Program, doesn\u2019t take any of this into account, which some board members were not happy about. (At one point, board member Michael Graham held up an NCAP evaluation of a car one of his staffers owns and said it was \u201cpretty worthless.\u201d)\nThe safety board asked NHTSA to start evaluating the limits of Autopilot, and to determine the likelihood that it will be misused. If safety defects are identified, they wrote, NHTSA should use its regulatory authority to make sure Tesla \u201ctakes corrective action.\u201d\nThe NTSB also asked NHTSA to work with the Society of Automotive Engineers to draw up standards for driver monitoring systems that would \u201cminimize driver disengagement, prevent automation complacency, and account for foreseeable misuse of the automation,\u201d and require that technology in all vehicles with Autopilot-like features.\nOn the distracted driving side, the NTSB recommended that Apple adopt a policy that bans the nonemergency use of smartphones and tablets in company-owned vehicles, or while doing company work. And it asked the Occupational Safety and Health Administration help increase employers\u2019 awareness of the dangers of distracted driving, and to step in when employers don\u2019t comply. The board also recommended that smartphone manufacturers develop better in-car \u201cdo not disturb\u201d modes that \u201cautomatically disable any driver-distracting functions when a vehicle is in motion\u201d (but allows the device to be used in an emergency), and make that the default setting.\nIs anybody listening?\nOne of the pervasive themes at Tuesday\u2019s meeting, and in the NTSB\u2019s findings and recommendations, is that the safety board believes companies and other government agencies are basically exploiting its lack of regulatory authority.\nThe NTSB is an independent government agency, but it only has the authority to investigate and make recommendations. It\u2019s still up to these other parties to act. And according to the NTSB, that\u2019s not really happening.\nTesla still hasn\u2019t formally responded to NTSB recommendations issued 881 days ago. NHTSA hasn\u2019t properly responded to a previous NTSB recommendation to improve the New Car Assessment Program. And Caltrans has gone 1,044 days without formally responding to an NTSB recommendation about improving its crash attenuator replacement process.\n\u201cIt is frankly disheartening.\u201d\n\u201cI am disturbed that in this report we call out a number of recommendations that are very late in being responded to,\u201d Sumwalt said at Tuesday\u2019s hearing. \u201cAgain, this is how we affect change, through our recommendations. It is frankly disheartening.\u201d\nThis exasperation was codified, too, as the NTSB on Tuesday \u201creiterated\u201d five previous recommendations that various governing bodies and companies have not met. To Tesla, for instance, it once again asked that the company add safeguards that limit the use of Autopilot only to situations that it was designed for. The board also re-recommended that Tesla develop a better driver monitoring system.\nSumwalt and the other board members held little back when expressing their frustration with NHTSA and Tesla throughout the hearing. They accused the former of basically abandoning its role as a regulatory body when it comes to systems like Autopilot (\u201cI wanted to join expressing disappointment for the lack of leadership\u201d from NHTSA, Chapman said at one point) and Tesla for how it\u2019s acted in that regulatory vacuum. Sumwalt even took issue with the name Autopilot, saying he has \u201ca personal belief that the term Autopilot may not be, from a safety point of view, the most well-branded name.\u201d\nBut since the NTSB can\u2019t force any of the changes it\u2019s seeking, it\u2019s unlikely that anything will change in the near term. And until it does, the safety board will probably have to keep investigating similar crashes like the one discussed on Tuesday.\nIn fact, when the NTSB released the documents in the Huang investigation last week, it also published documents related to its probe of another fatal crash involving Autopilot that happened last year. The full report is expected to be released in the next few weeks.\nMost Popular\n- Reddit will block the Internet Archive\n- GitHub just got less independent at Microsoft after CEO resignation\n- Ford reveals breakthrough process for lower priced EVs\n- RFK Jr. wants a wearable on every American \u2014 that future\u2019s not as healthy as he thinks\n- Microsoft releases lightweight Office taskbar apps for Windows 11"
    },
    {
      "url": "https://techcrunch.com/2025/06/22/tesla-launches-robotaxi-rides-in-austin-with-big-promises-and-unanswered-questions/",
      "text": "Tesla has started giving rides in driverless Model Y SUVs in Austin, a decade after CEO Elon Musk began making \u2014 and breaking \u2014 myriad promises about his company\u2019s ability to launch such a service.\nThe rollout will become the first big test of Musk\u2019s belief that it\u2019s possible to safely deploy fully autonomous vehicles using just cameras and end-to-end AI \u2013 an approach that differs from other players in the space like Waymo.\nOn Sunday, numerous videos shared on social media, as well as sources in the city, confirmed what Musk has been teasing for months: that the rides are finally happening, at a surely coincidental flat fee of $4.20 per ride.\nTesla sent early-access invitations in the past week to vetted customers, who were able to download and use the new robotaxi app on Sunday to hail rides. It\u2019s unclear how many people have received this invitation. But posts on Musk\u2019s social media platform X show that many of them went to Tesla\u2019s loudest online supporters.\nThe invitations, along with a new robotaxi information page published on Tesla\u2019s website on June 22, confirm the service will operate every day from 6:00 a.m. to 12:00 a.m. but \u201cmay be limited or unavailable in the event of inclement weather.\u201d And, notably, a Tesla employee will be sitting in the right front passenger seat as a \u201csafety monitor.\u201d\nThe robotaxi information page also includes instructions on downloading the app, how to report a lost item, and general rules for riders. It still glosses over the kind of specifics that Waymo \u2014 the Alphabet-owned AV company that operates commercial robotaxis in Phoenix, Los Angeles, San Francisco, and Austin \u2014 has historically provided.\nThe robotaxi service will be small to start, according to Musk. The initial fleet will include about ten 2025 Model Y SUVs operating in a narrowly defined area of South Austin. That\u2019s in line with a firsthand account by Ed Niedermeyer, author of \u201cLudicrous: The Unvarnished Story of Tesla Motors,\u201d who is in Austin to monitor the robotaxi rollout. (Niedermeyer is a co-host of The Autonocast with TechCrunch editor Kirsten Korosec.)\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $675 before prices rise.\nNiedermeyer found what appears to be a Tesla robotaxi depot \u2014 a nondescript parking lot dotted with trees near Oltorf Street in South Austin. The day before the launch, he spotted several driverless Model Ys \u2014 always with an employee behind the steering wheel \u2014 entering and exiting the parking lot. Groups of other Tesla Model Y vehicles, most with manufacturer plates, were also parked there.\nThis morning, he spotted the branded Tesla Model Y robotaxis, this time with the employee in the front passenger seat, leaving the holding area. He observed one of the branded robotaxis, which had not yet picked up a rider, suddenly hitting its brakes two separate times \u2014 once in the middle of an intersection. It\u2019s unclear why the vehicle behaved that way. However, in a video, which TechCrunch has viewed and which has since been posted on YouTube, both instances occurred as the Tesla passed by police vehicles that were located in parking lots adjacent to the roadway.\nInformation gaps\nLeading up to the launch, Musk shared dribs and drabs about the Tesla robotaxi launch in a few interviews and posts on X. Even now, nearly all of the information about the robotaxi launch has been provided by the company\u2019s biggest supporters.\nIn fact, Tesla has actively tried to suppress information about the robotaxi service. Tesla tried to block TechCrunch\u2019s public records request with the Texas Department of Transportation (TxDOT). The company has also tried to block the city of Austin from fulfilling a records request by Reuters, according to the news service.\n\u201cTesla seeks to be as transparent as possible, however, as explained further below, some of the requested information cannot be released because it is confidential information, trade secrets, and/or business information exchanged with the TxDOT in conjunction with conducting business with TxDOT,\u201d Taylor White, senior counsel on infrastructure for Tesla, wrote in a letter to the Texas Attorney General\u2019s office in April.\nOne of the more interesting rollout strategies is the company\u2019s use of a human \u201csafety monitor.\u201d\nIt\u2019s unclear what role these safety monitors will play and how much, if any control, they will have. These employees are likely not meant to intervene if the software is about to do something wrong. But they may have access to some sort of kill switch that can stop the car if that does happen.\nHistorically, autonomous vehicle companies like Waymo and Cruise tested their respective self-driving technology by having a human safety operator behind the wheel and a second engineer in the front passenger seat. Eventually, that might be reduced to one person sitting in the passenger seat before removing them altogether. This practice was traditionally done during the testing phase \u2014 not commercial operations.\nTesla is not using the futuristic vehicles, dubbed Cybercabs, that were revealed on October 10, 2024. Instead, the 2025 Tesla Model Y vehicles are equipped with what Musk describes as a new, \u201cunsupervised\u201d version of Tesla\u2019s Full Self-Driving software.\nTesla will not be using its in-cabin camera during rides by default. The company says it will only be used if a rider requests support or in the case of an emergency. It will use the camera after a ride ends to \u201cconfirm Robotaxi\u2019s readiness for its next trip.\u201d\nTesla is encouraging early-access riders to take photos and videos of their experiences, although it says it \u201cmay suspend or terminate Robotaxi access\u201d if riders violate its rules, including if they \u201cdisseminate content on a social media platform or similar medium depicting a violation of these Rules or misuse of the Robotaxi.\u201d (That includes riders agreeing not to smoke, vape, drink alcohol, do drugs, or use the robotaxi in connection with a crime.)\nMusk and other Tesla executives praised the milestone on X, with Ashok Elluswamy, the head of the company\u2019s self-driving team, posting a photo of the \u201cRobotaxi launch party\u201d from an undisclosed location.\n\u201cSuper congratulations to the @Tesla_AI software & chip design teams on a successful @Robotaxi launch!! Culmination of a decade of hard work,\u201d Musk wrote.\nBut at least one rider on Sunday reported having an experience where Tesla\u2019s remote support team had to help in some way. It\u2019s not immediately clear what happened during that ride, but that same rider later said the ride was very smooth."
    },
    {
      "url": "https://www.theverge.com/2018/5/2/17313324/tesla-autopilot-safety-statistics-elon-musk-q1-earnings",
      "text": "Tesla will publish quarterly reports about the safety of its Autopilot driver assistance feature, CEO Elon Musk announced on a call with analysts Wednesday. Musk didn\u2019t elaborate on exactly what the reports will entail, and a representative for Tesla declined to add any further detail. But the move could represent a major change in how the company treats data related to Autopilot, which is typically closely guarded.\nTesla will regularly release data about the safety of Autopilot, Elon Musk says\nQuarterly reports will outline Autopilot\u2019s performance as the company pushes for full self-driving functionality \u2014 and a shared autonomous fleet by the end of 2019\nMusk argued on the call that there is \u201cno question\u201d that Autopilot reduces the chance of a driver getting in an accident, something both he and his company have often claimed in the past. \u201cThe statistics are unequivocal that Autopilot improves safety,\u201d he said. Publishing these statistics about Autopilot\u2019s performance will let the public know \u201cexactly what Autopilot\u2019s safety [level] is,\u201d Musk said. \u201cIs it getting better, is it getting worse?\u201d\nMusk said he wants to let people know if Autopilot is getting better or worse, and by how much\nIt has not been easy to determine exactly how much Autopilot improves the safety of drivers, or even how to measure that in the first place. The most common figure Tesla and Musk use when making claims about Autopilot\u2019s safety is that it was \u201cfound by the U.S. government to reduce crash rates by as much as 40%.\u201d This statistic comes from the report that was filed at the conclusion of the National Highway Traffic Safety Administration\u2019s investigation into the 2016 death of Joshua Brown, who was using Autopilot when his Tesla Model S crashed into a tractor trailer. But the veracity of the statistic has recently come under fire, and today NHTSA distanced itself from the claim. Tesla declined to comment on the news.\nMusk announced the plan to increase transparency around Autopilot after repeatedly criticizing press coverage of the company\u2019s driver assistance feature, something he\u2019s done on analyst calls in the past. Autopilot has faced increased scrutiny after a driver of a Tesla Model X died while using the driver assistance feature on a California highway in March.\n\u201cI think theres 1.2 million automotive deaths per year, and how many do you read about? Basically none of them,\u201d Musk said, referring to global statistics. \u201cBut if it\u2019s an autonomous situation, it\u2019s headline news. And the media fails to mention that, actually, they shouldn\u2019t really be writing this story, they should be writing a story about how autonomous cars are really safe. But that\u2019s not the story that people want to click on. So they write inflammatory headlines that are fundamentally misleading to the readers.\u201d Musk didn\u2019t clarify which reports he takes umbrage with, or whether he\u2019s including local news coverage of vehicle fatalities in his criticism.\nThe problem, Musk says, isn\u2019t just that these \u201cinflammatory headlines\u201d are \u201cmisleading\u201d to readers. He also argued that it\u2019s affecting public policy. \u201cRegulators respond to public pressure, and the press,\u201d Musk said. \u201cSo if the press is hounding the regulators and the public is laboring on misapprehension that autonomy is less safe because of misleading press, then this is where I find things, the challenge for predicting it to be very difficult.\u201d\nMusk argued that \u201cnegative\u201d press coverage of Autopilot is affecting public policy around self-driving cars\nThe \u201cit\u201d that Musk was referring to was the timeframe for the launch of the shared, autonomous fleet of Teslas that he referenced in the second company \u201cmaster plan\u201d that was released in 2016. Despite the difficulty in guessing how fast (or slow) regulations will move around self-driving cars in the coming years, Musk did say that on the technical side, he thinks a shared fleet of autonomous Teslas \u2014 one that he said would be like a mix of \u201cUber Lyft and AirBnB,\u201d will \u201cprobably be ready by the end of next year.\u201d\nBefore Tesla gets there, though, it still needs to roll out full self-driving capability to the cars it is and has been making. While Tesla currently offers customers the ability to pre-pay for full self driving capability, the current version of Autopilot still more closely resembles advanced driver assistance systems like Cadillac\u2019s Super Cruise.\nMeanwhile, a coast-to-coast demonstration of Tesla\u2019s full self-driving capabilities has been delayed. Musk recently promised that the cross country drive is coming this year, though, and that the same capability will be made available to paying customers soon. Cars currently being made by Tesla should be able to handle full autonomy, Musk said, though he reiterated that the company may need to swap in more powerful computers to handle the processing power required.\nWhile Tesla is often cagey about the number of miles driven using Autopilot, Musk did say on the call that overall usage is increasing. For cars equipped with the feature, a third or \u201cmaybe half\u201d of highway miles in \u201csome regions\u201d are now driven using Autopilot, he said.\n\u201cBut then of course when there\u2019s negative news in the press, then that dips,\u201d he added. \u201cAnd then I was like, okay, this is not good, because people are reading things in the press that cause them to use Autopilot less, and then that makes it dangerous for our customers. And that\u2019s not cool, that\u2019s why I get upset.\u201d\nMusk has made this argument in the past \u2014 that \u201cnegative\u201d press coverage about Autopilot could scare people into using the feature less, which in turn could put more people in harm\u2019s way. But the Tesla CEO made a new argument about something he says the press gets wrong near the end of the call. Journalists often claim that a lack of understanding is to blame for Autopilot accidents, Musk said, but he believes the opposite is the case.\n\u201cWhen there is a serious accident it is almost always, in fact maybe always, the case that it is an experienced user, and the issue is more one of complacency,\u201d Musk said. \u201cThey just get too used to it. That tends to be more of an issue. It\u2019s not a lack of understanding of what Autopilot can do. It\u2019s [drivers] thinking they know more about Autopilot than they do.\u201d\nOverconfidence in the system does seem to be a problem, whether it\u2019s in the form of drivers hopping into the passenger seat of a Tesla or, as the company says happened in the fatal crash in March, when it appears that the driver received and ignored numerous prompts to retake control of the car.\nSome experts think problems like these could be mitigated with driver monitoring systems, like how Super Cruise watches drivers\u2019 eyes to make sure they\u2019re paying attention. In the Model S, X, and 3, Tesla only monitors driver attention by measuring resistance in the steering wheel. While it\u2019s thought that a small camera in the Model 3 might someday used for driver monitoring, it has not yet been activated. A lack of robust driver monitoring systems was one of the criticisms laid out in the National Transportation Safety Board\u2019s investigation into Brown\u2019s death. Back then, the NTSB recommended that Tesla \u2014 along with other automakers \u2014 find ways to monitor driver attention that go beyond detecting steering-wheel engagement.\nMost Popular\n- Reddit will block the Internet Archive\n- GitHub just got less independent at Microsoft after CEO resignation\n- Ford reveals breakthrough process for lower priced EVs\n- RFK Jr. wants a wearable on every American \u2014 that future\u2019s not as healthy as he thinks\n- Microsoft releases lightweight Office taskbar apps for Windows 11"
    }
  ],
  "argos_summary": "A federal jury in Miami has found Tesla partially liable for a fatal 2019 crash involving its Autopilot system, assigning one-third of the blame to the company and two-thirds to the driver. The jury awarded approximately $242.5 million in damages, including $200 million in punitive damages against Tesla. This verdict is significant as it marks one of the first major legal setbacks for Tesla regarding its driver assistance technology, with the company planning to appeal the decision.",
  "argos_id": "QFP6SFTL8"
}