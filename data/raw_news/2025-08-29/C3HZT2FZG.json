{
  "url": "https://lifehacker.com/tech/anthropic-training-ai-claude-user-conversations",
  "authorsByline": "Jake Peterson",
  "articleId": "ece1b83062f245cfae8cfe113e67449d",
  "source": {
    "domain": "lifehacker.com",
    "paywall": false,
    "location": null
  },
  "imageUrl": "https://lifehacker.com/imagery/articles/01K3RXSZ48E80J6H4DYHW9NCBG/hero-image.fill.size_1200x675.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-28T20:30:00+00:00",
  "addDate": "2025-08-28T20:39:40.576330+00:00",
  "refreshDate": "2025-08-28T20:39:40.576331+00:00",
  "score": 1.0,
  "title": "How to Stop Anthropic From Training Its AI Models on Your Conversations",
  "description": "Anthropic is changing its data policies, and will now start training its AI models on your chats with Claude. Luckily, you can opt-out.",
  "content": "Did you know you can customize Google to filter out garbage? Take these steps for better search results, including adding my work at Lifehacker as a preferred source.\n\nYou should never assume what you say to a chatbot is private. When you interact with one of these tools, the company behind it likely scrapes the data from the session, often using it to train the underlying AI models. Unless you explicitly opt out of this practice, you've probably unwittingly trained many models in your time using AI.\n\nAnthropic, the company behind Claude, has taken a different approach. The company's privacy policy has stated that Anthropic does not collect user inputs or outputs to train Claude, unless you either report the material to the company, or opt in to training. While that doesn't mean Anthropic was abstaining from collecting data in general, you could rest easy knowing your conversations weren't feeding future versions of Claude.\n\nThat's now changing. As reported by The Verge, Anthropic will now start training its AI models, Claude, on user data. That means new chats or coding sessions you engage with Claude on will be fed to Anthropic to adjust and improve the models' performances.\n\nThis will not affect past sessions if you leave them be. However, if you re-engage with a past chat or coding sessions following the change, Anthropic will scrape any new data generated from the session for its training purposes.\n\nThis won't just happen without your permission\u2014at least, not right away. Anthropic is giving users until Sept. 28 to make a decision. New users will see the option when they set up their accounts, while existing users will see a permission popup when they login. However, it's reasonable to think that some of us will be clicking through these menus and popups too quickly, and accidentally agree to data collection that we might not otherwise mean to.\n\nTo Anthropic's credit, the company says it does try to hide sensitive user data through \"a combination of tools and automated processes,\" and that it does not sell your data to third parties. Still, I certainly don't want my conversations with AI to train future models. If you feel the same, here's how to opt out.\n\nHow to opt out of Anthropic AI training\n\nIf you're an existing Claude user, you'll see a popup warning the next time you log into your account. This popup, titled \"Updates to Consumer Terms and Policies,\" explains the new rules, and, by default, opts you into the training. To opt out, make sure the toggle next to \"You can help improve Claude\" is turned off. (The toggle will be set to the left with an (X), rather than to the right with a checkmark.) Hit \"Accept\" to lock in your choice.\n\nIf you've already accepted this popup and aren't sure if you opted in to this data collection, you can still opt out. To check, open Claude and head to Settings > Privacy > Privacy Settings, then make sure the \"Help improve Claude\" toggle is turned off. Note that this setting will not undo any data that Anthropic has collected since you opted in.",
  "medium": "Article",
  "links": [
    "https://www.theverge.com/anthropic/767507/anthropic-user-data-consumers-ai-models-training-privacy",
    "https://lifehacker.com/tech/you-can-now-tell-google-which-websites-you-prefer-search-results",
    "https://www.google.com/preferences/source?q=lifehacker.com",
    "https://lifehacker.com/tech/your-conversations-with-chatbots-are-not-private",
    "https://web.archive.org/web/20250820124110/https://privacy.anthropic.com/en/articles/10023580-is-my-data-used-for-model-training"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "user data",
      "weight": 0.11338965
    },
    {
      "name": "sensitive user data",
      "weight": 0.10901605
    },
    {
      "name": "New users",
      "weight": 0.09522351
    },
    {
      "name": "data collection",
      "weight": 0.09095776
    },
    {
      "name": "data",
      "weight": 0.08868612
    },
    {
      "name": "Anthropic",
      "weight": 0.08706569
    },
    {
      "name": "Claude",
      "weight": 0.08680037
    },
    {
      "name": "existing users",
      "weight": 0.08420616
    },
    {
      "name": "user inputs",
      "weight": 0.0832978
    },
    {
      "name": "users",
      "weight": 0.081235066
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.8642578125
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.7001953125
    },
    {
      "name": "/News/Technology News",
      "score": 0.489013671875
    },
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.378662109375
    },
    {
      "name": "/Reference/General Reference/How-To, DIY & Expert Content",
      "score": 0.343017578125
    }
  ],
  "sentiment": {
    "positive": 0.10088595,
    "negative": 0.4886407,
    "neutral": 0.41047335
  },
  "summary": "Anthropic, the company behind Claude, will begin training its AI models on user data, which will be used to improve the performance of these models. The company's privacy policy states that Anthropic does not collect user inputs or outputs to train Claude, unless you either report the material to the company, or opt in to training. However, if you re-engage with a past chat or coding session after the change, Anthropic will use any new data generated from the session for its training purposes. This will not affect past sessions if you leave them be.",
  "shortSummary": "Anthropic now trains AI models on user data, requiring explicit opt-out, though this won't affect future models if you opt out.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": true,
  "reprintGroupId": "403eccef56294018913323d52a668ab7",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://web.archive.org/web/20250820124110/https://privacy.anthropic.com/en/articles/10023580-is-my-data-used-for-model-training",
      "text": "This article is about our consumer products (e.g. Claude Free, Claude Pro). For our commercial products (e.g. Claude for Work, Anthropic API), see here.\nWe will not use your Inputs or Outputs to train our generative models (i.e. Claude), unless you\u2019ve explicitly reported the materials to us (for example via our feedback mechanisms as noted below) or you\u2019ve explicitly opted in to training (for example by joining our trusted tester program).\nIf your conversations are flagged for violating our Usage Policy, we may use or analyze them to improve our ability to detect and enforce against harmful activity on our platform, including training internal-only models used in our safety systems.\nFeedback\nWhen you send us feedback, we will store the entire related conversation, including any content, custom styles or conversation preferences, in our secured back-end for up to 10 years as part of your feedback. Feedback data does not include raw content from integrations (e.g. Google Drive), though data may be included if it\u2019s directly copied into your conversation with Claude.\nWe de-link your feedback from your user ID (e.g. email address) before it\u2019s used by Anthropic. We may use your feedback to analyze the effectiveness of our Services, conduct research, study user behavior, and train our AI models as permitted under applicable laws. We do not combine your feedback with your other conversations with Claude.\nHere\u2019s an example of what you\u2019ll see when using the thumbs up/thumbs down icons to start a feedback report:"
    },
    {
      "url": "https://lifehacker.com/tech/you-can-now-tell-google-which-websites-you-prefer-search-results",
      "text": "Search ain't what it used to be. In the not-so-distant past, your query would return a simple list of links, which you could peruse at your convenience. Now, a search can yield a myriad of result types, from AI summaries to obnoxious or even malicious ads, with the links buried in between. There are ways to make search more useful, but they aren't always obvious.\nThat's why I'm curious about Google's latest search addition. Last week, Google announced that its feature, Preferred Sources, is rolling out to all English users in the U.S. and India. If you frequently test features with Google Labs, you might remember trying this one out: Preferred Sources, as the name implies, lets you tell Google which websites you prefer to read news from. The goal, then, is to see pages from those sites in future Google searches about news stories, assuming those sites publish content related to your search.\nHow to choose your preferred news sources in Google search\nHere's how it works: When you search for something covered in the news, Google will display a \"Top stories\" section at the top of the search results page. While you can simply browse the stories that Google curates for you, you can also now click a new button to the right of the \"Top stories\" title. From here, you can search for any website, and click a checkbox next to its name to save it to your list of preferred sources. Google doesn't put a cap on the number of sources you can add, so you can fill your list with as many sites as you want. You'll also see any sources you previously saved here (including during the Labs experiment period) and you can uncheck them as you wish.\nNow, you can refresh your search results, which, with any luck, will populate with more of the sources you added to this list. Not only will they appear more frequently in the \"Top stories\" section going forward, Google may offer you a new \"From your sources\" section as well, which should only contain the websites you've added to your list. As this feature saves your preferences, you do need to be signed into your Google Account, though it seems to work across a variety of browsers. (Safari and Firefox both worked fine for me.)\nIt's not necessarily groundbreaking, but I do see the utility here. Any chance you get to personalize your search experience is probably worth taking advantage of, and with Preferred Sources, you can (and should) improve the odds the websites you know and trust appear alongside any given news story. It's also an interesting juxtaposition alongside other changes to Google Search, such as Web Guide, an AI-powered Search experience that automatically groups links into different categories.\nIt can be difficult to find high-quality information on the web these days, but I have to say, between these two features, Google is making some decent changes here."
    },
    {
      "url": "https://lifehacker.com/tech/your-conversations-with-chatbots-are-not-private",
      "text": "When I was in college back in 2004 (I'm old) I installed an \"AI\" plugin that would automatically respond to incoming AIM messages (old) when I was away. This simple plugin automatically responded to messages using my chat history; if someone asked \"How are you?\" the bot would respond with an answer I recently gave to that question. You probably know where this is going: it took roughly two days for this plugin to repeat something nasty I said about a friend to said friend. I uninstalled it, having learned a lesson about AI privacy (and friendship).\nAI has come a long ways in 20 years, but the privacy problems haven't changed: anything you say to an AI chatbot might be read and potentially repeated.\nBe careful with what you say to AI chatbots\nJack Wallen, writing for ZDNet, pointed out that the privacy statement for Google's Gemini clearly states that all information in chats with Gemini (previously called Bard) is stored for three years and that humans routinely review the data. The privacy document also states, outright, that you shouldn't use the service for anything private. To quote the terms:\nDon\u2019t enter anything you wouldn\u2019t want a human reviewer to see or Google to use. For example, don\u2019t enter info you consider confidential or data you don\u2019t want to be used to improve Google products, services, and machine-learning technologies.\nThis is Google outright saying, in plain language, that humans may review your conversations and that they will be used to improve their AI products.\nNow, does this mean that Gemini is going to repeat private information you type in the chat box, the way my crappy AIM chatbot did? No, and the page does say that human reviewers work to remove obviously private data such as phone numbers and email addresses. But a ChatGPT leak late last year, wherein a security researcher managed to access training info, shows that anything a large language model has access to could\u2014at least in theory\u2014leak eventually.\nAnd this is all assuming the companies running your chatbots are at least attempting to be trustworthy. Both Google and OpenAI have clear privacy policies that state they do not sell personal information. But Thomas Germain, writing for Gizmodo, reported that AI \"girlfriends\" are encouraging users to share private information and then actively selling it. From the article:\nYou\u2019ve heard stories about data problems before, but according to Mozilla, AI girlfriends violate your privacy in \"disturbing new ways.\" For example, CrushOn.AI collects details including information about sexual health, use of medication, and gender-affirming care. 90% of the apps may sell or share user data for targeted ads and other purposes, and more than half won\u2019t let you delete the data they collect.\nSo not only may your chat data leak, but some companies in the AI space are actively collecting and selling private information.\nThe takeaway is basically to never talk about anything private with any sort of large language model. This means obvious things, like Social Security numbers, phone numbers, and addresses, but it extends to anything you'd rather not see leaked eventually. These applications simply aren't intended for private information."
    },
    {
      "url": "https://www.theverge.com/anthropic/767507/anthropic-user-data-consumers-ai-models-training-privacy",
      "text": "Anthropic will start training its AI models on user data, including new chat transcripts and coding sessions, unless users choose to opt out. It\u2019s also extending its data retention policy to five years \u2014 again, for users that don\u2019t choose to opt out.\nAnthropic will start training its AI models on chat transcripts\nYou can choose to opt out.\nYou can choose to opt out.\nAll users will have to make a decision by September 28th. For users that click \u201cAccept\u201d now, Anthropic will immediately begin training its models on their data and keeping said data for up to five years, according to a blog post published by Anthropic on Thursday.\nThe setting applies to \u201cnew or resumed chats and coding sessions.\u201d Even if you do agree to Anthropic training its AI models on your data, it won\u2019t do so with previous chats or coding sessions that you haven\u2019t resumed. But if you do continue an old chat or coding session, all bets are off.\nThe updates apply to all of Claude\u2019s consumer subscription tiers, including Claude Free, Pro, and Max, \u201cincluding when they use Claude Code from accounts associated with those plans,\u201d Anthropic wrote. But they don\u2019t apply to Anthropic\u2019s commercial usage tiers, such as Claude Gov, Claude for Work, Claude for Education, or API use, \u201cincluding via third parties such as Amazon Bedrock and Google Cloud\u2019s Vertex AI.\u201d\nNew users will have to select their preference via the Claude signup process. Existing users must decide via a pop-up, which they can defer by clicking a \u201cNot now\u201d button \u2014 though they will be forced to make a decision on September 28th.\nBut it\u2019s important to note that many users may accidentally and quickly hit \u201cAccept\u201d without reading what they\u2019re agreeing to.\nThe pop-up that users will see reads, in large letters, \u201cUpdates to Consumer Terms and Policies,\u201d and the lines below it say, \u201cAn update to our Consumer Terms and Privacy Policy will take effect on September 28, 2025. You can accept the updated terms today.\u201d There\u2019s a big black \u201cAccept\u201d button at the bottom.\nIn smaller print below that, a few lines say, \u201cAllow the use of your chats and coding sessions to train and improve Anthropic AI models,\u201d with a toggle on / off switch next to it. It\u2019s automatically set to \u201cOn.\u201d Ostensibly, many users will immediately click the large \u201cAccept\u201d button without changing the toggle switch, even if they haven\u2019t read it.\nIf you want to opt out, you can toggle the switch to \u201cOff\u201d when you see the pop-up. If you already accepted without realizing and want to change your decision, navigate to your Settings, then the Privacy tab, then the Privacy Settings section, and, finally, toggle to \u201cOff\u201d under the \u201cHelp improve Claude\u201d option. Consumers can change their decision anytime via their privacy settings, but that new decision will just apply to future data \u2014 you can\u2019t take back the data that the system has already been trained on.\n\u201cTo protect users\u2019 privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data,\u201d Anthropic wrote in the blog post. \u201cWe do not sell users\u2019 data to third-parties.\u201d\nMost Popular\n- Taco Bell\u2019s AI drive-thru plan gets caught up on trolls and glitches\n- Microsoft\u2019s employee protests have reached a boiling point\n- Honor\u2019s Magic V5 is the thinnest foldable yet, but that\u2019s not why it matters\n- DJI\u2019s Mic 3 crams more features into a smaller package\n- Google Pixel 10 Pro review: AI, Qi2, and a spec bump too"
    }
  ],
  "argos_summary": "Anthropic is changing its policy to train its Claude AI models on user data, including new chat and coding sessions, unless users opt out. The change, effective September\u202f28\u202f2025, applies to all consumer tiers and will retain data for up to five years, while existing chats remain untouched unless resumed. Users can opt out via a toggle in the settings or during account setup, but many may inadvertently accept the default. The article also briefly discusses Google\u2019s new Preferred Sources feature for news searches, but the main focus is Anthropic\u2019s data\u2011collection update.",
  "argos_id": "C3HZT2FZG"
}