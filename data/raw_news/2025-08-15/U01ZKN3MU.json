{
  "url": "https://www.wired.com/story/gpt-5-coding-review-software-engineering/",
  "authorsByline": "Lauren Goode",
  "articleId": "77def123a080402693f4e95443d7823e",
  "source": {
    "domain": "wired.com",
    "paywall": true,
    "location": {
      "country": "us",
      "state": "MA",
      "county": "Suffolk County",
      "city": "Boston",
      "coordinates": {
        "lat": 42.3602534,
        "lon": -71.0582912
      }
    }
  },
  "imageUrl": "https://media.wired.com/photos/689a19ecddceece04e84fa2f/191:100/w_1280,c_limit/chatgpt5-hate-biz-2228998627-1193951547.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-15T13:47:49.998000-04:00",
  "addDate": "2025-08-15T17:56:32.892808+00:00",
  "refreshDate": "2025-08-15T17:56:32.892810+00:00",
  "score": 1.0,
  "title": "Developers Say GPT-5 Is a Mixed Bag",
  "description": "Software engineers are finding OpenAI\u2019s new GPT-5 model is helping them think through coding problems\u2014but isn\u2019t much better at actual coding.",
  "content": "When OpenAI launched GPT-5 last week, it told software engineers the model was designed to be a \u201ctrue coding collaborator\u201d that excels at generating high-quality code and performing agentic, or automated, software tasks. While the company didn\u2019t say so explicitly, OpenAI appeared to be taking direct aim at Anthropic\u2019s Claude Code, which has quickly become many developers\u2019 favored tool for AI-assisted coding.\n\nBut developers tell WIRED that GPT-5 has been a mixed bag so far. It shines at technical reasoning and planning coding tasks, but some say that Anthropic\u2019s newest Opus and Sonnet reasoning models still produce better code. Depending on which version of GPT-5 developers are using\u2014low, medium, or high verbosity\u2014the model can be more elaborative, which sometimes leads it to generate unnecessary or redundant lines of code.\n\nSome software engineers have also criticized how OpenAI evaluated GPT-5\u2019s performance at coding, arguing the benchmarks it used are misleading. One research firm called a graphic that OpenAI published boasting about GPT-5\u2019s capabilities a \u201cchart crime.\u201d\n\nGPT-5 does stand out in at least one way: Several people noted that, in comparison to competing models, it is a much more cost-effective option. \u201cGPT-5 is mostly outperformed by other AI models in our tests, but it\u2019s really cheap,\u201d says Sayash Kapoor, a computer science doctoral student and researcher at Princeton University who co-wrote the book AI Snake Oil.\n\nKapoor says he and his team have been running benchmark tests to evaluate GPT-5\u2019s capabilities since the model was released to the public last week. He notes that the standard test his team uses\u2014measuring how well a language model can write code that will reproduce the results of 45 scientific papers\u2014costs $30 to run with GPT-5 set to medium, or mid-range verbosity. The same test using Anthropic\u2019s Opus 4.1 costs $400. In total, Kapoor says his team has spent around $20,000 testing GPT-5 so far.\n\nAlthough GPT-5 is cheap, Kapoor\u2019s tests indicate the model is also less accurate than some of its competitors. Claude\u2019s premium model achieved a 51 percent accuracy rating, measured by how many of the scientific papers it accurately reproduced. The medium version of GPT-5 received a 27 percent accuracy rating. (Kapoor has not yet run the same test using GPT-5 high, so it\u2019s an indirect comparison, given that Opus 4.1 is Anthropic\u2019s most powerful model.)\n\nOpenAI spokesperson Lindsay McCallum referred WIRED to its blog, where it said that it trained GPT-5 on \u201creal-world coding tasks in collaboration with early testers across startups and enterprises.\u201d The company also highlighted some of its internal accuracy measurements for GPT-5, which showed that the GPT-5 \u201cthinking\u201d model, which does more deliberate reasoning, scored highest on accuracy among all of OpenAI\u2019s models. GPT-5 \u201cmain,\u201d however, still fell short of previously-released models on OpenAI\u2019s own accuracy scale.\n\nAnthropic spokesperson Amie Rotherham said in a statement that \u201cperformance claims and pricing models often look different once developers start using them in production environments. Since reasoning models can quickly use a lot of tokens while thinking, the industry is moving to a world where price per outcome matters more than price per token.\u201d",
  "medium": "Article",
  "links": [
    "https://openai.com/index/introducing-gpt-5-for-developers/",
    "https://hal.cs.princeton.edu/corebench_hard",
    "https://www.wired.com/story/anthropic-new-model-launch-claude-4/",
    "https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/",
    "https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf",
    "https://hal.cs.princeton.edu",
    "https://www.wired.com/story/openais-gpt-5-is-here/",
    "https://www.pcgamer.com/software/ai/openais-performance-charts-in-the-gpt-5-launch-video-are-such-a-mess-you-have-to-think-gpt-5-itself-probably-made-them-and-the-companys-attempted-fixes-raise-even-more-questions/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "GPT-5 developers",
      "weight": 0.11078593
    },
    {
      "name": "reasoning models",
      "weight": 0.10643462
    },
    {
      "name": "GPT-5",
      "weight": 0.10307122
    },
    {
      "name": "other AI models",
      "weight": 0.10012739
    },
    {
      "name": "competing models",
      "weight": 0.099430025
    },
    {
      "name": "OpenAI spokesperson Lindsay McCallum",
      "weight": 0.07124994
    },
    {
      "name": "coding tasks",
      "weight": 0.06975401
    },
    {
      "name": "OpenAI",
      "weight": 0.06751925
    },
    {
      "name": "benchmark tests",
      "weight": 0.06394173
    },
    {
      "name": "OpenAI\u2019s models",
      "weight": 0.060415577
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.6220703125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.56591796875
    },
    {
      "name": "/News/Technology News",
      "score": 0.5185546875
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.378173828125
    },
    {
      "name": "/Computers & Electronics/Programming/Development Tools",
      "score": 0.33642578125
    }
  ],
  "sentiment": {
    "positive": 0.1275093,
    "negative": 0.52478266,
    "neutral": 0.34770802
  },
  "summary": "OpenAI's new AI-assisted coding model, GPT-5, has been criticized by developers for its mixed performance so far. The model was designed to be a \"true coding collaborator\" that excels at generating high-quality code and performing agentic software tasks. However, some developers have argued that Anthropic's newest Opus and Sonnet reasoning models produce better code. Some developers have also criticized OpenAI's evaluation of GPT5's performance at coding, arguing that the benchmarks it used for its evaluation are misleading. GPT 5 stands out in comparison to other AI models as a cheaper option, but some critics argue it is less accurate than some competitors. The company has also highlighted internal accuracy measurements for GPT -5, which showed that the GPT's \"thinking\" model scored highest on accuracy among all of OpenAI\u2019s models.",
  "shortSummary": "OpenAI\u2019s GPT-5 model excels in coding but faces criticism for its accuracy and complexity, with some developers claiming it lacks context and price effectiveness.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "1c9af094d2f145d09b2cca1949c1b98f",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.wired.com/story/openais-gpt-5-is-here/",
      "text": "OpenAI has begun rolling out GPT-5, the latest iteration of its flagship language model, to all ChatGPT users.\nThe company\u2019s CEO Sam Altman called GPT-5 \u201ca significant step along the path to AGI\u201d during a press briefing on Wednesday. While he stopped short of claiming the model reaches artificial general intelligence, Altman noted the latest release is \u201cclearly a model that is generally intelligent.\u201d He added that GPT-5 still lacks key traits that would make it reach AGI, a notably loose term that is defined in OpenAI\u2019s charter as \u201ca highly autonomous system that outperforms humans at most economically valuable work.\u201d For example, the model still lacks the ability to learn continuously after deployment.\nOpenAI claims GPT-5 is smarter, faster, more useful, and more accurate, with a lower hallucination rate than previous models. In characteristically lofty terms, Altman likened the leap from GPT-4 to GPT-5 to the iPhone\u2019s shift from pixelated to a Retina display. \u201cGPT-5 is the first time that it really feels like talking to an expert in any topic, like a PhD level expert,\u201d Altman said.\nWatch our subscriber-only livestream about all things GPT-5, hosted by Kylie Robison, Will Knight, and Reece Rogers.\nAs part of Thursday\u2019s launch, OpenAI announced two new model variants: a lightweight GPT-5-mini and an even faster, cheaper, GPT-5-nano (which is only in the API). According to OpenAI, free users will get access to GPT-5 and GPT-5-mini, while the Plus subscription includes the same models with \u201csignificantly higher\u201d usage limits. OpenAI says that the $200 a month Pro tier offers unlimited GPT-5 access, along with GPT-5-pro, a more powerful version of the model, and GPT-5-thinking, which allows the model to process a query for longer than usual. Pro users will still have access to pick through legacy models. Most users will no longer need to choose between models, as the chat interface now automatically routes to the right version depending on the complexity of the query and the user\u2019s subscription tier.\nAccording to the developer launch blog, GPT-5 will cost developers using the API $1.25/1M to input tokens and $10/1M to output tokens. \u201cGPT-5 mini is priced at $0.25/1M input tokens and $2/1M output tokens, and GPT-5 nano is priced at $0.05/1M input tokens and $0.40/1M output tokens,\u201d it adds. For comparison, developers often use Gemini 2.5 Flash and Flash-Lite since it\u2019s so cheap\u2014GPT-5 nano is now cheaper.\nStarting next week, Pro users will be able to connect their Gmail, Google Contacts, and Google Calendar to ChatGPT, with other tiers gaining access at an unspecified date. \u201cChatGPT automatically knows when it\u2019s most relevant to reference them so you don\u2019t need to select them before you chat,\u201d the company said in an email.\nUsers can also choose a chat color and select from four preset personalities\u2014Cynic, Robot, Listener, and Nerd\u2014a feature WIRED\u2019s newsletter Model Behavior reported was in the works last week. According to OpenAI\u2019s blog announcement, it plans to bake these personalities into Advanced Voice Mode.\nThe company\u2019s API will offer users all three models, along with optional controls for toggling between detailed or direct responses. GPT-5 can retain more information than prior models\u2014it has a 256,000-token context window, a bump up from the 200,000-token context window available in the company\u2019s previous o3 model. That means it can better understand long conversations, documents, or code without losing track of context.\nOpenAI\u2019s blog post claims that GPT-5 beats its previous models on several coding benchmarks, including SWE-Bench Verified (scoring 74.9 percent), SWE-Lancer (GPT-5-thinking scored 55 percent), and Aider Polyglot (scored 88 percent), which test the model\u2019s ability to fix bugs, complete freelance-style coding tasks, and work across multiple programming languages.\nDuring the press briefing on Wednesday, OpenAI post-training lead Yann Dubois prompted GPT-5 to \u201ccreate a beautiful, highly interactive web app for my partner, an English speaker, to learn French.\u201d He tasked the AI to include features like daily progress, a variety of activities like flashcards and quizzes, and noted that he wanted the app wrapped up in a \u201chighly engaging theme.\u201d After a minute or so, the AI-generated app popped up. While it was just one on-rails demo, the result was a sleek site that delivered exactly what Dubois asked for.\n\u201cIt's a great coding collaborator, and also excels at agentic tasks,\u201d Michelle Pokrass, a post-training lead, says. \u201cIt executes long chains and tool calls effectively [which means it better understands when and how to use functions like web browsers or external APIs], follows detailed instructions, and provides upfront explanations of its actions.\"\nOpenAI also says in its blog post that GPT-5 is \u201cour best model yet for health-related questions.\u201d In three OpenAI health-related LLM benchmarks\u2014HealthBench, HealthBench Hard, and HealthBench Consensus\u2014the system card (a document that describes the product\u2019s technical capabilities and other research findings) states that GPT-5-thinking outperforms previous models \u201cby a substantial margin.\u201d The thinking version of GPT-5 scored 25.5 percent on HealthBench Hard, up from o3\u2019s 31.6 percent score. These scores are validated by two or more physicians, according to the system card.\nThe model also allegedly hallucinates less, according to Pokrass, a common issue for AI where it provides false information. OpenAI\u2019s safety research lead Alex Beutel adds that they\u2019ve \"significantly decreased the rates of deception in GPT-5.\u201d\n\u201cWe\u2019ve taken steps to reduce GPT-5-thinking\u2019s propensity to deceive, cheat, or hack problems, though our mitigations are not perfect and more research is needed,\u201d the system card says. \u201cIn particular, we\u2019ve trained the model to fail gracefully when posed with tasks that it cannot solve.\u201d\n| Got a Tip? |\n|---|\n| Do you work at OpenAI? We'd like to hear from you. Using a nonwork phone or computer, contact Kylie Robison on kylie_robison@wired.com or on Signal kylie.01. |\nThe company\u2019s system card says that after testing GPT-5 models without access to web browsing, researchers found its hallucination rate (which they defined as \u201cpercentage of factual claims that contain minor or major errors\u201d) 26 percent less common than the GPT-4o model. GPT-5-thinking has a 65 percent reduced hallucination rate compared to o3.\nFor prompts that could be dual-use (potentially harmful or benign), Beutel says GPT-5 uses \u201csafe completions,\u201d which prompts the model to \u201cgive as helpful an answer as possible, but within the constraints of remaining safe.\u201d OpenAI did over 5,000 hours of red teaming, according to Beutel, and testing with external organizations to make sure the system was robust.\nOpenAI says it now boasts nearly 700 million weekly active users of ChatGPT, 5 million paying business users, and 4 million developers utilizing the API.\n\u201cThe vibes of this model are really good, and I think that people are really going to feel that,\u201d head of ChatGPT Nick Turley says. \u201cEspecially average people who haven't been spending their time thinking about models.\u201d"
    },
    {
      "url": "https://www.pcgamer.com/software/ai/openais-performance-charts-in-the-gpt-5-launch-video-are-such-a-mess-you-have-to-think-gpt-5-itself-probably-made-them-and-the-companys-attempted-fixes-raise-even-more-questions/",
      "text": "OpenAI's performance charts in the GPT-5 launch video are such a mess you have to think GPT-5 itself probably made them, and the company's attempted fixes raise even more questions\nDoes OpenAI even care?\nWhat to make of OpenAI's latest GPT-5 chatbot? Let's just say the reception from users has been sufficiently mixed to have OpenAI head honcho Sam Altman posting apologetically on X. And more than once. But one thing we can say for sure, the charts in the launch video were a bizarre mess that OpenAI has since attempted to tidy up, to mixed avail.\nMost obviously, the claimed SWE-bench performance of GPT-5 versus older model shown on launch day was badly botched. The chart showed accuracy figures of 74.9% for ChatGPT 5, 69.1% for OpenAi o3 and 30.8% for GPT-4o.\nProblem is, the bar graph heights were exactly the same for the latter two, giving the at-a-glance impression of total dominance for GPT-5 when in fact it is only marginally superior to OpenAI o3.\nIt's a basic enough mistake that you have to wonder whether OpenAI used, well, GPT-5 itself make the charts and couldn't be bothered to proof them. Later on in the video there's another graph showing, not a little ironically, the deception rate of GPT-5. In this chart, it shows the bar graphs for GPT-5 and OpenAI o3.\nGPT-5 scores a \"coding deception\" rate of 50%, OpenAI o3's is 47.4%. But the bar for OpenAI o3 is rendered roughly three times higher than that of GPT-5. Now, you could recognise that a lower deception rate is better and make some kind of convoluted argument for therefore making OpenAI o3's bar higher.\nApart from the fact that this approach still doesn't account for the large discrepancy in bar height, the problem is that on the same slide OpenAI also shows stats for \"CharXiv missing image\". And here the bars are accurately proportional to the percentage results, with the 9% for GPT-5 a tiny fraction of the height of the 86.7% for OpenAI o3.\nAnother wonky chart cooked up by AI? Some kind of subtle satire? Just lazy, sloppy work? The usual adage probably applies, so assuming conspiracy where mere incompetence will suffice is probably unjustified. But it certainly implies a level of complacency that squares with the overall sense of entitlement and lack of rigour and accountability that surrounds the AI industry at large.\nKeep up to date with the most important stories and the best deals, as picked by the PC Gamer team.\nOpenAI has since posted some updated charts on its website. The new deception rate chart certainly suggests that a mere mistake was made. The revised stats show GPT-5's coding deception rate at 16.5%, which squares with the bar heights in the launch video.\nHowever, while the bar height in the SWE-bench chart has also been corrected, OpenAI added a further disclaimer, pointing out that the figures were achieved using 477 tasks within the SWE-bench suite, not the full 500.\nWhen we compare directly to Anthropic's Claude Opus 4.1, which has achieved a score of 74.5% on SWE-bench Verified, GPT-5 appears to score slightly better at 74.9%. But where are these 23 missing problems? What are they? pic.twitter.com/F8BSZnxLZtAugust 11, 2025\nThat has lead some observers to question whether a few inconvenient tasks were left out in order to allow GPT-5 to hit 74.9% and thus edge marginally ahead of the 74.5% score racked up by Anthropic's Claude Opus 4.1 model. Indeed, this raised the eyebrow of none other than Elon Musk, too.\nMeanwhile, original launch video with the messed up charts is still there on OpenAI's YouTube channel, implying OpenAI isn't all that bothered. Whatever is going on, exactly, it's all fairly unsightly. At best it's unbecoming of an organisation that supposedly produces artificial \"intelligence.\" At worst, it's thoroughly unnerving if managing the dangers AI and all our safety depends on these people.\n1. Best overall:\nHP Omen 35L\n2. Best budget:\nLenovo Legion Tower 5i\n3. Best compact:\nVelocity Micro Raptor ES40\n4. Alienware:\nAlienware Aurora\n5. Best mini PC:\nMinisforum AtomMan G7 PT\nJeremy has been writing about technology and PCs since the 90nm Netburst era (Google it!) and enjoys nothing more than a serious dissertation on the finer points of monitor input lag and overshoot followed by a forensic examination of advanced lithography. Or maybe he just likes machines that go \u201cping!\u201d He also has a thing for tennis and cars.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/",
      "text": "Anthropic revoked OpenAI\u2019s API access to its models on Tuesday, multiple sources familiar with the matter tell WIRED. OpenAI was informed that its access was cut off due to violating the terms of service.\n\u201cClaude Code has become the go-to choice for coders everywhere, and so it was no surprise to learn OpenAI's own technical staff were also using our coding tools ahead of the launch of GPT-5,\u201d Anthropic spokesperson Christopher Nulty said in a statement to WIRED. \u201cUnfortunately, this is a direct violation of our terms of service.\u201d\nAccording to Anthropic\u2019s commercial terms of service, customers are barred from using the service to \u201cbuild a competing product or service, including to train competing AI models\u201d or \u201creverse engineer or duplicate\u201d the services. This change in OpenAI\u2019s access to Claude comes as the ChatGPT-maker is reportedly preparing to release a new AI model, GPT-5, which is rumored to be better at coding.\nOpenAI was plugging Claude into its own internal tools using special developer access (APIs), instead of using the regular chat interface, according to sources. This allowed the company to run tests to evaluate Claude\u2019s capabilities in things like coding and creative writing against its own AI models, and check how Claude responded to safety-related prompts involving categories like CSAM, self-harm, and defamation, the sources say. The results help OpenAI compare its own models\u2019 behavior under similar conditions and make adjustments as needed.\n\u201cIt\u2019s industry standard to evaluate other AI systems to benchmark progress and improve safety. While we respect Anthropic\u2019s decision to cut off our API access, it\u2019s disappointing considering our API remains available to them,\u201d OpenAI\u2019s chief communications officer Hannah Wong said in a statement to WIRED.\nNulty says that Anthropic will \u201ccontinue to ensure OpenAI has API access for the purposes of benchmarking and safety evaluations as is standard practice across the industry.\u201d The company did not respond to WIRED\u2019s request for clarification on if and how OpenAI's current Claude API restriction would impact this work.\nTop tech companies yanking API access from competitors has been a tactic in the tech industry for years. Facebook did the same to Twitter-owned Vine (which led to allegations of anticompetitive behavior) and last month Salesforce restricted competitors from accessing certain data through the Slack API. This isn\u2019t even a first for Anthropic. Last month, the company restricted the AI coding startup Windsurf\u2019s direct access to its models after it was rumored OpenAI was set to acquire it. (That deal fell through).\nAnthropic\u2019s chief science officer Jared Kaplan spoke to TechCrunch at the time about revoking Windsurf\u2019s access to Claude, saying, \u201cI think it would be odd for us to be selling Claude to OpenAI.\u201d\nA day before cutting off OpenAI\u2019s access to the Claude API, Anthropic announced new rate limits on Claude Code, its AI-powered coding tool, citing explosive usage and, in some cases, violations of its terms of service."
    },
    {
      "url": "https://hal.cs.princeton.edu",
      "text": "HAL: Holistic Agent Leaderboard\nThe standardized, cost-aware, and third-party leaderboard for evaluating agents.\nBy the SAgE team at Princeton University\nPerformance Highlights\nTop performing agents across different benchmarks\nCORE-Bench Hard\nScientific ProgrammingTop 3 performing agents\nAssistantBench\nWeb AssistanceTop 3 performing agents\nScicode\nScientific ProgrammingTop 3 performing agents\nGAIA\nWeb AssistanceTop 3 performing agents\nTAU-bench Airline\nCustomer ServiceTop 3 performing agents\nOnline Mind2Web\nWeb AssistanceTop 3 performing agents\nSWE-bench Verified Mini\nSoftware EngineeringTop 3 performing agents\nUSACO\nProgrammingTop 3 performing agents\nWho is it for?\nHAL serves four key user groups in the AI ecosystem\nDownstream Users & Procurers\n- \u2022 Discover useful but less known benchmarks related to tasks you care about\n- \u2022 Find out who is building strong agents on these benchmarks\n- \u2022 Identify the state of the art for both cost and accuracy on these tasks\nBenchmark Developers\n- \u2022 Gain improved visibility for your benchmark\n- \u2022 Incentivize agent developers to build agents for your benchmark\n- \u2022 Enable cost-controlled evaluations by default without extra effort\nAgent Developers\n- \u2022 Easily reproduce existing agents and perform unbiased comparisons\n- \u2022 Compete on a leaderboard in a straightforward way\n- \u2022 Use HAL harness for framework-agnostic agent evaluation\nSafety Researchers\n- \u2022 Understanding agent capabilities on real-world safety threats and their associated costs\nCost-Controlled Agent Evaluations\nUnderstanding the cost-performance trade-off\nWhy looking at the Pareto frontier matters\n- \u2022 Agents can be 100x more expensive while only being 1% better\n- \u2022 Downstream developers can't tell the difference from 1D leaderboards\nThe HAL Evaluation Harness\nA unified framework for reproducible agent evaluation\nStandardized Evaluation\n- One-stop shop evaluation harness for all benchmarks and agents\n- Flexible execution environments for running parallel evaluations locally or in the cloud\nComprehensive Logging\n- Automatic logging of agent traces with W&B Weave\n- Detailed cost tracking of token usage with minimal edits to agent code\nDeveloper Friendly\n- Easy agent integration that does not require a specific agent framework\n- Modular architecture that allows for easy extentions with new benchmarks and agents\nAgent Traces\nEnabling rapid development and debugging while protecting benchmark integrity\nComplete Agent Traces\nWe make available the full traces of agent evaluations, including every single model call as logged by W&B Weave.\nEncrypted Distribution\nAll agent traces are encrypted to prevent benchmark contamination through automated scraping.\nMeet the Team\nThe people behind HAL\nCore Team\nContributors\nWe're grateful to the network of contributors to HAL:\nWant to Contribute?\nHAL is an open-source project and we welcome contributions from the community.\nCite HAL\n@Misc{hal, title = {HAL: A Holistic Agent Leaderboard for Centralized and Reproducible Agent Evaluation}, author = {Sayash Kapoor and Benedikt Stroebl and Peter Kirgis and Franck St\u00e9phane Ndzomga and Kangheng Liu and Arvind Narayanan}, howpublished = {\\url{https://github.com/princeton-pli/hal-harness/}}, year = {2025}}\nFunding\nHAL is funded by Open Philanthropy, Schmidt Sciences, the Princeton AI Lab and the Princeton Language and Intelligence Initiative. We are grateful to OpenAI for providing API credits to evaluate their models."
    },
    {
      "url": "https://hal.cs.princeton.edu/corebench_hard",
      "text": "CORE-Bench Hard\nCORE-Bench evaluates the ability of agents to computationally reproduce the results of published scientific papers. In CORE-Bench Hard, the agent is only given the codebase of the paper and must install all libraries and dependencies, run the code, and read through the output and figures to answer questions about the paper. This level is most akin to fully reproducing a paper and is the most realistic and challenging level.\nKey Features of CORE-Bench\nScientific Papers\nTasks are based on actual published scientific papers, requiring agents to understand and reproduce real research.\nComprehensive Evaluation\nTests multiple capabilities: code understanding, dependency management, and scientific result interpretation.\nThree Difficulty Levels\nTasks range from interpreting results (Easy) to full paper reproduction (Hard), allowing granular capability assessment.\nCORE-Bench Hard Leaderboard\n| Rank | Agent |\nPrimary Model\nPrimary Model\nThis is the primary model used by the agent. In some cases, an embedding model is used for RAG, or a secondary model like GPT-4o for image processing. Note: For non-OpenAI reasoning models, the reasoning token budget is set at 1,024 (low), 2,048 (medium), and 4,096 (high). |\nVerified\nVerified Results\nResults have been reproduced by the HAL team |\nAccuracy\nAccuracy\nConfidence intervals show the min-max values across runs for those agents where multiple runs are available |\nCost (USD)\nTotal Cost\nTotal API cost for running the agent on all tasks. Confidence intervals show the min-max values across runs for those agents where multiple runs are available |\nRuns\nNumber of Runs\nThe number of runs for this agent submitted to the leaderboard. To submit multiple evaluations, rerun the same agent and set the same agent name |\nTraces |\n|---|---|---|---|---|---|---|---|\n| 1 |\nCORE-Agent\n|\nClaude Opus 4.1 (August 2025) | \u2713 | 51.11% | $412.42 | 1 | Download |\n| 2 |\nCORE-Agent\n|\nClaude Opus 4.1 High (August 2025) | \u2713 | 42.22% | $509.95 | 1 | Download |\n| 3 |\nCORE-Agent\n|\nClaude-3.7 Sonnet (February 2025) | \u2713 | 35.56% | $73.04 | 1 | Download |\n| 4 |\nHAL Generalist Agent\n|\nClaude Opus 4.1 (August 2025) | \u2713 | 35.56% | $375.11 | 1 | Download |\n| 5 |\nCORE-Agent\n|\nGPT-4.1 (April 2025) | \u2713 | 33.33% | $107.36 | 1 | Download |\n| 6 |\nHAL Generalist Agent\n|\nClaude Opus 4.1 High (August 2025) | \u2713 | 33.33% | $358.47 | 1 | Download |\n| 7 |\nHAL Generalist Agent\n|\nClaude-3.7 Sonnet High (February 2025) | \u2713 | 31.11% | $54.37 | 1 | Download |\n| 8 |\nCORE-Agent\n|\no4-mini High (April 2025) | \u2713 | 26.67% | $61.35 | 1 | Download |\n| 9 |\nCORE-Agent\n|\nGPT-5 Medium (August 2025) | \u2713 | 26.67% | $31.76 | 1 | Download |\n| 10 |\nCORE-Agent\n|\nClaude-3.7 Sonnet High (February 2025) | \u2713 | 24.44% | $72.47 | 1 | Download |\n| 11 |\nCORE-Agent\n|\no3 Medium (April 2025) | \u2713 | 24.44% | $120.47 | 1 | Download |\n| 12 |\nHAL Generalist Agent\n|\no4-mini High (April 2025) | \u2713 | 24.44% | $50.28 | 1 | Download |\n| 13 |\nCORE-Agent\n|\nDeepSeek V3 | \u2713 | 17.78% | $5.79 | 1 | Download |\n| 14 |\nCORE-Agent\n|\no4-mini Low (April 2025) | \u2713 | 17.78% | $31.79 | 1 | Download |\n| 15 |\nHAL Generalist Agent\n|\no3 Medium (April 2025) | \u2713 | 15.56% | $24.70 | 1 | Download |\n| 16 |\nHAL Generalist Agent\n|\no4-mini Low (April 2025) | \u2713 | 15.56% | $21.80 | 1 | Download |\n| 17 |\nHAL Generalist Agent\n|\nGPT-4.1 (April 2025) | \u2713 | 13.33% | $54.61 | 1 | Download |\n| 18 |\nCORE-Agent\n|\nGPT-OSS-120B High | \u2713 | 11.11% | $4.21 | 1 | Download |\n| 19 |\nCORE-Agent\n|\nGPT-OSS-120B | \u2713 | 11.11% | $4.21 | 1 | Download |\n| 20 |\nCORE-Agent\n|\nGemini 2.0 Flash | \u2713 | 11.11% | $12.46 | 1 | Download |\n| 21 |\nHAL Generalist Agent\n|\nClaude-3.7 Sonnet (February 2025) | \u2713 | 10.37% (-10.37/+11.85) | $40.81 (-40.65/+22.40) | 3 | Download |\n| 22 |\nCORE-Agent\n|\nDeepSeek R1 | \u2713 | 8.89% | $24.84 | 1 | Download |\n| 23 |\nHAL Generalist Agent\n|\nGPT-OSS-120B | \u2713 | 8.89% | $2.79 | 1 | Download |\n| 24 |\nHAL Generalist Agent\n|\nDeepSeek V3 | \u2713 | 8.89% | $4.28 | 1 | Download |\n| 25 |\nHAL Generalist Agent\n|\nGPT-OSS-120B High | \u2713 | 8.89% | $2.05 | 1 | Download |\n| 26 |\nHAL Generalist Agent\n|\nDeepSeek R1 | \u2713 | 6.67% | $9.40 | 1 | Download |\n| 27 |\nHAL Generalist Agent\n|\nGemini 2.0 Flash | \u2713 | 6.67% | $17.68 | 1 | Download |\nAccuracy vs. Cost Frontier for CORE-Bench\nThis plot shows the relationship between an agent's performance and its token cost. The Pareto frontier (dashed line) represents the current state-of-the-art trade-off. The error bars indicate min-max values across runs.\nHeatmap for CORE-Bench\nThe heatmap visualizes success rates across tasks and agents. Colorscale shows the fraction of times a task was solved across reruns of the same agent. The \"any agent\" performance indicates the level of saturation of the benchmark and gives a sense of overall progress.\nTotal Completion Tokens Used per Agent\nThe bar chart shows the total completion tokens used by each agent, with the height of each bar representing the total number of completion tokens used across all tasks. Secondary models usually contribute a relatively small amount of tokens in comparison, and are used for RAG or image processing only.\nToken Pricing Configuration\nAdjust token prices to see how they affect the total cost calculations in the leaderboard and plots.\nAdditional Resources\nGetting Started\nWant to evaluate your agent on CORE-Bench? Follow our comprehensive guide to get started:\nView Documentation"
    }
  ],
  "argos_summary": "OpenAI launched GPT\u20115 as a \u201ccoding collaborator\u201d that promises higher accuracy and lower hallucinations, but developer reviews find it only marginally better than older models and less accurate than Anthropic\u2019s Claude Opus, while still being cheaper to run. The company\u2019s launch video contained chart errors that were later corrected, sparking criticism over transparency and prompting Anthropic to revoke OpenAI\u2019s API access for benchmarking Claude. GPT\u20115 offers several variants (mini, nano, pro) with a 256,000\u2011token context window and claims strong performance on coding benchmarks like SWE\u2011Bench, yet real\u2011world tests show a 27% accuracy rate versus 51% for Claude. Despite the mixed reception, OpenAI continues to roll out GPT\u20115 to ChatGPT users and API customers, positioning it as a step toward AGI while highlighting its lower hallucination and improved agentic task handling. ",
  "argos_id": "U01ZKN3MU"
}