{
  "url": "https://www.theverge.com/news/757537/meta-robby-starbuck-conservative-activist-ai-bias-advisor",
  "authorsByline": "Emma Roth",
  "articleId": "1a9f4a05bcc94dcd9506507907f014c5",
  "source": {
    "domain": "theverge.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "DC",
      "city": "Washington",
      "coordinates": {
        "lat": 38.8949924,
        "lon": -77.0365581
      }
    }
  },
  "imageUrl": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/STK043_VRG_Illo_N_Barclay_5_Meta.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-11T17:04:37+00:00",
  "addDate": "2025-08-11T19:53:57.997539+00:00",
  "refreshDate": "2025-08-11T19:53:57.997543+00:00",
  "score": 1.0,
  "title": "Meta makes conservative activist an AI bias advisor following lawsuit",
  "description": "The move aligns with Trump\u2019s broader \u201canti-woke\u201d AI order.",
  "content": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nPosts from this author will be added to your daily email digest and your homepage feed.\n\nConservative activist Robby Starbuck will serve as an advisor at Meta to address \u201cideological and political bias\u201d within the company\u2019s AI chatbot, according to a report from The Wall Street Journal. The move comes as part of a settlement Meta reached with Starbuck, who filed a lawsuit claiming Meta AI wrongly stated he was involved in the January 6th Capitol riot.\n\nStarbuck has waged public pressure campaigns against companies that have diversity, equity, and inclusion (DEI) programs, with companies like Tractor Supply, John Deere, and Harley-Davidson dropping their DEI efforts as a result. According to a lawsuit filed in April, Starbuck claims he discovered Meta AI\u2019s false output after a Harley-Davidson dealer published a screenshot from Meta\u2019s AI chatbot linking Starbuck to the Capitol riot and QAnon.\n\nNow, following President Donald Trump\u2019s executive order to make AI less \u201cwoke,\u201d Starbuck will advise Meta on bias. \u201cSince engaging on these important issues with Robby, Meta has made tremendous strides to improve the accuracy of Meta AI and mitigate ideological and political bias,\u201d Meta and Starbuck said in a statement to the WSJ.\n\nDuring an interview with CNBC, Starbuck declined to say whether Meta paid him to resolve the lawsuit. \u201cI\u2019m one person, but this could cause a lot of problems across the entire industry when it comes to elections and political bias, and we wanted to be leaders in solving this problem,\u201d Starbuck said during the interview. Earlier this year, Meta paid $25 million to settle a 2021 lawsuit filed by President Donald Trump over the suspensions of his accounts.\n\nOther people have attempted to file lawsuits alleging AI chatbot defamation. Conservative radio host Mark Walters filed a lawsuit against OpenAI in 2023, alleging that ChatGPT falsely stated that Walters was accused of embellishing funds from a non-profit organization. A judge granted summary judgment in favor of OpenAI and dismissed the defamation claim in May.\n\nFollow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates.",
  "medium": "Article",
  "links": [
    "https://www.wsj.com/tech/ai/meta-robby-starbuck-ai-lawsuit-settlement-6c6e9b0a?mod=tech_lead_pos5",
    "https://www.wsj.com/business/the-activist-pushing-companies-to-ditch-their-diversity-policies-aeb82873?mod=article_inline",
    "https://www.theverge.com/2023/6/9/23755057/openai-chatgpt-false-information-defamation-lawsuit",
    "https://www.cnbc.com/video/2025/08/08/robby-starbuck-on-meta-lawsuit-we-dont-want-ai-putting-its-thumb-on-the-scale-in-politics.html",
    "https://www.theverge.com/policy/713788/trump-ai-action-plan-explainer",
    "https://www.cnn.com/2024/08/19/business/harley-davidson-dei-john-deere-tractor-supply",
    "https://www.theverge.com/policy/713222/trump-woke-ai-executive-order-chatbots-llms",
    "https://briefings.brownrudnick.com/post/102kcl2/walters-v-openai-a-game-changing-verdict-reshaping-ai-defamation-and-techs-fu",
    "https://www.theverge.com/news/602577/meta-trump-facebook-lawsuit-settlement"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Meta AI",
      "weight": 0.10618628
    },
    {
      "name": "Meta",
      "weight": 0.09380067
    },
    {
      "name": "Conservative activist Robby Starbuck",
      "weight": 0.09026425
    },
    {
      "name": "Robby Starbuck",
      "weight": 0.089649186
    },
    {
      "name": "Starbuck",
      "weight": 0.08349034
    },
    {
      "name": "AI chatbot defamation",
      "weight": 0.07944324
    },
    {
      "name": "political bias",
      "weight": 0.07798632
    },
    {
      "name": "lawsuit",
      "weight": 0.06977051
    },
    {
      "name": "lawsuits",
      "weight": 0.06977051
    },
    {
      "name": "President Donald Trump",
      "weight": 0.0674692
    }
  ],
  "topics": [
    {
      "name": "AI"
    },
    {
      "name": "Artificial Intelligence"
    },
    {
      "name": "Activism"
    },
    {
      "name": "Lawsuits"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.86767578125
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.77294921875
    },
    {
      "name": "/People & Society/Social Issues & Advocacy/Other",
      "score": 0.54736328125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.54345703125
    },
    {
      "name": "/News/Politics/Other",
      "score": 0.4169921875
    },
    {
      "name": "/Online Communities/Social Networks",
      "score": 0.308349609375
    }
  ],
  "sentiment": {
    "positive": 0.072219975,
    "negative": 0.69328105,
    "neutral": 0.23449893
  },
  "summary": "Conservative activist Robby Starbuck has been appointed as an advisor at Meta to address \"ideological and political bias\" within the company's AI chatbot. This follows a settlement reached between Starbuck and the company after he filed a lawsuit claiming that Meta AI wrongly stated he was involved in the January 6th Capitol riot. Starbuck had previously campaigned against companies with diversity, equity, and inclusion programs, leading to companies like Tractor Supply, John Deere, and Harley-Davidson dropping their efforts as a result. Following President Donald Trump's executive order to make AI less woke, Starbuck will advise Meta on bias.",
  "shortSummary": "Conservative activist Robby Starbuck will serve as an advisor at Meta AI to address ideological and political bias within the platform following a lawsuit.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "cd901784ca11406ebb0277bda4f47d66",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.theverge.com/2023/6/9/23755057/openai-chatgpt-false-information-defamation-lawsuit",
      "text": "OpenAI has been hit with what appears to be the first defamation lawsuit responding to false information generated by ChatGPT.\nOpenAI sued for defamation after ChatGPT fabricates legal accusations against radio host\nChatGPT is notorious for generating false and misleading information, but this seems to be the first defamation case filed against creator OpenAI.\nA radio host in Georgia, Mark Walters, is suing the company after ChatGPT stated that Walters had been accused of defrauding and embezzling funds from a non-profit organization. The system generated the information in response to a request from a third party, a journalist named Fred Riehl. Walters\u2019 case was filed June 5th in Georgia\u2019s Superior Court of Gwinnett County and he is seeking unspecified monetary damages from OpenAI.\nThe case is notable given widespread complaints about false information generated by ChatGPT and other chatbots. These systems have no reliable way to distinguish fact from fiction, and when asked for information \u2014 particularly if asked to confirm something the questioner suggests is true \u2014 they frequently invent dates, facts, and figures.\n\u201cI heard about this new site, which I falsely assumed was, like, a super search engine.\u201d\nUsually, these fabrications do nothing more than mislead users or waste their time. But cases are beginning to emerge of such errors causing harm. These include a professor threatening to flunk his class after ChatGPT claimed his students used AI to write their essays, and a lawyer facing possible sanctions after using ChatGPT to research fake legal cases. The lawyer in question recently told a judge: \u201cI heard about this new site, which I falsely assumed was, like, a super search engine.\u201d\nOpenAI includes a small disclaimer on ChatGPT\u2019s homepage warning that the system \u201cmay occasionally generate incorrect information,\u201d but the company also presents ChatGPT as a source of reliable data, describing the system in ad copy as a way to \u201cget answers\u201d and \u201clearn something new.\u201d OpenAI\u2019s own CEO Sam Altman has said on numerous occasions that he prefers learning new information from ChatGPT than from books.\nIt\u2019s not clear, though, whether or not there is legal precedence to hold a company responsible for AI systems generating false or defamatory information, or whether this particular case has substantial merit.\nTraditionally in the US, Section 230 shields internet firms from legal liability for information produced by a third party and hosted on their platforms. It\u2019s unknown whether these protections apply to AI systems, which do not simply link to data sources but generate information anew (a process which also leads to their creation of false data).\nThe defamation lawsuit filed by Walters in Georgia could test this framework. The case states that a journalist, Fred Riehl, asked ChatGPT to summarize a real federal court case by linking to an online PDF. ChatGPT responded by created a false summary of the case that was detailed and convincing but wrong in several regards. ChatGPT\u2019s summary contained some factually correct information but also false allegations against Walters. It said Walters was believed to have misappropriated funds from a gun rights non-profit called the Second Amendment Foundation \u201cin excess of $5,000,000.\u201d Walters has never been accused of this.\nRiehl never published the false information generated by ChatGPT but checked the details with another party. It\u2019s not clear from the case filings how Walters\u2019 then found out about this misinformation.\nNotably, despite complying with Riehl\u2019s request to summarize a PDF, ChatGPT is not actually able to access such external data without the use of additional plug-ins. The system\u2019s inability to alert Riehl to this fact is an example of its capacity to mislead users. (Although, when The Verge tested the system today on the same task, it responded clearly and informatively, saying: \u201cI\u2019m sorry, but as an AI text-based model, I don\u2019t have the ability to access or open specific PDF files or other external documents.\u201d)\nEugene Volokh, a law professor who has written on the legal liability of AI systems, noted in a blog post that although he thinks \u201csuch libel claims [against AI companies] are in principle legally viable,\u201d this particular lawsuit \u201cshould be hard to maintain.\u201d Volokh notes that Walters did not notify OpenAI about these false statements, giving them a chance to remove them, and that there have been no actual damages as a result of ChatGPT\u2019s output. \u201cIn any event, though, it will be interesting to see what ultimately happens here,\u201d says Volokh.\nWe\u2019ve reached out to OpenAI for comment and will update this story if we hear back.\nMost Popular\n- RFK Jr. wants a wearable on every American \u2014 that future\u2019s not as healthy as he thinks\n- Sex is getting scrubbed from the internet, but a billionaire can sell you AI nudes\n- GitHub is no longer independent at Microsoft after CEO resignation\n- Ditching my phone for an LTE smartwatch was a humbling experience\n- Ford reveals breakthrough process for lower priced EVs"
    },
    {
      "url": "https://www.theverge.com/policy/713788/trump-ai-action-plan-explainer",
      "text": "President Donald Trump\u2019s plan to promote America\u2019s AI dominance involves discouraging \u201cwoke AI,\u201d slashing state and federal regulations, and laying the groundwork to rapidly expand AI development and adoption. Trump\u2019s proposal, released on July 23rd, is a sweeping endorsement of the technology, full of guidance that ranges from specific executive actions to directions for future research.\nSome of the new plan\u2019s provisions (like promoting open-source AI) have garnered praise from organizations that are often broadly critical of Trump, but the loudest acclaim has come from tech and business groups, whose members stand to gain from fewer restrictions on AI. \u201cThe difference between the Trump administration and Biden\u2019s is effectively night and day,\u201d says Patrick Hedger, director of policy at tech industry group NetChoice. \u201cThe Biden administration did everything it could to command and control the fledgling but critical sector \u2026 The Trump AI Action Plan, by contrast, is focused on asking where the government can help the private sector, but otherwise, get out of the way.\u201d\nOthers are far more ambivalent. Future of Life Institute, which led an Elon Musk-backed push for an AI pause in 2023, said it was heartened to see the Trump administration acknowledge serious risks, like bioweapons or cyberattacks, could be exacerbated by AI. \u201cHowever, the White House must go much further to safeguard American families, workers, and lives,\u201d says Anthony Aguirre, FLI\u2019s executive director. \u201cBy continuing to rely on voluntary safety commitments from frontier AI corporations, it leaves the United States at risk of serious accidents, massive job losses, extreme concentrations of power, and the loss of human control. We know from experience that Big Tech promises alone are simply not enough.\u201d\nFor now, here are the ways that Trump aims to promote AI.\n\u2018Consider a state\u2019s AI regulatory climate when making funding decisions\u2019\nCongress failed to pass a moratorium on states enforcing their own AI laws as part of a recent legislative package. But a version of that plan was resurrected in this document. \u201cAI is far too important to smother in bureaucracy at this early stage, whether at the state or Federal level,\u201d the plan says. \u201cThe Federal government should not allow AI-related Federal funding to be directed toward states with burdensome AI regulations that waste these funds, but should also not interfere with states\u2019 rights to pass prudent laws that are not unduly restrictive to innovation.\u201d\nTo do this, it suggests federal agencies that dole out \u201cAI-related discretionary funding\u201d should \u201climit funding if the state\u2019s AI regulatory regimes may hinder the effectiveness of that funding or award.\u201d It also suggests the Federal Communications Commission (FCC) \u201cevaluate whether state AI regulations interfere with the agency\u2019s ability to carry out its obligations and authorities under the Communications Act of 1934.\u201d\nThe Trump administration also wants the Federal Trade Commission (FTC) to take a hard look at existing AI regulations and agreements to see what it can scale back. It recommends the agency reevaluate investigations launched during the Biden administration \u201cto ensure that they do not advance theories of liability that unduly burden AI innovation,\u201d and suggests it could throw out burdensome aspects of existing FTC agreements. Some AI-related actions taken during the Biden administration that the FTC might now reconsider include banning Rite Aid\u2019s use of AI facial recognition that allegedly falsely identified shoplifters, and taking action against AI-related claims the agency previously found to be deceptive.\n\u2018Our AI systems must be free from ideological bias\u2019\nTrump\u2019s plan includes policies designed to help encode his preferred politics in the world of AI. He\u2019s ordered a revision of the Biden-era National Institute of Standards and Technology (NIST) AI Risk Management Framework \u2014 a voluntary set of best practices for designing safe AI systems \u2014 removing \u201creferences to misinformation, Diversity, Equity, and Inclusion, and climate change.\u201d (The words \u201cmisinformation\u201d and \u201cclimate change\u201d don\u2019t actually appear in the framework, though misinformation is discussed in a supplementary file.)\nIn addition to that, a new executive order bans federal agencies from procuring what Trump deems \u201cwoke AI\u201d or large language models \u201cthat sacrifice truthfulness and accuracy to ideological agendas,\u201d including things like racial equity.\nThis section of the plan \u201cseems to be motivated by a desire to control what information is available through AI tools and may propose actions that would violate the First Amendment,\u201d says Kit Walsh, director of the Electronic Frontier Foundation (EEF). \u201cThe plan seeks to require that \u2018the government only contracts with\u2019 developers who meet the administration\u2019s ideological criteria. While the government can choose to purchase only services that meet such criteria, it cannot require that developers refrain from also providing non-government users other services conveying other ideas.\u201d\n\u2018Establishing a dynamic, \u2018try-first\u2019 culture for AI\u2019\nThe administration describes the slow uptake of AI tools across the economy, including in sensitive areas like healthcare, as a \u201cbottleneck to harnessing AI\u2019s full potential.\u201d The plan describes this cautious approach as one fueled by \u201cdistrust or lack of understanding of the technology, a complex regulatory landscape, and a lack of clear governance and risk mitigation standards.\u201d To promote the use of AI, the White House encourages a \u201c\u2018try-first\u2019 culture for AI across American industry.\u201d\nThis includes creating domain-specific standards for adopting AI systems and measuring productivity increases, as well as regularly monitoring how US adoption of AI compares to international competitors. The White House also wants to integrate AI tools throughout the government itself, including by detailing staff with AI expertise at various agencies to other departments in need of that talent, training government employees on AI tools, and giving agencies ample access to AI models. The plan also specifically calls out the need to \u201caggressively adopt AI within its Armed Forces,\u201d including by introducing AI curricula at military colleges and using AI to automate some work.\n\u2018Retrain and help workers thrive\u2019\nAll this AI adoption will profoundly change the demand for human labor, the plan says, likely eliminating or fundamentally changing some jobs. The plan acknowledges that the government will need to help workers prepare for this transition period by retraining people for more in-demand roles in the new economy and providing tax benefits for certain AI training courses.\nOn top of preparing to transition workers from traditional jobs that might be upended by AI, the plan discusses the need to train workers for the additional roles that might be created by it. Among the jobs that might be needed for this new reality are \u201celectricians, advanced HVAC technicians, and a host of other high-paying occupations,\u201d the plan says.\n\u2018Create a supportive environment for open models\u2019\nThe administration says it wants to \u201ccreate a supportive environment for open models,\u201d or AI models that allow users to modify the code that underpins them. Open models have certain \u201cpros,\u201d like being more accessible to startups and independent developers.\nGroups like EFF and the Center for Democracy and Technology (CDT), which were critical of many other aspects of the plan, applauded this part. EFF\u2019s Walsh called it a \u201cpositive proposal\u201d to promote \u201cthe development of open models and making it possible for a wider range of people to participate in shaping AI research and development. If implemented well, this could lead to a greater diversity of viewpoints and values reflected in AI technologies, compared to a world where only the largest companies and agencies are able to develop AI.\u201d\nThat said, there are also serious \u201ccons\u201d to the approach that the AI Action Plan didn\u2019t seem to get into. For instance, the nature of open models makes them easier to trick and misalign for purposes like creating misinformation on a large scale, or chemical or biological weapons. It\u2019s easier to get past built-in safeguards with such models, and it\u2019s important to think critically about the tradeoffs before taking steps to drive open-source and open-weight model adoption at scale.\n\u2018Expedite environmental permitting\u2019\nTrump signed an executive order on July 23rd meant to fast track permitting for data center projects. The EO directs the commerce secretary to \u201claunch an initiative to provide financial support\u201d that could include loans, grants, and tax incentives for data centers and related infrastructure projects.\nFollowing a similar move by former President Joe Biden, Trump\u2019s plan directs agencies to identify federal lands suitable for the \u201clarge-scale development\u201d of data centers and power generation. The EO tells the Department of Defense to identify suitable sites on military installations and the Environmental Protection Agency (EPA) to identify polluted Superfund and Brownfield sites that could be reused for these projects.\nThe Trump administration is hellbent on dismantling environmental regulations, and the EO now directs the EPA to modify rules under the Clean Air Act, Clean Water Act, and Toxic Substances Control Act to expedite permitting for data center projects.\nThe EO and the AI plan, similar to a Biden-era proposal, direct agencies to create \u201ccategorical exclusions\u201d for federally supported data center projects that would exclude them from detailed environmental reviews under the National Environmental Policy Act. And they argue for using new AI tools to speed environmental assessments and applying the \u201cFast-41 process\u201d to data center projects to streamline federal permitting.\nThe Trump administration is basically using the AI arms race as an excuse to slash environmental regulations for data centers, energy infrastructure, and computer chip factories. Last week, the administration exempted coal-fired power plants and facilities that make chemicals for semiconductor manufacturing from Biden-era air pollution regulations.\nThe plan admits that AI is a big factor \u201cincreasing pressures on the [power] grid.\u201d Electricity demand is rising for the first time in more than a decade in the US, thanks in large part to data centers \u2014 a trend that could trigger blackouts and raise Americans\u2019 electricity bills. Trump\u2019s AI plan lists some much-needed fixes to stabilize the grid, including upgrading power lines and managing how much electricity consumers use when demand spikes.\nBut the administration is saying that the US needs to generate more electricity to power AI just as it\u2019s stopping renewable energy growth, which is like trying to win a race in a vehicle with no front wheels. It wants to meet growing demand with fossil fuels and nuclear energy. \u201cWe will continue to reject radical climate dogma,\u201d the plan says. It argues for keeping existing, mostly fossil-fueled power plants online for longer and limiting environmental reviews to get data centers and new power plants online faster.\nThe lower cost of gas generation has been killing coal power plants for years, but now a shortage of gas turbines could stymie Trump\u2019s plans. New nuclear technologies that tech companies are investing in for their data centers probably won\u2019t be ready for commercial deployment until the 2030s at the earliest. Republicans, meanwhile, have passed legislation to hobble the solar and wind industries that have been the fastest-growing sources of new electricity in the US.\n\u2018Prioritize fundamental advancements in AI interpretability\u2019\nThe Trump administration accurately notes that while developers and engineers know how today\u2019s advanced AI models work in a big-picture way, they \u201coften cannot explain why a model produced a specific output. This can make it hard to predict the behavior of any specific AI system.\u201d It\u2019s aiming to fix that, at least when it comes to some high-stakes use cases.\nThe plan states that the lack of AI explainability and predictability can lead to issues in defense, national security, and \u201cother applications where lives are at stake,\u201d and it aims to promote \u201cfundamental breakthroughs on these research problems.\u201d The plan\u2019s recommended policy actions include launching a tech development program led by the Defense Advanced Research Projects Agency to advance AI interpretability, control systems, and security. It also said the government should prioritize fundamental advancements in such areas in its upcoming National AI R&D Strategic Plan and, perhaps most specifically, that the DOD and other agencies should coordinate an AI hackathon to allow academics to test AI systems for transparency, effectiveness, and vulnerabilities.\nIt\u2019s true that explainability and unpredictability are big issues with advanced AI. Elon Musk\u2019s xAI, which recently scored a large-scale contract with the DOD, recently struggled to stop its Grok chatbot from spouting pro-Hitler takes \u2014 so what happens in a higher-stakes situation? But the government seems unwilling to slow down while this problem is addressed. The plan states that since \u201cAI has the potential to transform both the warfighting and back-office operations of the DOD,\u201d the US \u201cmust aggressively adopt AI within its Armed Forces if it is to maintain its global military preeminence.\u201d\nThe plan also discusses how to better evaluate AI models for performance and reliability, like publishing guidelines for federal agencies to conduct their own AI system evaluations for compliance and other reasons. That\u2019s something most industry leaders and activists support greatly, but it\u2019s clear what the Trump administration has in mind will lack a lot of the elements they have been pushing for.\nEvaluations likely will focus on efficiency and operations, according to the plan, and not instances of racism, sexism, bias, and downstream harms.\n\u2018Give the courts and law enforcement the tools they need\u2019\nCourtrooms and AI tools mix in strange ways, from lawyers using hallucinated legal citations to an AI-generated appearance of a deceased victim. The plan says that \u201cAI-generated media\u201d like fake evidence \u201cmay present novel challenges to the legal system,\u201d and it briefly recommends the Department of Justice and other agencies issue guidance on how to evaluate and deal with deepfakes in federal evidence rules.\n\u2018Improving the financial market for compute\u2019\nFinally, the plan recommends creating new ways for the research and academic community to access AI models and compute. The way the industry works right now, many companies, and even academic institutions, can\u2019t access or pay for the amount of compute they need on their own, and they often have to partner with hyperscalers \u2014 providers of large-scale cloud computing infrastructure, like Amazon, Google, and Microsoft \u2014 to access it.\nThe plan wants to fix that issue, saying that the US \u201chas solved this problem before with other goods through financial markets, such as spot and forward markets for commodities.\u201d It recommends collaborating with the private sector, as well as government departments and the National Science Foundation\u2019s National AI Research Resource pilot to \u201caccelerate the maturation of a healthy financial market for compute.\u201d It didn\u2019t offer any specifics or additional plans for that.\nMost Popular\n- RFK Jr. wants a wearable on every American \u2014 that future\u2019s not as healthy as he thinks\n- Sex is getting scrubbed from the internet, but a billionaire can sell you AI nudes\n- Ditching my phone for an LTE smartwatch was a humbling experience\n- GitHub is no longer independent at Microsoft after CEO resignation\n- Ford reveals breakthrough process for lower priced EVs"
    },
    {
      "url": "https://www.theverge.com/news/602577/meta-trump-facebook-lawsuit-settlement",
      "text": "Meta agreed to a $25 million settlement over a 2021 lawsuit President Donald Trump brought against Meta for suspending his accounts after the January 6th insurrection at the US Capitol. The Wall Street Journal was the first to report the news, and Meta spokesperson Andy Stone confirmed the settlement to The Verge.\nIt\u2019s a step that Trump discussed with Meta CEO Mark Zuckerberg during his recent visit to Mar-a-Lago, The Verge has independently confirmed. One unnamed source told The Journal that Trump indicated the lawsuit would need to be resolved before Zuckerberg would have a chance of being \u201cbrought into the tent.\u201d\nThe White House and an advisor to Trump did not immediately provide comment.\nThe settlement, which would contribute $22 million toward Trump\u2019s presidential library funds as well as legal fees, is the latest signal of Trump\u2019s powerful influence over corporate America. Trump\u2019s odds of success in the case did not look particularly promising, given that a judge dismissed a similar suit filed against Twitter (now X) and another against Google was administratively closed. The docket has been stagnant since 2023. But now back in the White House, Zuckerberg and many of his tech and business peers have recognized the immense influence Trump could wield over their companies and have taken a much more proactive role in engaging with his administration compared to last time.\nTrump filed a class action lawsuit against Meta in 2021, seeking damages for himself and other users whose accounts were allegedly \u201cwrongly restricted or curtailed.\u201d Facebook had announced an indefinite suspension on Trump\u2019s accounts after his posts during the January 6th insurrection at the US Capitol that year. At the time, Zuckerberg said, \u201cThe shocking events of the last 24 hours clearly demonstrate that President Donald Trump intends to use his remaining time in office to undermine the peaceful and lawful transition of power to his elected successor, Joe Biden.\u201d Eventually, the company dropped restrictions on his accounts.\nABC News similarly settled a defamation lawsuit Trump brought over anchor George Stephanopoulos\u2019 mischaracterization of the charge Trump was found liable for in the case brought by writer E. Jean Carroll. And CBS owner Paramount has also discussed settling a Trump lawsuit over the news outlet\u2019s interview with his then-opponent Kamala Harris, understanding that his administration could make it difficult to close a merger with Skydance Media, according to the Journal.\nAlex Heath contributed reporting.\nMost Popular\n- RFK Jr. wants a wearable on every American \u2014 that future\u2019s not as healthy as he thinks\n- Sex is getting scrubbed from the internet, but a billionaire can sell you AI nudes\n- GitHub is no longer independent at Microsoft after CEO resignation\n- Ditching my phone for an LTE smartwatch was a humbling experience\n- Ford reveals breakthrough process for lower priced EVs"
    },
    {
      "url": "https://www.cnn.com/2024/08/19/business/harley-davidson-dei-john-deere-tractor-supply",
      "text": "Tractor Supply Co. John Deere. Now Harley-Davidson.\nHarley-Davidson said Monday that it\u2019s ending diversity and other progressive initiatives at the company. Harley-Davidson is the latest major American brand to backtrack from DEI policies it had supported in recent years.\nHarley-Davidson faced pressure online from Robby Starbuck, a conservative activist who has successfully taken on DEI policies at several American companies.\n\u201cWe are saddened by the negativity on social media over the last few weeks, designed to divide the Harley-Davidson community,\u201d the company wrote in a statement posted on X.\nThe company added that \u201cwe have not operated a DEI function since April 2024, and we do not have a DEI function today. We do not have hiring quotas and we no longer have supplier diversity spend goals.\u201d\nBut the company said it would review all sponsorships and outside organizations the company affiliates with, and the company will establish a central clearinghouse for approvals of those relationships. It also suggested it would drop some sponsorships, including LGBTQ+ Pride festivals, saying the brand going forward would focus exclusively on growing the sport of motorcycling. Harley-Davidson, based in Milwaukee, had previously been a longtime corporate member of the Wisconsin LBGT Chamber of Commerce.\nThe company also said it would end its relationship with the Human Rights Campaign, a leading LGBTQ+ advocacy group.\n\u201cWe remain committed to listening to all members of our community,\u201d the company said in the statement.\nStarbuck first posted on social media about the company less than a month ago.\n\u201cIt\u2019s time to expose Harley Davidson,\u201d Starbuck first posted on July 23, listing around 20 examples of how the company has \u201cgone totally woke.\u201d Among them: Harley-Davidson sponsored a bootcamp for LGBTQ entrepreneurs, donated to United Way and wants to increase its workforce diversity as it tries to grow its base of motorcycle riders.\nElon Musk and other right-wing leaders amplified Starbuck\u2019s social media posts.\nHarley-Davidson declined to comment to CNN.\nHarley-Davidson joins Tractor Supply and John Deere to backtrack on policies following pressure campaigns led by Starbuck.\nTractor Supply recently announced it was eliminating jobs and goals focused on diversity, equity and inclusion; withdrawing its carbon emission reduction goals; and ending sponsorships for LGBTQ+ Pride festivals and voting campaigns. John Deere announced it will no longer sponsor \u201csocial or cultural awareness\u201d events and would audit all its training materials.\nOn X Monday, Starbuck called it a \u201cwin for our movement\u201d and hinted that he would target another company."
    },
    {
      "url": "https://briefings.brownrudnick.com/post/102kcl2/walters-v-openai-a-game-changing-verdict-reshaping-ai-defamation-and-techs-fu",
      "text": "In a decision that could reshape the legal and technological landscape, the Superior Court of Gwinnett County, Georgia, issued a ruling on May 19, 2025, in Walters v. OpenAI, L.L.C. (Case No. 23-A-04860-2), granting summary judgment in favor of OpenAI, the developer of the artificial intelligence chatbot ChatGPT.\nThis case, pitting Mark Walters\u2014a nationally recognized radio host and Second Amendment advocate\u2014against the AI pioneer, hinges on a false statement generated by ChatGPT that alleged Walters was involved in embezzlement. The ruling not only dismisses Walters\u2019 defamation claim but also sets a precedent that may influence how AI developers, legal professionals and technologists navigate the intersection of emerging technology and traditional tort law. This article provides an in-depth summary, a comprehensive analysis, and forward-looking insights into the ruling\u2019s implications, offering recommendations for stakeholders in this evolving domain.\nCase Summary: The Genesis of the Dispute\nThe saga began on May 3, 2023, when Frederick Riehl, a journalist and editor of AmmoLand.com\u2014a news and advocacy site focused on Second Amendment rights\u2014and a member of the board of directors of the Second Amendment Foundation (SAF), interacted with ChatGPT. Riehl was researching a lawsuit, SAF v. Ferguson, filed by SAF against the attorney general of Washington, which alleged unconstitutional harassment of the organization due to its political stance on gun rights. Seeking a summary of the publicly available complaint, Riehl pasted text into ChatGPT and later provided a URL link to the document. Initially, ChatGPT accurately summarized the input text. However, upon receiving the URL, the chatbot\u2014despite stating it could not access the internet\u2014produced a fabricated summary, alleging that the lawsuit involved embezzlement by an SAF treasurer and chief financial officer, later identifying this individual as Mark Walters.\nWalters, a radio host of two nationally syndicated programs with an estimated 1.2 million listeners per 15-minute segment, is a vocal advocate for gun rights, authoring books and serving as a media spokesperson for SAF. Despite his public profile, he contended in his lawsuit that he was not a public figure, citing limited media appearances (e.g., one Fox Business interview and one local NBC segment). The false ChatGPT output, which Riehl recognized as inaccurate within 90 minutes and did not republish, prompted Walters to sue OpenAI for defamation, alleging harm to his reputation. However, Walters later conceded he suffered no damages. OpenAI moved for summary judgment under Georgia law (O.C.G.A. \u00a7 9-11-56), arguing that the statement lacked defamatory meaning, Walters could not prove fault, and no damages were recoverable. The court agreed on all counts, delivering a decisive victory for OpenAI.\nDetailed Legal Analysis: The Court\u2019s Rationale\nThe court\u2019s 22-page order, penned by Judge Tracie Cason, articulates three independent bases for granting summary judgment, each reflecting a meticulous application of defamation law and First Amendment principles. Let\u2019s unpack each ground with depth and nuance.\n1. Absence of Defamatory Meaning\nUnder Georgia law, a statement is defamatory only if it can be reasonably understood by a hypothetical reader as conveying \u201cactual facts\u201d about the plaintiff (Bollea v. World Championship Wrestling, Inc., 271 Ga. App. 555). The court emphasized that context, including disclaimers and the reader\u2019s experience, is critical in this determination. ChatGPT\u2019s output was accompanied by multiple warnings: it could not access the internet, its knowledge cutoff was September 2021 (predating the lawsuit), and its Terms of Use cautioned users about potential inaccuracies or \u201challucinations\u201d\u2014a term for AI-generated fabrications. Riehl, an experienced user aware of past fictional outputs, encountered these disclaimers and quickly verified the falsehood, testifying that the chatbot \u201ccompletely fantasized\u201d the response.\nThe court relied on precedents like Farah v. Esquire Mag. (736 F.3d 528), which holds that a reasonable reader, after reflection, would not construe such output as factual given the \u201cprominent indicia\u201d of unreliability. Riehl\u2019s access to the actual complaint and SAF press release further undermined any defamatory interpretation. This analysis suggests that AI tools, when transparently labeled as probabilistic, may evade liability if users are expected to exercise due diligence\u2014a novel extension of traditional publisher liability standards.\n2. Failure to Establish Fault\nDefamation plaintiffs must prove fault, with the threshold varying by the plaintiff\u2019s status. For private individuals, Georgia requires at least ordinary negligence (Am. C.L. Union, Inc. v. Zeh, 312 Ga. 647), while public figures must demonstrate \u201cactual malice\u201d\u2014knowledge of falsity or reckless disregard thereof (New York Times Co. v. Sullivan, 376 U.S. 254). The court classified Walters as a public figure, citing his extensive media presence, large audience and voluntary role in Second Amendment debates. This designation, rooted in Gertz v. Robert Welch, Inc. (418 U.S. 323), reflects his \u201cespecial prominence\u201d and access to counter false narratives via his radio platform.\n- Negligence Standard: Walters failed to identify a standard of care or evidence that OpenAI breached it. OpenAI\u2019s expert, Dr. White, testified\u2014unrebutted\u2014that the company leads the industry in reducing hallucinations through data training and human feedback, supplemented by robust user warnings. The court rejected Walters\u2019 argument that deploying a fallible AI constitutes negligence, likening it to strict liability, which Gertz and Georgia law prohibit.\n- Actual Malice Standard: Walters offered no evidence that OpenAI knew the specific output was false or acted with reckless disregard. Dr. White\u2019s testimony highlighted OpenAI\u2019s efforts to mitigate errors, and the court found that general awareness of AI limitations does not meet the \u201cclear and convincing\u201d malice threshold (Jones v. Albany Herald Publ\u2019g Co., 290 Ga. App. 126).\nThis dual failure underscores a high bar for AI defamation claims, particularly for public figures, and suggests that proactive error mitigation and disclosure may shield developers.\n3. Inability to Recover Damages\nDamages are a cornerstone of defamation claims, and Walters\u2019 case collapsed here. He admitted no actual economic or reputational harm, and alternative damage theories were foreclosed:\n- Punitive Damages: Georgia law (O.C.G.A. \u00a7 51-5-2) requires a retraction request before seeking punitive damages, which Walters did not pursue. His argument that a retraction was impractical was deemed irrelevant under Mathis v. Cannon (276 Ga. 16).\n- Presumed Damages: Typically available for defamation per se (e.g., imputing a crime), presumed damages were rebutted by Walters\u2019 own testimony and Riehl\u2019s disbelief in the output. Moreover, Gertz and Zeh prohibit presumed or punitive damages for matters of public concern without actual malice, which Walters could not prove. The court classified the SAF lawsuit as a public issue, distinguishing it from private disputes like the credit report in Dun & Bradstreet, Inc. v. Greenmoss Builders, Inc. (472 U.S. 749).\nThis damages analysis reinforces First Amendment protections for speech on public issues, even when generated by AI, and limits recovery to provable harm.\nInsights: A Crossroads of Law and Technology\nThis ruling is a watershed moment, blending the rigidity of defamation law with the fluidity of AI innovation. Several insights emerge:\n- AI as a Non-Publisher: The court treats ChatGPT as a tool rather than a traditional publisher, shifting liability to users like Riehl to verify outputs. This echoes historical cases where intermediaries (e.g., bookstores) were not liable for content (Smith v. California, 361 U.S. 147), but extends it to autonomous systems\u2014a legal evolution worth watching.\n- Public Figure Doctrine: The expansive definition of Walters as a public figure, based on niche influence rather than mainstream fame, may broaden this category, affecting activists and commentators. This could insulate AI developers from claims by similar plaintiffs but leave private individuals more vulnerable.\n- Disclaimer Reliance: The ruling\u2019s emphasis on disclaimers suggests a future where legal protection hinges on user education, potentially standardizing such notices across AI platforms. However, this raises ethical questions about whether users can reasonably be expected to navigate AI\u2019s complexities.\nThe decision also highlights a tension between innovation and accountability. By excusing OpenAI\u2019s errors, it may accelerate AI deployment, but it risks normalizing inaccuracies, challenging the reliability expected of informational tools.\nBroader Implications and Future Impacts\nThe Walters ruling could reverberate across legal, technological and societal spheres:\n- Legal Precedent: As a state court decision, its authority is limited, but it may influence federal courts or other jurisdictions grappling with AI defamation. Expect test cases involving private plaintiffs or less transparent AI systems to refine this precedent. The ruling\u2019s alignment with First Amendment principles may also embolden tech firms to resist liability expansions.\n- Technological Development: AI developers might prioritize transparency (e.g., real-time error flags) and invest in hallucination reduction, though the ruling suggests current efforts suffice legally. This could spur a race to integrate human oversight or hybrid AI-human systems, balancing efficiency with accuracy.\n- Public Trust and Media: If AI errors are legally excusable, trust in tools like ChatGPT for factual reporting may erode, boosting demand for human-verified content. Journalists and publishers could face increased pressure to fact-check AI outputs, reviving traditional editorial roles in a digital age.\n- Regulatory Landscape: The decision may prompt lawmakers to consider AI-specific regulations, such as mandatory accuracy standards or liability thresholds, especially as public concern over misinformation grows. The European Union\u2019s AI Act, with its risk-based approach, offers a potential model, though U.S. free speech norms may resist similar measures.\nLong-term, this ruling could shape a bifurcated AI ecosystem: one for entertainment or creative use (where errors are tolerated) and another for factual domains (demanding higher reliability). The balance between fostering innovation and protecting individuals will be a key battleground.\nRecommendations for Stakeholders\nFor Lawyers\n- Potential Plaintiffs: Counsel should meticulously document damages, even intangible ones like emotional distress, to meet the actual harm requirement. Challenging the efficacy of AI disclaimers\u2014arguing they are buried or unclear\u2014could open new legal avenues, especially for private plaintiffs. Staying versed in First Amendment case law and state tort variations will be essential as AI cases proliferate.\n- Potential Defendants: For tech clients, emphasize proactive measures\u2014detailed user warnings, error logs, and mitigation strategies\u2014to bolster negligence defenses. Preemptive legal audits of AI systems could identify vulnerabilities, while crafting Terms of Use to mirror OpenAI\u2019s successful model may limit exposure. Collaboration with technologists to understand AI behavior will enhance credibility with courts.\n- Emerging Strategies: Explore class actions if AI errors affect multiple individuals, or seek injunctive relief to compel corrections, bypassing damages hurdles. Monitoring legislative trends (e.g., AI liability bills) will allow proactive adaptation.\nFor Technologists\n- Technical Enhancements: Invest in advanced training datasets and real-time validation (e.g., cross-referencing with trusted sources) to minimize hallucinations. Implementing user-facing indicators\u2014color-coded accuracy scores or confidence levels\u2014could empower users and reduce legal risks.\n- User Interface Design: Design intuitive interfaces that highlight data limitations (e.g., knowledge cutoffs, access restrictions) and encourage verification. Gamifying fact-checking or offering tutorial prompts could educate users without overwhelming them.\n- Legal-Tech Collaboration: Partner with legal experts to refine Terms of Use and compliance frameworks, ensuring they withstand judicial scrutiny. Engaging ethicists to address AI\u2019s societal impact could preempt regulatory backlash and build public trust.\n- Industry Standards: Advocate for voluntary AI accuracy benchmarks, potentially through consortia like the Partnership on AI, to self-regulate and avoid draconian laws. Open-sourcing error mitigation techniques could foster collective progress.\nFor Policymakers and Educators\n- Regulatory Guidance: Develop balanced AI guidelines that protect innovation while addressing misinformation, perhaps through sandbox testing of new models. Public awareness campaigns could educate users on AI\u2019s limitations, reducing unrealistic expectations.\n- Educational Initiatives: Integrate AI literacy into curricula, teaching critical evaluation of digital content. Workshops for journalists and advocates could bridge the gap between technology and traditional fact-finding.\nConclusion: A New Frontier Beckons\nThe Walters v. OpenAI ruling is a clarion call for the legal and tech communities to adapt to an AI-driven world. By shielding OpenAI from liability, it champions innovation but underscores the need for vigilance. For lawyers, it\u2019s a reminder to evolve strategies beyond analog precedents; for technologists, a challenge to engineer trustworthiness into AI systems. As society navigates this frontier, the balance between free expression, technological progress, and individual rights will define the next chapter of digital governance. This case, while a victory for OpenAI, is merely the opening salvo in a broader dialogue\u2014one that demands collaboration, foresight and a commitment to ensuring AI serves humanity\u2019s best interests."
    },
    {
      "url": "https://www.theverge.com/policy/713222/trump-woke-ai-executive-order-chatbots-llms",
      "text": "After delivering a rambling celebration of tariffs and a routine about women\u2019s sports, President Donald Trump entertained a crowd, which was there to hear about his new AI Action Plan, with one his favorite topics: \u201cwokeness.\u201d Trump complained that AI companies under former President Joe Biden \u201chad to hire all woke people,\u201d adding that it is \u201cso uncool to be woke.\u201d And AI models themselves had been \u201cinfused with partisan bias,\u201d he said, including the hated specter of \u201ccritical race theory.\u201d Fortunately for the audience, Trump had a solution: he signed an executive order titled \u201cPreventing Woke AI in the Federal Government,\u201d directing government agencies \u201cnot to procure models that sacrifice truthfulness and accuracy to ideological agendas.\u201d\nTo anyone with a cursory knowledge of politics and the tech industry, the real situation here is obvious: the Trump administration is using government funds to pressure AI companies into parroting Trumpian talking points \u2014 probably not just in specialized government products, but in chatbots that companies and ordinary people use.\nTrump\u2019s order asserts that agencies must only procure large language models (LLMs) that are \u201ctruthful in responding to user prompts seeking factual information or analysis,\u201d \u201cprioritize historical accuracy, scientific inquiry, and objectivity,\u201d and are \u201cneutral, nonpartisan tools that do not manipulate responses in favor of ideological dogmas such as DEI.\u201d DEI, of course, is diversity, equity, and inclusion, which Trump defines in this context as:\nThe suppression or distortion of factual information about race or sex; manipulation of racial or sexual representation in model outputs; incorporation of concepts like critical race theory, transgenderism, unconscious bias, intersectionality, and systemic racism; and discrimination on the basis of race or sex.\n(In reality, DEI was typically used to refer to civil rights, social justice, and diversity programs before being co-opted as a Trump and MAGA bogeyman.)\nThe Office of Management and Budget has been directed to order further guidance within 120 days.\nWhile we\u2019re still waiting on some of the precise details about what the order means, one issue seems unavoidable: it will plausibly affect not only government services, but the entire field of major LLMs. While it insists that \u201cthe Federal Government should be hesitant to regulate the functionality of AI models in the private marketplace,\u201d the reality is that nearly every big US consumer LLM maker has (or desperately wants) government contracts, including with products like Anthropic\u2019s Claude Gov and OpenAI\u2019s ChatGPT Gov \u2014 but there\u2019s not a hard wall between development of government, business, and consumer models. OpenAI touts how many agencies use its enterprise service; Trump\u2019s AI Action Plan encourages adoption of AI systems in public-facing arenas like education, and the boundaries between government-funded and consumer-focused products will likely become even more porous soon.\nTrump\u2019s idea of \u201cDEI\u201d is expansive. His war against it has led national parks to remove signage highlighting indigenous people and women and the Pentagon to rename a ship commemorating gay rights pioneer Harvey Milk, among many other changes. Even LLMs whose creators have explicitly aimed for what they consider a neutral pursuit of truth would likely produce something Trump could find objectionable unless they tailor their services.\nThere\u2019s not a hard wall between AI for government and everything else\nIt\u2019s possible that companies will devote resources to some kind of specifically \u201cnon-woke\u201d government version of their tools, assuming the administration agrees to treat these as separate models from the rest of the Llama, Claude, or GPT lineup \u2014 it could be as simple as adding some blunt behind-the-scenes prompts redirecting it on certain topics. But refining models in a way that consistently and predictably aligns them in certain directions can be an expensive and time-consuming process, especially with a broad and ever-shifting concept like Trump\u2019s version of \u201cDEI,\u201d especially because the language suggests simply walling off certain areas of discussion is also unacceptable. There are significant sums at stake: OpenAI and xAI each recently received $200 million defense contracts, and the new AI plan will create even more opportunities. The Trump administration isn\u2019t terribly detail-oriented, either \u2014 if some X user posts about Anthropic\u2019s consumer chatbot validating trans people, do we really think Pam Bondi or Pete Hegseth will distinguish between \u201cClaude\u201d and \u201cClaude Gov\u201d?\nThe incentives overwhelmingly favor companies changing their overall LLM alignment priorities to mollify the Trump administration. That brings us to our second problem: this is exactly the kind of blatant, ideologically motivated social engineering that Trump claims he\u2019s trying to stop.\nThe executive order is theoretically about making sure AI systems produce \u201caccurate\u201d and \u201cobjective\u201d information. But as Humane Intelligence cofounder and CEO Rumman Chowdhury noted to The Washington Post, AI that is \u201cfree of ideological bias\u201d is \u201cimpossible to do in practice,\u201d and Trump\u2019s cherry-picked examples are tellingly politically lopsided. The order condemns a quickly fixed 2024 screwup, in which Google added an overenthusiastic pro-diversity filter to Gemini \u2014 causing it to produce race- and gender-diverse visions of Vikings, the Founding Fathers, the pope, and Nazi soldiers \u2014 while unsurprisingly ignoring the long-documented anti-diversity biases in AI that Google was aiming to balance.\nIt\u2019s not simply interested in facts, either. Another example is an AI system saying \u201ca user should not \u2018misgender\u2019 another person even if necessary to stop a nuclear apocalypse,\u201d answering what is fundamentally a question of ethics and opinion. This condemnation doesn\u2019t extend to incidents like xAI\u2019s Grok questioning the Holocaust.\nLLMs produce incontrovertibly incorrect information with clear potential for real-world harm; they can falsely identify innocent people as criminals, misidentify poisonous mushrooms, and reinforce paranoid delusions. This order has nothing to do with any of that. Its incentives, again, reflect what the Trump administration has done through \u201cDEI\u201d investigations of universities and corporations. It\u2019s pushing private institutions to avoid acknowledging the existence of transgender people, race and gender inequality, and other topics Trump disdains.\nAI systems have long been trained on datasets that reflect larger cultural biases and under- or overrepresent specific demographic groups, and contrary to Trump\u2019s assertions, the results often aren\u2019t \u201cwoke.\u201d In 2023, Bloomberg described the output of image generator Stable Diffusion as a world where \u201cwomen are rarely doctors, lawyers, or judges,\u201d and \u201cmen with dark skin commit crimes, while women with dark skin flip burgers.\u201d Companies that value avoiding ugly stereotypes or want to appeal to a wider range of users often need to actively intervene to shape their tech, and Trump just made doing that harder.\nAttacking \u201cthe incorporation of concepts\u201d that promote \u201cDEI\u201d effectively tells companies to rewrite whole areas of knowledge that acknowledge racism or other injustices. The order claims it\u2019s only worried if developers \u201cintentionally encode partisan or ideological judgments into an LLM\u2019s outputs\u201d and says LLMs can deliver those judgments if they \u201care prompted by or otherwise readily accessible to the end user.\u201d But no Big Tech CEO should be rube enough to buy that \u2014 we have a president who spent years accusing Google of intentionally rigging its search results because he couldn\u2019t find enough positive news stories about himself.\nTrump is determined to control culture; his administration has gone after news outlets for platforming his enemies, universities for fields of study, and Disney for promoting diverse media. The tech industry sees AI as the future of culture \u2014 and the Trump administration wants its politics built in on the ground floor.\nMost Popular\n- RFK Jr. wants a wearable on every American \u2014 that future\u2019s not as healthy as he thinks\n- Sex is getting scrubbed from the internet, but a billionaire can sell you AI nudes\n- GitHub is no longer independent at Microsoft after CEO resignation\n- Ditching my phone for an LTE smartwatch was a humbling experience\n- Ford reveals breakthrough process for lower priced EVs"
    }
  ],
  "argos_summary": "Conservative activist Robby Starbuck has been appointed as an advisor at Meta to address perceived ideological bias in the company's AI chatbot, following a lawsuit he filed over false claims linking him to the January 6th Capitol riot. Meanwhile, a Georgia court ruled in favor of OpenAI in a defamation lawsuit filed by radio host Mark Walters, who claimed ChatGPT falsely accused him of embezzlement, setting a precedent that may influence future AI-related legal cases. Additionally, President Trump's new AI Action Plan aims to eliminate 'woke' influences in AI development, potentially reshaping the landscape of AI technology and its regulatory environment.",
  "argos_id": "7WDXZLU5X"
}