{
  "url": "https://www.zdnet.com/article/gnomes-new-ai-assistant-can-even-run-linux-commands-for-you-heres-how/",
  "authorsByline": "Jack Wallen",
  "articleId": "c69e3b67cb1d4bca850d4993bace5d1a",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/8ac352efaa4b0069b735c6513ae09523bbe3f22a/2025/08/05/61fb80d6-8d09-419f-81e8-c55949ed515e/gettyimages-2195307970.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-05T15:29:57+00:00",
  "addDate": "2025-08-05T15:38:28.442632+00:00",
  "refreshDate": "2025-08-05T15:38:28.442633+00:00",
  "score": 1.0,
  "title": "GNOME's new AI assistant can even run Linux commands for you - here's how",
  "description": "If your Linux desktop environment of choice is GNOME, there's a new AI-based assistant that you need to try.",
  "content": "\u2022 Newelle is an AI assistant for the GNOME desktop.\n\u2022 It's capable of standard chats and even running commands.\n\u2022 However, Newelle does require Flatseal to run commands on Linux.\n\nThere's a new AI assistant available for the GNOME desktop, and it just reached version 1.0 status. That new AI assistant is called Newelle, and it's already proven to be a worthy contender for your desktop.\n\nNewelle isn't just another large language model manager, but a full-blown assistant that can run Linux commands from human-readable descriptions (more on that in a bit), serve as a traditional AI chatbot, and more. Newelle uses Bai Chat as its backend and allows you to download and select from different LLMs (some of which will require an API key).\n\nAlso: The top 5 GNOME extensions I install first (and what they can do for you)\n\nThis app can easily serve as a missing link to add something similar to what Gemini is to Android (although it does take a bit more work to get it there).\n\nI've been using Newelle for a few days now and have found it to be quite a handy app. In fact, it's replaced Ollama/Msty as my go-to GUI for local LLMs. Not only is Newelle as easy to use as Ollama/Msty, it better fits the GNOME aesthetic, is faster, and doesn't take nearly the system resources.\n\nThe only caveat I've found with Newelle is that getting certain commands to run properly can be a challenge. There are specific steps you must take to allow commands to run, but once you've taken care of those configurations, you can use Newelle to run commands on your Linux system.\n\nLet's install Newelle and then configure it to run commands.\n\nWhat you'll need: Newelle is installed on Linux via Flatpak, so you'll need a distribution with that universal package manager installed and working.\n\nWhen the command finishes, you can close the terminal window with the exit command.\n\nAlso: How I feed my files to a local AI for better, more relevant responses\n\nBefore you configure Newelle to run commands, you'll need to select which large language model (LLM) to use. Some LLMs are easy to add, while others require an API key to function properly. Let's make this easy and set up a local LLM. Here's how.\n\nOpen the Newelle app from your desktop menu.\n\nFrom the Newelle main window, click the three horizontal line menu button near the top left and select Settings.\n\nFrom the LLM listing, click the downward-pointing arrow to download an LLM and then, once it's downloaded, select it by clicking the associated radio button. You can download as many LLMs as you want, but obviously, you can only use one at a time. Once you've taken care of this, you can close the Settings window.\n\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\n\nThis is where it gets a bit tricky. You have to change a few specific bits to enable Newelle to run commands for you.\n\nGo back to the Settings window and click on the General tab.\n\nBecause Newelle is installed within a sandboxed environment, you have to give it permission to access your file system. To do that, you must install Flatseal with the command:\n\nOpen Flatseal and click on Newelle in the left sidebar. Scroll down until you see the Filesystem section. In that section, enable \"All user files.\"\n\nScroll down to the System Bus section and click the + button associated with Talks. In the new field, add:\n\nYou can now close Flatseal.\n\nOnce you've taken care of the above, you can then run commands within Newelle. For instance, in the chat field, type:\n\nNewelle will create the folder, and it's ready to use.\n\nAlso: How I made Perplexity AI the default search engine in my browser (and why you should too)\n\nAnd that's the gist of Newelle. Using it as a traditional AI chatbot is straightforward and simple enough that anyone can use it. Sure, getting it to run commands takes a few extra steps, but if you're using Linux, you should be OK with that process.\n\nGet the morning's top stories in your inbox each day with our Tech Today newsletter.",
  "medium": "Article",
  "links": [
    "https://www.zdnet.com/article/my-go-to-llm-tool-just-dropped-a-super-simple-mac-and-pc-app-for-local-ai-why-you-should-try-it",
    "https://www.zdnet.com/article/this-app-makes-using-ollama-local-ai-on-macos-devices-so-easy",
    "https://github.com/qwersyk/Newelle",
    "https://www.zdnet.com/newsletters/",
    "https://www.zdnet.com/article/how-i-feed-my-files-to-a-local-ai-for-better-more-relevant-responses/",
    "https://www.zdnet.com/article/5-gnome-extensions-that-make-my-desktop-environment-more-useful-and-enjoyable/",
    "https://www.zdnet.com/article/6-linux-myths-busted/",
    "https://www.zdnet.com/article/how-i-made-perplexity-ai-the-default-search-engine-in-my-browser-and-why-you-should-too/",
    "https://www.zdnet.com/article/i-tried-sanctums-local-ai-app-and-its-exactly-what-i-needed-to-keep-my-data-private/",
    "https://www.zdnet.com/article/the-open-source-tools-that-could-disrupt-the-entire-it-incident-management-market/",
    "https://www.zdnet.com/article/your-old-laptop-can-shine-on-after-windows-10s-sunset-with-this-linux-distro/",
    "https://www.zdnet.com/article/5-lightweight-linux-distributions-that-will-bring-your-old-pc-back-to-life/"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Newelle",
      "weight": 0.094993696
    },
    {
      "name": "Linux commands",
      "weight": 0.08903541
    },
    {
      "name": "certain commands",
      "weight": 0.07646582
    },
    {
      "name": "commands",
      "weight": 0.075002775
    },
    {
      "name": "local LLMs",
      "weight": 0.06330248
    },
    {
      "name": "GNOMEs new AI assistant",
      "weight": 0.05861294
    },
    {
      "name": "the Newelle main window",
      "weight": 0.057876438
    },
    {
      "name": "Linux",
      "weight": 0.057382934
    },
    {
      "name": "GNOME",
      "weight": 0.054301832
    },
    {
      "name": "the Newelle app",
      "weight": 0.053436227
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.69970703125
    },
    {
      "name": "/Reference/General Reference/How-To, DIY & Expert Content",
      "score": 0.552734375
    },
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.541015625
    },
    {
      "name": "/Computers & Electronics/Software/Operating Systems",
      "score": 0.50927734375
    },
    {
      "name": "/Reference/Technical Reference",
      "score": 0.427490234375
    },
    {
      "name": "/Computers & Electronics/Programming/Development Tools",
      "score": 0.369384765625
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.353759765625
    }
  ],
  "sentiment": {
    "positive": 0.28051758,
    "negative": 0.17456055,
    "neutral": 0.5449219
  },
  "summary": "GNP's new AI assistant, Newelle, is available for the GNOME desktop and can run Linux commands from human-readable descriptions and is capable of standard chats and even running commands. However, it requires Flatseal to run commands on Linux. Newelle uses Bai Chat as its backend and can download and select from different language model managers, some which require an API key. The app can be used as a replacement for Ollama/Msty for local LLMs and is easy to use, faster, and doesn't take nearly the system resources. However there are specific steps to allow commands to run, which can be difficult but simple to configure Newelle to run. Once you have installed Newelle and configure it, you can run commands within Newelle.",
  "shortSummary": "Newelle, an AI assistant for the GNOME desktop, can run Linux commands using Flatseal, but requires specific configuration to run certain commands.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": true,
  "reprintGroupId": "6e3e12e2230746488620cea1375e1727",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/how-i-feed-my-files-to-a-local-ai-for-better-more-relevant-responses/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nHow I feed my files to a local AI for better, more relevant responses\nFor a few weeks, I've been using Msty for research purposes. One of the main reasons I chose this route is that I like the idea of keeping those interactions isolated to my local machine. Thanks to Msty and Ollama, that's a fairly easy task.\nOne Msty/Ollama feature that has intrigued me is the ability to add your own content to what's called a Knowledge Stack, which can enable you to integrate local data sources to enhance the AI's ability to provide more relevant and contextual responses. Once you've added a document stack, Msty can then serve as a smart assistant with knowledge of whatever it is you deem worth knowing. Once you've created a Knowledge Stack, Msty indexes it with a separate model to be used for a chat session.\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nThink about it this way: You've written several documents about a particular topic and you want to use them to fuel your AI chats. With Knowledge Stacks, this is not only possible -- it's easy.\nYou can add different items to a Knowledge Stack, such as:\n- Files (PDF, CSV, MD, JSON, EPUB, DOCX, RTF, and TXT)\n- Folders\n- Obsidian vaults (from the Obsidian note-taking app)\n- Notes\n- YouTube transcripts\nThe process is much simpler than you might think and the results are impressive.\nLet me show you how it works.\nHow to create your first Knowledge Stack\nWhat you'll need: To make this work, you'll need both Ollama and Msty installed. You'll also needs some supported files to add (see the list above).\n1. Create your Knowledge Stack\nThe first thing to do is open Msty and then click the Knowledge Stack button in the sidebar. On the first page of the pop-up, click Use Local AI Model and then, in the resulting window, click Add Your First Knowledge Stack.\n2. Name your Knowledge Stack\nIn the next pop-up, give your Knowledge Stack a name, leave the model drop-down as is, and click Add.\n3. Add your documents\nIn the resulting window, you can either drag and drop documents or click Browse Files and locate the files in your default file manager. You can also add Obsidian Vaults, Folders, Notes, and/or YouTube transcripts. Once you've done that, click Save as Draft and then click Compose.\nDepending on how many documents you've added to your Knowledge Stack, the compose process can take some time. Wait until it completes. Once the compose process is finished, close the Knowledge Stack Pop-up.\nHow to add your Knowledge Stack to a chat\nWith your Knowledge Stack added, it's time to start chatting. It's actually very simple. Click the New button (or hit Ctrl+N) to start a new chat. In the chat window, click the Knowledge Stacks button above the chat window (not the one in the sidebar).\nFrom the pop-up menu, select your new Knowledge Stack to add it to the chat. Once you've done that, start chatting. Ask a question that would be answered in one of your documents, and you should get the expected answer.\nAlso: How I made Perplexity AI the default search engine in my browser (and why you should too)\nI've run a few tests with Msty Knowledge Stacks, and every time I came away impressed. This feature is a great way to use your own documents/data for an AI chat, while ensuring it remains on your local machine and isn't saved or used by a third party. It's also pretty cool to see how many of your documents are cited in a chat.\nThis should get you started with Msty Knowledge Stacks. Once you're up to speed with how they work, I would suggest you poke around some of the more advanced features (like Similarity and Chunks)."
    },
    {
      "url": "https://www.zdnet.com/article/the-open-source-tools-that-could-disrupt-the-entire-it-incident-management-market/",
      "text": "The open-source tools that could disrupt the entire IT incident management market\nThere are a handful of leading commercial toolmakers to help IT detect and respond to system outages and application failures, commonly referred to as \"incident management and response,\" including companies such as PagerDuty, as well as various \"observability\" companies like Datadog and Dynatrace.\nAlso: 7 solid reasons to consider AIOps\nHowever, the market is finally opening up to open-source software approaches, according to a report released last week by JP Morgan's software analysts. The open-source offerings, riding a wave of \"AIOps\" and other new industry approaches, have a serious shot at giving PagerDuty and the others a run for their money.\nThe rise of open-source alternatives to PagerDuty\n\"There has also been a lot of progress made in the open-source world,\" wrote JP Morgan software analyst Pinjalim Bora.\nBora cites as examples the open-source startup Raintank of New York City, which does business as Grafana Labs. The company has introduced \"an on-call solution as an open-source project, which is free to use for self-managed and on-premise deployment.\" The company also sells cloud-based managed services that are not open-source.\nJP Morgan participated in a $240 million round of funding for Grafana in 2022. The company has raised a total of $840 million from venture capitalists, including Coatue Management and Lightspeed Management, according to FactSet.\nAlso: Grafana 7.0 promises to connect, unify, and visualize all your data\nBora notes that the rise of open source is just one component of an explosion in incident response tools vendors in recent years. The market has gone from around 70 such offerings in early 2022, both open and closed source, to a hundred or more now, \"with the number of vendors serving the enterprise doubling in that time period from 15 to 30.\"\nAI is automating a lot of IT's problem-solving\nThe AIOps category, which has long been debated as a viable category by IBM and others, is getting a shot in the arm from generative AI investments. A report last month by venture capitalists at Menlo Ventures noted that IT operations currently make up the largest single category of enterprise spending on Gen AI, at 22%.\nAlso: Enterprises are struggling with what to do with Gen AI\nBora casts the matter of open and closed source in a brighter light: AI is going to automate a lot of problem-solving that is currently IT's job.\nThe use of Gen AI-based coding assistants, such as Microsoft's GitHub Copilot, he believes, will change the cycle of code writing, checking, and remediation.\n\"The increasing use of AI code assistants in building of applications likely will have some impacts in this space as well,\" wrote Bora. \"While on one hand it will likely drive up workload growth, it could also lower mean-time-to-resolution.\n\"For instance, we think as more machines write code, it could create patterns that are easier to find and remediate vs. human-written code, potentially reducing the number of critical P1 events [Priority 1, high-priority incidents for IT], and thus likely somewhat diluting the value proposition of a premium on-call scheduling tool.\""
    },
    {
      "url": "https://www.zdnet.com/article/how-i-made-perplexity-ai-the-default-search-engine-in-my-browser-and-why-you-should-too/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nHow I made Perplexity AI the default search engine in my browser (and why you should too)\nSome time ago, I mentioned in an article how I believed AI could replace a traditional search engine, and the reasons behind that were focused on the lack of advertisements, improved results, and better information.\nTo that claim, I'll add this: I only use AI for specific types of searches, mostly research, where I find Google and DuckDuckGo to be less than helpful. For everything else, I'll go with a traditional search tool.\nRecently, I discovered I could set Perplexity as my default search engine in my web browser, and it's been a game-changer. I'm not going to say the results are 100% helpful and 100% accurate, but the same thing could be said of Google, Bing, DuckDuckGo, and every search engine on the market. In fact, I take this one step further. With the Arc browser, I can set a specific search engine for different profiles. To that end, I have Perplexity set as the search engine for my writing profile and DuckDuckGo set for my user profile. This way, when I'm in my writing or work Workspaces (which are associated with my writing profile), I get the power of AI for searches, and when I'm in any other Workspace, I get the simplicity of a traditional search engine.\nAlso: Zen Browser is the customizable Firefox I've been waiting for - here's why\nBut with Perplexity, you get more than just a search engine. You get news summaries (\"What's in the news today?\"), a deep research tool that allows for follow-up questions, and you don't have to worry about ads or paid sites bubbling to the top of your search queries. For example, in my current book, which is a sci-fi series about rehousing the population on the moon, I have to find out what can cause a \"dust storm\" of regolith on the moon. With Perplexity, I get my answer, but then I can follow up by asking questions regarding the answer, while still retaining the original answer.\nFor me, this combination of AI and traditional search has done wonders for my ability to get the information I need quickly, and as long as I always vet any suspect AI answer, it's all good.\nLet me show you how I added Perplexity to both Chrome and Firefox.\nHow to add Perplexity as a search engine for Chrome\nWhat you'll need: The only thing you'll need for this is a running instance of the Chrome (or Chromium) web browser.\n1. Open Chrome Settings\nThe first thing to do is open Chrome, then click on the menu button (three-dot button near the upper right corner), and click Settings.\n2. Manage your search engine\nOn the resulting page, click Search Engine in the left navigation and then click \"Manage search engines and site search.\"\n3. Add Perplexity\nIn the resulting \"Site Search\" section, click Add to open the \"Add search site\" pop-up. In this pop-up, add the following information:\n- Name: Perplexity\n- Shortcut: perp\n- URL: https://www.perplexity.ai/?q=%s\nWhen finished, click Add search engine.\n4. Set Perplexity as the default\nThis is optional, as you can always search with Perplexity by opening a new Chrome tab, typing \"perp\" in the address bar, hitting your Tab key, and then typing your search string. If you want to make Perplexity your default search engine, go back to the Search engine section of Chrome Settings, scroll down to Site search, click the three-dot menu button for Perplexity, and click Make default.\nBingo, you're done.\nAlso: I tried Perplexity's assistant, and only one thing stops it from being my default phone AI\nHow to make Perplexity your default search engine in Firefox\nWith Firefox, we'll add a third-party add-on.\n1. Install the add-on\nOpen Firefox and point it to the Search Engines Helper extension from within the Firefox Add-ons store. If you'd like, you can also check out the source for the extension on GitHub. Click Add to Firefox and then, when prompted, click Add.\n2. Configure Perplexity as a search engine\nClick the Search Engines Helper extension on the Firefox toolbar and click Add New Search Engine. In the resulting page, type https://www.perplexity.ai/?q=%s and everything else will auto-populate.\n3. Add the search engine\nRight-click within the Firefox address bar and select Add \"perplexity.ai.\" Now, if you go to Settings > Search, you can select Perplexity as your default search engine.\nAnd that, my friends, is all there is to adding Perplexity as your default search engine in Chrome and Firefox."
    },
    {
      "url": "https://www.zdnet.com/article/this-app-makes-using-ollama-local-ai-on-macos-devices-so-easy",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThis app makes using Ollama local AI on MacOS devices so easy\nI've turned to locally installed AI for research because I don't want third parties using my information to either build a profile or train their local language models (LLMs).\nMy local AI of choice is the open-source Ollama. I recently wrote a piece on how to make using this local LLM easier with the help of a browser extension, which I use on Linux. But on MacOS, I turn to an easy-to-use, free app called Msty.\nAlso: How to turn Ollama from a terminal tool into a browser-based AI with this free extension\nMsty allows you to use locally installed and online AI models. However, I default to the locally installed option. And, unlike the other options for Ollama, there's no container to deploy, no terminal to use, and no need to open another browser tab.\nMsty features things like split chats (so you can run more than one query at a time), regenerate model response, clone chats, add multiple models, real-time data summoning (which only works with certain models), create Knowledge Stacks (where you can add files, folders, Obsidian vaults, notes, and more to be used to train your local model), a prompt library, and more.\nMsty is one of the best tools for interacting with Ollama. Here's how to use it.\nInstalling Msty\nWhat you'll need: The only things you'll need for this are a MacOS device, and Ollama installed and running. If you haven't installed Ollama, do that first (here's how). You'll also need to pull down one of the local models (which is demonstrated in the article above).\n1. Download the installer\nHead to the Msty website, click the Download Msty dropdown, select Mac, and then select either Apple Silicon or Intel.\n2. Install Msty\nWhen the installation is complete, double-click on the file and, when prompted, drag the Msty icon to the Applications folder.\nUsing Msty\n1. Open Msty\nNext, open Launchpad and locate the launcher for Msty. Click the launcher to open the app.\n2. Connect your local Ollama model\nWhen you first run Msty, click Setup Local AI and it will download the necessary components. Once the download completes, it will take care of the configuration and download a local model other than Ollama.\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nTo connect Msty to Ollama, click Local AI Models in the sidebar and then click the download button associated with Llama 3.2. Once downloaded, you can select it from the models dropdown. You can also add other models, for which you'll need to retrieve an API key from your account for that particular model. Msty should now be connected to the local Ollama LLM.\nAt this point, you can type your first query and wait for the response.\n3. Model Instructions\nOne of the cool features of Msty is that it allows you to change the model instructions.\nFor example, you might want to use the local LLM as an AI-assisted doctor, for writing, accounting, as an alien anthropologist, or as an artistic advisor.\nTo change the model instructions, click Edit Model Instructions in the center of the app and then click the tiny chat button to the left of the broom icon.\nAlso: The best AI for coding in 2025 (and what not to use)\nFrom the popup menu, you can select the instructions you want to apply. Click \"Apply to this chat\" before running your first query.\nThere are many other things Msty can do, but this guide will get you up and running quickly. I would suggest starting with the basics and, as you get used to the app, venture into more complicated processes."
    },
    {
      "url": "https://www.zdnet.com/article/6-linux-myths-busted/",
      "text": "6 Linux myths, busted\nI've been covering Linux and open-source since 1999 (the year of Prince). During that long stretch, a year hasn't gone by that I haven't experienced someone (or a group of someones) still believing the myths that seem to be forever churning about the open-source operating system.\nAlso: The 3 most Windows-like Linux distros you can try because change is hard\nIt would be impossible to calculate the hours I've spent trying to help people understand that the vast majority of those myths simply are not true. Hopefully every time I do, someone's eyes are opened, and they see what they've been misunderstanding.\n1. Linux requires the use of the terminal\nWhen I first started using Linux in 1997, the terminal was an absolute necessity. There was no way around it. I had to write scripts to keep a modem connected, install software from source, manage users and permissions, and just about everything from the terminal. Sure, there were GUI apps for some of those tasks, but those graphical tools weren't up to snuff.\nThat was then, and this is now.\nAlso: I'm a Linux power user, and I recommend this distro to newbies and experts alike\nI've tested this myth. I've made myself refrain from using the Command Line Interface (CLI) for weeks at a time and never once felt it was something I had to use. How is that possible? The desktop environments and various settings apps have come a very long way over the years, to the point that they've become equally as effective as the terminal.\nEven with servers, there are now web-based tools like Cockpit that make it possible to do much of what used to be required of the command line. That doesn't mean you can just as easily skip the terminal when using Linux on a server, though. In that case, yeah, you had better learn CLI. But on the desktop? No.\nDo yourself a favor and go a week without opening the terminal on desktop Linux and see how it goes. You'll be surprised that tasks every average computer user requires can be managed without having to echo >> \"Using the terminal.\"\n2. Linux users don't want Microsoft-like desktops\nI, for one, opt to go a very non-Microsoft route for my desktop environment. But the average computer user who's spent years (or decades) with Windows and is looking to migrate to an alternative? They need familiarity.\nAlso: This Linux distro is so Windows-like, it even comes with Microsoft apps\nThere's a reason why the old adage \"people fear change\" is bandied about\u2026 because, for the most part, it's true. Most people who'd like to migrate away from Microsoft aren't doing so because they hate the interface. The majority of users don't care what the interface looks like, only that it works and is easy to learn.\nThat's the very reason why Linux distributions that look like Windows (and MacOS) exist. Developers know that some users simply aren't going to adopt Linux if what they see is totally foreign to them. The beauty of those distributions is that they lure users to Linux and then allow them to grow from the standard Windows interface. But the important idea is to woo users over with something familiar that's also better.\n3. Linux is totally virus-free\nIt used to be that viruses didn't stand a chance against Linux. For the most part, that's still true. However, it doesn't mean that Linux is 100% safe from malicious software, URLs, and code. The way I see it is that if your computer is connected to a network, it's vulnerable. But instead of having to constantly worry about these threats, just make sure you're installing software from reputable repositories, and don't click on random links (especially from within email).\nAlso: Ready to ditch Windows? 5 factors to help you decide between Linux or MacOS\nI've used Linux for decades and, on the desktop, have never once suffered a virus, malware, or ransomware. I have, on one occasion, dealt with a nasty rootkit on a Linux server, but that's a different story altogether.\n4. Linux doesn't have the software you need\nConsider this -- there are multiple routes to installing software, including:\n- Standard repositories\n- Third-party repositories\n- Source\n- Snap\n- Flatpak\n- AppImage\n- Wine\n- Steam\nAll told, there is a wealth of apps available for Linux.\nAlso: Leaving Windows 10 for Linux? 5 security differences to consider first\nBut here's the thing: the majority of users spend their time in a web browser (6.36 hours a day on average, according to Statista). If we're talking about a typical workday, that doesn't leave much time for other applications.\nIf you need to use MS Office, Office365 works great in most browsers. Do you use Spotify for music? It's there. What about Slack? Yup. Microsoft Teams? You know it.\nThe truth is Linux does not lack software. If you do run into a Windows app that doesn't have a Linux version, you can always try running it with Wine (which is a compatibility layer that allows for the installation of Windows software on Linux).\n5. Linux is hard\nWhen I first started using Linux, it was quite challenging. But over the past two decades, desktop Linux has become so easy anyone can use it.\nAlso: I converted this Mini PC from Windows to Linux, and it came alive. Here's how\nMy father was not terribly adept with computers. When he purchased his first computer, it had Windows 7, and he had nothing but problems. I installed Ubuntu Linux over Windows, and it was smooth sailing. He was able to use it with ease. Part of the reason why he picked it up so quickly was because he was never told it was hard. He went into it assuming it would be easier than what he had, and that assumption proved to be true.\nI've watched Linux evolve since the early days, and what we have today is exponentially easier than it was back then.\n6. Linux doesn't support many peripherals\nI have peripherals that will not work on MacOS. I have peripherals Windows cannot detect unless I track down the proper driver. For example, my Brother laser printer gives MacOS fits, and Windows simply cannot see it until I install a driver, which takes me a while to track down.\nAlso: System76 just took everything that was good in Pop!_OS and made it even better\nLinux, on the other hand, sees it immediately and works without installing any sort of driver. At the same time, I've had scanners that I couldn't get work, no matter what I did.\nThe thing about peripherals is that they are all hit-or-miss. Printers are a nightmare. Back when I was doing PC support, I probably spent 80% of my time solving printer problems, and in every single instance, the problem was caused by Windows. Most peripherals were designed with Windows in mind because Microsoft's desktop OS is the most widely used on the planet. Even so, Windows can struggle to work with some peripherals.\nFrom my perspective, this is a crap shoot, regardless of what OS you're on. That being said, since 1997, I've only had a handful of peripherals that I could not get to work with Linux, and that is a pretty good track record."
    },
    {
      "url": "https://www.zdnet.com/article/5-gnome-extensions-that-make-my-desktop-environment-more-useful-and-enjoyable/",
      "text": "The top 6 GNOME extensions I install first (and what they can do for you)\nI've been using the GNOME desktop environment since its beta. Sure, there have been short periods when I've migrated away (such as when I moved to elementary OS or Bodhi Linux), but there's no place like GNOME. This open-source desktop environment is the sweet spot between minimalism and over-the-top functionality.\nBut that doesn't mean GNOME is perfect. Out of the box, it can use a bit of help. That's why the developers created extensions. These tiny applications (and there are many) bring useful features, behaviors, and even visuals to GNOME.\nAlso: How to change DNS servers on a GNOME-based Linux distribution\nUpon installing GNOME, one of the first things I do is head over to the extensions home page and install a handful of additional features. Here are my top six.\n1. Dash to Panel\nAlthough I appreciate the default GNOME layout, I do prefer a panel. Given that I'm at my desktop all day, I like to minimize mouse or key clicks to help make my experience as efficient as possible. For that reason, Dash To Panel is the first extension I install on GNOME.\nDash To Panel moves the GNOME Dash from within the Overview to the main desktop window, effectively turning it into a traditional panel. Installing this extension means you don't have to worry about getting a third-party dock.\nAlso: The best-looking Linux desktop I've seen so far in 2025 - and it's not even close\nTo add more launchers to the panel, open the app in question, right-click its icon, and select Add to Favorites. In the Dash to Panel settings (which can be accessed by right-clicking on the panel), you can change its position, style, behavior, action, and more. This extension should be considered a must-have for every GNOME user.\n2. Bluetooth Quick Connect\nIf you connect Bluetooth devices to your desktop or laptop, do yourself a favor and add the Bluetooth Quick Connect extension. By default, when adding or managing Bluetooth connections, you must go through the Settings app, which isn't exactly efficient.\nWith this extension, you get a Bluetooth menu in the system tray menu to help you easily manage your Bluetooth connections. You also get battery status indicators for all applicable devices. This extension is so valuable that it's hard to believe it's not part of the stock GNOME experience.\n3. Just Perfection\nThis extension has more features than you'll ever need. Essentially, Just Perfection is your one-stop shop for tweaking GNOME Shell, allowing you to change its behavior and even disable elements you don't want or need.\nAlso: This Linux distro can be used without installation (and it's totally free)\nDon't like having a top bar? Disable it. Prefer faster or slower animations? Just Perfection has you covered.\nYou can select from the Default, Minimal, or Super Minimal profiles, or create a custom profile that allows you to do things like enable/disable the panel, Activities button, App Menu, Clock Menu, System Menu, Weather, Calendar, Events, Search, various icons, Workspace Wraparound, Workspace Peek, Type to Search, and more.\nYou can also customize the Overview spacing size, panel size, panel position, animation speeds, and more. This extension is a tinkerer's dream come true.\n4. Gnome Clipboard\nI rely on my clipboard throughout the day. Unfortunately, the default GNOME clipboard works like any other and saves the last copied string -- and that's it.\nWith GNOME Clipboard, you get many more options. This extension retains a history of copied items you can select from and then paste with the usual Ctrl-V shortcut. You can also search through your saved items, define how many items to retain (the default is 100), change the sort history, and delete any entries you don't want to save.\nAlso: How to use Linux without ever touching the terminal\nOne thing this extension doesn't offer is the ability to ignore certain apps (such as password managers). Other than that issue, this extension is another one I can't go without.\n5. Blur My Shell\nIf you're like me and prefer a desktop with panache, you'll want Blur My Shell. With Blur My Shell, you can add a blurred look to various aspects of the GNOME desktop, such as the top panel, dash, and overview.\nAlthough I use Pop!_OS (which doesn't use a stock overview), I still use Blur My Shell for the top panel blur. Out of the box, the GNOME top panel always feels in the way or doesn't quite fit my aesthetic preference. With Blur My Shell, the top panel blends in with the wallpaper for a smoother transition. No, this extension won't make your day any easier, but it will make it more pleasing to the eye\u2026 and that still counts for something.\n6. Removable Drive Menu\nI regularly insert removable drives and don't always want to go through the file manager to unmount them. With the Removable Drive Menu extension, I can click an icon in the top bar and then click the unmount icon for the drive I want to remove. With this extension, you can also quickly access the removable media you need to work with. Click the Removable Drive Menu icon in the top bar and select the removable drive you want to access. Your default file manager will open to that location.\nAlso: I've used Linux for 30 years. Here are 5 reasons why I'll never switch to Windows or MacOS\nIf you want a more efficient way of interacting with your removable drives, this is the extension to use.\nThere are plenty of extensions for you to choose from on the GNOME Extensions site. Just remember to use the Firefox web browser to avoid any installation issues.\nIf this is your first time diving into the world of GNOME extensions, enjoy the ride."
    },
    {
      "url": "https://github.com/qwersyk/Newelle",
      "text": "newelle.mp4\n- \ud83c\udfa8 Advanced Customization: Tailor the application with a wide range of settings\n- \ud83d\ude80 Flexible Model Support: Choose from mutliple AI models and providers to fit your specific needs\n- \ud83d\udcbb Terminal Command Exection: Execute commands suggested by the AI on the fly\n- \ud83e\udde9 Extensions: Add your own functionalities and models to Newelle\n- \ud83d\udde3 Voice support: Chat hands free with Newelle, supporting many Speech To Text and TTS models, with translation options\n- \ud83e\udde0 Long Term Memory: Remember conversations from previous chats\n- \ud83d\udcbc Chat with documents: Chat with your own documents\n- \ud83d\udd0e Web Search: Provide reliable answers using Web Search\n- \ud83c\udf10 Website Reading: Scrap informations from websites by appending the prefix #https://.. in the prompt\n- \ud83d\udc64 Profile Manager: Create settings profiles and switch between them\n- \ud83d\udcc1 Builtin File Manager: Manage you files with the help of AI\n- \ud83d\udcdd Rich Formatting: Supports both Markdown and LaTeX\n- \u270f\ufe0f Chat editing: Edit or remove any message and manage your prompts easily\nNewelle supports extensions to extend its functionality. You can either use existing extensions or create your own to add new features to the application.\nA lightweight version of Newelle that can be triggered via keyboard shortcuts.\nAs an example, to set the mini window launch's hotkey to Ctrl+Space, execute this command:\n/bin/bash -c 'flatpak run --command=gsettings io.github.qwersyk.Newelle set io.github.qwersyk.Newelle startup-mode \"mini\" && flatpak run io.github.qwersyk.Newelle'\nAfter that, enable the hotkey in settings.\nFor GNOME desktop environment users, you may need to enable automatic window centering:\ngsettings set org.gnome.mutter center-new-windows true\ninstall.sh\n- Install the latest Gnome SDK\n- Run\nsh install.sh\n- Profit!\n- Gnome Builder\n- Install GNOME Builder on your system.\n- Clone the Newelle repository from GitHub.\n- Open the project in GNOME Builder and compile it.\n- Once compiled, you can run the program from the compiled executable.\nWith nix, you can run the app without installing by executing this command:\nnix run github:qwersyk/Newelle\nIf you want the latest commit version, you can clone this repository and execute nix run .\nto start the program or nix develop .\nto start a developer shell\nWarning\nLocalizations will not work on these builds! If you want your language to work, go to the \"Builder\" section instead\n- Download the latest release from the Github Actions\n- Extract the downloaded package.\n- Install a flatpak package.\n- Ensure you have set up both flatpak and flathub\n- Install Newelle by executing:\nflatpak install flathub io.github.qwersyk.Newelle\nImportant\nThe Flathub version of Newelle is restricted to the .var/app/io.github.qwersyk.Newelle\nfolder and operates within a\nFlatpak virtualized environment, limiting its capabilities.\nTo extend Newelle's permissions, either execute this command to temporarily grant its access:\nflatpak run --talk-name=org.freedesktop.Flatpak --filesystem=home io.github.qwersyk.Newelle\nor adjust the permissions permanently using Flatseal:\n- Open Flatseal, find \"newelle\" and enable both \"All user files\" and \"Session Bus\"\n- Add\norg.freedesktop.Flatpak\nto run outside the sandbox.\n- Add\nWarning\nBe cautious when enabling these options. They reduce security by exposing your data and terminal. Avoid sharing personal information, and understand that we can't guarantee the privacy of your chat data or prevent potential risks from proprietary models.\nNewelle Lite - Your Virtual Assistant for aarch64\nNyarch Assistant - Your ultimate Waifu AI Assistant"
    },
    {
      "url": "https://www.zdnet.com/article/i-tried-sanctums-local-ai-app-and-its-exactly-what-i-needed-to-keep-my-data-private/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nI tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nLocally installed AI is the way to go, especially if privacy is important to you. Instead of sending your queries to a third party, you can keep them private, so no one else has access to your questions or the generated answers. When you run a query with the locally installed Sanctum, your data is encrypted, secure, and never leaves the app.\nAlso: How I made Perplexity AI the default search engine in my browser (and why you should too)\nI've been using locally installed AI for a while now (mostly Ollama with the addition of the Msty front-end) and have found it to be quite useful.\nBut why Sanctum?\n- It's local\n- Data remains private\n- Thousands of GGUF models on Hugging Face\n- PDF summaries\n- Work even in an internet outage\n- It's open-source\n- You can choose your LLM (from Gemma, Llama, Mistral, and more)\n- You can choose if any information is shared\n- Available prompt templates\n- Real-time information on system resources in use\nSanctum could easily become instrumental for research on any given subject, especially when you don't want your queries to go beyond your local machine. No matter what you're researching, Sanctum can help.\nAlso: How to install Perplexity AI's app on Linux (I found an easier way)\nLet me walk you through the process of getting Sanctum up and running. It's quite easy.\nHow to install Sanctum\nWhat you'll need: The only things you'll need are either a MacOS or Windows computer (Linux version coming soon) and a network connection. I'll demonstrate the installation on MacOS. If you're using a Windows computer, the installation is as simple as installing any other application.\n1. Download the installer\nThe first thing to be done is downloading the installer. For that, head to the Sanctum site and click the Download drop-down. Select your OS and the download will start.\n2. Install the app\nOnce the download has finished, locate and double-click the file in Finder. A new window will pop up, asking you to drag the Sanctum icon to Applications. Do that, and the installation is done. You can then eject the Sanctum drive on your desktop and delete the download.\nHow to set up Sanctum\nIt's now time to configure Sanctum.\n1. Save your recovery phrase\nAs soon as you start Sanctum, click the Get Started button on the main window. In the resulting window, you'll need to copy and paste your recovery phrase. This is important because if you lose your login credentials, you'll need it for recovery.\n2. Set a password\nIn the next window, you are required to create a password. Make sure this password is strong and unique. When you've done that, click Continue.\n3. Download an AI model\nYou can now decide which LLM to download and use. You can select others later, but you'll want to select one here. After making your selection (I opted for Mistral), click Continue.\n4. Select your privacy level\nYou can now select the level of privacy you want for Sanctum. I selected \"Don't share a single byte,\" and I suggest you make the same choice. Once you've done that, click Continue.\nAt this point, you can start using Sanctum as a locally installed AI tool to assist you with all (or some) of your research."
    },
    {
      "url": "https://www.zdnet.com/article/5-lightweight-linux-distributions-that-will-bring-your-old-pc-back-to-life/",
      "text": "5 lightweight Linux distributions that will bring your old PC back to life\nDo you have an old PC lying around? Maybe your Windows 10 PC won't support Windows 11, and you're not interested in purchasing a new machine. What do you do?\nYou can turn to a lightweight Linux distribution. This type of operating system typically has very low system requirements and can make older machines feel as if they're brand new.\nAlso: 10 Linux apps I install on every new machine (and why you should, too)\nAlthough there are some distributions that claim to be lightweight, I'm limiting my choices to those whose system requirements are equal to or less than a 1GHz CPU, equal to or less than 2GB of RAM, and a minimum of 10GB storage. I've also only chosen those distributions that don't skimp on the features in favor of speed. But which distribution should you choose? Let me help you with that.\n1. Linux Lite\nLinux Lite is probably the best of all lightweight Linux distributions. Based on Ubuntu LTS and designed with user-friendliness in mind, Linux Lite uses the Xfce desktop to create a UI that's clean and simple but doesn't miss out on features.\nAlso: This lightweight Linux distro is the best (and easiest) way to revive your old computer. Here's how\nOnce installed, you'll find Linux Lite includes the apps you need to get started, such as Chrome, LibreOffice, GIMP, Photo Manager, Deja Dup Backups, VLC media player, a USB image writer, an onscreen keyboard, Timeshift (for restore points), and more. If it doesn't include the app you need, there's always the Synaptic Package Manager GUI to help you find what you're looking for.\nThe system requirements for Linux Lite are:\n- CPU: 1GHz or faster\n- 1GB of RAM\n- 8GB of drive space\nLinux Lite can be downloaded and installed for free.\n2. AntiX\nAntiX is another lightweight distribution, but I wouldn't recommend it to those new to Linux. AntiX is geared more toward those with at least a bit of Linux experience. The reason for that is AntiX opts for the zzzFM-IceWM window manager. Even though the developers have configured the window manager to look fairly familiar, if you go to make any changes to the look and feel, you'll find it's not quite as simple as KDE Plasma or GNOME.\nAlso: Why this lightweight Linux distro won't win any popularity contests but is perfect for power users\nAntiX ships with plenty of pre-installed apps, such as Firefox, LibreOffice, Firejail, plenty of media players, and more. More than anything, though, AntiX is crazy fast, which means an aging computer will feel like you just purchased it off the shelf.\nThe system requirements for AntiX are:\n- CPU: Intel Core 2 Duo or AMD Athlon 64 X2\n- 1GB or more RAM\n- 10GB or more drive space\n- Graphics: Any graphics card with 3D acceleration\n3. Bodhi Linux\nNot gonna lie -- I do love me some Bodhi Linux. The Moksha Desktop is a fork of Enlightenment, and it looks gorgeous. Of all the lightweight distributions, Bodhi is, by far, the most unique. But don't think that uniqueness makes Bodhi Linux challenging to use. You'd be surprised at how easy this distribution is.\nAlso: Bodhi Linux can make an old computer feel brand new\nOne of my favorite features of Bodhi Linux is the right-click mouse menu. Right-click anywhere on the desktop, and a menu will appear where you can launch applications and much more. The right-click menu is the same as the desktop menu, so it's very efficient. Although Bodhi Linux doesn't include much in the way of eye candy, you'll find plenty of beautiful themes to choose from.\nThe Bodhi Linux system requirements are:\n- CPU: 1.0GHz or faster\n- 768MB of RAM\n- 10GB of drive space\n4. BunsenLabs\nBunsenLabs Linux will surprise you with how it is functional and user-friendly. On top of that, it's beautiful. This distribution is a continuation of the beloved CrunchBang Linux and uses a Debian base, the OpenBox window manager, the tint2 panel, Conky, and jgmenu. All of those bits come together to form a lightweight Linux distribution that not only performs incredibly well but looks very modern.\nAlso: This Linux distribution surprised me with its minimalistic yet highly-functional operating system\nOn top of all this, it shouldn't take you any time to get up to speed with how the distribution works. Even better, BunsenLabs Linux includes a desktop mouse menu that is similar to the one found in Bodhi Linux.\nThe system requirements for BunsenLabs Linux are:\n- CPU: 1GHz or faster\n- 2GB of RAM\n- 20GB of drive space\n5. Lubuntu\nLubuntu is another distribution based on Ubuntu LTS. This take, however, uses the LXQt desktop, which is one of the most full-featured of all the lightweight desktop environments. LXQt is very fast and easy to use. Anyone who's worked with Windows of any iteration will find LXQt familiar. Lubuntu is an official spin of Ubuntu and retains things like Snap package support, a stable kernel, a similar set of pre-installed software, and access to the standard Ubuntu repositories.\nAnd although the Lubuntu desktop may give you early-2000s vibes, that doesn't mean it's too old-school for use. Lubuntu even includes KDE Plasma's Discover app store, so you won't have any problem installing all of the apps you need. The best thing about Lubuntu, however, is the speed. This lightweight Linux distribution will blow you away with how fast it runs, even on older hardware.\nSpeaking of which, the system requirements for Lubuntu are:\n- CPU: 1GHz or faster\n- 1GB of RAM\n- 5GB of drive space\nAlso: Lubuntu is Linux for those looking to simplify and speed up their desktop experience\nAny one of the above Linux distributions will give your aging PC new life -- so, if you're looking at the end of Windows 10 and your computer won't support Windows 11, instead of throwing that machine away, install one of these distributions and keep that computer around for a few more years."
    },
    {
      "url": "https://www.zdnet.com/article/my-go-to-llm-tool-just-dropped-a-super-simple-mac-and-pc-app-for-local-ai-why-you-should-try-it",
      "text": "My go-to LLM tool just dropped a super simple Mac and PC app for local AI - why you should try it\nZDNET's key takeaways\n- Ollama AI devs have released a native GUI for MacOS and Windows.\n- The new GUI greatly simplifies using AI locally.\n- The app is easy to install, and allows you to pull different LLMs.\nIf you use AI, there are several reasons why you would want to work with it locally instead of from the cloud.\nFirst, it offers much more privacy. When using a Large Language Model (LLM) in the cloud, you never know if your queries or results are being tracked or even saved by a third party. Also, using an LLM locally saves energy. The amount of energy required to use a cloud-based LLM is growing and could be a problem in the future.\nErgo, locally hosted LLMs.\nAlso: How to run DeepSeek AI locally to protect your privacy \u2013 2 easy ways\nOllama is a tool that allows you to run different LLMs. I've been using it for some time and have found it to simplify the process of downloading and using various models. Although it does require serious system resources (you wouldn't want to use it on an aging machine), it does run fast, and allows you to use different models.\nBut Ollama by itself has been a command-line-only affair. There are some third-party GUIs (such as Msty, which has been my go-to). Until now, the developers behind Ollama hadn't produced their own GUI.\nThat all changed recently, and there's now a straightforward, user-friendly GUI, aptly named Ollama.\nWorks with common LLMs - but you can pull others\nThe GUI is fairly basic, but it's designed so that anyone can jump in right away and start using it. There is also a short list of LLMs that can easily be pulled from the LLM drop-down list. Those models are fairly common (such as the Gemma, DeepSeek, and Qwen models). Select one of those models, and the Ollama GUI will pull it for you.\nIf you want to use a model not listed, you would have to pull it from the command line like so:\nollama pull MODEL\nWhere MODEL is the name of the model you want.\nAlso: How I feed my files to a local AI for better, more relevant responses\nYou can find a full list of available models in the Ollama Library.\nAfter you've pulled a model, it appears in the drop-down to the right of the query bar.\nThe Ollama app is as easy to use as any cloud-based AI interface on the market, and it's free to use for MacOS and Windows (sadly, there's no Linux version of the GUI).\nI've kicked the tires of the Ollama app and found that, although it doesn't have quite the feature set of Msty, it's easier to use and fits in better with the MacOS aesthetic. The Ollama app also seems to be a bit faster than Msty (in both opening and responding to queries), which is a good thing because local AI can often be a bit slow (due to a lack of system resources).\nHow to install the Ollama app on Mac or Windows\nYou're in luck, as installing the Ollama app is as easy as installing any app on either MacOS or Windows. You simply point your browser to the Ollama download page, download the app for your OS, double-click the downloaded file, and follow the directions. For example, on MacOS, you drag the Ollama app icon into the Applications folder, and you're done.\nUsing Ollama is equally easy: select the model you want, let it download, then query away.\nShould you try the Ollama app?\nIf you've been looking for a reason to try local AI, now is the perfect time.\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nThe Ollama app makes migrating away from cloud-based AI as easy as it can get. The app is free to install and use, as are the LLMs in the Ollama library. Give this a chance, and see if it doesn't become your go-to AI tool.\nWant more stories about AI? Check out AI Leaderboard, our weekly newsletter."
    }
  ],
  "argos_summary": "Newelle is a newly released AI assistant for the GNOME desktop that combines traditional chatbot functionalities with the ability to execute Linux commands based on user descriptions. It requires Flatseal for command execution and allows users to choose from various large language models (LLMs), some of which may need an API key. While it offers a user-friendly interface and integrates well with the GNOME aesthetic, users may face challenges in configuring command execution properly.",
  "argos_id": "Z40LK37P5"
}