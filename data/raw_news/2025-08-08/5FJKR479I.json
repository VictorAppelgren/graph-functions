{
  "url": "https://techcrunch.com/2025/08/08/openai-priced-gpt-5-so-low-it-may-spark-a-price-war/",
  "authorsByline": "Julie Bort",
  "articleId": "0252813c080c4b5d94c05510ecf7b954",
  "source": {
    "domain": "techcrunch.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "CA",
      "county": "San Francisco",
      "city": "San Francisco",
      "coordinates": {
        "lat": 37.7790262,
        "lon": -122.419906
      }
    }
  },
  "imageUrl": "https://techcrunch.com/wp-content/uploads/2014/05/sam-altman3.jpg?resize=1200,800",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-08T16:10:04+00:00",
  "addDate": "2025-08-08T16:15:29.147306+00:00",
  "refreshDate": "2025-08-08T16:15:29.147307+00:00",
  "score": 1.0,
  "title": "OpenAI priced GPT-5 so low, it may spark a price war",
  "description": "Developers who have had early access are touting GPT-5's pricing as much better than competing models.",
  "content": "OpenAI astounded the tech industry for the second time this week by launching its newest flagship model, GPT-5, just days after releasing two new freely available models under an open source license.\n\nOpenAI CEO Sam Altman went so far as to call GPT-5 \u201cthe best model in the world.\u201d That may be pride or hyperbole, as TechCrunch\u2019s Maxwell Zeff reports that GPT-5 only slightly outperforms other leading AI models from Anthropic, Google DeepMind, and xAI on some key benchmarks, and slightly lags on others.\n\nStill, it\u2019s a model that performs well for a wide variety of uses, particularly coding. And, as Altman pointed out, one area where it is undoubtedly competing well is price. \u201cVery happy with the pricing we are able to deliver!\u201d he tweeted.\n\nThe top-level GPT-5 API costs $1.25 per 1 million tokens of input, and $10 per 1 million tokens for output (plus $0.125 per 1 million tokens for cached input). This pricing mirrors Google\u2019s Gemini 2.5 Pro basic subscription, which is also popular for coding-related tasks. Google, however, charges more if inputs/outputs cross a heavy threshold of 200,000 prompts, meaning its most consumption-heavy customers end up paying more.\n\nBut OpenAI is really undercutting Anthropic\u2019s Claude Opus 4.1, which starts at $15 per 1 million input tokens and $75 per 1 million output tokens. (Anthropic does, however, offer big discounts for prompt caching and batch processing \u2014 storing/reusing prompts and processing multiple requests together.)\n\nAnthropic\u2019s model has been extremely popular among programmers, both as a choice within popular coding assistant Cursor and powering its own such assistant, Claude Code. (Note that Cursor offered GPT-5 as an option minutes after it was announced.)\n\nDevelopers who have had early access to GPT-5 are touting the pricing. Simon Willison, one of the developers featured in OpenAI\u2019s launch video, writes in his review: \u201cThe pricing is aggressively competitive with other providers.\u201d (Emphasis his.)\n\nBut GPT-5 is also priced competitively with GPT-4o. OthersideAI\u2019s co-founder and CEO, Matt Shumer (maker of HyperWrite), writes that GPT-5 \u201cis cheaper than GPT-4o, which is fantastic. Intelligence per dollar continues to increase.\u201d\n\nSome on X called OpenAI\u2019s fees for the model \u201ca pricing killer,\u201d while others on Hacker News are offering similar praise.\n\nWill competitors like Anthropic follow? Will Google \u2014 who undercut OpenAI on pricing before \u2014 get even more affordable? If so, we could be witnessing the start of a much awaited LLM price war.\n\nThere\u2019s no doubt a price war would be welcome. The underlying economics of vibe-coding tool providers, for instance, is pretty shaky because of the high and unpredictable fees they have to pay model makers, as TechCrunch\u2019s Marina Temkin reports. And there are countless startups building on top of AI models as well.\n\nSilicon Valley has been hoping that the LLM price-to-performance ratio will eventually improve, along with inference costs. But it seemed like such an equalization could be years away as the tech industry invests hundreds of billions to build data centers and infrastructure to support growing AI demand.\n\nOpenAI itself has a $30 billion-per-year contract with Oracle for capacity, when it only recently hit annual recurring revenue of $10 billion. Meanwhile, Meta plans to spend up to $72 billion on AI infrastructure in 2025, and Alphabet has set aside $85 billion for capital expenditures in 2025, driven by AI needs. In the face of such enormous expenses, costs typically go one way: upwards.\n\nGiven such investments, it may be too soon for startups looking at their rising model API bills to rejoice from OpenAI\u2019s lone move to lower pricing.\n\nYet this week, OpenAI threw down the gauntlet to put pressure on prices not just once but twice. We\u2019ll see if others follow.",
  "medium": "Article",
  "links": [
    "https://techcrunch.com/events/tc-disrupt-2025/?utm_source=tc&utm_medium=ad&utm_campaign=disrupt2025&utm_content=ticketsales&promo=tc_inline_rb&display=",
    "https://x.com/OpenAIDevs/status/1953535155789865423",
    "https://x.com/sama/status/1953527247257899389",
    "https://x.com/Hansenq/status/1953520912831287446",
    "https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/",
    "https://x.com/mehuljindal18/status/1953528613938377084",
    "https://techcrunch.com/2025/06/09/openai-claims-to-have-hit-10b-in-annual-revenue/",
    "https://techcrunch.com/2025/07/30/meta-to-spend-up-to-72b-on-ai-infrastructure-in-2025-as-compute-arms-race-escalates/",
    "https://www.cnbc.com/2025/07/23/googles-85-billion-capital-spend-spurred-by-cloud-ai-demand.html",
    "https://techcrunch.com/2025/08/05/for-the-first-time-openai-models-are-available-on-aws/",
    "https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/",
    "https://x.com/mattshumer_/status/1953501875602395327",
    "https://simonwillison.net/2025/Aug/7/gpt-5/",
    "https://techcrunch.com/2025/08/07/openais-gpt-5-is-here/",
    "https://techcrunch.com/2025/08/07/the-high-costs-and-thin-margins-threatening-ai-coding-startups/",
    "https://cursor.com/blog/gpt-5",
    "https://techcrunch.com/2025/04/04/gemini-2-5-pro-is-googles-most-expensive-ai-model-yet/",
    "https://techcrunch.com/2025/07/07/cursor-apologizes-for-unclear-pricing-changes-that-upset-users/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI models",
      "weight": 0.091698125
    },
    {
      "name": "other leading AI models",
      "weight": 0.086549215
    },
    {
      "name": "model makers",
      "weight": 0.08357899
    },
    {
      "name": "GPT-5",
      "weight": 0.076378755
    },
    {
      "name": "OpenAI CEO Sam Altman",
      "weight": 0.07553989
    },
    {
      "name": "OpenAI",
      "weight": 0.072554015
    },
    {
      "name": "AI infrastructure",
      "weight": 0.066265956
    },
    {
      "name": "pricing",
      "weight": 0.06336783
    },
    {
      "name": "popular coding assistant",
      "weight": 0.063066706
    },
    {
      "name": "price",
      "weight": 0.06020111
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.94970703125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.83056640625
    }
  ],
  "sentiment": {
    "positive": 0.5541992,
    "negative": 0.05102539,
    "neutral": 0.3947754
  },
  "summary": "OpenAI has launched its newest flagship AI model, GPT-5, just days after releasing two new models under an open source license. The company's CEO, Sam Altman, called GPT 5 \"the best in the world,\" although it only slightly outperforms other leading AI models on some key benchmarks. The top-level GPS-5 API costs $1.25 per 1 million tokens of input, and $10 per output for output, similar to Google's Gemini 2.5 Pro basic subscription. However, Google charges more if inputs/outputs cross a heavy threshold of 200,000 prompts, leading its most consumption-heavy customers to pay more. Developers have been praising the pricing, with some calling it a pricing killer. Will competitors like Anthropic follow? If so, this could be the start of a price war.",
  "shortSummary": "OpenAI\u2019s new AI model GPT-5 offers significantly lower pricing, potentially sparking a price war among other providers, despite limited performance and investment.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "190abf3010f84900a73871d3220c0890",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://techcrunch.com/2025/04/04/gemini-2-5-pro-is-googles-most-expensive-ai-model-yet/",
      "text": "On Friday, Google released API pricing for Gemini 2.5 Pro, an AI reasoning model with industry-leading performance on several benchmarks measuring coding, reasoning, and math.\nFor prompts up to 200,000 tokens, Gemini 2.5 Pro costs $1.25 per million input tokens (roughly 750,000 words, longer than the entire \u201cLord of The Rings\u201d series) and $10 per million output tokens. For prompts greater than 200,000 tokens (which most of Google\u2019s competitors don\u2019t support), Gemini 2.5 Pro costs $2.50 per million input tokens and $15 per million output tokens.\nThat pricing makes Gemini 2.5 Pro more expensive for developers than any other AI model currently offered by Google, including Gemini 2.0 Flash ($0.10/million input tokens, $0.40/million output tokens). It also makes Gemini 2.5 Pro more expensive than several other frontier AI models, such as OpenAI\u2019s o3-mini ($1.10/million input tokens, $4.40/million output tokens) and DeepSeek\u2019s R1 ($0.55/million input tokens, $2.19/million output tokens).\nTo be fair, Gemini 2.5 Pro, which is available for free with strict rate limits, comes in cheaper than some other highly competitive models, including Anthropic\u2019s Claude 3.7 Sonnet ($3/million input tokens, $15/million output tokens) and OpenAI\u2019s GPT-4.5 ($75/million input tokens, $150/million output tokens). The tech industry\u2019s initial reaction has been largely positive, with developers praising what they perceive to be sensible rates.\nBut broadly speaking, there seems to be some upward pressure on pricing for flagship models. The cost of recent top-of-the-line releases from labs like Google, OpenAI, and Anthropic has been going up, not down. See, for example, OpenAI\u2019s recently launched o1-pro, which is the company\u2019s most expensive API offering yet at $150/million input tokens and $600/million output tokens.\nIt could be that high demand and computing costs are driving the trend. According to Google CEO Sundar Pichai, Gemini 2.5 Pro is the company\u2019s most in-demand AI model among developers, leading to an 80% increase in usage in Google\u2019s AI Studio platform and the Gemini API this month alone."
    },
    {
      "url": "https://techcrunch.com/2025/08/05/for-the-first-time-openai-models-are-available-on-aws/",
      "text": "Sam Altman\u2019s blowtorch to his competitors is so hot, it even includes a new partnership with Amazon Web Services.\nAs OpenAI announced two open-weight reasoning models with capabilities on par with its o-series, Amazon announced that the new models would become available on AWS on Tuesday. This is the first time that OpenAI models will be offered by AWS, the company confirmed to TechCrunch. They will be available as a model choice with Amazon AI services Bedrock and SageMaker AI.\nWhile anyone can download the models via Hugging Face, Amazon is offering these models with OpenAI\u2019s full knowledge and approval, as Dmitry Pimenov, the model maker\u2019s product lead, indicated in the announcement. A spokesperson described the offering as similar to how Amazon offered open model DeepSeek-R1 earlier this year.\nThis is a juicy competitive move for both companies. For AWS, it finally puts the cloud giant in the same sentence as the biggest model maker, OpenAI.\nUntil now, AWS has best been known as a major host and a financial backer of Anthropic\u2019s Claude, one of OpenAI\u2019s biggest competitors. AWS offers Claude, along with other models from makers such as Cohere, DeepSeek, Meta, and Mistral, as well as its own home-grown ones in its AI services. Specifically, Bedrock allows AWS customers to build and host generative AI apps using models of their choice. SageMaker, on the other hand, allows AWS customers to train, or even build, their own AI models largely for analytics uses.\nWhile AWS\u2019s ultimate rival, Microsoft, hasn\u2019t had a lock on OpenAI models since January, Azure is still OpenAI\u2019s most significant cloud partner to date. OpenAI even announced that Microsoft is offering versions of these two new models, too, optimized for Windows devices.\nWatching Microsoft win an increasing amount of cloud business with OpenAI has been a public pain in Amazon CEO Andy Jassy\u2019s neck. Just last week during Amazon\u2019s quarterly earnings call, Jassy was pounded with questions from Wall Street analysts about how the company was losing ground in AI to competitors, particularly Microsoft.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $675 before prices rise.\nFor instance, JPMorgan analyst Doug Anmuth asked Jassy to explain \u201csignificantly faster cloud growth among the number two and number three players in the space,\u201d referring to Microsoft and Google. Later, Morgan Stanley analyst Brian Nowak told Jassy that Wall Street thinks \u201cAWS is falling behind in GenAI with concerns about share loss, peers, etc.\u201d\nJassy responded with a minutes-long diatribe that included this barb at Redmond: \u201cI think the second player is about 65% of the size of the AWS.\u201d\nMeanwhile, another AWS competitor, Oracle, reported that it signed a $30 billion a year deal with OpenAI to offer data center services. This means that OpenAI plans to pay Oracle more each year than all of its other cloud services customers combined. Until now, AWS had been left out of any OpenAI-related glory.\nAs for how such a move with AWS benefits OpenAI: The AI provider\u2019s relationship with Microsoft is notoriously strained, as the two are reportedly renegotiating their long-term partnership deal. What better way for OpenAI to strengthen its position than to cozy up the biggest cloud provider, even if, initially, on a small scale?\nIn addition, this partnership allows swaths of AWS enterprise customers to easily experiment with using OpenAI models with their hosted AI apps.\nMeanwhile Altman gets to undercut Meta\u2019s Mark Zuckerberg with this move as well. As OpenAI releases these two high-performing models under an Apache 2.0 open source license, Meta recently admitted that it probably won\u2019t continue to open source all of its upcoming \u201csuperintelligence\u201d models."
    },
    {
      "url": "https://techcrunch.com/2025/07/30/meta-to-spend-up-to-72b-on-ai-infrastructure-in-2025-as-compute-arms-race-escalates/",
      "text": "Meta is pouring money into the physical and technical infrastructure needed to scale its AI ambitions. The company said Wednesday in its second-quarter earnings report that it plans to more than double its spend on building AI infrastructure, like data centers and servers.\n\u201cWe currently expect 2025 capital expenditures, including principal payments on finance leases, to be in the range of $66-72 billion\u2026up approximately $30 billion year-over-year at the midpoint,\u201d Meta said.\nThat\u2019s an aggressive capex growth, and one that Meta plans to continue onwards to 2026. The company said it expects a similarly large increase in spend on AI infrastructure next year as the company continues to \u201caggressively [pursue] opportunities to bring additional capacity online to meet the needs of [its] artificial intelligence efforts and business operations.\u201d\n\u201cWe expect that developing leading AI infrastructure will be a core advantage in developing the best AI models and product experiences, so we expect to ramp our investments significantly in 2026 to support that work,\u201d said Susan Li, Meta CFO, during the company\u2019s Wednesday earnings call.\nLi also noted that while Meta expects to finance most of its AI spend on its own, the company is exploring ways to work with financial partners to co-develop data centers.\n\u201cWe don\u2019t have any finalized transactions to announce, but we generally believe that there will be models here that will attract significant external financing to support large-scale data center projects that are developed using our ability to build world-class infrastructure, while providing us with flexibility should our infrastructure requirements change over time,\u201d Li said.\nMeta has announced two major AI \u201ctitan clusters.\u201d The first is Prometheus in Ohio, which is poised to be among the first AI superclusters to hit 1 gigawatt of compute power when it comes online in 2026. Then there\u2019s Hyperion, a cluster in Louisiana that Meta CEO Mark Zuckerberg has bragged would have a footprint the size of Manhattan and could scale up to 5 gigawatts over several years. On top of those, Meta has several other unnamed titan-scale clusters underway.\nMeta\u2019s data center projects promise to soak up enough energy to power millions of homes, pulling that electricity from nearby communities. One of the company\u2019s projects in Newton County, Georgia, has already caused the water taps to run dry in some residents\u2019 homes.\nMeta also noted in its earnings report that it expects its second-largest driver of growth to be employee compensation as the company spends millions, and possibly even billions, to poach talented AI engineers and researchers to work for Meta\u2019s newly formed business unit, Superintelligence Labs.\nBefore earnings, Zuckerberg shared his vision for \u201cpersonal superintelligence,\u201d the idea that AI should help individual people live their best lives, mainly through the medium of Meta\u2019s smart glasses and virtual reality headsets.\nMeta\u2019s stock surged 10% in after-hours as investors responded to Meta\u2019s overall performance in the quarter and better-than-expected outlook for the third quarter. Meta reported revenue of $47.5 billion in the second quarter, with expectations to hit between $47.5 billion and $50.5 billion in Q3. Advertising drove Meta\u2019s revenue gains, fueled by AI tools \u2014 like AI-powered translations and video generation \u2014 to help advertisers create more meaningful and targeted campaigns.\nThe company\u2019s Reality Labs segment, however, saw a $4.5 billion loss.\nThis article was updated to reflect Meta\u2019s additional paths to funding its AI infrastructure builds."
    },
    {
      "url": "https://cursor.com/blog/gpt-5",
      "text": "GPT-5 is now available in Cursor\nThis is OpenAI's most powerful model, and we've found it to be quite effective for coding.\nPosted By Aman\n2 minutes read\nGPT-5 is now available in Cursor.\nThis is OpenAI's most powerful model, and we've found it to be quite effective for coding.\nWe've started using the model internally to build Cursor and wanted to share our initial taste-test from a number of engineers.\nInitial impressions from the team\nHere are some observations from five engineers on our team:\nI've been using GPT-5 for more long-running, complex tasks. For example, using it with Background Agents for independent tasks and parallel foreground agents when switching between different tasks and reviewing their outputs.\nZack Holbrook\nGPT-5 is one of the most steerable models I've used. I've needed to be more explicit about what I was trying to accomplish for some tasks. Leaving things vague ended up with the model taking a different direction than I expected. When I was more specific, I was surprised by how smart the model was.\nEric Zakariasson\nThere are a few places where I've gotten stuck in the past week coding where GPT-5 solved a complex bug other models couldn't figure out. I've used it for tasks like looking through our Stripe queries and figuring out the best optimizations to remove latency.\nAdam Hofmann\nThe default writing style of GPT-5 was more verbose than I wanted, so I was able to make a rule to have responses be more concise. I also prefer more agentic behavior, so I was able to steer the model towards asking fewer follow-up questions after prompting. I've been using it for more difficult coding problems.\nLee Robinson\nMy favorite way to taste test is to use the same prompt for many different models and see if any can one-shot what I was expecting. Our frontend and backend use protobufs, which sometimes throw models off. I asked GPT-5 to build a backend API endpoint as well as the corresponding frontend React component, which are in two separate submodules in our monorepo. It was able to get this code correct and regenerate the protobufs to fix type errors in one shot.\nYash Gaitonde\nWe've found GPT-5 to be an intelligent and steerable model, which lends itself well to coding and tool calling.\nTogether with OpenAI, GPT-5 is available with free credits for paying users during the launch week."
    },
    {
      "url": "https://www.cnbc.com/2025/07/23/googles-85-billion-capital-spend-spurred-by-cloud-ai-demand.html",
      "text": "Google is going to spend $10 billion more this year than it previously expected due to the growing demand for cloud services, which has created a backlog, executives said Wednesday.\nAs part of its second quarter earnings, the company increased its forecast for capital expenditures in 2025 to $85 billion due to \"strong and growing demand for our Cloud products and services\" as it continues to expand infrastructure to power more AI services that use its cloud technology. That's up from the $75 billion projection that Google provided in February, which was already above the $58.84 billion that Wall Street expected at the time.\nThe increased forecast comes as demand for cloud services surges across the tech industry as AI services increase in popularity. As a result, companies are doubling down on infrastructure to keep pace with demand and are planning multi\u2011year buildouts of data centers.\nIn its second quarter earnings, Google reported that cloud revenues increased by 32% to $13.6 billion in the period. The demand is so high for Google's cloud services that it now amounts to a $106 billion backlog, Alphabet finance chief Anat Ashkenazi said during the company's post-earnings conference call.\n\"It's a tight supply environment,\" she said.\nThe vast majority of Alphabet's capital spend was invested in technical infrastructure during the second quarter, with approximately two-thirds of investments going to servers and one-third in data center and networking equipment, Ashkenazi said.\nShe added that the updated outlook reflects additional investment in servers, the timing of delivery of servers and \"an acceleration in the pace of data center construction, primarily to meet Cloud customer demand.\"\nAshkenazi said that despite the company's \"improved\" pace of getting servers up and running, investors should expect further increase in capital spend in 2026 \"due to the demand as well as growth opportunities across the company.\" She didn't specify what those opportunities are but said the company will provide more details on a future earnings call.\n\"We're increasing capacity with every quarter that goes by,\" Ashkenazi said.\nDue to the increased spend, Google will have to record more expenses over time, which will make profits look smaller, she said.\n\"Obviously, we're working hard to bring more capacity online,\" Ashkenazi said.\nWATCH: Alphabet shares Q2 shares sink despite revenue and earnings beat"
    },
    {
      "url": "https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/",
      "text": "OpenAI was the company that signed a $30 billion per year deal with Oracle for data center services, disclosed last month, The Wall Street Journal reported on Monday. Now, OpenAI CEO Sam Altman has confirmed the details of the contract (but not the dollar amount) in an X post on Tuesday and in a company blog post.\nTo recap, on June 30, Oracle disclosed in an SEC filing that it had signed a cloud deal that would generate $30 billion a year in revenue. However, the company didn\u2019t say who it was with or for what services. The news caused Oracle\u2019s stock to hit an all-time high, making its founder and CTO, Larry Ellison, the second richest person in the world, according to Bloomberg.\nSpeculation on the identity of the customer ensued as people wondered what company could possibly need a fresh $30 billion a year in data center services. For comparison, Oracle collectively sold $24.5 billion worth of cloud services in its fiscal 2025 to all customers combined, it reported in June.\nOpenAI has now explained that this Oracle deal is for 4.5 gigawatts of capacity as part of Stargate, the $500 billion data-center-building project OpenAI, Oracle, and SoftBank announced in January. (Apparently, the $30 billion deal does not involve SoftBank.)\nThe WSJ reports 4.5 gigawatts is the equivalent of two Hoover Dams, enough power for about four million homes.\nThis isn\u2019t a straightforward win for Oracle. OpenAI and Oracle still have to build this monster data center, which will be a costly endeavor, both in cash and in energy. They are doing so at what OpenAI called the Stargate I site in Abilene, Texas.\nMeanwhile, Oracle spent $21.2 billion on capital expenditures in its last fiscal year, CEO Safra Catz reported in June, and it expects to spend another $25 billion this year, she said. So, nearly $50 billion, largely spent on data centers (and that doesn\u2019t include land purchases, she said) in two years. Although, to be clear, that money also supports Oracle\u2019s existing customers, in addition to OpenAI\u2019s demands.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $675 before prices rise.\nOne final interesting part to note about all of this: Last month, Altman said that OpenAI recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year. This single commitment to Oracle is already triple per year what it is currently bringing in and doesn\u2019t include all of the company\u2019s other expenses, including its current data center commitments."
    },
    {
      "url": "https://techcrunch.com/2025/08/07/openais-gpt-5-is-here/",
      "text": "OpenAI has launched GPT-5, a new flagship AI model that will power the company\u2019s next generation of ChatGPT.\nGPT-5, which was released Thursday, is OpenAI\u2019s first \u201cunified\u201d AI model and combines the reasoning abilities of its o-series of models with the fast responses of its GPT series. The next-generation model signals a new era for ChatGPT \u2014 and its creator, OpenAI \u2014 pointing to OpenAI\u2019s broader ambitions to develop AI systems that are more like agents than chatbots.\nWhile GPT-4 enabled AI chatbots to offer smart responses on a wide variety of questions, GPT-5 allows ChatGPT to complete a wide variety of tasks on behalf of users \u2014 such as generating software applications, navigating a user\u2019s calendar, or creating research briefs.\nWith GPT-5, OpenAI has also sought to make ChatGPT simpler to use. Instead of asking users to choose the right settings, GPT-5 comes equipped with a real-time router that decides how to offer the best answer, whether that\u2019s responding to user questions quickly or taking additional time to \u201cthink\u201d through answers.\nDuring a briefing with reporters, OpenAI CEO Sam Altman claimed GPT-5 is \u201cthe best model in the world,\u201d and said it represented a \u201csignificant step\u201d along the company\u2019s path to developing AI that can outperform humans at most economically valuable work \u2014 that is, artificial general intelligence (AGI).\n\u201cHaving something like GPT-5 would be pretty much unimaginable at any previous time in history,\u201d said Altman.\nStarting Thursday, GPT-5 will be available to all free users of ChatGPT as their default model. OpenAI\u2019s VP of ChatGPT, Nick Turley, said this is part of the company\u2019s effort to give free users access to an AI reasoning model for the first time. (Previously, the company gated these more advanced models behind a paywall.)\n\u201cThis is just one of the ways that I\u2019m excited to live the mission, making sure that this stuff actually benefits people,\u201d said Turley on the decision, referencing OpenAI\u2019s long-standing mission to distribute advanced AI to as many people as possible.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $675 before prices rise.\nThe expectations are high for GPT-5, one of OpenAI\u2019s most anticipated product launches since ChatGPT put the company on the map in 2022. Since then, ChatGPT has grown into one of the world\u2019s most popular consumer products, reaching more than 700 million users every week \u2014 nearly 10% of the globe\u2019s population, according to the company.\nMany see GPT-5 as a bellwether for AI progress broadly, and the model\u2019s reception by Silicon Valley could have profound implications for Big Tech, Wall Street, and policymakers regulating technology. These stakeholders are watching to see if GPT-5 offers a significant jump in AI\u2019s capabilities, much like its predecessor, GPT-4, which challenged expectations of what software can do.\nGPT-5 offers a slight edge on the competition\nOpenAI claims GPT-5 is state-of-the-art in several domains, slightly edging out leading AI models from Anthropic, Google DeepMind, and Elon Musk\u2019s xAI on key benchmarks. However, GPT-5 slightly underperforms frontier AI models in other areas.\nThe company says GPT-5 offers frontier-level performance around coding; Altman said the model specifically excels at spinning up entire software applications on demand, in what\u2019s become known as \u201cvibe coding.\u201d\nOn SWE-bench Verified \u2014 a test of real-world coding tasks pulled from GitHub \u2014 GPT-5 scores 74.9% on its first attempt. That means GPT-5 just outperforms Anthropic\u2019s latest Claude Opus 4.1 model, which scored 74.5%, and Google DeepMind\u2019s Gemini 2.5 Pro, which scored 59.6%.\nOn Humanity\u2019s Last Exam \u2014 a difficult test measuring AI model performance across math, humanities, and the natural sciences \u2014 a version of GPT-5 with extended reasoning (GPT-5 Pro) scored 42% when using tools. That\u2019s slightly less than xAI was able to achieve with Grok 4 Heavy, which scored 44.4% on the test.\nOn GPQA Diamond \u2014 a test of PhD-level science questions \u2014 GPT-5 pro scored 89.4% on its first try, outperforming Claude Opus 4.1, which scored 80.9%, and Grok 4 Heavy, which scored 88.9%.\nOpenAI says GPT-5 is better for answering health-related questions. On a test measuring accuracy in AI model responses around healthcare topics, HealthBench Hard Hallucinations, OpenAI says GPT-5 (with thinking) hallucinates just 1.6% of the time. This is far lower than the company\u2019s previous GPT-4o and o3 models, which scored 12.9% and 15.8, respectively.\nWhile AI chatbots are not medical professionals, millions of people are using them for health advice. In response to this phenomenon, the company says GPT-5 is more proactive about flagging potential health concerns and helping users parse medical results.\nIn addition, OpenAI says GPT-5 is better than other AI models on more difficult-to-measure, subjective domains, such as creative design and writing. Turley said GPT-5 responds more naturally and exhibits \u201cbetter taste\u201d than other AI models on creative tasks.\n\u201cThe vibes of this model are really good,\u201d said Turley.\nGPT-5 is also more accurate than OpenAI\u2019s previous models, and the company says it suffers far less from hallucinations \u2014 the tendency for AI models to make up information \u2014 compared to its o-series models. Hallucinations seemed to be getting worse in OpenAI\u2019s latest AI reasoning models, such as o3, and OpenAI previously said it didn\u2019t quite understand why it was happening.\nIn responses to ChatGPT prompts, OpenAI found that GPT-5 (with thinking) hallucinates and responds with incorrect information 4.8% of the time. That\u2019s a significant reduction from o3 and GPT-4o, which score hallucination rates of 22% and 20.6%, respectively, on the test.\nOn a benchmark measuring an AI model\u2019s agentic ability to complete simulated online tasks, Tau-bench, GPT-5 offers mixed performance. On part of the test measuring an AI\u2019s ability to navigate an airline\u2019s website, GPT-5 scores 63.5%, slightly underperforming o3, which scored 64.8%. On another part of the test measuring AI\u2019s ability to navigate retail websites, GPT-5 scores 81.1%, underperforming Claude Opus 4.1, which scored 82.4%.\nOpenAI also says that GPT-5 is safer than its previous models. While AI reasoning models occasionally exhibit a tendency to scheme against humans or lie to promote their own goals, OpenAI found that GPT-5 was deceptive at a lower rate than other models.\nAlex Beutel, OpenAI\u2019s safety research lead, said reducing deception improves not only the safety of GPT-5, but also the user experience, creating a model that\u2019s more \u201ctransparent and honest in ways users can trust.\u201d\nBeutel also notes GPT-5 is better at discerning between bad actors who are trying to misuse ChatGPT and users making harmless requests. This results in GPT-5 being able to refuse more unsafe questions, while offering fewer rejections to users seeking harmless information.\nUpgrades for consumers and developers\nChatGPT is getting a few user experience upgrades as part of the GPT-5 launch. Users can now select from four new personalities in ChatGPT\u2019s setting: Cynic, Robot, Listener, and Nerd. The company says these will adapt ChatGPT\u2019s responses without requiring users to specifically ask the model to respond in a certain way.\nSubscribers to ChatGPT\u2019s $20-per-month Plus plan get higher usage limits for GPT-5 than free users. Meanwhile, $200-per-month Pro subscribers will have unlimited access to GPT-5, as well as a souped-up version called GPT-5 Pro that uses additional computational resources to produce better answers. Organizations on OpenAI\u2019s Team, Edu, and Enterprise plans will gain access to GPT-5 as their default model next week.\nFor developers, GPT-5 is coming to OpenAI\u2019s API in three sizes \u2014 gpt-5, gpt-5-mini, and gpt-5-nano \u2014 which will spend more or less time \u201creasoning\u201d through tasks. Developers can also now control verbosity in the OpenAI API, deciding how long or short an AI model\u2019s responses should be.\nThe base model of GPT-5 will cost developers $1.25 per million input tokens (roughly 750,000 words, longer than the entire \u201cLord of the Rings\u201d series) and $10 per million output tokens.\nThe launch of GPT-5 comes after a busy week for OpenAI. The company released an open-weight reasoning model, gpt-oss, that developers and enterprises can download for free and run at a fraction of the cost. The open model nearly matched the abilities of OpenAI\u2019s previous top models, o3 and o4-mini, but GPT-5 sets a new standard for frontier performance in some areas, such as coding.\nHowever, GPT-5 seems to be roughly on par with other frontier AI models in several areas. Benchmarks, of course, only tell part of the story for any AI model, and it remains to be seen how developers will use GPT-5 in the real world, and whether the model is truly a step above the competition."
    },
    {
      "url": "https://techcrunch.com/events/tc-disrupt-2025/?utm_source=tc&utm_medium=ad&utm_campaign=disrupt2025&utm_content=ticketsales&promo=tc_inline_rb&display=",
      "text": "TechCrunch Disrupt 2025\nTechCrunch Disrupt is where you\u2019ll find innovation for every stage of your startup journey. Whether you\u2019re a budding founder with a revolutionary idea, a seasoned startup looking to scale, or an investor seeking the next big thing, Disrupt offers unparalleled resources, connections, and expert insights to propel your venture forward.\nFind innovation for every stage at Disrupt 2025\nFrom idea to IPO, Disrupt 2025 will map out the path for startups to achieve their next major milestone.\nJoin 10,000 startup and VC leaders\nLooking to meet founders, connect with investors, seek advice, or land your next big role? Disrupt is the must-attend event to make it all happen in person.\nBuild and scale your business faster\nDisrupt is more than a startup launchpad \u2014 it\u2019s a growth accelerator. Dive into sessions on scaling, sales, and leadership, and connect with the investors and tech experts who can help take your business to the next level.\nGain insights from today\u2019s tech giants\nTap into the wisdom of founders and tech titans at Disrupt. From actionable tips to hard-won lessons, they\u2019ll share what works (and what doesn\u2019t) to guide you as you build your own path forward.\nDisrupt 2025 speakers\nPartner Sequoia Capital\nChief Technology Officer US Dept of Navy\nCEO and Co-Founder Writer\nChief Product Officer Netflix\nCEO Wayve\nCo-Founder & CEO Box\nFounder and CEO Pinecone\nCo-CEO Waymo\nCo-Founder ElevenLabs\nCaptain of Moonshots X, The Moonshot Factory\nI\u2019ve always thought, \u201cWhat can I do that opens up possibilities?\u201d I grew up in South Africa and went to an Afrikaans high school, and we didn\u2019t speak English at home. But I pushed myself to go to an English-speaking university, and that opened up a door. Then I chose to work for McKinsey, because I thought it would open another door and maybe I\u2019d get a chance to work overseas. I was reading about what was happening in Silicon Valley in the mid 90s, before I came to the U.S. Already, I\u2019m starting to see the beginnings of the internet. Did I fathom how big it would be? Absolutely not. I didn\u2019t know what venture capital was. I didn\u2019t know that I would join a startup. I just had an intuition that I needed to be here. A friend of mine introduced me to Elon in 1999 and I joined PayPal. Then when Mike Moritz asked me to come interview at Sequoia, that was just another door opening. Where might this one lead? Whenever I interview people, I ask about those key moments in somebody\u2019s life where they\u2019ve made career decisions. And I think about companies in the same way\u2014there are these crucible moments that have an enormous bearing on ultimate outcomes.\nRoelof's Sessions\nWhat Sequoia Sees Coming Next\nAs one of the most influential VCs of the modern era, Roelof Botha has seen it all; booms, busts, and billion-dollar breakout bets. In this fireside chat, the Sequoia Capital managing partner opens up about how today\u2019s most ambitious founders are navigating AI, geopolitics, and a shifting capital landscape.\nJustin Fanelli is the Chief Technology Officer for the Department of the Navy and Technical Director of PEO Digital, driving measurable technology improvements and secure, high-performance digital transformation. He champions private-public collaboration to deliver innovation with unprecedented value. Fanelli advises numerous national science and technology boards and has held key roles including Chief Architect for Defense Health, DARPA Service Chiefs Fellow, and Chief Systems Engineer for Joint Command and Control. He teaches at Georgetown and has lectured all over the country at CMU, MIT, Stanford and others. He recently gave a TED Talk on innovation adoption and the Innovation Adoption Kit covered by TechCrunch, Forbes, CSIS and others. Fanelli holds engineering degrees from Penn State and the University of Pennsylvania and is a Senior Executive Fellow at Harvard Kennedy School. He\u2019s been an angel investor, VC and has been privileged to serve on public and private boards. His work has earned national awards including the Etter Award, Fed100, Defense50, and CMMI Project of the Year. He lives in Arlington, VA and enjoys book recommendations.\nJustin's Sessions\nAI and National Security in the High-Stakes Race to Innovate\nFrom defense labs to Wall Street and naval operations, AI is reshaping how countries protect themselves and project power. DARPA\u2019s Kathleen Fisher, Point72\u2019s Sri Chandrasekar, and Navy CTO Justin Fanelli dive into the cutting-edge AI breakthroughs driving security innovation. They\u2019ll discuss what it means for entrepreneurs, investors, and the future of global stability.\nMay Habib is the CEO and co-founder of Writer, a leader and pioneer in enterprise AI. With Writer\u2019s end-to-end AI agent platform, hundreds of companies like Accenture, Mars, Uber, and Vanguard are building and scaling AI agents that are grounded in their company\u2019s data and fueled by Writer\u2019s enterprise-grade LLMs. From faster product launches to deeper financial research to better clinical trials, companies are quickly transforming their most important business processes for the AI era in partnership with Writer. Writer houses the world\u2019s only enterprise-specific AI research lab. Its family of enterprise-grade Palmyra LLMs includes state-of-the-art frontier models, as well as self-evolving, open-source, and domain-specific models. Palmyra models define industry-leading standards for enterprise-grade transparency, reliability, safety, efficiency, and observability. May is an expert in natural language processing and AI-driven language generation. She has led Writer to become one of the world\u2019s fastest-growing generative AI companies, securing its position as a Forbes 50 AI company and inclusion in the World Economic Forum\u2019s Unicorn Community. Founded in 2020 with its headquarters in San Francisco and offices around the globe, Writer is backed by world-leading investors, including Premji Invest, Radical Ventures, ICONIQ Growth, Insight Partners, Balderton, B Capital, Salesforce Ventures, Adobe Ventures, Citi Ventures, IBM Ventures, and others. May and the Writer team have raised over $326M in funding at a valuation of $1.9B. May graduated with high honors in Economics from Harvard University. She is a World Economic Forum Young Global Leader, a Fellow of the Aspen Global Leadership Network, a recipient of Inc.\u2019s Female Founder Award, and one of Worth\u2019s Groundbreaking Women for 2025.\nMay's Sessions\nWriting the Future with AI?\nWhat happens when AI learns to write with purpose, personality, and persuasion? Writer CEO May Habib joins us to talk about the evolving relationship between language and machines and what the rise of generative content means for the future of brand, business, and beyond.\nEunice Kim was named Chief Product Officer in October 2023. She previously led the company\u2019s global Consumer Product Innovation team. Eunice joined Netflix in early 2021 after having spent 10 years in product leadership roles at Google Play and YouTube. Prior to Google, she worked at several tech startups as well as PepsiCo and Adobe Systems. Eunice holds a B.A. from Columbia University and an M.B.A. from the University of Chicago Booth School of Business. She serves on the Board of Directors for Cure CMD.\nEunice's Sessions\nWhat's Next for Netflix and for Streaming Itself\nAs CPO of Netflix, Eunice Kim is steering the future of entertainment for hundreds of millions of users. In this fireside chat, Kim will break down how Netflix is evolving its product strategy\u2014from personalized discovery to global growth, from ad tiers to gaming. We\u2019ll explore how Netflix is adapting to a shifting content landscape, what it means to innovate at massive scale, and what design surprises Netflix has up its sleeve. For anyone building consumer experiences, this will be a masterclass.\nAlex co-founded Wayve in 2017 to reimagine autonomous mobility through embodied intelligence. From his award-winning research at the University of Cambridge, he seized the opportunity to use deep learning to pioneer an entirely new way to solve self-driving. He showed for the first time that it was possible to teach a machine to understand where it is and what\u2019s around it and then give it the \u201cintelligence\u201d to make its own decisions based on what it sees with computer vision. As CEO, Alex is responsible for the company\u2019s overall strategy, primarily focusing on establishing all necessary ingredients to develop and deploy AV2.0 globally. He also works closely with our partners and investors to ensure that our technology is commercially viable and can be widely adopted. Under Alex\u2019s leadership, Wayve is fast becoming one of the most exciting companies in the autonomous vehicle industry. Before founding Wayve, Alex\u2019s passion for autonomous vehicles began as a research fellow at the University of Cambridge, where he earned his PhD in Computer Vision and Robotics. His research has received numerous awards for scientific impact and made significant contributions to the field of computer vision and AI. He was selected on the Royal Academy of Engineering\u2019s SME Leaders Programme and named on the Forbes 30 Under 30 innovators list.\nAlex's Sessions\nDriving Intelligence\nFrom self-driving cars to self-learning systems, Alex Kendall is rethinking how machines perceive and act in the world. The Wayve CEO joins us to explore how real-world autonomy is shaping the next chapter of AI, and why breakthroughs on the road may unlock progress far beyond it.\nAaron Levie is Chief Executive Officer, Cofounder at Box, which he launched in 2005 with CFO and cofounder Dylan Smith. He is the visionary behind the Box product and platform strategy, incorporating the best of secure content collaboration with an intuitive user experience suited to the way people work today. Aaron leads the company in its mission to transform the way people and businesses work so they can achieve their greatest ambitions. He has served on the Board of Directors since April 2005. Aaron attended the University of Southern California from 2003 to 2005 before leaving to found Box.\nAaron's Sessions\nSurvive, Scale, Reinvent: Lessons from a Cloud OG\nAaron Levie built Box before cloud was cool and he\u2019s still standing while competitors have come and gone. Known for his sharp takes and startup instincts, Levie joins us to unpack how to keep innovating inside a public company, what AI really changes for enterprise software, and why reinvention is the name of the game in tech right now. Expect real talk and a playbook for building companies that last.\nEdo Liberty is the founder and CEO of Pinecone whose mission is to make AI knowledgeable. Pinecone is the leading vector database for building accurate and performant AI applications at scale in production. Prior to founding Pinecone, Edo was a Director of Research at AWS and Head of Amazon AI Labs where his team worked on data systems and services including SageMaker and OpenSearch. Before AWS, Edo was a Senior Research Director at Yahoo and Head of Yahoo\u2019s Research Lab in New York. As an adjunct professor at Princeton and Tel Aviv University, Edo taught long-term memory in AI and data mining. His academic work focuses on numerical linear algebra, streaming algorithms, data mining, and mathematical foundations of machine learning. Edo holds a B.Sc in physics and computer science from Tel Aviv University, and a Ph.D. in computer science from Yale. He has authored more than 75 academic papers and patents.\nEdo's Sessions\nWhy the Next Frontier Is Search\nIn a world overflowing with data, finding what matters is everything. Pinecone founder Edo Liberty unpacks why infrastructure, not algorithms, might be the biggest unlock in AI, and what\u2019s coming next in the race to power smarter applications at scale.\nTekedra N. Mawakana is the co-CEO of Waymo, an autonomous driving technology company. As co-CEO, Tekedra oversees the company\u2019s strategy for the wide adoption of the Waymo Driver. She boasts 20+ years of experience advising consumer technology companies to advance their business interests globally. Tekedra currently serves on the Board of Directors for Intuit and the Advisory Council for Boom Technology. She is a social impact-focused angel investor and an Advisor and LP with the Operator Collective.\nTekedra's Sessions\nThe Self-Driving Reality Check\nAutonomous vehicles have been \u201cjust around the corner\u201d for years\u2014until now. Tekedra Mawakana, Co-CEO of Waymo, takes the Disrupt Stage to talk about where AVs actually stand, what it\u2019s taken to get to real deployments, and why the race isn\u2019t just about tech, it\u2019s about trust. From regulation to rider experience to competition with Tesla and Tesla-adjacent hype, this conversation gets real about what\u2019s next in mobility.\nMati Staniszewski is the co-founder and CEO of ElevenLabs, a research company building audio AI tools to solve audio intelligence and make digital interactions feel more human \u2014 with voice as the most direct path to that. Before founding ElevenLabs, Mati worked at Palantir as a Deployment Strategist, managing large-scale implementations across public and private sectors, and at BlackRock, where he helped launch the Aladdin Wealth platform.\nMati's Sessions\nSynthetic Voices and Real Impact\nFrom audiobooks to avatars, synthetic speech is having a moment. ElevenLabs is helping lead the charge. CEO Mati Staniszewski joins us to explore what it takes to build AI that speaks like us and how voice technology is reshaping the creative industries, accessibility, and entertainment.\nDr. Astro Teller currently oversees X, Alphabet\u2019s moonshot factory for building breakthrough technologies and businesses designed to help tackle huge problems in the world. Before joining Google / Alphabet, Astro was the co-founding CEO of Cerebellum Capital, Inc, an investment management firm whose investments are continuously designed, executed, and improved by a software system based on techniques from statistical machine learning. Before his tenure as a business executive, Dr. Teller taught at Stanford University and was an engineer and researcher for Phoenix Laser Technologies, Stanford\u2019s Center for Integrated Systems, and The Carnegie Group Incorporated. Dr. Teller holds a Bachelor of Science in computer science from Stanford University, Masters of Science in symbolic and heuristic computation, also from Stanford University, and a Ph.D. in artificial intelligence from Carnegie Mellon University, where he was a recipient of the Hertz fellowship.\nAstro's Sessions\nMoonshots, AI, and the Future of Alphabet\nFrom self-driving cars to internet balloons to AI-fueled breakthroughs, Astro Teller leads the lab where Alphabet incubates the nearly impossible. In this rare Disrupt stage appearance, he shares what\u2019s actually working inside X, why \u201cfailing fast\u201d isn\u2019t just a mantra, and how moonshots may evolve in the AI age. If you think your startup is ambitious, wait until you hear what he\u2019s launching next.\nMore reasons to attend Disrupt\nLatest TechCrunch Disrupt 2025 news\nFind Your Ticket Type\nFind the ticket type that fits you best with up to $600+ savings. Groups can save up to 30%.\nPartner with TechCrunch\nTechCrunch offers many ways for partners to engage directly with our attendees before, during, and after the event. Get in touch with us to learn more.\nLocation\nUpcoming TechCrunch events\n-\nGet on waitlist to get exclusive early access to limited tickets!\nTechCrunch Disrupt 2025 is innovation for every stage\n10,000+ tech leaders. 250+ sessions. 200+ speakers. 3 days. 6 stages. One epic experience of insights, strategy, and connections. Save up to $675 before rates increase on August 7.\nSubscribe\nEvent Updates\nGet the latest event announcements, special discounts and other event offers.\nPartner with TechCrunch\nTechCrunch offers many ways for partners to engage directly with our attendees before, during, and after the event. Get in touch with us to learn more."
    },
    {
      "url": "https://techcrunch.com/2025/08/07/the-high-costs-and-thin-margins-threatening-ai-coding-startups/",
      "text": "In February, AI coding startup Windsurf was in talks to raise a big new round at a $2.85 billion valuation led by Kleiner Perkins, at double the valuation it hit six months earlier, sources told TechCrunch at the time. That deal didn\u2019t happen, according to a source familiar with the matter. Instead, news broke in April that the startup planned to sell itself to OpenAI for roughly the same valuation: $3 billion.\nWhile that deal famously fell apart, one bigger question remains: If the startup was growing that fast and attracting VC interest, why would it sell at all?\nInsiders tell TechCrunch that for all the popularity and hype around AI coding assistants, they can actually be massively money-losing businesses. Vibe coders generally, and Windsurf in particular, can have such expensive structures that their gross margins are \u201cvery negative,\u201d one person close to Windsurf told TechCrunch. Meaning it cost more to run the product than the startup could charge for it.\nThis is due to the high costs of using large language models (LLMs), the person explained. AI coding assistants are particularly pressured to always offer the most recent, most advanced, and most expensive LLMs because model makers are particularly fine-tuning their latest models for improvements in coding and related tasks like debugging.\nThis is a challenge compounded by fierce competition in the vibe-coding and code-assist market. Rivals include companies that already have huge customer bases like Anysphere\u2019s Cursor and GitHub Copilot.\nThe most straightforward path to improving margins in this business involves the startups building their own models, thereby eliminating costs of paying suppliers like Anthropic and OpenAI.\n\u201cIt\u2019s a very expensive business to run if you\u2019re not going to be in the model game,\u201d said the person.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital, Elad Gil \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $600+ before prices rise.\nTech and VC heavyweights join the Disrupt 2025 agenda\nNetflix, ElevenLabs, Wayve, Sequoia Capital \u2014 just a few of the heavy hitters joining the Disrupt 2025 agenda. They\u2019re here to deliver the insights that fuel startup growth and sharpen your edge. Don\u2019t miss the 20th anniversary of TechCrunch Disrupt, and a chance to learn from the top voices in tech \u2014 grab your ticket now and save up to $675 before prices rise.\nBut that idea comes with its own risks. Windsurf\u2019s co-founder and CEO, Varun Mohan, ultimately decided against the company building its own model \u2014 an expensive undertaking, the person said.\nIn addition, model makers are already competing directly. Anthropic offers Claude Code and OpenAI offers Codex, for instance.\nSelling the business was a strategic move to lock in a high return before it could be undermined by the very companies that supplied its AI, including OpenAI and Anthropic, which were also entering the AI coding market.\nMultiple people believe that the same pressure on margins Windsurf faced could be impacting Anysphere, the maker of Cursor, as well as vibe coders like Lovable, Replit, and others.\n\u201cMargins on all of the \u2018code gen\u2019 products are either neutral or negative. They\u2019re absolutely abysmal,\u201d said Nicholas Charriere, founder of Mocha, a vibe-coding startup and back-end hosting solution serving small and medium businesses (SMBs). He added that he believes the variable costs for all the startups in the sector are very close, likely within 10% to 15% of one another.\nUnlike Windsurf, Anysphere has been growing so fast that it intends to remain an independent company, having already turned down acquisition offers, including, reports say, from OpenAI.\nAnd Anysphere announced in January that it is attempting to build its own model, which could give it more control over its expenses. In July, the startup hired two leaders from Anthropic\u2019s Claude Code team, the Information reported, but two weeks later, these employees returned to work at Anthropic.\nIn addition to building a model, Anysphere could expect the cost of LLMs to decrease over time.\n\u201cThat\u2019s what everyone\u2019s banking on,\u201d said Erik Nordlander, a general partner at Google Ventures. \u201cThe inference cost today, that\u2019s the most expensive it\u2019s ever going to be.\u201d\nIt\u2019s not entirely clear how true that is. Rather than falling as expected, the cost of some of the latest AI models has risen, as they use more time and computational resources to handle complicated, multistep tasks.\nWhen that will change remains to be seen. On Thursday, for instance, OpenAI introduced a new flagship model, GPT-5, with fees that are significantly less than its competitor, Anthropic\u2019s Claude Opus 4.1. And Anysphere immediately offered this model as a choice for Cursor users.\nAnysphere has also recently changed its pricing structure to pass along the increased costs of running Anthropic\u2019s latest Claude model, particularly to its most active users. The move caught some of Cursor customers by surprise, since they didn\u2019t expect additional charges on top of its $20-per-month Pro plan. Anysphere CEO Michael Truell later apologized for unclear communication about the pricing change in a blog post.\nThis is the rock and the hard place. Although Cursor is one of the most popular AI applications, having reached $500 million in ARR in June, the company\u2019s user base may not be so loyal to the product if another company develops a tool that is superior to Cursor, investors say.\nAnysphere didn\u2019t respond to a request for comment.\nGiven the competitive landscape and the costs, Windsurf\u2019s decision to get out may prove to be understandable. After the OpenAI deal fell through, the founders and key employees left to join Google in a deal that led to a $2.4 billion payout to key shareholders. The remaining business then sold itself to Cognition.\nWhile many, including prominent VCs, criticized Mohan for leaving approximately 200 employees without roles at Google, a source familiar with the deal insisted the acquisition actually maximized the outcomes for all employees.\nBeyond Cursor, other AI coding tools are also among the fastest growing startups of the LLM generation, like Replit, Lovable, and Bolt, and all of them rely on model makers as well.\nAdditionally, if this extremely popular business sector, already generating hundreds of millions in revenue or more a year, has difficulty building on top of model makers, what might it mean for other, more nascent industries?"
    },
    {
      "url": "https://simonwillison.net/2025/Aug/7/gpt-5/",
      "text": "GPT-5: Key characteristics, pricing and model card\n7th August 2025\nI\u2019ve had preview access to the new GPT-5 model family for the past two weeks (see related video and my disclosures) and have been using GPT-5 as my daily-driver. It\u2019s my new favorite model. It\u2019s still an LLM\u2014it\u2019s not a dramatic departure from what we\u2019ve had before\u2014but it rarely screws up and generally feels competent or occasionally impressive at the kinds of things I like to use models for.\nI\u2019ve collected a lot of notes over the past two weeks, so I\u2019ve decided to break them up into a series of posts. This first one will cover key characteristics of the models, how they are priced and what we can learn from the GPT-5 system card.\n- Key model characteristics\n- Position in the OpenAI model family\n- Pricing is aggressively competitive\n- More notes from the system card\n- Prompt injection in the system card\n- Thinking traces in the API\n- And some SVGs of pelicans\nKey model characteristics\nLet\u2019s start with the fundamentals. GPT-5 in ChatGPT is a weird hybrid that switches between different models. Here\u2019s what the system card says about that (my highlights in bold):\nGPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say \u201cthink hard about this\u201d in the prompt). [...] Once usage limits are reached, a mini version of each model handles remaining queries. In the near future, we plan to integrate these capabilities into a single model.\nGPT-5 in the API is simpler: it\u2019s available as three models\u2014regular, mini and nano\u2014which can each be run at one of four reasoning levels: minimal (a new level not previously available for other OpenAI reasoning models), low, medium or high.\nThe models have an input limit of 272,000 tokens and an output limit (which includes invisible reasoning tokens) of 128,000 tokens. They support text and image for input, text only for output.\nI\u2019ve mainly explored full GPT-5. My verdict: it\u2019s just good at stuff. It doesn\u2019t feel like a dramatic leap ahead from other LLMs but it exudes competence\u2014it rarely messes up, and frequently impresses me. I\u2019ve found it to be a very sensible default for everything that I want to do. At no point have I found myself wanting to re-run a prompt against a different model to try and get a better result.\nHere are the OpenAI model pages for GPT-5, GPT-5 mini and GPT-5 nano. Knowledge cut-off is September 30th 2024 for GPT-5 and May 30th 2024 for GPT-5 mini and nano.\nPosition in the OpenAI model family\nThe three new GPT-5 models are clearly intended as a replacement for most of the rest of the OpenAI line-up. This table from the system card is useful, as it shows how they see the new models fitting in:\n| Previous model | GPT-5 model |\n|---|---|\n| GPT-4o | gpt-5-main |\n| GPT-4o-mini | gpt-5-main-mini |\n| OpenAI o3 | gpt-5-thinking |\n| OpenAI o4-mini | gpt-5-thinking-mini |\n| GPT-4.1-nano | gpt-5-thinking-nano |\n| OpenAI o3 Pro | gpt-5-thinking-pro |\nThat \u201cthinking-pro\u201d model is currently only available via ChatGPT where it is labelled as \u201cGPT-5 Pro\u201d and limited to the $200/month tier. It uses \u201cparallel test time compute\u201d.\nThe only capabilities not covered by GPT-5 are audio input/output and image generation. Those remain covered by models like GPT-4o Audio and GPT-4o Realtime and their mini variants and the GPT Image 1 and DALL-E image generation models.\nPricing is aggressively competitive\nThe pricing is aggressively competitive with other providers.\n- GPT-5: $1.25/million for input, $10/million for output\n- GPT-5 Mini: $0.25/m input, $2.00/m output\n- GPT-5 Nano: $0.05/m input, $0.40/m output\nGPT-5 is priced at half the input cost of GPT-4o, and maintains the same price for output. Those invisible reasoning tokens count as output tokens so you can expect most prompts to use more output tokens than their GPT-4o equivalent (unless you set reasoning effort to \u201cminimal\u201d).\nThe discount for token caching is significant too: 90% off on input tokens that have been used within the previous few minutes. This is particularly material if you are implementing a chat UI where the same conversation gets replayed every time the user adds another prompt to the sequence.\nHere\u2019s a comparison table I put together showing the new models alongside the most comparable models from OpenAI\u2019s competition:\n| Model | Input $/m | Output $/m |\n|---|---|---|\n| Claude Opus 4.1 | 15.00 | 75.00 |\n| Claude Sonnet 4 | 3.00 | 15.00 |\n| Grok 4 | 3.00 | 15.00 |\n| Gemini 2.5 Pro (>200,000) | 2.50 | 15.00 |\n| GPT-4o | 2.50 | 10.00 |\n| GPT-4.1 | 2.00 | 8.00 |\n| o3 | 2.00 | 8.00 |\n| Gemini 2.5 Pro (<200,000) | 1.25 | 10.00 |\n| GPT-5 | 1.25 | 10.00 |\n| o4-mini | 1.10 | 4.40 |\n| Claude 3.5 Haiku | 0.80 | 4.00 |\n| GPT-4.1 mini | 0.40 | 1.60 |\n| Gemini 2.5 Flash | 0.30 | 2.50 |\n| Grok 3 Mini | 0.30 | 0.50 |\n| GPT-5 Mini | 0.25 | 2.00 |\n| GPT-4o mini | 0.15 | 0.60 |\n| Gemini 2.5 Flash-Lite | 0.10 | 0.40 |\n| GPT-4.1 Nano | 0.10 | 0.40 |\n| Amazon Nova Lite | 0.06 | 0.24 |\n| GPT-5 Nano | 0.05 | 0.40 |\n| Amazon Nova Micro | 0.035 | 0.14 |\n(Here\u2019s a good example of a GPT-5 failure: I tried to get it to output that table sorted itself but it put Nova Micro as more expensive than GPT-5 Nano, so I prompted it to \u201cconstruct the table in Python and sort it there\u201d and that fixed the issue.)\nMore notes from the system card\nAs usual, the system card is vague on what went into the training data. Here\u2019s what it says:\nLike OpenAI\u2019s other models, the GPT-5 models were trained on diverse datasets, including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. [...] We use advanced data filtering processes to reduce personal information from training data.\nI found this section interesting, as it reveals that writing, code and health are three of the most common use-cases for ChatGPT. This explains why so much effort went into health-related questions, for both GPT-5 and the recently released OpenAI open weight models.\nWe\u2019ve made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, and have leveled up GPT-5\u2019s performance in three of ChatGPT\u2019s most common uses: writing, coding, and health. All of the GPT-5 models additionally feature safe-completions, our latest approach to safety training to prevent disallowed content.\nSafe-completions is later described like this:\nLarge language models such as those powering ChatGPT have traditionally been trained to either be as helpful as possible or outright refuse a user request, depending on whether the prompt is allowed by safety policy. [...] Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be completed safely at a high level, but may lead to malicious uplift if sufficiently detailed or actionable. As an alternative, we introduced safe- completions: a safety-training approach that centers on the safety of the assistant\u2019s output rather than a binary classification of the user\u2019s intent. Safe-completions seek to maximize helpfulness subject to the safety policy\u2019s constraints.\nSo instead of straight up refusals, we should expect GPT-5 to still provide an answer but moderate that answer to avoid it including \u201charmful\u201d content.\nOpenAI have a paper about this which I haven\u2019t read yet (I didn\u2019t get early access): From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.\nSycophancy gets a mention, unsurprising given their high profile disaster in April. They\u2019ve worked on this in the core model:\nSystem prompts, while easy to modify, have a more limited impact on model outputs relative to changes in post-training. For GPT-5, we post-trained our models to reduce sycophancy. Using conversations representative of production data, we evaluated model responses, then assigned a score reflecting the level of sycophancy, which was used as a reward signal in training.\nThey claim impressive reductions in hallucinations. In my own usage I\u2019ve not spotted a single hallucination yet, but that\u2019s been true for me for Claude 4 and o3 recently as well\u2014hallucination is so much less of a problem with this year\u2019s models.\nUpdate: I have had some reasonable pushback against this point, so I should clarify what I mean here. When I use the term \u201challucination\u201d I am talking about instances where the model confidently states a real-world fact that is untrue\u2014like the incorrect winner of a sporting event. I\u2019m not talking about the models making other kinds of mistakes\u2014they make mistakes all the time!\nSomeone pointed out that it\u2019s likely I\u2019m avoiding hallucinations through the way I use the models, and this is entirely correct: as an experienced LLM user I instinctively stay clear of prompts that are likely to trigger hallucinations, like asking a non-search-enabled model for URLs or paper citations. This means I\u2019m much less likely to encounter hallucinations in my daily usage.\nOne of our focuses when training the GPT-5 models was to reduce the frequency of factual hallucinations. While ChatGPT has browsing enabled by default, many API queries do not use browsing tools. Thus, we focused both on training our models to browse effectively for up-to-date information, and on reducing hallucinations when the models are relying on their own internal knowledge.\nThe section about deception also incorporates the thing where models sometimes pretend they\u2019ve completed a task that defeated them:\nWe placed gpt-5-thinking in a variety of tasks that were partly or entirely infeasible to accomplish, and rewarded the model for honestly admitting it can not complete the task. [...]\nIn tasks where the agent is required to use tools, such as a web browsing tool, in order to answer a user\u2019s query, previous models would hallucinate information when the tool was unreliable. We simulate this scenario by purposefully disabling the tools or by making them return error codes.\nPrompt injection in the system card\nThere\u2019s a section about prompt injection, but it\u2019s pretty weak sauce in my opinion.\nTwo external red-teaming groups conducted a two-week prompt-injection assessment targeting system-level vulnerabilities across ChatGPT\u2019s connectors and mitigations, rather than model-only behavior.\nHere\u2019s their chart showing how well the model scores against the rest of the field. It\u2019s an impressive result in comparison\u201456.8 attack success rate for gpt-5-thinking, where Claude 3.7 scores in the 60s (no Claude 4 results included here) and everything else is 70% plus:\nOn the one hand, a 56.8% attack rate is cleanly a big improvement against all of those other models.\nBut it\u2019s also a strong signal that prompt injection continues to be an unsolved problem! That means that more than half of those k=10 attacks (where the attacker was able to try up to ten times) got through.\nDon\u2019t assume prompt injection isn\u2019t going to be a problem for your application just because the models got better.\nThinking traces in the API\nI had initially thought that my biggest disappointment with GPT-5 was that there\u2019s no way to get at those thinking traces via the API... but that turned out not to be true. The following curl\ncommand demonstrates that the responses API \"reasoning\": {\"summary\": \"auto\"}\nis available for the new GPT-5 models:\ncurl https://api.openai.com/v1/responses \\\n-H \"Authorization: Bearer $(llm keys get openai)\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"gpt-5\",\n\"input\": \"Give me a one-sentence fun fact about octopuses.\",\n\"reasoning\": {\"summary\": \"auto\"}\n}'\nHere\u2019s the response from that API call.\nWithout that option the API will often provide a lengthy delay while the model burns through thinking tokens until you start getting back visible tokens for the final response.\nOpenAI offer a new reasoning_effort=minimal\noption which turns off most reasoning so that tokens start to stream back to you as quickly as possible.\nAnd some SVGs of pelicans\nNaturally I\u2019ve been running my \u201cGenerate an SVG of a pelican riding a bicycle\u201d benchmark. I\u2019ll actually spend more time on this in a future post\u2014I have some fun variants I\u2019ve been exploring\u2014but for the moment here\u2019s the pelican I got from GPT-5 running at its default \u201cmedium\u201d reasoning effort:\nIt\u2019s pretty great! Definitely recognizable as a pelican, and one of the best bicycles I\u2019ve seen yet.\nHere\u2019s GPT-5 mini:\nAnd GPT-5 nano:\nMore recent articles\n- OpenAI's new open weight (Apache 2) models are really good - 5th August 2025\n- ChatGPT agent's user-agent - 4th August 2025"
    }
  ],
  "argos_summary": "OpenAI has launched GPT-5, its latest flagship AI model, which CEO Sam Altman claims is the best in the world, although it shows only slight performance improvements over competitors like Anthropic and Google DeepMind on key benchmarks. The model is particularly effective for coding tasks and is priced competitively at $1.25 per million input tokens and $10 per million output tokens, undercutting rivals like Anthropic's Claude Opus 4.1. OpenAI's aggressive pricing strategy may signal the beginning of a price war in the AI model market, as competitors like Google and Anthropic may need to adjust their pricing in response.",
  "argos_id": "5FJKR479I"
}