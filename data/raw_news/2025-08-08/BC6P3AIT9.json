{
  "url": "https://www.techradar.com/ai-platforms-assistants/apple-intelligence/gpt-5-is-coming-to-apple-intelligence-soon-but-it-wont-solve-apples-biggest-ai-problem",
  "authorsByline": "Alex Blake",
  "articleId": "bf59283f46b245688e23c406455f3bb2",
  "source": {
    "domain": "techradar.com",
    "paywall": false,
    "location": {
      "country": "in",
      "city": "New Delhi",
      "coordinates": {
        "lat": 28.6138954,
        "lon": 77.2090057
      }
    }
  },
  "imageUrl": "https://cdn.mos.cms.futurecdn.net/RY8MTV54n5gyfz3FuafX6c.png",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-08T11:20:46+00:00",
  "addDate": "2025-08-08T11:27:48.501701+00:00",
  "refreshDate": "2025-08-08T11:27:48.501702+00:00",
  "score": 1.0,
  "title": "GPT-5 tipped to come to Apple Intelligence in iOS 26 \u2013 here\u2019s how it\u2019ll boost Apple\u2019s AI",
  "description": "New AI powers at your fingertips",
  "content": "\u2022 OpenAI\u2019s GPT-5 model will soon be available alongside Apple Intelligence\n\u2022 That means more powerful ChatGPT features on your Apple device\n\u2022 But this move may make life harder for Apple\u2019s rumored chatbot\n\nOpenAI has just released its latest GPT-5 artificial intelligence (AI) model, and it\u2019s set to bring a whole host of powerful new features to the ChatGPT chatbot. Thanks to Apple\u2019s integration of ChatGPT into Apple Intelligence, that could bring benefits to a wider set of people than merely ChatGPT users \u2013 but it also brings some serious risks to Apple.\n\nWhen might we see GPT-5 in Apple Intelligence? Well, news outlet 9to5Mac thinks it has the answer. The publication claims that Apple told it that GPT-5 will arrive on Apple devices with the release of iOS 26, iPadOS 26, and macOS Tahoe. That\u2019s due to happen in early September, meaning there\u2019s only about a month to wait before GPT-5 is directly incorporated into Apple\u2019s products.\n\nYou'll apparently be able to access GPT-5 in a few different ways on iPhones and Macs. The most obvious being when you use Siri for more complex questions that it can't answer, but also with Apple's Writing Tools and Visual Intelligence, which uses your iPhone's camera to help answer questions about what you're looking at.\n\nThe new features in GPT-5 include different personalities (including Cynic, Listener, Nerd, and Robot), fresh theming options, and an improved Voice Mode. Google users will also soon be able to connect their Gmail, Google Calendar, and Google Contacts accounts directly to ChatGPT.\n\nGPT-5 comes in a few different sizes, including the regular GPT-5 model, GPT-5 mini, and GPT-5 nano. These offer different levels of reasoning and are all available to free users. If you\u2019re paying $200 a month for ChatGPT Pro, you\u2019ll also get access to the deeper GPT-5 pro and GPT-5 thinking models, which take longer to respond but offer more insightful responses.\n\nThe wait goes on\n\nWhile ChatGPT can be used in conjunction with Apple Intelligence, you have to specifically invoke it. Most queries to Apple\u2019s AI will use Apple Intelligence, but if there\u2019s something that Apple\u2019s model can\u2019t answer, it will ask if you want to hand off to ChatGPT.\n\nRight now, relying on ChatGPT for more in-depth queries doesn\u2019t reflect particularly well on Apple Intelligence, but there are signs that Apple is working to correct that. The company is reportedly developing its own AI chatbot to rival ChatGPT and others (despite previously telling TechRadar \u201cthat was never the goal, and it remains not our primary goal\u201d).\n\nUnfortunately, we don\u2019t know when that chatbot will be ready, which could be why Apple is moving so swiftly to incorporate GPT-5 into Apple Intelligence: it might be an attempt to keep Apple\u2019s AI relevant while the company works on its own solution. But the longer we have to wait \u2013 and the more features OpenAI adds to ChatGPT \u2013 the higher the expectations will be on Apple\u2019s chatbot, and the harder it will be for Apple Intelligence to eventually usurp its rivals.\n\nYou might also like\n\u2022 OpenAI GPT-5 launch live \u2013 all the latest news as Sam Altman unveils the new model\n\u2022 GPT-5 is here \u2013 5 things you need to know about OpenAI\u2019s \u2018most useful\u2019 model yet\n\u2022 Apple\u2019s rumored ChatGPT rival could fix the thing I hate most about Siri \u2013 and it can\u2019t come soon enough",
  "medium": "Article",
  "links": [
    "https://www.techradar.com/computing/artificial-intelligence/this-is-what-really-happened-with-siri-and-apple-intelligence-according-to-apple",
    "https://www.techradar.com/ai-platforms-assistants/chatgpt/gpt-5-is-here-5-things-you-need-to-know-about-openais-most-useful-model-yet",
    "https://www.techradar.com/computing/artificial-intelligence/what-is-apple-intelligence-everything-you-need-to-know-about-the-ai-toolkit",
    "https://www.techradar.com/news/live/openai-chatgpt5-launch",
    "https://www.techradar.com/computing/mac-os/macos-tahoe",
    "https://www.techradar.com/ai-platforms-assistants/chatgpt/gpt-5-free-tier-3-cool-features-you-can-try-now-for-nothing",
    "https://www.techradar.com/ai-platforms-assistants/apple-intelligence/apples-rumored-chatgpt-rival-could-fix-the-thing-i-hate-most-about-siri-and-it-cant-come-soon-enough",
    "https://9to5mac.com/2025/08/07/apple-intelligence-gpt-5-chatgpt-integration/",
    "https://www.techradar.com/computing/artificial-intelligence/openai-really-helped-me-finish-my-to-do-list-when-it-launched-chatgpts-advanced-voice-mode-and-it-can-do-the-same-for-you",
    "https://www.techradar.com/tag/chatbot",
    "https://www.techradar.com/phones/ios/how-to-download-the-ios-26-public-beta",
    "https://www.techradar.com/tag/apple"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Apple Intelligence",
      "weight": 0.12796184
    },
    {
      "name": "GPT-5 thinking models",
      "weight": 0.1112444
    },
    {
      "name": "Apple devices",
      "weight": 0.11042112
    },
    {
      "name": "Apple",
      "weight": 0.11034885
    },
    {
      "name": "OpenAI GPT-5 launch",
      "weight": 0.10609501
    },
    {
      "name": "GPT-5",
      "weight": 0.10159998
    },
    {
      "name": "GPT-5 pro",
      "weight": 0.10011639
    },
    {
      "name": "GPT-5 mini",
      "weight": 0.098838456
    },
    {
      "name": "ChatGPT Pro",
      "weight": 0.08884
    },
    {
      "name": "ChatGPT",
      "weight": 0.087872535
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.97705078125
    },
    {
      "name": "/Computers & Electronics/Software/Operating Systems",
      "score": 0.89501953125
    },
    {
      "name": "/Internet & Telecom/Mobile & Wireless/Mobile Phones",
      "score": 0.73046875
    },
    {
      "name": "/Internet & Telecom/Mobile & Wireless/Mobile Apps & Add-Ons",
      "score": 0.35498046875
    }
  ],
  "sentiment": {
    "positive": 0.24829102,
    "negative": 0.15795898,
    "neutral": 0.59375
  },
  "summary": "OpenAI has released its latest GPT-5 artificial intelligence (AI) model, which is set to be integrated into Apple Intelligence in iOS 26, iPadOS 26, and macOS Tahoe. This could provide more powerful ChatGPT features on your Apple device, but could pose significant challenges to Apple's rumored chatbot. The integration could benefit a wide range of users, but also potentially hinder Apple's AI. GPT5 will be available on Apple devices with the release of iOS 26 in early September, giving Apple just about a month to wait before GPT is directly incorporated into Apple\u2019s products. The new features in GPT 5 include different personalities, improved Voice Mode, and Gmail, Calendar, and Contacts accounts directly linked to Chat GPT.",
  "shortSummary": "OpenAI\u2019s GPT-5 model will soon integrate ChatGPT features into Apple Intelligence, enhancing Apple's AI capabilities and potentially slowing down its upcoming chatbot.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "c3a8d7e822ee476ebcae3012850b27b1",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.techradar.com/tag/chatbot",
      "text": "Chatbot\nLatest about Chatbot\nIs your cloud hosting ready for AI GPU accelerators? Here are 5 things you need to know!\nBy Marty Puranik published\nChoosing the right cloud GPU hosting means looking beyond just raw power.\nThe next big thing in AI is agents, but is your data ready?\nBy Derek Slager published\nDespite growing popularity, AI agents are only as good as the data they run on.\nHarnessing the chaos: the strategic imperative of the Generative AI era\nBy Curt Raffi published\nA look at the infrastructure behind a compliant, scalable AI strategy.\nAre you a browser or a builder? Why AI\u2019s real disruption demands a different mindset\nBy Ankit Sinha published\nAI agents are just the beginning\u2014true transformation starts when businesses rebuild from the ground up.\nAgents are advancing, fast. Security needs to keep pace\nBy Suja Viswesan published\nAgent adoption is racing ahead, but we can't afford to let innovation outpace security and governance.\nDOGE employee leaks private xAI API key from sensitive database\nBy Ellen Jennings-Trace published\nThe staffer had access to millions of Americans\u2019 personal data.\nAI \u2018wingman\u2019 app leaks 160,000 screenshots of private chats - here's what we know\nBy Ellen Jennings-Trace published\nFlirty chats have been exposed after leak at AI dating advice chatbot.\nBrainpower unleashed: agentic AI and beyond bots\nBy Arvind Rao published\nAgentic AI moves beyond isolated bots, orchestrating intelligence to unlock unprecedented scale, resilience, and adaptability for a digital future.\nSign up for breaking news, reviews, opinion, top tech deals, and more."
    },
    {
      "url": "https://www.techradar.com/ai-platforms-assistants/chatgpt/gpt-5-free-tier-3-cool-features-you-can-try-now-for-nothing",
      "text": "GPT-5 free tier \u2013 3 cool features you can try now for nothing\nThe best thing about ChatGPT-5 is that you can use it for free\n- Access to ChatGPT-5 is being given to all ChatGPT users\n- You get new features like ChatGPT Voice and better writing\n- ChatGPT-5 access is rolling out right now\nAt today\u2019s ChatGPT-5 launch event, we got to see exactly why OpenAI is so excited about the new version of ChatGPT. It\u2019s faster, more intuitive, less likely to hallucinate, and can integrate with your Gmail in even more useful ways to know more about you.\nIn fact, at one point, the OpenAI presenters actually got ChatGPT-5 to write a eulogy to the previous ChatGPT-4o model as a demonstration about how much better its writing capabilities had become.\nThe eulogy was warranted because OpenAI isn\u2019t restricting access to ChatGPT-5 to Plus and Pro subscribers \u2013 it is giving it out to free tier users too, so everybody now gets access to a better ChatGPT. That effectively means the end for ChatGPT-4o.\nThe vast majority of today\u2019s presentation was about how much better ChatGPT-5 was at writing code than previous versions, and also how much better it was at enterprise applications, like law, health care, education, and finance.\nOf course, most of us simply use ChatGPT to help us get things done in our daily lives, but there were plenty of improvements that are worth noting for everyday users, too, and they\u2019re mostly available in the free tiers.\nLet\u2019s take a look at what you get access to without paying.\n1. Writing improvements\nAs with GPT\u20114o, the difference between free and paid access to GPT\u20115 is usage volume. At some point on the free tier, your access to GPT-5 will reach the usage limit, and then you\u2019ll be kicked back to GPT-5 mini.\nPlus subscribers will get significantly higher usage volume before that happens. That means you\u2019ll get access to the improved writing ability in GPT-5. You\u2019ll find that its prose has a lot more rhythm and beat to it than the prose in GPT-4o did.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\n2. ChatGPT Voice\nWith the free tier, you still only get limited access to ChatGPT-5\u2019s voice mode (along with file uploads, image creation, and data analysis), but you should get more access per day now than you did before.\nWhat used to be called 'Advanced voice mode' is now called 'ChatGPT voice'. OpenAI says that \u201cStandard Voice Mode retires on September 9, 2025, unifying all users on ChatGPT Voice\u201d.\nChatGPT Voice is better than before, with more natural-sounding conversations. Hit the voice button on the mobile app for ChatGPT to give it a go right now.\n3. New highlight colors\nYou\u2019ve always been able to change the theme of ChatGPT from light to dark to System, but now you can add accent colors to your chats in the Settings. These apply to elements in ChatGPT-5 like conversation bubbles and highlighted text.\nThis is the first time color has been used in the ChatGPT interface, and it takes some getting used to, but I think this is a feature you\u2019ll come to enjoy.\nWhat you don\u2019t get\nChatGPT-5 has better memory, reduces hallucinations, and as a free user, you\u2019ll get all that too. So what don\u2019t you get?\nWell, as before, access to the Sora AI video generator is restricted to Plus and Pro users, as is ChatGPT Agent. And in terms of new models, the super-powerful ChatGPT-5 Pro is only available to Pro users.\nBut I\u2019m still suck on GPT-4o?\nIf you\u2019re frantically hitting the refresh button on your ChatGPT page waiting for access to GPT-5 then you\u2019re not alone right now, but it is on the way.\nA message on the OpenAI website says: \u201cWe are gradually rolling out GPT-5 to ensure stability during launch. Some users may not yet see GPT-5 in their account as we increase availability in stages.\u201d\nYou might also like\nGraham is the Senior Editor for AI at TechRadar. With over 25 years of experience in both online and print journalism, Graham has worked for various market-leading tech brands including Computeractive, PC Pro, iMore, MacFormat, Mac|Life, Maximum PC, and more. He specializes in reporting on everything to do with AI and has appeared on BBC TV shows like BBC One Breakfast and on Radio 4 commenting on the latest trends in tech. Graham has an honors degree in Computer Science and spends his spare time podcasting and blogging.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/computing/artificial-intelligence/openai-really-helped-me-finish-my-to-do-list-when-it-launched-chatgpts-advanced-voice-mode-and-it-can-do-the-same-for-you",
      "text": "OpenAI really helped me finish my to-do list when it launched ChatGPT's Advanced Voice Mode, and it can do the same for you\n5 tips for checking off your daily tasks\nI was skeptical when OpenAI announced ChatGPT's Advanced Voice Mode. It was one thing to be able to talk and have my words transcribed for the text conversation, but to engage in a back-and-forth chat with the AI chatbot and not be able to see what it was writing as it spoke felt unnecessary and a little unnerving.\nI've since changed my mind. Using Advanced Voice Mode isn't just dictating what you would normally type; the AI genuinely changes its behavior to mimic human conversation better. It's possible to interrupt the AI and even change your mind mid-sentence when using AV Mode. And the voice you hear is a long way from the robotic tones of even a few years ago, especially after some recent upgrades to the tone and rhythm of how ChatGPT speaks.\nWhen you're using it to help you organize your life and get through chores, it's much more like talking to a human assistant on the phone than I would have once believed. And while it can't physically help out, it can create digital records that can last well beyond the conversation's end.\nHere are some tricks I've found for getting the most out of Advanced Voice Mode that you might not have thought of.\n1. Don't overthink it\nAI chatbots are not human, but they are improving at mimicking human interactions. That includes getting motivated and figuring out how to start something.\nSo, say you want to clean your kitchen. You can put ChatGPT in AV Mode and tell it that you want to start cleaning your kitchen, but you're unsure where to begin. The AI could then start guiding you by saying, \"How about you start by clearing off the counters?\" And once you\u2019ve cleared the counter, it\u2019s easier to keep going.\nYou can then say, \u201cOkay, what\u2019s next?\u201d and it will suggest actions, like wiping surfaces or loading the dishwasher. The point is that you don't have to do a lot of planning \u2013 you simply have a chat with ChatGPT and figure it out as you go.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nYou can even describe your kitchen or share photos, and it will help you go through the whole room until it's sparkling clean. The idea is to use Advanced Voice Mode not to plan the entire project, but to reduce inertia by vocalizing a specific and achievable first move. It\u2019s less about perfect productivity and more about interrupting the avoidance routine with a verbal nudge.\n2. Ask for micro-missions\nRelatedly, even if you do plan ahead, putting \"Clean the house\" on your to-do list is not exactly a single activity. Figuring out all those sub-sections is exhausting to me. But now I let Voice Mode help me divide everything into bite-sized missions that feel achievable, even when the bigger task looms like a mountain.\nIf I want to plan a dinner party, there are numerous moving pieces. I tell ChatGPT that I'm in AV Mode, and the AI will not only start making lists of tasks for every aspect but also explain what it's doing and allow me to add things that I think of based on what it comes up with.\nIt might have \"Come up with a menu\" as a task, with each course as a sub-section, to which I might add, \"Oh, and make sure I have that ready by a couple of days before so I can go shopping in time.\" After the initial lists are made, you can get started with the first, and, when it's done, ask \u201cWhat\u2019s next?\u201d and move naturally from one task to the next.\n3. Timely reminders\nOne of the most unexpectedly helpful ways I\u2019ve used ChatGPT\u2019s voice mode is to run focused work sprints. I\u2019m talking about those relatively brief sessions for fully concentrating on a single task where you need both accountability and structure, but don\u2019t want to stop what you\u2019re doing to mess with a timer app or check your phone.\nHere's how to do it: while standing in front of a messy drawer or prepping to write that awkward birthday card, I\u2019ll say, \u201cChatGPT, I'm going to organize these drawers but don't want to spend more than 20 minutes on it so keep me focused and let me know when it's almost up so I can wrap up whatever bit I'm doing then. It\u2019s like the Pomodoro Technique with AI instead of tomatoes.\nYou can also pair the timer with micro\u2011goals: \u201cChatGPT, I want to clean this entire drawer in the next 20 minutes. Start the timer and prompt me to switch sections every 5.\u201d You\u2019ll hear a friendly verbal nudge to help keep you moving. You can even stack sprints: \u201cAfter this one, let\u2019s do 10 minutes on folding my laundry.\u201d The key is you don\u2019t have to look at a screen or lose momentum, just keep your hands busy and let the voice mode keep you on track.\n4. The rolling to-do list\nI've casually implied I make a lot of to-do lists before I start any task. I wish that were true, but I'm what would charitably be called dynamic and accurately called chaotic in my approach. Using Advanced Voice Mode works with that scatter-brained way of organizing, though. It can provide a running commentary on your mental tabs and have it all in one place whenever I need to review it.\nYou can do the same. Whenever something pops into your head, simply tell ChatGPT, \u201cAdd that to my to-do list\u201d or \u201cMake a note: order more baby wipes.\u201d ChatGPT will acknowledge the item and log it, whether you're sweeping the floor or walking the dog. To keep things organized, you can also categorize the activities. \u201cAdd \u2018schedule vet appointment\u2019 to my errands list,\u201d or \u201cAdd \u2018email Liz\u2019 to my work list,\u201d just whenever it comes to mind. The model can segment by list name, so you can later say, \u201cWhat\u2019s on my errands list today?\u201d and it\u2019ll rattle off only those relevant tasks.\nIt's great because when you talk to Advanced Voice Mode for all these things, you can check into it as more than just random reminders in a notes app. At the end of the day, you can ask, \u201cWhat did I add to my to-do list today?\u201d and hear a replay of your mental to\u2011do arc. That not only jogs your memory, but lets you follow up with, \u201cGreat, now move those to tomorrow\u2019s list,\u201d or \u201cDelete the ones I finished.\u201d\n5. Make your spreadsheets\nThose records are great even when you're done with Advanced Voice Mode. Because it's ChatGPT, it can set up spreadsheets and documents and update them based on your conversation simply by narrating what you\u2019re doing.\nFor instance, I recently reorganized my fridge and freezer. I asked ChatGPT in AV Mode to track where I'm storing food and to create a spreadsheet listing the food, organized by the shelf and drawer it is stored in. I also noted things like expiration dates where relevant and had those added to the list. Then I could just describe the items I put away as I did it, saying \"Milk on shelf two, expiring in six weeks.\nI could update it in real time, too, if I needed to have an item already placed moved to a different shelf. Then the sheet was ready to export and could be shared with my family, so we all know not to buy frozen peas for a while and where the mustard my wife likes is located.\nYou might also like\nEric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI\u2019s ChatGPT, Anthropic\u2019s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/ai-platforms-assistants/apple-intelligence/apples-rumored-chatgpt-rival-could-fix-the-thing-i-hate-most-about-siri-and-it-cant-come-soon-enough",
      "text": "Apple\u2019s rumored ChatGPT rival could fix the thing I hate most about Siri \u2013 and it can\u2019t come soon enough\nWe demand answers\nIt\u2019s pretty much common knowledge by this point that Siri is lacking compared to rival assistants and artificial intelligence (AI) tools like ChatGPT. But according to a new report, Apple might be about to inject it with some much-needed power that could potentially transform it into a true force in AI.\nIn the latest edition of his Power On newsletter, Bloomberg journalist Mark Gurman claims that Apple is essentially building its own ChatGPT rival that would bring chatbot capabilities to Siri.\nThe company has reportedly created its own 'Answers, Knowledge and Information' team to do this \u2013 and if it works, it could fix one of the biggest gripes I have with Siri. Too often, I ask Siri a question, only for it to bring up a search engine results page and cheerily respond with \u201cHere\u2019s what I found on the web.\u201d\nOther times, it\u2019ll suggest handing over to ChatGPT instead of answering the question itself. For example, just try asking Siri a relatively simple question \u2013 \u201cwhat is the most common type of tree in North America?\u201d, for instance \u2013 and it\u2019ll ask if you want to enlist ChatGPT for this task.\nBoth outcomes highlight how incapable Siri is compared to AI tools like ChatGPT. It\u2019s like Siri is hoisting the white flag. \u201cI don\u2019t know what the answer is, but maybe you can find it yourself?\u201d seems to be the message it\u2019s conveying.\nAnd there\u2019s another aspect to this. Whereas ChatGPT can actually find the answer for me, Siri\u2019s response is the equivalent of those sarcastic \u201clet me Google that for you\u201d web pages that rudely hint at your own stupidity for not simply searching for the answer yourself. That\u2019s never a good look for a personal assistant like Siri.\nBut the bigger problem is that handing off to Google or ChatGPT doesn\u2019t actually solve the problem I wanted a solution for in the first place, and it misses the entire reason why people ask Siri questions in the first place.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nPeople ask Siri because they want answers, not because they want to do the heavy lifting themselves. If the outcome I\u2019m looking for is a Google search results page, I could have opened Safari and done that myself. I wanted an answer, not more hunting around.\nAnd that\u2019s the key difference between tools like ChatGPT and Siri right now: ChatGPT provides answers, while Siri does not. Even when Siri ropes ChatGPT into the situation, it\u2019s an extra step compared to Siri just giving you the result you wanted. Whatever the question, Siri is rarely the answer.\nA shot in the arm\nHopefully, Apple adding these \u201canswer\u201d capabilities to Siri could be the shot in the arm its virtual assistant desperately needs. That prospect is certainly more exciting to me than some of the other delayed Siri features, like its ability to work within apps or understand your personal context.\nWhile those features are interesting and might grab the headlines, I don\u2019t see myself using them very often. They\u2019re more complex and specialized, whereas asking Siri a simple question is a much more common occurrence for me and, I suspect, the majority of Apple users.\nStill, questions remain over how Apple will transform Siri into a proper answer engine. How will the company train Siri to be able to answer more questions? Will Siri simply search online for results, or will it be able to rely on stored knowledge? How will Siri be trained in a way that aligns with Apple\u2019s much-vaunted privacy principles? And will it take away traffic from third-party websites, withering the web in the way ChatGPT and Google\u2019s own AI overviews are threatening to do?\nAnd then there\u2019s the question of when Siri\u2019s chatbot update will arrive. Apple is reportedly targeting a spring 2026 launch date for its next-generation Siri features, but that hasn\u2019t previously included any mention of a chatbot, which could take a long time to develop. With AI developments moving at breakneck speed, will a Siri-based chatbot be able to keep up with its rivals?\nThose questions will be answered in due course, but in the meantime, I\u2019m looking forward to a more capable (and, frankly, less useless) virtual assistant. If Apple can offer these features in a way that respects your privacy and doesn\u2019t rip off the world\u2019s online content, then I won\u2019t mind if Siri is a little less capable than ChatGPT. As long as it\u2019s more capable than it is right now.\nAfter all, the whole point of using an AI helper for search is to get actual answers, not a list of results that I could have searched for myself. If Apple can pull it off, this could be the upgrade Siri has been in dire need of for years.\nYou might also like\nAlex Blake has been fooling around with computers since the early 1990s, and since that time he's learned a thing or two about tech. No more than two things, though. That's all his brain can hold. As well as TechRadar, Alex writes for iMore, Digital Trends and Creative Bloq, among others. He was previously commissioning editor at MacFormat magazine. That means he mostly covers the world of Apple and its latest products, but also Windows, computer peripherals, mobile apps, and much more beyond. When not writing, you can find him hiking the English countryside and gaming on his PC.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/ai-platforms-assistants/chatgpt/gpt-5-is-here-5-things-you-need-to-know-about-openais-most-useful-model-yet",
      "text": "GPT-5 is here \u2013 5 things you need to know about OpenAI\u2019s \u2018most useful\u2019 model yet\nSam Altman shows off the model he claims also scares him\nOpenAI\u2019s much\u2011anticipated livestream reveal of GPT\u20115 crammed a lot into about an hour and a half of announcements and demonstrations. CEO Sam Altman didn't show any of the fear he claimed to feel about the new AI model, just a lot of pride at what he and his team had accomplished.\nMuch of the presentation showed off GPT-5's technical milestones and how they translate into powerful new and upgraded AI features for users. Altman also had some lighter fare to unveil for ChatGPT, offering more customization options and ways for users to link their existing online footprint to ChatGPT.\nHere are the five most notable pieces to emerge from GPT-5's debut.\nGPT-5 arrives\nGPT-5 is the next iteration of OpenAI's models, bigger and more powerful, but not dissimilar in its basic form. Even so, GPT-5 is big and complex enough to reach a new level in how it seems to reason.\nEssentially, you no longer have to spoon-feed it context or restate complex prompts three times, or at least not nearly as often. Multifaceted questions like how changing interest rates might affect Gen Z homeownership trends in mixed markets might take several prompts refined multiple times to provide the answers you seek with earlier ChatGPT models, but GPT\u20115 can unpack the whole thing.\nBased on the demonstrations, GPT\u20115 seems to parse each part separately and stitch it together. And it flags when there's a gap in its knowledge, which is far better than confidently hallucinating. It applies that way of thinking to how it interacts with users, too. While obviously not 'thinking', it does appear to read between the lines well enough to reflect a user's mood and even adjust its response to an expressed emotion.\nThe model is supposed to be particularly good with math and coding software. Good enough to handle the increasingly popular pursuit of 'vibe coding,' where you simply describe a feeling or a mood of a piece of software, and the AI produces the HTML, CSS, and JavaScript to match your design vision.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nGPT-5 sizes\nGPT-5 comes in many sizes in addition to the standard version. There's also the smaller gpt-5-mini, and an even leaner gpt-5-nano, which lives solely in the API. The big news is that free ChatGPT users now get access to both GPT-5 and mini, while ChatGPT Plus subscribers enjoy higher usage limits across the board.\nIf you\u2019re a Pro user paying $200 a month, you\u2019ll now get unlimited GPT-5 access, along with access to the more powerful gpt-5-pro model and gpt-5-thinking. These both take longer to provide answers, but come back with deeper, more thoughtful responses.\nThere is no need to pick and choose yourself, either, though. ChatGPT now picks the right model automatically based on what you're asking and what plan you\u2019re on.\nChatGPT custom personalities and colors\nChatGPT has a default, pleasantly bland personality, but GPT\u20115 is advanced enough to offer more variety in the tone and style of the AI chatbot. If you don't want the usual neutral mode, you can choose \u201cCynic\u201d for sarcasm with your answer, \u201cListener\u201d if you\u2019re venting and just need it to echo back understanding, \"Nerd\" for a side of geeky trivia, and \"Robot\" for the purely mechanical response.\nThese personalities don\u2019t undercut the answers you get, but they do flavor the response. Therefore, you might get dry wit with productivity tips from the \u201cCynic\u201d tone or gentle encouragement in your goals from \u201cListener.\u201d\nAdditionally, the chatbot's appearance can now be altered with the new color themes. If you're a paid subscriber, you'll soon be able to adjust the look of ChatGPT instead of switching between the usual black or white.\nVoice Mode\nChatGPT's Voice Mode is getting an audio glow-up of its own. OpenAI is rolling out a much-improved version that not only works with custom GPTs but also adapts its tone and speech style based on your instructions and overall vibe.\nYou can ask it to be snappier, slower, warmer, or whatever else you want. For ChatGPT Plus users, voice responses are now nearly unlimited. Free users still get access, too, with a few hours a day to chat hands-free.\nTo streamline things, the old Standard Voice Mode is being phased out entirely within 30 days. After that, everyone will be on the same upgraded experience.\nGoogle connections for ChatGPT\nNext week, ChatGPT Pro users will be able to hook up their Gmail, Google Calendar, and Google Contacts directly to ChatGPT. That means no more switching tabs to check if you're free next Tuesday or digging through threads to find that one email you definitely forgot to reply to.\nOnce connected, ChatGPT will pull in what it needs to help respond to your queries. OpenAI assured users that it will only pull in the minimum needed and only when it\u2019s helpful.\nYou don\u2019t need to say \u201ccheck my calendar\u201d or \u201cpull that contact.\u201d The AI will do so based on whether you request something that requires it, like scheduling a meeting. It will pick a time that works for you and write the email on your behalf. Other subscription tiers are scheduled to get access to the connections in the near future, so this won't be limited to Pro forever.\nAll of these upgrades leveraging GPT-5 point to OpenAI's bigger plans to make its AI models an intimate part of your life, not just a tool you occasionally turn to and feel annoyed about having to carefully parse answers. Smarter reasoning means less cleanup for the user. Vibe coding shifts AI from merely aping code to interpreting your intended use with software. The personalities and colors make the AI feel like it's more unique to you, not just a one-size-fits-all tool, especially with the more realistic voice and access to your email account and calendar\nGPT\u20115 mimics awareness better than any of its predecessors. That means it could blend into our routines and become as second nature to use as our smartphones. Or at least, that's what OpenAI and its investors likely hope to see happen.\nYou might also like\n- OpenAI's first AI Agent is here, and Operator can make a dinner reservation and complete other tasks on the web for you\n- There\u2019s a new AI agent ready to browse the web and fill in forms without the need to touch your mouse\n- I tried this new AI agent that takes control of your mouse and keyboard to help get tasks done, and I can\u2019t believe how good it is\nEric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI\u2019s ChatGPT, Anthropic\u2019s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/computing/artificial-intelligence/this-is-what-really-happened-with-siri-and-apple-intelligence-according-to-apple",
      "text": "This is what really happened with Siri and Apple Intelligence, according to Apple\nIs it a delay, a strategy... or something else?\nThere's no denying that Apple's Siri digital chatbot didn't exactly hold a place of honor at this year's WWDC 2025 keynote. Apple mentioned it, and reiterated that it was taking longer than it had anticipated to bring everyone the Siri it promised a year ago, saying the full Apple Integration would arrive \"in the coming year.\"\nApple has since confirmed this means 2026. That means we won't be seeing the kind of deep integration that would have let Siri use what it knew about you and your iOS-running iPhone to become a better digital companion in 2025. It won't, as part of the just-announced iOS 26, use app intents to understand what's happening on the screen and take action on your behalf based on that.\nI have my theories about the reason for the delay, most of which revolve around the tension between delivering a rich AI experience and Apple's core principles regarding privacy. They often seem at cross purposes. This, though, is guesswork. Only Apple can tell us exactly what's going on \u2013 and now they have.\nI, along with Tom's Guide Global Editor-in-Chief Mark Spoonauer, sat down shortly after the keynote with Apple's Senior Vice President of Software Engineering Craig Federighi and Apple Global VP of Marketing Greg Joswiak for a wide-ranging podcast discussion about virtually everything Apple unveiled during its 90-minute keynote.\nWe started by asking Federighi about what Apple delivered regarding Apple Intelligence, as well as the status of Siri, and what iPhone users might expect this year or next. Federighi was surprisingly transparent, offering a window into Apple's strategic thinking when it comes to Apple Intelligence, Siri, and AI.\nFar from nothing\nFederighi started by walking us through all that Apple has delivered with Apple Intelligence thus far, and, to be fair, it's a considerable amount\n\"We were very focused on creating a broad platform for really integrated personal experiences into the OS.\" recalled Federighi, referring to the original Apple Intelligence announcement at WWDC 2024.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nAt the time, Apple demonstrated Writing Tools, summarizations, notifications, movie memories, semantic search of the Photos library, and Clean Up for photos. It delivered on all those features, but even as Apple was building those tools, it recognized, Federighi told us, that \"we could, on that foundation of large language models on device, private cloud compute as a foundation for even more intelligence, [and] semantic indexing on device to retrieve keep knowledge, build a better Siri.\"\nOver-confidence?\nA year ago, Apple's confidence in its ability to build such a Siri led it to demonstrate a platform that could handle more conversational context, mispeaking, Type to Siri, and a significantly redesigned UI. Again, all things Apple delivered.\n\"We also talked about [...] things like being able to invoke a broader range of actions across your device by app intents being orchestrated by Siri to let it do more things,\" added Federighi. \"We also talked about the ability to use personal knowledge from that semantic index so if you ask for things like, \"What's that podcast, that 'Joz' sent me?' that we could find it, whether it was in your messages or in your email, and call it out, and then maybe even act on it using those app intents. That piece is the piece that we have not delivered, yet.\"\nThis is known history. Apple overpromised and underdelivered, failing to deliver a vaguely promised end-of-year Apple Intelligence Siri update in 2024 and admitting by spring 2025 that it would not be ready any time soon. As to why it happened, it's been, up to now, a bit of a mystery. Apple is not in the habit of demonstrating technology or products that it does not know for certain that it will be able to deliver on schedule.\nFederighi, however, explained in some detail where things went awry, and how Apple progresses from here.\n\"We found that when we were developing this feature that we had, really, two phases, two versions of the ultimate architecture that we were going to create,\" he explained. \"Version one we had working here at the time that we were getting close to the conference, and had, at the time, high confidence that we could deliver it. We thought by December, and if not, we figured by spring, until we announced it as part of WWDC. Because we knew the world wanted a really complete picture of, 'What's Apple thinking about the implications of Apple intelligence and where is it going?'\"\nA tale of two architectures\nAs Apple was working on a V1 of the Siri architecture, it was also working on what Federighi called V2, \"a deeper end-to-end architecture that we knew was ultimately what we wanted to create, to get to a full set of capabilities that we wanted for Siri.\"\nWhat everyone saw during WWDC 2024 were videos of that V1 architecture, and that was the foundation for work that began in earnest after the WWDC 2024 reveal, in preparation for the full Apple Intelligence Siri launch.\n\"We set about for months, making it work better and better across more app intents, better and better for doing search,\" Federighi added. \"But fundamentally, we found that the limitations of the V1 architecture weren't getting us to the quality level that we knew our customers needed and expected. We realized that V1 architecture, you know, we could push and push and push and put in more time, but if we tried to push that out in the state it was going to be in, it would not meet our customer expectations or Apple standards, and that we had to move to the V2 architecture.\n\"As soon as we realized that, and that was during the spring, we let the world know that we weren't going to be able to put that out, and we were going to keep working on really shifting to the new architecture and releasing something.\"\nThat switch, though, and what Apple learned along the way, meant that Apple would not make the same mistake again, and promise a new Siri for a date that it could not guarantee to hit. Instead. Apple won't \"precommunicate a date,\" explained Federighi, \"until we have in-house, the V2 architecture delivering not just in a form that we can demonstrate for you all\u2026\"\nHe then joked that, while, actually, he \"could\" demonstrate a working V2 model, he was not going to do it. Then he added, more seriously, \"We have, you know, the V2 architecture, of course, working in-house, but we're not yet to the point where it's delivering at the quality level that I think makes it a great Apple feature, and so we're not announcing the date for when that's happening. We will announce the date when we're ready to seed it, and you're all ready to be able to experience it.\"\nI asked Federighi if, by V2 architecture, he was talking about a wholesale rebuilding of Siri, but Federighi disabused me of that notion.\n\"I should say the V2 architecture is not, it wasn't a star-over. The V1 architecture was sort of half of the V2 architecture, and now we extend it across, sort of make it a pure architecture that extends across the entire Siri experience. So we've been very much building up upon what we have been building for V1, but now extending it more completely, and that more homogeneous end-to-end architecture gives us much higher quality and much better capability. And so that's what we're building now.\"\nA different AI strategy\nSome might view Apple's failure to deliver the full Siri on its original schedule as a strategic stumble. But Apple's approach to AI and product is also utterly different than that of OpenAI or Google Gemini. It does not revolve around a singular product or a powerful chatbot. Siri is not necessarily the centerpiece we all imagined.\nFederighi doesn't dispute that \"AI is this transformational technology [\u2026] All that's growing out of this architecture is going to have decades-long impact across the industry and the economy, and much like the internet, much like mobility, and it's going to touch Apple's products and it's going to touch experiences that are well outside of Apple products.\"\nApple clearly wants to be part of this revolution, but on its terms and in ways that most benefit its users while, of course, protecting their privacy. Siri, though, was never the end game, as Federighi explained.\n\"When we started with Apple Intelligence, we were very clear: this wasn't about just building a chatbot. So, seemingly, when some of these Siri capabilities I mentioned didn't show up, people were like, 'What happened, Apple? I thought you were going to give us your chatbot. That was never the goal, and it remains not our primary goal.\"\nSo what is the goal? I think it may be fairly obvious from the WWDC 2025 keynote. Apple is intent on integrating Apple Intelligence across all its platforms. Instead of heading over to a singular app like ChatGPT for your AI needs, Apple's putting it, in a way, everywhere. It's done, Federighi explains, \"in a way that meets you where you are, not that you're going off to some chat experience in order to get things done.\"\nApple understands the allure of conversational bots. \"I know a lot of people find it to be a really powerful way to gather their thoughts, brainstorm [...] So, sure, these are great things,\" Federighi says. \"Are they the most important thing for Apple to develop? Well, time will tell where we go there, but that's not the main thing we set out to do at this time.\"\nSee below for the full interview with Federighi and Joswiak.\nYou might also like\nA 38-year industry veteran and award-winning journalist, Lance has covered technology since PCs were the size of suitcases and \u201con line\u201d meant \u201cwaiting.\u201d He\u2019s a former Lifewire Editor-in-Chief, Mashable Editor-in-Chief, and, before that, Editor in Chief of PCMag.com and Senior Vice President of Content for Ziff Davis, Inc. He also wrote a popular, weekly tech column for Medium called The Upgrade.\nLance Ulanoff makes frequent appearances on national, international, and local news programs including Live with Kelly and Mark, the Today Show, Good Morning America, CNBC, CNN, and the BBC.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://9to5mac.com/2025/08/07/apple-intelligence-gpt-5-chatgpt-integration/",
      "text": "Earlier today, OpenAI announced GPT-5, its latest model to power ChatGPT. The new frontier model is a significant leap forward in a lot of areas. But when will ChatGPT integration within Apple Intelligence adopt it? That\u2019s actually happening rather soon\u2026\nUsing ChatGPT through Apple Intelligence is optional. If you allow Apple Intelligence to work with ChatGPT, it offers three things. These include:\n- Use Siri to access ChatGPT: Siri can tap into ChatGPT to provide answers when that might be helpful for certain requests including questions about photos and documents.\n- Use ChatGPT with Writing Tools: ChatGPT can compose text or images from just a description.\n- Use ChatGPT with visual intelligence: Use visual intelligence with Camera Control to quickly learn more about the places and objects around you.\nIn iOS 18, iPadOS 18, macOS Sequoia, and later visionOS 2, ChatGPT integration within Apple Intelligence is powered by OpenAI\u2019s GPT-4o model.\nFollowing today\u2019s reveal of OpenAI\u2019s latest model, Apple tells me that ChatGPT integration within Apple Intelligence will use GPT-5 with iOS 26, iPadOS 26, and macOS Tahoe 26.\nThose software updates are expected to arrive next month, meaning ChatGPT integration within Apple Intelligence will benefit from the new model soon.\nAs a reminder, Apple includes privacy protections for users accessing ChatGPT through Apple Intelligence. These include obscuring IP addresses and ensuring that OpenAI doesn\u2019t store requests. If you decide to connect your OpenAI account with Apple Intelligence, then OpenAI\u2019s data-use policy applies.\nIn iOS 26, Apple Intelligence is gaining new capabilities across the iPhone and beyond. That includes Live Translation, which can interpret conversations in FaceTime, Phone, and Messages in real time, and upgrades to Visual Intelligence, a systemwide tool for searching and interacting with content directly on the screen.\nApple is also opening up its on-device foundation model to developers, paving the way for smarter, more capable apps built on the same technology that powers Apple Intelligence.\nIn addition to GPT-5, OpenAI announced two open weight large language models, including one that runs on Apple silicon Macs, and changes to encourage healthier ChatGPT usage.\nCheck out these great accessories\nFTC: We use income earning auto affiliate links. More.\nComments"
    },
    {
      "url": "https://www.techradar.com/computing/artificial-intelligence/what-is-apple-intelligence-everything-you-need-to-know-about-the-ai-toolkit",
      "text": "What is Apple Intelligence: everything you need to know about the AI toolkit\nHow to get AI assistance on your iPhone, iPad and Mac\nApple Intelligence is the Cupertino company\u2019s take on artificial intelligence. It\u2019s a suite of tools deeply integrated into compatible iPhone, iPad and Mac devices.\nThe idea behind Apple Intelligence is to make life easier for Apple users, using AI to add value through suggestions, automations and personalization. That includes everything from Writing Tools to message summaries.\nNew AI-powered features are being released in stages. With more set to come in 2025, here\u2019s what you need to know about Apple Intelligence.\nThis article was correct as of February 2025. AI tools are updated regularly and it is possible that some features have changed since this article was written. Some features may also only be available in certain countries.\nWhat is Apple Intelligence?\nAnnounced in 2024, Apple Intelligence is designed to enhance the everyday user experience with iPhone, iPad and Mac devices. It\u2019s a suite of AI-powered features integrated into Apple\u2019s core apps and services, including generative tools for editing writing, summarizing conversations and organizing photos. It also promises a context-aware Siri. Rollout has been piecemeal, with Apple drip-feeding features through a series of software updates.\nPrivacy is a key component of Apple Intelligence. The toolkit\u2019s contextual awareness depends on unprecedented access to your data, which is why Apple is ensuring that as little as possible actually leaves your device. It runs all the tasks it can on device. When they\u2019re not, Apple uses Private Cloud Compute, a security protocol designed to set a new privacy standard for AI tools.\nWhat can you use Apple Intelligence for?\nApple Intelligence enhances the user experience in several ways. Here are some of the features which have already rolled out to Apple devices:\nSign up for breaking news, reviews, opinion, top tech deals, and more.\n- Writing Tools is an editing assistant which can proofread text, summarize passages and rewrite your words using a different tone of voice.\n- Summaries work across everything from group chats and emails to web pages and notifications, condensing information into digestible overviews.\n- Siri gets smarter, allowing you type commands and ask follow-up queries, making it closer to an AI chatbot. It also gains ChatGPT support.\n- Photos are easier to search for with natural language prompts; Clean Up works like Google\u2019s Magic Eraser; and you can create Memories from prompts.\n- Mail gains support for smart replies based on the contents of emails. Apple Intelligence also automatically categorizes, prioritizes and summarizes emails.\n- Transcription is enhanced by Apple Intelligence, allowing you to record phone calls or voice notes and have them automatically transcribed.\n- Visual Intelligence works with the Camera Control button on the iPhone 16, allowing you to get search results based on what you point the camera at.\n- Genmoji is a generative emoji tool, which allows you to type in a description of the emoji that you want to send to generate it.\n- Image Playground is a place to generate AI images, which can be based on your real friends and family.\nWe\u2019ve put together a full run-down of all the Apple Intelligence intelligence features here, including when you can use them.\nWhat can\u2019t you use Apple Intelligence for?\nAs above, several Apple Intelligence features haven\u2019t arrived yet. Chief among them is a more intelligent Siri. This is expected in the near future, introducing awareness of what you\u2019re looking at on screen, as well as deep personalization based on information from your device.\nBecause it prioritizes privacy, there are also ways in which Apple Intelligence can\u2019t offer the same real-time, cloud-based performance as other AI tools. It\u2019s not a fully fledged chatbot like ChatGPT or Google Gemini, for example, and it can\u2019t do photorealistic image generation. In fact, if you ask Siri a question that's too complex it will hand you over to ChatGPT for an answer.\nUnlike Google Gemini, you can\u2019t use Apple Intelligence on non-Apple devices. You need to own one of the following compatible Apple devices to use at least some of the AI features: iPhone 15 Pro, iPhone 15 Pro Max, iPhone 16, iPhone 16 Plus, iPhone 16 Pro, iPhone 16 Pro Max, iPad with M-series chip, Mac with M-series chip.\nHow much does Apple Intelligence cost?\nAt present, Apple Intelligence is completely free for users of compatible Apple devices. The AI features are being rolled out in a series of iOS 18 software updates at no additional cost.\nThis could change in future. Some analysts have claimed that Apple may start charging for some Apple Intelligence features. Luckily, other industry insiders have suggested that paid AI tools are several years away, with the first wave unlikely to be introduced before 2027.\nWhere can you use Apple Intelligence?\nApple Intelligence initially rolled out with language support for US English only. In December 2024, this expanded to include localized English for the UK, Australia, New Zealand and Canada. Support for more languages is coming in April 2025. For now, your device needs to be set to one of the supported languages to use Apple Intelligence.\nAccess is limited for users in the European Union and China. Apple Intelligence is available with macOS Sequoia 15.1 in these regions, but users will need to wait until April to use the AI features on iPhone or iPad.\nIf you are able to access Apple Intelligence, you\u2019ll find its features deeply integrated into your device at a system level. Siri might be the voice of Apple Intelligence, but there\u2019s no single \u2018hub\u2019 to access the full suite of apps. Instead, you\u2019ll find that the tools become available in different apps when relevant to the context, whether that\u2019s in Photos, Mail or Notifications. The toolkit is cross-platform, designed to work seamlessly across apps on iPhone, iPad and macOS.\nIs Apple Intelligence any good?\nBased on our experience, Apple Intelligence meaningfully improves the user experience across iPhone, iPad and Mac. Several members of our team have been using the toolkit on their devices, with positive feedback. Writing Tools work like a supercharged autocorrect, significantly improving the Notes app experience. We also rate the usefulness of Reduce Interruptions, which only allows key alerts through. Other Apple Intelligence features we keep coming back to include article summaries in Safari and the Clean Up tool in Photos.\nWhile typing to Siri is also a useful addition, we can\u2019t help but feel that the best is yet to come, including situational awareness. Although on-device processing enhances privacy, it could also be one of the reasons behind the slow rollout of Apple Intelligence features. The result is that users can get more advanced AI support from other chatbot apps, such as Gemini, or by accessing ChatGPT through Siri.\nUse Apple Intelligence if...\nYou want native AI on your Apple device\nDesigned for Apple devices and integrated deeply with their operating systems, Apple Intelligence builds AI assistance right into core apps and services, offering contextual assistance within Mail, Photos and more.\nYou want secure, on-device AI\nBuilt into OpenAI\u2019s chatbot, ChatGPT search allows you to find detailed answers from the web through conversational dialogue. That includes the ability to submit follow-up queries within the same thread, written in natural language.\nDon't use Apple Intelligence if...\nYou don\u2019t have an Apple device\nUnlike Google\u2019s Gemini chatbot, Apple Intelligence can only be used on compatible iPhone, iPad and Mac devices, which means you\u2019ll need to find a different solution if you\u2019re using an Android device or Windows PC.\nYou need an full AI chatbot\nUpdates mean you can type and respond to Siri, but Apple\u2019s smart assistant is a long way off the cloud-powered capabilities of AI chatbots such as ChatGPT and Gemini. Advancements are coming soon, though.\nAlso consider\n- Gemini is Google\u2019s AI multimodal chatbot. Available as an app for iOS users, it\u2019s also increasingly integrated into the Android operating system. Hundreds of devices are supported, as are more than 40 languages. Features include spoken search and image generation.\n- Galaxy AI is a suite of AI-powered features baked in to Samsung Galaxy mobile devices. Like Apple Intelligence, it\u2019s designed to streamline the user experience through automation and personalization. Tools include real-time translation, automatic transcription and note organisation.\nYou might also like...\nFor more than a decade, Chris has been finding and featuring the best kit you can carry. When he's not writing about his favourite things for Stuff, you'll find Chris field-testing the latest gear for TechRadar. From cameras to classic cars, he appreciates anything that gets better with age.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/computing/mac-os/macos-tahoe",
      "text": "macOS Tahoe 26 is official: here\u2019s everything we know about the release date, compatibility and all the new features\nA whole new look for macOS and a big change for Spotlight\nApple officially revealed macOS Tahoe 26 at WWDC 25 and as expected, we were treated to an all-new look for this latest version of the desktop operating system \u2013 and a lot more besides.\nBut what are the new features coming with macOS Tahoe? And when will the OS be out? Indeed, you may be wondering whether your model of Mac will be compatible with the next incarnation of Apple\u2019s OS. Read on to find out everything you need to know about macOS Tahoe 26.\nmacOS 26: release date\nApple typically deploys its new version of macOS in September or October, so we can likely expect macOS Tahoe 26 to roll out to Macs around that time.\nBefore that, we will see the release of a developer beta (which should be imminent), and later, a public beta, for testing macOS Tahoe as the year rolls onwards.\nRemember, though, that pre-release software is often a rollercoaster ride of glitches and potentially show-stopping bugs, so it\u2019s not for the faint-hearted. Most folks will want to wait for the full release of macOS Tahoe.\nmacOS 26: Compatibility\nWant to know if your Mac will be compatible with macOS Tahoe 26? Here\u2019s the full list of Macs that\u2019ll be able to run the operating system:\n- MacBook Air M1 or later\n- MacBook Pro M1 or later\n- MacBook Pro 16-inch 2019\n- MacBook Pro 13-inch 2020 with four Thunderbolt 3 ports\n- iMac 2020 or later\n- Mac mini 2020 or later\n- Mac Pro 2019 or later\n- Mac Studio 2022 and later\nmacOS 26: New features\n1. An all-new look\nOne of the most insistent rumors was that Apple\u2019s desktop OS is getting a visual overhaul, and that is indeed the case: macOS Tahoe uses the \u2018Liquid Glass\u2019 design language. Get ready for translucency aplenty across the interface, a refined dock and toolbars, and a transparent menu bar which allows for the desktop to feel a bit larger.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nApple is offering more ways to customize the interface in macOS 26, too. You can tailor what controls are present in the menu bar and Control Center, tinker with the layout, and add controls for your favorite apps. A greater level of personalization is also being ushered in with moves such as allowing the user to change the color of folders, and add a symbol (or emoji) to those folders to make them more readily identifiable.\nIt looks to be a substantial makeover, and the emphasis on clean lines, greater levels of customization, and adding more personality with the likes of jazzing up folders all seems very promising.\n2. Under the Spotlight\nSpotlight is the system-wide search feature which hasn\u2019t got much love in recent times, but Apple is changing that with macOS 26. It\u2019s being powered up with AI so that it surfaces results that are ranked more intelligently, we\u2019re told, based on relevance to the user. Spotlight also pops up filtering options to allow you to swiftly whittle down search results, too.\nWith macOS Tahoe, it can be used to search within apps for features, and the more you use it, the more Spotlight acclimatizes to your workflow and typical usage patterns to make more relevant suggestions. Overall, this looks like a big step forwards for Spotlight.\n3. Continuity continued\nApple is bringing new Continuity features to macOS Tahoe, with the big move here being the introduction of the Phone app to the Mac. You\u2019ll be able to make or take calls, access voicemail, and benefit from the new features on iPhone as well, such as Call Screening or Hold Assist.\nTighter integration is also present with Live Activities from the Mac owner\u2019s iPhone popping up in the menu bar on macOS. Clicking a Live Activity will open the relevant app via iPhone Mirroring, and it all looks very neat and convenient.\n4. More AI (of course)\nIt\u2019s no surprise that Apple is adding more AI to macOS 26, as well as juicing up Spotlight which we\u2019ve already discussed.\nThe Shortcuts app is getting fresh AI abilities, and that incudes summarizing text using Writing Tools, and image generation via Image Playground. Furthermore, Mac users will be able to access Apple Intelligence models (on-device, or in the cloud) to use as part of their shortcuts. Shortcuts can also be cued up to run at a certain time of day, or when completing a specific action like connecting an external monitor.\nAI is also set to drive Live Translation in the Messages app, translating text into another language as you type it (and vice versa for replies). FaceTime will benefit from translated live captions, and calls in the Phone app can be translated, too.\nThere are some further minor trimmings here, including macOS 26 providing new ways to create Genmoji.\n5. Apple Games\nThere\u2019s a new dedicated app for gamers with macOS Tahoe, imaginatively named Apple Games. It acts as a hub for your Mac gaming activities, allowing you to jump back into titles you\u2019ve been recently playing.\nThere\u2019s a Game Overlay that allows you to quickly adjust settings (volume, brightness), chat with friends, send invites to them and so forth. A new Low Power Mode is present for MacBooks to give them better battery life when gaming, too.\nOn the developer side, Metal 4 arrives with macOS 26, ushering in new tech including MetalFX Frame Interpolation (for faster frame rates) and Denoising.\n6. Additional accessibility features\nApple has brought in new accessibility functionality with macOS Tahoe, including Magnifier for Mac. This leverages your iPhone (or a USB camera) to be able to zoom in to, say, a distant whiteboard, and present the writing on the board to the user on their Mac\u2019s screen \u2013 seriously nifty stuff for those who are vision-impaired.\nIn a similar vein, Accessibility Reader can take images of book pages and show them on the Mac display, allowing for zooming, or changing fonts or colors to make things clearer. There\u2019s also a new Braille Access experience providing an interface for those using a connected braille display, and Vehicle Motion Cues, a feature to help combat motion sickness when on, say, a long car journey.\nYou might also like...\nDarren is a freelancer writing news and features for TechRadar (and occasionally T3) across a broad range of computing topics including CPUs, GPUs, various other hardware, VPNs, antivirus and more. He has written about tech for the best part of three decades, and writes books in his spare time (his debut novel - 'I Know What You Did Last Supper' - was published by Hachette UK in 2013).\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/tag/apple",
      "text": "Apple\nApple is arguably the world's most important consumer tech company, having played a hugely influential role in shaping the smartphone, laptop, desktop computer, tablet, and wearable markets.\nIt continues to dominate many of those areas today through the iPhone 14, iPad, MacBook Air and Pro, iMac, and Apple Watch product lines, and with new forays into the likes of AV/VR to come. Its decision to build its own silicon, meanwhile, has given it an advantage over rivals such as Samsung and Microsoft - one that's only slowly being closed.\nIt's been every bit as dominant in the software space through iOS, macOS, watchOS, iTunes, Apple Pay, and more, while its Apple TV Plus streaming service is now regularly home to top-quality, award-winning original content.\nApple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976, and has been led by CEO Tim Cook since 2011. It is currently valued at around $2.6 trillion; not bad for a company that started in a garage.\nLatest about Apple\nGPT-5 is coming to Apple Intelligence soon, but it won\u2019t solve Apple\u2019s biggest AI problem\nBy Alex Blake published\nChatGPT\u2019s GPT-5 model will arrive on Apple devices when iOS 26 launches next month.\niOS 26 beta gives Apple Maps an AI upgrade \u2013 and it might be enough to tempt you from Google Maps\nBy David Nield published\nIt seems Apple Maps is getting an AI-powered search upgrade with iOS 26, though it hasn't been officially announced.\niPhone 17 Pro Max: latest news, rumors, and everything we\u2019ve heard so far\nBy James Rogerson last updated\nUpdated The iPhone 17 Pro Max is sure to be a beast, and leaks are already painting a picture of it.\niPhone 17 Pro: latest rumors, tips and everything we\u2019ve heard so far\nBy James Rogerson last updated\nUpdated The iPhone 17 Pro could have better cameras than any current iPhone and also have a new button. Here's what we've heard so far.\niPhone 17: latest news and rumors for every expected model\nBy James Rogerson last updated\nSupposed leaks and rumors around th iPhone 17 line are already popping up, with talk of a new button and a new model.\n'This looks hideous': the iPhone 17 Pro has been shown off in a new orange shade that you\u2019re going to either love or hate\nBy James Rogerson published\nA video of an iPhone 17 Pro in an orange shade is delivering mixed reactions, but some people love it.\nThe iPhone 17 Pro\u2019s rumored price hike could be softened by a much-needed storage upgrade\nBy Alex Blake published\nA new rumor says the iPhone 17 Pro could get more storage and a $50 price rise.\niOS 26 could give your AirPods charging case a helpful lighting system boost \u2013 here\u2019s how\nBy David Nield published\nIf you're running the latest developer beta of iOS 26, you may have noticed a change to the charging lights.\nSign up for breaking news, reviews, opinion, top tech deals, and more."
    },
    {
      "url": "https://www.techradar.com/phones/ios/how-to-download-the-ios-26-public-beta",
      "text": "How to download the iOS 26 public beta\nBring on the beta\niOS 26 \u2013 the confusingly named follow-up to iOS 18 \u2013 was announced back at WWDC 2025 in June, and it launched into developer beta shortly after that. But as that name suggests, this very early version of the software was only really intended for developers \u2013 so while anyone could download it, for most people it wouldn\u2019t be advisable.\nNow though, the first iOS 26 public beta has launched, and this one is much easier to recommend, as it\u2019s actually intended for non-developers, and will likely be more stable than the first betas.\nIt still comes with the same general caveats though \u2013 this is incomplete software, so it may be buggy, features might be missing, and some things may not work as intended. So if you have a low tolerance for software quirks and tech issues, we\u2019d suggest maybe only downloading this onto a secondary phone \u2013 or simply waiting for the finished release, which will probably land in September, alongside the iPhone 17 series.\nStill, there\u2019s a lot to like in iOS 26, including a completely redesigned interface with a more glass-like appearance, so you might not want to wait.\nWith that all said then, here\u2019s how to download the iOS 26 public beta.\nHow to install the iOS 26 public beta on your iPhone\nBefore you start, it\u2019s worth making sure you have a compatible iPhone, but you likely do, since all models from the iPhone 11 onwards support iOS 26. You should also back up your iPhone, just in case you run into issues installing the beta.\nWith that done, actually getting the iOS 26 public beta is quite a quick and easy process. First, if you\u2019re not already enrolled in the Apple Beta Software Program then head to the Apple beta page, and hit the \u2018Sign up\u2019 button, at which point you\u2019ll be prompted to log into your Apple account \u2013 if you have multiple Apple accounts, make sure you use the one your iPhone is signed into.\nSign up for breaking news, reviews, opinion, top tech deals, and more.\nIf you\u2019re already signed up for the Apple Beta Software Program then you can skip this step, but if in doubt, head to the above link and select the \u2018Sign in\u2019 option. If you\u2019re already a member, then you\u2019ll be able to sign in, if not then go back and select \u2018Sign up\u2019.\nWith that done, it\u2019s now time to download the iOS 26 beta onto your iPhone. To do this, head to Settings > General > Software Update > Beta Updates, and select the beta you want. There may be multiple options, especially if you\u2019ve also enrolled into the developer beta program, but the one you want in this instance is \u2018iOS 26 Public Beta\u2019.\nHit the back button to return to the \u2018Software Update\u2019 page, and you should see details of the relevant beta. Then just tap \u2018Update Now\u2019 or \u2018Update Tonight\u2019 to download it either immediately or overnight.\nOnce the update starts, simply wait for the beta to install, and then you\u2019ll be able to try out all the iOS 26 goodies, which as well as a new look include a new Apple Games app, reworked Camera and Phone apps, improvements to Live Translation and the Shortcuts app, and many other things.\nYou might also like\nJames is a freelance phones, tablets and wearables writer and sub-editor at TechRadar. He has a love for everything \u2018smart\u2019, from watches to lights, and can often be found arguing with AI assistants or drowning in the latest apps. James also contributes to 3G.co.uk, 4G.co.uk and 5G.co.uk and has written for T3, Digital Camera World, Clarity Media and others, with work on the web, in print and on TV.\nYou must confirm your public display name before commenting\nPlease logout and then login again, you will then be prompted to enter your display name."
    },
    {
      "url": "https://www.techradar.com/news/live/openai-chatgpt5-launch",
      "text": "OpenAI GPT-5 launch live \u2013 all the latest news as Sam Altman unveils the new model\nThe next generation of ChatGPT is here\nIt's the one we've all been waiting for \u2013 ChatGPT-5 was finally unveiled by OpenAI on August 7! Earlier in the week OpenAI CEO Sam Altman teased that \"something big\" was coming on Friday, but then today the OpenAI account on X.com tweeted: \"LIVE5TREAM THURSDAY 10AM PT\".\nNotice the use of a 5 instead of an \"S\" in the word \"LIVE5TREAM\"? That can mean only one thing: ChatGPT-5 is coming!\nWith a livestream that kicked off at 10AM PT, 1PM ET, and 6PM BST \u2013 and you can watch the replay below \u2013 TechRadar broke down all the news on GPT-5 as the team at OpenAI, including Sam Altman, unveiled its next-generation model.\nAhead, you can read our live reporting and get everything you need to know about ChatGPT-5. It's rolling out right now to all users, and unlike previous models, even the Free tier as well Pas lus and Pro subscribers are getting access to it. GPT-5 brings with it a lot of new features and enhanced levels of performance. OpenAI says it's more reliable, better at crafting nuanced responses, and that it haullincates less.\nIn many of the demos during the kick-off event, ChatGPT-5 showed off it's advanced coding capabilities and the transformed Voice mode that sounds a lot more natural.\nIf you're curious about the most impactful features that GPT-5 is set to bring with it, you'll want to read through our list of the 5 most impactful aspects of OpenAI's latest model. Here, we're taking a close look at the different flavors that GPT-5 comes in, as well as features like the transformation of Advanced Voice Mode to simply Voice Mode.\nGPT-5 is rolling out slowly\nIt's a bit of a waiting game for access to GPT-5 at this time. While OpenAI did announce that it's rolling out for all users \u2013 yes, that includes the free tier \u2013 it's a staggered one. The company posted on its site, \"We are gradually rolling out GPT-5 to ensure stability during launch. Some users may not yet see GPT-5 in their account as we increase availability in stages.\u201d\nSo rather than instability, longer response times, or crashes, OpenAI's taking a slow and steady approach here. While we're all keen to try out GPT-5 \u2013 I've been refreshing my own Plus account, it gives us time to talk about what the AI giant has announced.\nTechRadar's Editor-at-Large Lance Ulanoff shared this on the announcement of GPT-5:\n\"Is it weird that I am slightly underwhelmed by the GPT-5 reveal? I get that it\u2019s much smarter and can do agentic-level things with more or less a single prompt, but I had sort of hoped that we\u2019d get our first glimpse of true AGI.\nPerhaps, considering all Sam Altman\u2019s publicly declared safety concerns, he held something back. GPT-5 puts us on the path to AGI but also keeps it at arm's length.\nI'm also surprised at the bold embrace of health advice and guidance. The fine print makes it clear this is no doctor replacement, but still, it\u2019s a ballsy move and one that I wonder if they might regret (at least in the short term).\"\nLance makes a crucial point here \u2013 while Sam Altman did say it's an important step towards AGI, the rest of the keynote didn't necessarily provide clear details around. It was more of a set of demos, largely focused on the coding capabilities, and then several new features that daily users of ChatGPT will appreciate.\nWe'll have to see whatever day uses think of GPT-5 pitching itself for health advice and guidance, remember that it still is not a doctor replacement by any measure.\nAbout an hour and 15 minutes later, and OpenAI's unveil of GPT-5 \u2013 dubbed the OpenAI Summer Update \u2013 has come to a close. Interestingly enough, OpenAI's chief scientist closed out the event, thanking the team, and his last line was \"we'll get back to sailing.\"\nWhile we weren't invited out to sea with them, we're constantly refreshing our ChatGPT pages as we await the GPT-5 rollout to hit our accounts. And whether you're on the free tier, Plus, or Pro, you can expect to see GPT-5 today and many of its new features.\nWhat's new with GPT-5\nOpenAI's already covered a lot of ground, but let's run through what's new with GPT-5 quickly:\n- Better reliability and more accurate answer with fewer hallucinations\n- Improved writing skills for more precise results and the ability to switch based on context\n- GPT-5 is the 'strongest coding model'\n- It's OpenAI's best model for health-related questions\n- GPT-5 will deliver safe completions rather than refusing a prompt\n- It will ask follow-ups when needed to get more context for a better result\n- The model can better adapt to context in prompts and can understand tone\n- GPT-5 will handle all the heavy-lifting for deciding on the right model and will show its work\n- It can connect with your Google account for access to Gmail and Google Calendar (Pro users get this first)\n- Paid subscribers can customize the look of ChatGPT\n- A more natural-sounding voice mode\n- GPT-5 arrives for all subscribers, even the free tier, with varying levels of use\nOpenAI is moving through this livestream pretty quickly. Now, Greg Brockman, President of OpenAI, is on stage, and the focus is shifting back to developers and coding. He says that GPT-5 is the best model at agentic coding tasks.\nSam Altman is now back onstage to start the discussion around ChatGPT-5 and how it handles the Health prompt. We now have two ChatGPT users on stage, including one, Carolina, who was diagnosed with three forms of cancer in one week.\nAfter receiving an email confirming the diagnosis, this user took a screenshot and uploaded it to ChatGPT and got an understandable explanation. Altman says that Health questions are one of the most common use cases for ChatGPT as well. A takeaway from both of them is that ChatGPT can be used to help learn a bit more and 'regain some agency.'\nBoth of these users have used GPT-5 and note that it is a much faster, \"almost a little alarmingly at first.\" Another key impression, though, is that GPT-5 is more of a thought partner than just delivering an answer based on the prompt. They also put the biopsy results back in, and GPT-5 asked follow-ups to get a bit more detail and deliver a more nuanced response.\nOf course, ChatGPT or GPT-5 is not a medical doctor, and you should still see one for diagnosis and emergencies.\nOpenAI is also focused on improving safety and deception; in this case, GPT-5 will aim to maximize helpfulness within the allowed safety constraints. This will be in contrast to how previous models, like o3, might have refused on the grounds of safety before.\nGPT-5 could answer the same prompt, by explaining why it can't offer help and point you towards a third-party or even a manual. The demo shown off by OpenAI here is a prompt asking for how to work with 'pyrogen.'\nChatGPT will be able to connect with your Google account\nWe're now switching gears to ChatGPT's Memory function. Pro, Plus, and Team subscribers will be able to connect ChatGPT to a Google Account for calendar access. In fact, it can connect with Gmail and Google Calendar, allowing GPT-5 to pull in your schedule and get some more context.\nIt can even remind you to respond to an email that you've received, read, but haven't yet responded to. This will roll out first to Pro users next week, then Plus and Team at some point in the future.\nIf you've been wanting to change the color of your ChatGPT interface, OpenAI is making that possible \u2013 to a degree \u2013 for paid users. Even more impressive, though, is the ability to customize the personality of GPT-5 a bit for research use cases.\nVoice mode is getting an upgrade\nThere is a lot of focus on coding with GPT-5, but we've also seen improvements in writing thus far. The teams at OpenAI have also been focused on further improving Voice and video functionality.\nWith GPT-5, voice mode is coming to all users, and in this demo, OpenAI is showing that you can be specific with how you want an answer. In this case, they demonstrated ChatGPT Voice's ability to respond with just one word, summarizing Pride and Prejudice as 'relationships.'\nThe biggest improvment, though, is that it sounds a lot more natural than even the current version in the app.\nNow we're in the midst of another GPT-5 demo that's centered around its ability to write code, spanning over 200 lines in just a few minutes of this demo. It actually created a site, with many visual elements and even audio ones, to help one learn French with a partner.\nAnd like that, GPT-5 completed the prompt and coded the demo, which the OpenAI team is now testing during the live-stream. Pretty, pretty fast.\nFor the next demo, OpenAI is pitting GPT-4o and GPT-5 against each other in writing a eulogy for its previous models. Here's a look at what GPT-4o produced.\nGPT-5 on the other hand has more of an understanding of tone and nuances. It sounds a bit more like a person with emotions wrote it ... less dry.\nOur first demo of GPT-5\nWe're now getting our first demo of GPT-5, first with the model being asked to explain the Bernoulli effect, and it responded pretty quickly. The follow-up, though, is asking GPT-5 to create a visual, and OpenAI is explaining that it will take a bit longer to respond, and in that time also suggest the best course of action.\nYou can also ask GPT-5 to 'think harder' or 'more precisely' in the actual prompt. Even neater, you can see GPT-5's thought process while it's answering the prompt.\nGPT-5 will start rolling out to all users today\nHere's some good news \u2013 OpenAI is rolling out GPT-5 to all users, with that process beginning today, including the Free tier as well as Plus and Pro subscribers. It won't be a free-for-all with GPT-5 in the free tier, but you'll be able to use it until allotments are hit. Plus and Pro subscribers will expectedly get higher rate limits.\nA key area of focus with GPT-5 was improving reliability and being accurate with facts. OpenAI is also saying that it has specifically focused on reliability in health.\nNow, we're getting a closer look at GPT-5 from more team members at OpenAI. Here's the kicker, also, OpenAI says that GPT-5 will even be available for free users.\nAs for what we'll see during this livestream, we'll be getting plenty of demos but are starting first with evaluations. OpenAI has just announced that GPT-5 has set a new level on several benchmarks, including SWE-Bench \u2013 it's not the full story, but it's a good tell of the performance here.\nSam Altman announces GPT-5\nAnd here we go, OpenAI CEO Sam Altman is walking onto a stage, and that's certainly a different setting than previous livestreams.\nAltman is wasting no time, saying that they're ushering in GPT-5 and it's another important step to AGI. He believes that folks will enjoy using GPT-5 a lot more. Why? Well, the early sell here, as Altman puts it, is that chatting or talking with GPT-5 is like talking to an expert, and it can even write software on demand.\n2 minutes to go\nThe visuals have changed again, and we're now counting down from three minutes until the start. The music is also getting a bit more exciting with more sounds joining in.\nThe livestream has started\nOpenAI has officially kicked off the livestream with under 10 minutes to go until 1PM ET/ 6PM BST. Right now, we have some sonically soothing sounds over \"OpenAI Summer Update\" flashing with colors and then spinning in circles on the screen. No mention of GPT-5 though.\nWhile we wait for the livestream to kick off on OpenAI's YouTube channel \u2013 and in the video above \u2013 the description of the video does give us a bit more detail. We've already known, thanks to a post on X from Sam Altman, that this will be a longer livestream, and there will be many, many presenters.\nHere's what the description reads: \"Join Sam Altman, Greg Brockman, Sebastien Bubeck, Mark Chen, Yann Dubois, Brian Fioca, Adi Ganesh, Oliver Godement, Saachi Jain, Christina Kaplan, Tina Kim, Elaine Ya Le, Felipe Millon, Michelle Pokrass, Jakub Pachocki, Max Schwarzer, Rennie Song, Ruochen Wang as they introduce and demo GPT-5.\"\nFurther, if you had any doubt that we'd be seeing GPT-5 today, this confirms it will be unveiled and demoed.\nI just checked my ChatGPT Plus account one more time before this event kicks off, and still no sign of GPT-5. Just a long list of models, and as my colleague John-Anthony already wrote here, it could use a bit of a cleanup and streamlining.\nI'm particularly excited about GPT-5's rumored capability of automatically selecting the right model to handle the prompt or whatever the user has asked. That could speed things up before the AI even gets to work.\nLess than 30 minutes until kick off\nWe're just under 30 minutes until OpenAI tells us everything about GPT-5 and the livestream for the event is now appearing on YouTube. Though it's just a countdown and a still teaser image showing \"GPT-5\" and \"OpenAI\" in front of a colorful array of gradients.\nYou can see the livestream embedded above, and once it starts, you can watch it right from there.\nOne. Hour. Countdown\nOne hour to go until OpenAI reveals GPT-5 and we enter into the next generation of ChatGPT.\nOpenAI still hasn't got a livestream listing on YouTube yet but stay tuned to this liveblog and we'll update with a link as soon as we have access.\nOne last look before everything changes - again. pic.twitter.com/P4y0iUvjihAugust 7, 2025\nI've been moaning about the OpenAI model names for what feels like an eternity at this point, so I may as well add one last criticism before we move into the GPT-5 era.\nMy colleague Lance Ulanoff has shared an image of the ChatGPT model selection page as it stands right now, and quite honestly, it might just be the most confusing naming scheme on the planet.\nIn just over an hour, OpenAI will announce GPT-5 alongside other models that we believe to be called GPT-5-mini, GPT-5-nano, and GPT-5-chat. Hopefully, the company decides to add these new models and replace the likes of GPT-4o, o3, o4-mini, o4-mini-high, otherwise, the list in that image is going to get even longer and even more confusing.\nAnother ChatGPT livestream\nIt's only been two weeks since OpenAI revealed ChatGPT Agent in a livestream, and yet here we are waiting for what many believe to be the next generation of AI model.\nIs GPT-5 really going to be as impressive as its next generation name would suggest? Or are we going to see small performance improvements in line with an iterative upgrade?\nEither way, OpenAI needs to showcase why users should care about GPT-5 in this upcoming livestream, and so far the company's livestreams have been anything but engaging.\nWith less than 90 minutes until the reveal, we're all getting excited here at TechRadar. After all this could be the ChatGPT upgrade that pushes AI even further into the future.\nTwo hours to go\nOnly two hours until the GPT-5 reveal, and we still don't have a livestream link on YouTube to share.\nAs soon as OpenAI adds a listing for the livestream, we'll be sure to add it to this live blog, so stay tuned as we delve into the future of AI (and ChatGPT).\nPlease fix the naming scheme\nOpenAI's AI model naming scheme is confusing. There's 4o, o3, o3-mini, o3-high, just to name a few.\nIn a few hours, at least four more models will be added to the mix, and it begs the question: At what point will OpenAI's marketing step up and make ChatGPT more user-friendly?\nIf the company chooses to add another subscription tier with the aforementioned ChatGPT Go, not only will we have an AI model naming issue, but users will start to get confused by subscription tiers, too.\nPlease, OpenAI, fix your naming scheme.\nA new ChatGPT subscription\nNew images have appeared online detailing a ChatGPT Go subscription, likely to sit between free tier users and ChatGPT Plus subscriptions.\nAccording to the screenshot, Go subscribers will get more messages, more file uploads, more image generation, and more advanced data analysis compared to free users.\nThis does add a slight concern to the future of ChatGPT's free tier. Does this mean we'll all be paying for the AI we've become accustomed to using for free at some point? I sure hope not...\nThe hype is building\nDropping soon. pic.twitter.com/WSuOn3qE6yAugust 7, 2025\nOpenAI just tweeted a GPT-5 teaser, and as the name would suggest it doesn't give us any real insight other than the fact that it's likely going to replace GPT-4o.\nWith less than three hours to go until the reveal, I was hoping for a teaser to include a little bit more information. I guess not today.\nWhat does Microsoft Copilot think about GPT-5\nWith just over three hours until the OpenAI livestream, we've started to ask other AI tools what they think about GPT-5. After all, who doesn't want to hear AI's thoughts on its own evolution, right?\nLance Ulanoff, TechRadar's Editor At Large, asked Microsoft Copilot about GPT-5 and after a response with leaks and rumors, Copilot said, \"It\u2019s shaping up to be a major leap forward. If you could design your own GPT-5 variant, what would it specialize in\u2014creativity, emotional intelligence, or something else entirely?\"\nLance replied: \"Emotional intelligence, for sure. Have you tried GPT-5?\"\nCopilot responded, \"I haven\u2019t tried GPT-5 myself\u2014since I\u2019m not a user, I don\u2019t get to 'use' other models. But I do keep tabs on what\u2019s happening in the AI world, and GPT-5\u2019s rumored emotional intelligence upgrades are especially intriguing.\nIf it\u2019s anything like what\u2019s been hinted, it could:\nRead tone and subtext more accurately in conversations\nRespond with empathy and nuance, even in complex emotional situations\nAdapt its personality to match the user\u2019s mood or communication style\nSupport mental health applications with more sensitivity and care\nImagine an AI that not only understands what you say, but also how you feel when you say it. That\u2019s a whole new level of connection. What kind of emotionally intelligent features would you want it to have\u2014like being a better listener, helping with conflict resolution, or maybe even coaching through tough decisions?\"\nWOAH. This is probably all a pipe dream but Copilot has thought up a future OpenAI model that completely evolves ChatGPT's emotional intelligence. Is this something you'd be interested in?\nBenchmark leak?\nholy hell pic.twitter.com/ausKp4vancAugust 7, 2025\nLeaks are popping up all across social media including this rumored score on the ARC-AGI-2 test, a benchmark that highlights an AI model's capability of dealing with logic.\nIf these results are accurate, GPT-5 would be so far beyond anything we've seen before from OpenAI or its competitors.\nThat said, anyone can make a fake leak, and ultimately until we see the results from Altman and co themselves, there's no way to be sure of GPT-5's capabilities.\nJust imagine though...\nWhat ChatGPT says about GPT-5\nJohn-Anthony Disotto here, taking over from Graham while he gets fed and watered. I decided the best way to get a glimpse of GPT-5's capabilities would be to ask ChatGPT itself what we can expect during today's livestream.\nThe AI said, \"A Unified, Multi\u2011Variant System\" alongside \"Supercharged Reasoning, Coding, and Agentic Behavior\" and \"Ultra\u2011Large Context & Persistent Memory\" sets GPT-5 apart from its predecessor.\nObviously, ChatGPT won't be letting us into OpenAI's internal secrets, but considering unifying all ChatGPT capabilities would be at the top of my list for GPT-5, I'm really hoping the AI chatbot isn't too far off.\nThe livestream kicks off in just over four hours, so we don't have much longer to wait to hear about the future of ChatGPT and everything GPT-5 is capable of.\nChatGPT-5 is just the beginning\nAs I wait with bated breath for the release of ChatGPT-5 (at 10am PT), I'm thinking about what Sam Altman said just a few days ago. \"We have a ton of stuff to launch over the next couple of months \u2013 new models, products, features, and more\u201d.\nSo, earlier this week, OpenAI dropped its gpt-oss models, which are designed to run entirely on laptops and phones. In a few short hours, we'll get ChatGPT-5, but what next? The next couple of months are a long time, which means we might see quite a few new features being released.\nWhy the Death Star image for ChatGPT-5?\nAt first, Sam Altman posting a picture of the Death Star in relation to the launch of ChatGPT-5, his own product, seemed like an odd way to market something to me. I mean, what's he trying to say? Is it, \"This thing is so scary it could destroy a planet\"?\nHowever, when you view it in context of the things he has recently said about AI creating new and unparalleled security risks and that fact that he finds ChatGPT-5 scary, it fits in. On the one hand, Altman is always pushing the narrative of how great AI will be for the future of the world and for the next generation, but at the same time he's always talking about how AI has the potential to be dangerous.\nThis new type of marketing, where he both praises and criticizes his own product, is certainly nothing new for him, and perhaps it reframes the narrative and puts him ahead of OpenAI's critics? Whatever his reasons, it certainly seems to be working.\nLonger than usual\nSam Altman followed up his mysterious Death Star image post on X.com with a more down-to-earth statement in his next post:\n\"Our livestream tomorrow at 10 am PDT will be longer than usual, around an hour. We have a lot to show and hope you can find the the time to watch!\"\nour livestream tomorrow at 10 am PDT will be longer than usual, around an hour.we have a lot to show and hope you can find the the time to watch!August 7, 2025\nTo be honest Sam, an hour is not that long to reveal the details of what could potentially be the new standard in AI chatbots, so I think you should take as long as you need.\nThat's no moon... that's ChatGPT-5?\nSam Altman has confounded the Internet once again by posting a picture of what appears to be the Death Star from Star Wars rising menacingly from behind a planet.\npic.twitter.com/1u0MOGvJWUAugust 7, 2025\nEverybody is a bit confused by the meaning of this, so, who better to ask what it could mean than rival AI, Grok?\nWhen asked what the image could mean, Grok replied: \"Sam Altman's image of the Death Star symbolizes the rise of immensely powerful AI, like the anticipated GPT-5\u2014a transformative force with potential risks, echoing Star Wars' superweapon. It's likely teasing OpenAI's next big breakthrough.\"\nChatGPT-5 accidentally leaks hours before launch\nAs we sit down with snacks and drinks watching the screen and waiting for a live stream to appear for the imminent launch of ChatGPT-5 it appears that a leak on GitHub has revealed the name of the different ChatGPT-5 models.\nIt looks like we will have gpt-5, designed for logic and multi-step tasks, .gpt-5-mini, a lightweight version for cost-sensitive applications, gpt-5-nano, which is Optimized for speed and ideal for applications requiring low latency, and gpt-5-chat, designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications."
    }
  ],
  "argos_summary": "OpenAI's GPT-5 model is set to enhance ChatGPT features on Apple devices with its integration into Apple Intelligence, expected to launch alongside iOS 26 in early September. This new model introduces various functionalities, including different personalities, improved voice mode, and enhanced reasoning capabilities. However, the integration may complicate Apple's development of its own AI chatbot, as users may rely more on ChatGPT for complex queries, highlighting the challenges Apple faces in competing with established AI tools.",
  "argos_id": "BC6P3AIT9"
}