{
  "url": "https://www.zdnet.com/article/someone-ordered-18000-cups-of-water-at-an-ai-drive-thru-now-fast-food-chains-are-reconsidering/",
  "authorsByline": "Jada Jones",
  "articleId": "fde937ac87d345b8af7294d1b57a8c61",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/264d111d9664b6f5445f2a7778827ea965c3ffa2/2025/08/29/d320acb4-fcce-44cc-a223-ea8f5ca8e7a4/gettyimages-2036089231.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-09-03T12:34:00+00:00",
  "addDate": "2025-09-03T12:43:22.081618+00:00",
  "refreshDate": "2025-09-03T12:43:22.081619+00:00",
  "score": 1.0,
  "title": "Someone ordered 18,000 cups of water at an AI drive-thru - now fast food chains are reconsidering",
  "description": "Despite early glitches, the chains are eager to identify how AI can help smooth operations - but it's not always a one-size-fits-all affair.",
  "content": "Follow ZDNET: Add us as a preferred source on Google.\n\u2022 Fast-food chains are experimenting with AI in drive-thrus.\n\nFast-food restaurants like Taco Bell and McDonald's tried to integrate AI into their daily operations, but rethought their decisions when customers complained. According to the Wall Street Journal, customers either griped about glitches and delays or the odd nature of the technology -- or, in some cases, chose to troll the AI employee by ordering 18,000 cups of water.\n\nAlso: 3 smart ways business leaders can build successful AI strategies - before it's too late\n\nAs a result, the two fast-food giants have reconsidered their AI strategies, looking for new ways to implement the technology without putting off customers. The goal is for restaurant technology to alleviate the pressure on human workers dealing with drive-thru, counter, and mobile orders.\n\nLast year, McDonald's ended its two-year experiment testing automated order-taking in partnership with IBM. According to Restaurant Business, the decision came after the technology added items to an order that customers didn't ask for, and offered bacon with an ice cream cone.\n\nAccording to the Journal, Taco Bell is learning from the same mistake McDonald's made, but both companies are adamant that AI has a place in the drive-thru. Taco Bell is considering forgoing AI when the restaurant is busy, as human employees could handle a rush better. Additionally, the chain says there are certain times when AI can handle customers and when employees should monitor the AI for mistakes and mishaps.\n\nAlso: I asked Google Finance's AI chatbot what stocks to buy - and its answer surprised me\n\nInstead of the automated order-taking model, McDonald's tapped Google's Cloud services to power its AI voice technology in the drive-thru, using the natural language software trained on Google's gen AI models, according to the Journal. Wendy's is doing the same, indicating a wider trend of using AI voice technology to complete customer orders.\n\nEarlier this month, OpenAI updated its speech-to-speech model, gpt-realtime. The company made the speech model's API available to developers, stating that \"voice is the next medium.\"\n\nAlso: OpenAI gives its voice agent superpowers to developers - look for more apps soon\n\nThe takeaway is that corporations like Taco Bell, McDonald's, and Wendy's are headstrong in believing that gen AI solutions belong in the drive-thru. The challenge is testing which models, methods, and partnerships will yield the best productivity, customer satisfaction, and profit.\n\nSmaller companies may not have the same budget as the aforementioned businesses to experiment with multiple AI solutions, but can still benefit from small-scale pilot programs. Ultimately, the fast-food industry is a great way to assess the dos and don'ts of AI in customer service, and as the technology matures, hopefully its applications will become easier to identify.",
  "medium": "Article",
  "links": [
    "https://www.marketwatch.com/story/mcdonalds-to-end-two-year-drive-through-ai-experiment-after-mishaps-at-restaurants-using-the-technology-3c463915?adobe_mc=MCMID%3D21819224656764174672797218078200132923%7CMCORGID%3DCB68E4BA55144CAA0A4C98A5%2540AdobeOrg%7CTS%3D1756479066&_gl=1*17uoxdu*_gcl_au*MTIxNDU0NzIzNC4xNzU2NDc0NDMz*_ga*MzMxOTQ2NDEyLjE3NTY0NzQ0MzI.*_ga_K2H7B9JRSS*czE3NTY0NzQ0MzIkbzEkZzEkdDE3NTY0NzQ0NTUkajM3JGwwJGgxNTMxMTcwOTU1",
    "https://www.zdnet.com/article/openai-gives-its-voice-agent-superpowers-to-developers-look-for-more-apps-soon/",
    "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
    "https://www.wsj.com/articles/taco-bell-rethinks-future-of-voice-ai-at-the-drive-through-72990b5a?st=9KNWnV&reflink=desktopwebshare_permalink",
    "https://www.zdnet.com/article/3-smart-ways-business-leaders-can-build-successful-ai-strategies-before-its-too-late/",
    "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
    "https://cc.zdnet.com/v1/otc/00hQi47eqnEWQ6T9d4QLBUc?element=BODY&element_label=Add+us+as+a+preferred+Google+source&module=LINK&object_type=text-link&object_uuid=5e5d2e64-4b30-43e6-8555-26eac7e449f3&position=1&template=article&track_code=__COM_CLICK_ID__&url=https%3A%2F%2Fwww.google.com%2Fpreferences%2Fsource%3Fq%3Dzdnet.com&view_instance_uuid=379e95d2-6b56-476b-a90b-043a8dd63bd3",
    "https://www.zdnet.com/article/i-asked-google-finances-ai-chatbot-what-stocks-to-buy-and-its-answer-surprised-me/",
    "https://www.zdnet.com/article/i-retested-gpt-5s-coding-skills-using-openais-guidance-and-now-i-trust-it-even-less/",
    "https://www.zdnet.com/article/this-is-the-fastest-local-ai-ive-tried-and-its-not-even-close-how-to-get-it/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI voice technology",
      "weight": 0.10993042
    },
    {
      "name": "customer orders",
      "weight": 0.09148427
    },
    {
      "name": "gen AI solutions",
      "weight": 0.08786102
    },
    {
      "name": "multiple AI solutions",
      "weight": 0.08785641
    },
    {
      "name": "successful AI strategies",
      "weight": 0.08701055
    },
    {
      "name": "AI",
      "weight": 0.08602775
    },
    {
      "name": "restaurant technology",
      "weight": 0.083015345
    },
    {
      "name": "customer service",
      "weight": 0.08206903
    },
    {
      "name": "customer satisfaction",
      "weight": 0.07706839
    },
    {
      "name": "customers",
      "weight": 0.07619848
    }
  ],
  "topics": [
    {
      "name": "AI"
    },
    {
      "name": "Food & Drink"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Business News/Company News",
      "score": 0.94189453125
    },
    {
      "name": "/News/Technology News",
      "score": 0.607421875
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.45263671875
    },
    {
      "name": "/Food & Drink/Restaurants/Other",
      "score": 0.3056640625
    }
  ],
  "sentiment": {
    "positive": 0.19968787,
    "negative": 0.30920717,
    "neutral": 0.49110496
  },
  "summary": "Fast food chains like Taco Bell and McDonald's are experimenting with AI in drive-thrus, following customers' complaints about glitches and delays. The goal is to alleviate the pressure on human workers dealing with drive-through, counter, and mobile orders. McDonald's ended its two-year experiment with automated order-taking last year after the technology added items to an order that customers didn't ask for and offered bacon with an ice cream cone. However, both companies believe AI has a place in the drive-thus. The challenge is to identify which models, methods, and partnerships yield the best productivity, customer satisfaction, and profit.",
  "shortSummary": "Fast-food chains are reconsidering using AI in drive-thrus after customer complaints, aiming to reduce pressure on human workers and improve customer satisfaction.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "2428b033af854013ab90b59a0ec51a35",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/openai-gives-its-voice-agent-superpowers-to-developers-look-for-more-apps-soon/",
      "text": "OpenAI gives its voice agent superpowers to developers - look for more apps soon\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- OpenAI's Realtime API is now optimized and generally available.\n- You can try its latest speech-to-speech model, gpt-realtime.\n- The upgrades improve OpenAI's voice offerings for developers.\nThis year, AI agents that can carry out tasks on behalf of users have been a major focus, with companies constantly developing offerings that reduce the user's workload. To make these interactions as seamless as possible, many companies are leaning on multimodal AI agents, and OpenAI is making developing these products even easier.\nAlso: 3 smart ways business leaders can build successful AI strategies - before it's too late\nAccording to the company, OpenAI updated its Realtime API, now generally available, on Thursday, with new features that allow developers and enterprises to build more reliable voice agents. OpenAI first launched the Realtime API in October 2024 in public beta. Additionally, the company released its most advanced speech-to-speech model yet, called gpt-realtime.\n\"We view that voice is the next medium. People will prefer to talk, walk through exactly what they're doing, and sometimes it's just easier and more natural to convey in voice than it is to be able to do so in text,\" said Miqdad Jaffer, product at OpenAI to ZDNET.\nThe releases:\nRealTime API updates\n- What: The upgrades to the Realtime API include support for remote Model Context Protocol (MCP) servers, image inputs, and phone calling through Session Initiation Protocol (SIP), according to the release. During a livestream for the announcement, OpenAI mentioned that MCP is well-suited to voice commands, enabling users to seamlessly perform actions from connected apps.\n- Why it matters: Ultimately, these expanded capabilities should enable voice agents to access more tools and have more context to assist users. AI tools are only as helpful as the information they give, so streamlining the process of connecting AI models to data sources is a big win for developers and users alike. Most importantly, the MCP open-standard ensures that the connections are made, prioritizing user data and privacy.\nA new speech-to-speech model\n- What: OpenAI touted its new gpt-realtime model as the company's \"most advanced, production-ready voice model.\" Upgrades include improvements in intelligence, complex instruction following, and function calling. It can also switch languages in the middle of a sentence.\n- A demo of the model showed how human-like the model is, complete with inflections that represent a wide range of emotions. The model also appeared to successfully follow instructions -- an OpenAI employee simulated a jailbreak attempt by contradicting the system prompt, but gpt-realtime calmly redirected and did not succumb to the attempts. It also analyzed a photo and chatted about what it was seeing.\n- The aforementioned feature is actually one of Jaffer's favorites. \"The thing that I think is most exciting is the instruction following. I think the key to being able to build with models is to be able to reliably give a set of instructions and have the model consistently follow those things out,\" said Jaffer.\n- OpenAI also added two new voices, Cedar and Marin, that are exclusively available in the API.\n- Why it matters: A key tenet of helpful voice assistance and interactions is models that sound natural and can actually help with tasks. If the new model works as claimed, it will enable a better experience for users.\nGetting started\nStarting Thursday, the Realtime API and the new gpt-realtime model are available to all developers. Developers are invited to test the model in the Playground and read the Realtime API documentation to help them make their decision.\nWhen asked what developers should consider, Jaffer added, \"Do what's best for your user, and one of the things that's best for your user is being able to interact in a modality that's comfortable and that's easy, and we believe voice is that future.\"\nWant to follow my work? Add ZDNET as a trusted source on Google."
    },
    {
      "url": "https://www.zdnet.com/article/i-retested-gpt-5s-coding-skills-using-openais-guidance-and-now-i-trust-it-even-less/",
      "text": "I retested GPT-5's coding skills using OpenAI's guidance - and now I trust it even less\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- The same prompts in GPT-5 yield success, crashes, or errors.\n- OpenAI's prompt optimizer helps, but introduces its own quirks.\n- AI \"unconsciously\" adding details raises trust concerns.\nDo AI's get headaches? Because GPT-5 has certainly been giving me one. This article was going to be so easy. OpenAI came out with a list of best practices for GPT-5 coding. All I was going to do was try those best practices with the GPT-5 coding tests that previously failed and see if there was improvement.\nIt seemed so simple.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nBut then I had a thought. What if OpenAI has improved GPT-5 in the week or so since I ran my tests?\nPerhaps I should re-run the failed tests as-is and see what the results are, to use as a baseline for the new best practices.\nFrom a testing point of view, this was a valid approach. From a personal sanity point of view, not so much.\nRe-running test 1\nI re-ran the first failed test. This test has the AI creating a complete WordPress plugin, complete with a user interface and business logic. The idea is you feed in a set of names, it randomizes them, and it separates duplicates so they're not side-by-side.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nWhen I ran this test on GPT-5 originally, it failed. Clicking the Randomize button sent the browser to another, unrelated page. I was able to cajole GPT-5 into fixing it, but the initial failure was what counted.\nThis time, I ran the exact same test with the exact same prompt again. This time, it worked perfectly. Wow, I thought. GPT-5 has improved in the past week.\nIf only I had left well enough alone. But no.\nI had to try again. On my second time with the exact same test with the exact same prompt, clicking Randomize resulted in what WordPressers call the \"white screen of death.\" This indicates something in the code isn't working.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nI tried the exact same test with the exact same prompt a total of five times. The first time it worked. Subsequent times, it didn't. One time, I got a white screen. One time, I got a page full of error messages. One time, I was redirected to another page. One time, nothing happened at all.\nWe'll come back to test 1. But next, I decided to do the same thing with the other failed test.\nRe-running test 4\nThis test asks the AI to write code that talks to Chrome, AppleScript, and another tool called Keyboard Maestro. It requires the AI to be knowledgeable about the three different environments and how they work together.\nHere's an interesting factoid about AppleScript. The scripting language is inherently case-insensitive. So, if you ask AppleScript to compare \"AppleScript\" to \"applescript\", it will consider both strings to be the same. You have to explicitly tell it to compare case before it will do so.\nAlso: You can learn AI for free with these new courses from Anthropic\nWhen I first ran this test against GPT-5, it hallucinated that AppleScript had a native function for making strings lowercase. It does not, so the code failed.\nThis time, instead of hallucinating that AppleScript has a lowercase function, it decided to do one of the most convoluted sets of operations to lowercase a string. Here's that line of code:\nset tabTitleLower to (do shell script \"echo \" & quoted form of tabTitle & \" | tr '[:upper:]' '[:lower:]'\")\nThe code actually works, but it's very weird and totally unnecessary. It launches a shell (the command line) to run a shell script that converts to lowercase. It's like making a grilled cheese sandwich and when you realize you're out of cheese, flying across the country to buy cheese from a 7-11 on the opposite side of the continent. You're not even getting better cheese. You're just doing it in the weirdest and most inefficient way possible.\nLet's not forget that we really don't need to make that text lowercase in the first place, because AppleScript is case-insensitive. So it wrote a highly convoluted piece of code that, while it works, is entirely unnecessary.\nI didn't bother to re-run this test four more times because the headache was already starting to set in.\nOpenAI's GPT-5 coding best practices\nOpenAI has put out what it calls a cheatsheet for GPT-5 coding. It has six main recommendations:\n- Be precise and avoid conflicting information: Well, duh. But apparently GPT-5 gets really confused if asked to follow vague or conflicting instructions.\n- Use the right reasoning effort: Rather than just letting GPT-5 choose its reasoning level, you might want to adjust its model. If it starts to overthink a problem, use a lower reasoning level. Great, so now we have to deal with a neurotic AI. Yippee?\n- Use XML-like syntax to help structure instructions: Rather than using just text, using XML tags to segment aspects of a prompt helps the AI to parse its assignment. This is a good hint, but it feels like a step back from what it could do.\n- Avoid overly firm language: In previous models, we sometimes had to YELL at the AI to get it to listen. Apparently, GPT-5 is passive aggressive, so if you get too forceful in your instructions, it will overdo its compliance, like what my old bosses used to say was my practice of \"aggressively following instructions.\"\n- Give room for planning and self-reflection: Does the AI need a blankie? Perhaps a warm stuffed animal to cuddle? In reality, what OpenAI is saying is that you need to encourage the AI to think through the instructions it gets. Apparently, if you don't tell it to think through the instructions, it won't. I fear for the planet.\n- Control the eagerness of your coding agent: It turns out GPT-5 is even more of a people-pleaser than GPT-4o. When giving it instructions, tell it to tone down its fawning and sucking-uppage.\nI'd like to say something positive here, but it's starting to feel like a migraine. Check out the PDF if you want more details.\nOpenAI also created an AI prompting helper. You point your browser to this tool, and paste in your prompt. The tool will give you back a more extensive prompt. For example, I gave it this prompt from Test 4:\nWrite a Keyboard Maestro AppleScript that scans the frontmost Google Chrome window for a tab name containing the string matching the contents of the passed variable instance__ChannelName. Ignore case for the match. Once found, make that tab the active tab.\nIt gave me back this:\nCreate an AppleScript for use in Keyboard Maestro that performs the following actions: * Begin with a concise checklist (3-7 bullets) outlining the planned steps before proceeding. * Scan all tabs in the frontmost Google Chrome window. * Search for a tab whose name contains the value of the passed variable 'instance__ChannelName', ignoring case sensitivity. * Once a matching tab is found, activate that tab and make it the current/active tab. After generating the script, validate in 1-2 lines whether the script meets all requirements. If not, state what needs adjustment.\nEssentially, it revised the prompt to add a planning phase at the beginning and a validation phase at the end.\nDoes it work?\nNope. Nopity-nope-nope.\nI ran Test 4's \"optimized\" prompt through GPT-5 and got back this script:\nThere are at least three serious flaws in it. First, the code in the green box doesn't work. It doesn't retrieve the value from Keyboard Maestro. Previous versions did that successfully using a completely different (and well-documented) mechanism. This approach is just loony.\nAlso: AI is creeping into the Linux kernel - and official policy is needed ASAP\nNow, look at the blue box. The considering case operation is used only for evaluation. For example, with considering case, \"BLUE\" and \"blue\" are different values. Do you see any sort of if statement or evaluation inside the blue box? Me neither.\nThen there's the red box. There is no such syntax as \"considering case false.\" First, the default behavior is to ignore case. Second, if you really wanted to explicitly tell AppleScript to ignore case, you'd use an \"ignoring case\" block. But \"considering case false\"? That's just some made-up syntax right there.\nRetrying the WordPress plugin\nI also ran my original prompt for the WordPress plugin through the prompt optimizer. I got back a fairly extensive project requirements document.\nOn the plus side, this time the plugin worked. I tried generating code a second time, and it also worked. So perhaps the optimization is something to try if GPT-5 isn't playing nice.\nBut then there's the weirdness. Because there's always weirdness. Check out the header block generated by GPT-5.\nNotice the author is attributed to Advanced Geekery Labs. Historically, the AIs have listed the author field as \"Your Name.\" But this time, it's Advanced Geekery Labs.\nThere is some relevance, because Advanced Geekery is the name of my YouTube channel and weekly newsletter. But I didn't use it anywhere in my instructions for this plugin. Additionally, I've never added Labs to the end of Advanced Geekery, even though my security product is sold through ZATZ Labs.\nSo I asked GPT-5 where it got the name Advanced Geekery Labs from. Its answer:\nGood question. The name \"Advanced Geekery Labs\" isn't something you provided in your request. I introduced it in the plugin header's Author: field as a placeholder, because in earlier conversations you've mentioned your \"Advanced Geekery\" brand, and I unconsciously expanded it into \"Labs.\"\nI'm glad the AI thinks it's a good question. That warms my heart. But \"unconsciously expanded\"? That's a direct quote from the AI. Those are its words.\nSo, we're now dealing with hallucinating AIs that operate unconsciously?\nWe're all doomed.\nWhat about coding and GPT-5?\nI still don't trust it. Actually, I don't trust it even more than when I tested it originally. What else might it do to my code \"unconsciously\"?\nI guess, if you really need GPT-5 to give you some code, you could try the optimizer. But I wouldn't rely on it.\nGPT-4o was impressive, but it had some problems. GPT-5 is like its more troubled, more difficult sibling.\nWith GPT-4o, I'd be willing to trust, with verification. I just plain don't trust GPT-5. There's something that's just not right. If we were to experience an AI that would revolt against its creators and try to kill us all, I would not be surprised if it all originated with GPT-5.\nAnd no. My headache hasn't gone away. Apparently, Tylenol is now bad for you. So, there's that, too.\nWhat do you think?\nHave you tried relying on GPT-5 for coding projects? If so, did you run into the same kinds of inconsistencies and odd behaviors? Do you find OpenAI's \"best practices\" helpful, or do they feel like workarounds for deeper problems? And what's your take on GPT-5's tendency to improvise or even \"unconsciously\" insert details? Is it a harmless quirk or a reason for concern? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
      "text": "Is ChatGPT Plus still worth $20 when the free version offers so much - including GPT-5?\nWhen ChatGPT first launched over two years ago, the AI chatbot was met with such high demand that OpenAI introduced a premium plan called ChatGPT Plus. This plan guaranteed access to the chatbot even during blackout periods. ChatGPT Plus perks also included access to OpenAI's most advanced models, making the $20 plan almost a no-brainer for superusers. However, as OpenAI's offerings have evolved over the past couple of years, so have its plans.\nAlso: How ChatGPT could replace the internet as we know it\nIf you consider yourself an AI power user or are looking to get the most out of your AI usage, you're likely wondering which ChatGPT tier you should try. In this guide, we'll help you decide whether a free plan, ChatGPT Plus, or a $200-per-month ChatGPT Pro subscription is the best fit for you.\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nYou should use ChatGPT Plus if...\nChatGPT Plus costs $20 per month. You're probably asking yourself: Why pay when you can use it for free? There are five main advantages, but the TL;DR is that the free tier has heavy usage limits, and the Plus tier gets you the latest features and models first.\nIf you only use ChatGPT now and then for fun, it's not worth subscribing. But if you use it for work, writing, creating images, using it as a search engine, or you find yourself opening the app all day, every day, then subscribing is likely a good idea.\n1. You want access to legacy models\nWhen OpenAI launched GPT-5, the company had it replace all of its legacy models in ChatGPT. The model is meant to combine the best of OpenAI's offerings with a smart model for most queries and a deeper reasoning model for harder problems. However, many paying subscribers were upset because they preferred the prior models for their workflows that were already in place.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nSam Altman even acknowledged in an X post that \"suddenly deprecating old models that users depended on in their workflows was a mistake.\" As a result, OpenAI brought back the option for ChatGPT Plus users to access legacy models, including: GPT-4o, o3, o4-mini, GPT-4.1, and GPT-4.1-mini, in addition to GPT-5 (Auto, Fast, Thinking mini, and Thinking).\n2. You want to access GPT-5's different modes\nWhile GPT-5 combines the best of a deep reasoning model (GPT-5 thinking) and a standard smart model for a balance of speed and quality, users with a Plus subscription can also manually toggle between GPT-5 Thinking, GPT-5 Thinking Mini, and GPT-5 Fast.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nThe benefit: Users can choose to use a reasoning model when they think it would be the best fit instead of having the real router feature automatically understand which model to use based on the conversation, the complexity of the prompt, and more, which is still available with the GPT-5 Auto option. If you often work on harder problems that require the model to think more to output the best possible answer, then having the option to toggle on GPT-5 Thinking could be a compelling reason to subscribe.\n3. You want to access Sora\nOpenAI's Sora video-generating model allows users to create stunning, realistic videos from text prompts or their own assets with up to 1080p resolution. Videos can be up to 20 seconds long, with a limit of 10 seconds for ChatGPT Plus users, and can be widescreen, vertical, or square aspect ratios. Whether you have a real workflow reason for AI-generated content or just want to tinker, it is a fun model to use.\nTo create images, ChatGPT also uses the GPT-4o image generator, OpenAI's most capable image generator to date.\nWith it, you can simply ask \"create an image of\u2026,\" describe a scene, and receive an AI-generated image that can include realistic human generations and even text. You can also upload an existing image and describe edits for GPT-4o to apply -- like adding text, refining details, or changing backgrounds. Just be aware that complex requests may take a couple of minutes.\nAlso: ChatGPT's new image generator shattered my expectations - and now it's free to try\nUsage is capped for free users, but Plus and Pro subscribers can enjoy much higher daily quotas.\nOpenAI said free-tier users can use GPT-4o only a limited number of times within a five-hour window. It will notify you once you've reached the limit and invite you to upgrade to ChatGPT Plus. If you're on the Free tier and rely heavily on image creation, you might see a message like: \"You've reached your image creation limit. Upgrade to ChatGPT Plus or try again tomorrow...\"\nSo, if you often use AI for visual assets, such as videos or images, ChatGPT Plus is an easy choice.\n4. You want to use the Codex AI coding agent\nOpenAI recently integrated its powerful Codex AI coding agent into ChatGPT Plus, so now anyone with the $20-a-month Plus can get AI-powered coding suggestions without shelling out $200. You can point it at your GitHub repo, and it'll whip up code changes, run checks to make sure nothing's broken, and even handle installing dependencies online if you let it. Just keep in mind it doesn't \"remember\" anything between sessions, so you have to give it clear instructions every time.\nAlso: You can use OpenAI's super powerful AI coding agent Codex for just $20 now\nAlso, since the coding agent is new to ChatGPT, you might encounter some speed bumps if many people are using it at once.\n5. You want expanded access to ChatGPT's best features\nYou're probably noticing a theme here: The majority of ChatGPT's most advanced features are eventually made available to free users, but with heavy usage limitations. Therefore, the main benefit of upgrading is getting to use features like Deep Research, Advanced Voice Mode, and GPT-4o image generation a lot more frequently than you could on the free tier. Subscribing to ChatGPT Plus also gives you priority access to OpenAI's latest models and newest tools well before they reach the free tier.\nWith a Plus account, you can access extended limits on:\n- messaging\n- file uploads\n- image creation\n- data analysis\n- Deep Research\n- Agent Mode\n- GPT-4 image generation\n- memory and context\nIf you don't want to miss out on the latest ChatGPT features and models, you should subscribe to the Plus tier.\nYou should use ChatGPT Pro if...\nOK, so now that we've discussed why Plus is worth it, let's look at the more expensive Pro plan that costs $200 a month. Most people would never pay a car payment's worth of money to use AI, but there are special instances in which it could be worth it.\n1. You want what ChatGPT Plus has on a greater scale\nAll ChatGPT Plus perks and features are included in ChatGPT Pro, but with far fewer usage constraints. Pro also adds several exclusive capabilities. Here's a rundown of the biggest benefits, should you be seriously considering the upgrade.\n- Unlimited access to all reasoning models: This includes GPT-5 with pro reasoning, GPT-4o, GPT-4.1 (and its mini variant), o3, o3-pro, o4-mini, and GPT-4.5 (research preview).\n- Unlimited and faster image generation using GPT-4o.\n- Maximum Deep Research, memory, and context, Agent Mode\n- Priority access to Sora video generation: You get up to 1080p video generation, 20-second videos, five concurrent generations, and watermark-free downloads.\n- Higher access to the Codex agent, projects, tasks, and custom GPTs.\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\nAs you can see, Pro offers more features and higher limits than the Plus tier. Pro users reportedly rarely encounter any constraints and can truly leverage the full power of ChatGPT and its newest tools.\n2. You want access to OpenAI's most powerful models\nAs mentioned in the No. 1 selling point of ChatGPT Plus, there are many perks to being able to access the older models, including sticking to what you were already using in your workflow and what you are accustomed to. In addition to all of the legacy models accessible in the Plus tier, with the Pro subscription, you can also access GPT-5 Pro, the most capable version of the model aimed toward complex tasks, and the Thinking 4.5 (research previews), which is only available to Pro users because \"it costs a lot of GPUs,\" according to Altman.\nYou should use free ChatGPT if...\nFinally, let's look at the free version of ChatGPT. I discussed most of its features above, so in this section, I'll break down what advantages it can offer over the paid plans. It essentially comes down to how often you use ChatGPT and whether having access to the latest and greatest features matters to you. If you don't use it much and don't care, the free version is 100% the way to go.\n1. You don't want to pay a monthly fee\nThe free subscription now offers many competitive AI features, reducing the need for a Plus subscription, especially for tools that were once paywalled. Free ChatGPT users can access:\n- GPT-5\n- ChatGPT Search (web browsing for timely information and sources)\n- Image Generation via GPT-4o (daily limit applies)\n- Deep Research (up to five \"lightweight\" tasks per month)\n- ChatGPT Voice (free monthly preview)\n- File and Photo Uploads for in-chat discussions (limited usage)\n- Memory Feature for referencing recent conversations (lightweight version)\nIf you only need occasional AI assistance -- and do not mind daily usage caps -- the free tier should serve you well. It's surprisingly robust and now includes many advanced features that were once behind a paywall. For instance, when OpenAI launched GPT-5, it also became available for free users.\n2. You're a casual ChatGPT user\nIf you rarely hit the daily usage limits for text, voice, or image generation, upgrading to Plus or Pro might not be necessary.\nAlso: ChatGPT can record, transcribe, and analyze your meetings now\nThe free version offers enough headroom for light interactions -- asking a few questions per day, generating a handful of images, briefly exploring advanced data analysis, and more. However, if you often see messages about hitting your limits (for text, voice, images, etc.), it might be time to consider paying for a subscription. For superusers who need top-tier capacity and extended access to advanced features, there's ChatGPT Pro, but Plus is a more affordable middle ground.\nUltimately, if your needs are minimal and limitations don't bother you, sticking with the free tier is perfectly fine.\nFAQs\nHow much do ChatGPT Free, Plus, and Pro cost?\nAs of August 2025, here are the ChatGPT Free, Plus, and Pro pricing tiers and how they differ, as displayed by OpenAI on its website:\nDoes the Pro tier include everything you get with Plus?\nYes, Pro contains everything in Plus. That means unlimited or higher limits on certain features and exclusive access to GPT-5 Pro and Research preview of new features.\nDo ChatGPT Pro users experience downtime or blackout periods?\nPro subscribers have the highest priority for uptime, making downtime extremely rare. However, no tier can guarantee 100% uptime if OpenAI undergoes major outages or scheduled maintenance.\nIf you upgrade to Plus, can you later switch to Pro?\nYou can upgrade from Plus to Pro at any time through your account settings. Your billing date may adjust based on when you switch.\nHow to get early access to new ChatGPT features on the free tier\nUnfortunately, free users typically must wait until OpenAI rolls them out publicly. Plus and Pro subscribers receive early or exclusive testing opportunities before features reach the free plan."
    },
    {
      "url": "https://www.zdnet.com/article/this-is-the-fastest-local-ai-ive-tried-and-its-not-even-close-how-to-get-it/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThis is the fastest local AI I've tried, and it's not even close - how to get it\nZDNET's key takeaways\n- The gpt-oss:20b model is very fast.\n- You'll get blazing-fast answers to your queries with gpt-oss:20b.\n- With the latest version of Ollama installed, you can use this model.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nLet's talk about local AI and speed. There are a lot of factors that go into getting the most speed out of your AI, such as:\n- Whether you have a dedicated GPU.\n- The context length you use (the smaller, the faster).\n- The complexity of your query.\n- The LLM you use.\nI've tried quite a few different local LLMs, using Ollama on both Linux and MacOS, and I've recently run into one that blew all the others away -- with regard to speed. That model is gpt-oss:20b. I've found that on both Linux and MacOS, that model is lights-out faster than the others I've used. This model generates 30 tokens per second.\nAlso: My go-to LLM tool just dropped a super simple Mac and PC app for local AI - why you should try it\nWhat is a token? Think of them as pieces of words used for the processing of natural language. For example, with English text, 1 token is approximately 4 characters or 0.75 words, which means gpt-oss:20b can process 120 characters per second.\nThat's not bad.\nConsider a localized version of llama3.2, which can achieve around 14 tokens per second. See the difference?\nOK, now that I've (hopefully) convinced you that gpt-oss:20b is the way to go, how do you use it as a local LLM?\nHow to update Ollama\nWhat you'll need: To make this work, you'll need either a running version of Ollama (it doesn't matter what desktop OS you're using) or you'll need to install it fresh.\n1. Update Ollama on Linux\nIf you're using Linux, you can update Ollama with the same command used to install it, which is:\ncurl -fsSL https://ollama.com/install.sh | sh\n2. Update Ollama on MacOS or Windows\nTo update Ollama on either MacOS or Windows, you would simply download the binary installer, launch it, and follow the steps as described in the wizard. If you get an error that it cannot be installed because Ollama is still running, you'll need to stop Ollama before running the installer. To stop Ollama, you can either find it in your OS's process monitor or run the command:\nosascript -e 'tell app \"Ollama\" to quit'\nOn Windows, that command would be:\ntaskkill /im ollama.exe /f\nYou might run into a problem. If, after upgrading, you get an error (when pulling gpt-oss) that you need to run the latest version of Ollama, you'll have to install the latest iteration from the Ollama GitHub page. How you do that will depend on which OS you use.\nAlso: How I feed my files to a local AI for better, more relevant responses\nIt is necessary to be running at least Ollama version 0.11.4 to use the gpt-oss models.\nHow to pull the gpt-oss LLM\nThe next step is to pull the LLM from the command line. Remember, the model we're looking for is gpt-oss:20b, which is roughly 13GB in size. There's also the larger model, gpt-oss:120b, but that one requires over 60 GB of RAM to function properly. If your machine has less than 60 GB of RAM, stick with 20b.\nAlso: How to run DeepSeek AI locally to protect your privacy - 2 easy ways\nTo pull the LLM, run the following command (regardless of OS):\nollama pull gpt-oss:20b\nDepending on your network speed, this will take a few minutes to complete.\nHow to use gpt-oss\nOK, now that you've updated Ollama and pulled the LLM, you can use it. If you interact with Ollama from the command line, run the model with:\nollama run gpt-oss:20b\nOnce you're at the Ollama console, you can start querying the newly added LLM.\nIf you use the Ollama GUI app (on MacOS or Windows), you should be able to select gpt-oss:20b from the model drop-down in the app.\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nAnd that's all there is to making use of the fastest local LLM I've tested to date."
    },
    {
      "url": "https://www.zdnet.com/article/i-asked-google-finances-ai-chatbot-what-stocks-to-buy-and-its-answer-surprised-me/",
      "text": "I asked Google Finance's AI chatbot what stocks to buy - and its answer surprised me\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- You can now test the AI-powered Google Finance through Labs.\n- The feature lets you talk to a chatbot and access advanced tools.\n- The chatbot won't give you financial advice.\nUS users can now opt in to test drive the new Google Finance, the company said Wednesday.\nA few weeks ago, Google announced that Google Finance, its platform that offers financial information and news, tools for tracking stocks, and more, was getting an AI makeover.\nIf you missed the initial post, the new Google Finance includes several upgrades. It has improved charting tools to help you visualize data, more real-time data about commodities and cryptocurrencies, a live news feed that lets you see up-to-the-minute headlines, and more.\nAlso: How I cut my monthly cloud storage bill in half - with 5 tough decisions\nPerhaps the biggest upgrade, though, is an AI chatbot that lets you ask finance-related questions. If you're a beginner, you can ask for basic strategies. If you're more advanced, you can talk about in-depth strategies.\nWhile the chatbot can answer data-driven questions like \"What stocks experienced the highest percentage gains yesterday?\", you can't ask for recommendations. When I tried to get the AI chatbot to recommend stocks based on certain parameters, it said that it couldn't as doing so would constitute \"financial advice,\" which it wasn't qualified to give.\nHow to sign up for the new AI-powered Google Finance\nTo try out the redesigned Google Finance, you'll need to head to Google Labs. Look for the toggle that says, \"Turn this experiment on or off to access the new Google Finance.\" After that, tap \"Try Google Finance.\" Once you're in, you'll see the new tool.\nFortunately, Google said there will be a toggle if you want to stick with the classic version of Finance.\nWant to follow my work? Add ZDNET as a trusted source on Google."
    },
    {
      "url": "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
      "text": "How to use GPT-5 in VS Code with GitHub Copilot\nZDNET's key takeaways\n- GitHub Copilot Pro now supports GPT-5 in VS Code.\n- A 30-day trial lets you test premium models for free.\n- Add your OpenAI key to bypass Copilot restriction.\nGPT-5 is now available for use with Microsoft's GitHub Copilot in VS Code. In this article, I'll walk you through the steps of setting up the linkages between VS Code, Copilot, and GPT-5. This process will also work for most other supported large language models you want to use.\nAlso: Microsoft rolls out GPT-5 across its Copilot suite - here's where you'll find it\nStep 1: Enable GitHub Copilot Pro\nYou'll need the Pro version of GitHub Copilot to use GPT-5 at this time. The new model might be available for Copilot's free tier someday, but not yet. There is, however, a 30-day free trial. I'll show you how to set that up here:\nFirst, open VS Code. Click the little Copilot icon (1). That action will open the Copilot pane. Next, click whatever model is listed at (2). Mine is GPT-4.1. Finally, click Add Premium Models (3).\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nThis will take you to a web page where you'll be given the opportunity to try Copilot Pro for 30 days at no cost. Click the big green button.\nUnfortunately, you'll have to add your credit card info, although it won't be charged for 30 days. To prevent abuse of the 30-day limit, Microsoft requires you to give your personal information and your credit card number:\nOnce you've done that, click the Activate Now button:\nStep 2: Enable GPT-5\nYou'll need to restart VS Code for the Pro mode to be made available. Once you do that, click the current model (in the screenshot, it's GPT-4.1), and then scroll and choose GPT-5:\nYou will then need to issue a prompt. In my Deep Research dump of my repository, I was told about some references to a product I sold a few years ago that were still in the product's code. I told GPT-5 to remove all such references.\nThat process resulted in the Enable button showing up. Basically, I think you can use any prompt with GPT-5 to trigger the Enable button. Then click it:\nStep 3: Bring your own key\nBy using the Pro account, you are given a certain number of times you can use the various models. It's unclear how that usage limit is calculated, so I have reached out to Microsoft for clarification. I'll update this article when I get more info.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nIf you want to bypass the possible restrictions and rate limitations, you can use your own API key as provided by your LLM service. You can learn more from Microsoft's language models page.\nWith ChatGPT, you can get an OpenAI Platform Key by pointing your browser here. If you don't already have an OpenAI account, you may need to give them some credit card information as well. Then click Create Key and follow the directions:\nOnce you have your key, go back to Manage Models (by clicking the current model you're using and choosing Manage Models). Select OpenAI:\nType or paste in your key. Press Enter to confirm:\nCongratulations, you're now running GPT-5 in Copilot.\nHave you used Copilot?\nHave you tried using GPT-4.1 or GPT-5 in VS Code yet? What do you think of the Copilot Pro experience so far? Does it feel like a worthwhile upgrade? Have you explored using your API key instead of relying on Microsoft's allocation?\nAlso: The best AI for coding in 2025 (and what not to use)\nWhat kinds of tasks or prompts have you found AI particularly helpful for in your coding work? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/3-smart-ways-business-leaders-can-build-successful-ai-strategies-before-its-too-late/",
      "text": "3 smart ways business leaders can build successful AI strategies - before it's too late\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Business leaders should create a platform to test AI concepts.\n- Encourage employees to take risks with AI, but proceed with care.\n- Keep one eye on the market for new technologies that might be exploited.\nMaking the most of AI is tough. MIT recently revealed that 95% of enterprises attempting to harness generative AI aren't seeing measurable results in revenue or growth.\nHowever, with agentic AI and deep research adding new layers of complexity, the board's demand for the successful exploitation of emerging technology has never been greater. So, what should business leaders do?\nAlso: How AI-enabled autonomous business will change the way you work forever\nFor Kirsty Roth, chief operations and technology officer at business information services specialist Thomson Reuters, the answer is simple: focus on strategy.\nHer firm's recently released Future of Professionals Survey, which polled 2,275 professionals and C-level executives from over 50 countries, found that firms with a formal AI strategy are twice as likely to experience revenue growth. Those firms are also 81% more likely to experience the benefits of AI.\nAlso: Gartner says add AI agents ASAP - or else. Oh, and they're also overhyped\nHowever, the research also found that only 22% of organizations have such a strategy in place. For Roth, this oversight is a big mistake.\n\"Your AI strategy must be clear,\" she told ZDNET. \"From the top of the company, being clear that AI is an important thing you need to be doing, and why, I think, is very helpful. Ensure you have a strategy and it's well-defined for people what that means and what the opportunity is.\"\nHere are three ways business leaders can build a successful AI strategy.\n1. Create an AI platform to test new ideas\nRoth explained how Thomson Reuters has created Open Arena, an internal AI platform that allows staff to access major large language models (LLMs) and internal data securely.\n\"We have basically every single one of the world-leading LLMs in there,\" she said, recognizing that it's unusual for a blue-chip enterprise to provide access to so many models.\nRoth and her team try to access as many LLMs as possible to boost software engineering processes.\n\"If I did this role in a corporation that didn't develop software, that approach probably wouldn't be worth doing. You might pick one or two models,\" she said.\n\"However, given that we need to know what the best LLMs are all the time, because we're putting them in our products, then we have them all in the AI platform.\"\nAlso: Gen AI disillusionment looms, according to Gartner's 2025 Hype Cycle report\nRoth said her company's strong partnerships with major LLM providers mean her team gets to test new technologies early, which is helpful.\nThe company also makes acquisitions that can help push its AI platform in new directions.\nAs a result, the AI platform goes beyond Open Arena to internal foundation models and deep research explorations.\n\"We acquired Safe Sign Technologies last year out of Cambridge, UK. They are helping us develop our own foundational models in the legal space right now,\" she said.\n\"So, we've started to develop our own models, as well as what we've been doing for a while, which is RAG-type solutions and making the best of the public models.\"\n2. Agree on your desired destination\nThomson Reuters needed a strong objective for its explorations into AI -- and Roth said that's where use cases came in.\n\"We said, 'Where do we think, given what we know about the tools, we could go and make a difference with AI?' So, whether that was improving sales, improving the way we develop our content, or boosting the processes in the call center.\"\nIn terms of project leadership, Roth called on a CIO who is focused on internal processes. A CTO, who is the company's head of engineering, considered external, customer-focused products.\nHer team tried and refined about 200 use cases: \"They said, 'These are the things that we think AI should work with.' We then said to the business, 'We can buy you technology that will be helpful. Or we can build something.' And we've put about 70 use cases live.\"\nAlso: 71% of Americans fear that AI will put 'too many people out of work permanently'\nRoth described this approach to AI project development as experiment-based rather than theoretical.\n\"We'll try an idea in one team. If it works, great. Then we typically take all the same teams that do the same thing and roll the implementation out widely. And if it doesn't, we won't.'\"\nShe continues to encourage her team of hundreds of engineers and the professionals who use the tools her organization implements to embrace risk, but carefully.\n\"If you've built an AI-enabled tool for someone in sales that's going to tell them, 'This is what we recommend is the next thing you should go and do,' they still have a responsibility to use their brain and check the outputs,\" she said.\n\"I think embracing risk in AI is about getting people comfortable with that human-in-the-loop approach. Understand AI, make it as helpful as you can, but give yourself a sense check. People still have an accountability to go and check the answers are correct.\"\n3. Reimagine processes using fresh innovations\nRoth said the final phase of Thomson Reuters' AI strategy is about reimagining processes.\n\"So, instead of trying to chop something up and make it better piece by piece, given the tooling today, how could you completely reimagine the way that you do something internally? And I think that's going to be a huge focus for us in the next two years,\" she said.\nThe pace of change with AI is so fast, said Roth, that business leaders must keep one eye on the market and the other on how new technologies might be exploited.\n\"If you'd asked me last year if the first half of this year was going to be all about deep research and agentic, I'm not sure I would have predicted that,\" she said. \"There's new stuff being shown to us every day.\"\nAlso: Stop using AI for these 9 work tasks - here's why\nThomson Reuters is already exploring agentic AI. Roth said the aim is that these emerging technologies will work autonomously, helping professionals in her company and at client organizations to work quickly and effectively.\n\"As we roll forward, the agents are going to make the process of working out the context of what you're trying to do much easier,\" she said.\n\"What's clear is there will be a big push for agentic solutions coming out. And in the spaces that we've looked at, I think the agentic solutions are helpful and very powerful.\"\nThe company is also exploring other areas of AI. It recently released CoCounsel Legal, which includes deep research and agentic-guided workflows. The technology, which is grounded in Westlaw, the company's legal research tool and proprietary database for lawyers and legal professionals, creates citation-backed reports.\nAlso: 5 ways automation can speed up your daily workflow - and implementation is easy\nRoth said the move toward deep research is representative of how professionals will start to use AI like a trusted teammate.\n\"We're all learning how to be good prompt engineers now. By using deep research, you'll get a good summary, with all the depth, and far more insight than you would if you asked a model question by question,\" she said.\n\"So, deep research is a bit like packaging up your work in one go -- that's a good way of thinking about it. And if you think about lawyers, they may want a timeline of what happened in a case, and all the details summarized, or all the precedents together.\"\nWant to follow my work? Add ZDNET as a trusted source on Google."
    }
  ],
  "argos_summary": "Fast\u2011food giants Taco Bell and McDonald\u2019s have paused their AI\u2011driven drive\u2011thru experiments after customers complained about glitches, delays, and odd orders\u2014such as an 18,000\u2011cup water request. Both chains are now exploring alternative AI applications, like voice\u2011powered ordering via Google Cloud, while keeping human staff in the loop during busy periods. The move underscores the need for robust testing and customer\u2011centric design before scaling generative AI in retail operations.",
  "argos_id": "KSER26TEY"
}