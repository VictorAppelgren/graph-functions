{
  "url": "https://www.pocket-lint.com/vrr-on-tv/",
  "authorsByline": "Roger Fingas",
  "articleId": "4b5ea5522cc941e6b0fb2062dc2672fa",
  "source": {
    "domain": "pocket-lint.com",
    "location": {
      "country": "gb",
      "city": "London",
      "coordinates": {
        "lat": 51.5073219,
        "lon": -0.1276474
      }
    }
  },
  "imageUrl": "https://static1.pocketlintimages.com/wordpress/wp-content/uploads/wm/2025/08/untitled-design-1.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-11T16:44:53+00:00",
  "addDate": "2025-08-11T19:45:43.255438+00:00",
  "refreshDate": "2025-08-11T19:45:43.255439+00:00",
  "score": 1.0,
  "title": "This one TV feature matters more than having the fastest refresh rate",
  "description": "Raw speed hardly counts if it's not being used properly.",
  "content": "While most of the specs hype around smart TVs tends to revolve around their resolution, brightness, and HDR (high dynamic range) formats, you'll also see refresh rates (measured in hertz) come up as a frequent bragging point. Indeed, the way some product pages go on about them, you'd think that refresh rates were everything -- as if specs that weren't as fast as possible would be a disappointment. It's why motion smoothing is still an option on TVs, even though it tends to lead to the \"soap opera effect\" when you watch movies or shows shot at 24 frames per second (fps).\n\nWhat matters more these days is a feature called variable refresh rate, or VRR for short. In this guide, I'll cover not just what it is and how it works, but what's required for it, and why it matters far more than the fastest possible refresh technology. I guarantee it'll change how you go about TV shopping.\n\nWhat does variable refresh rate tech do on your TV?\n\nDealing with the realities of modern viewing\n\nOnce upon a time, refresh rates didn't really matter on TVs. People were mostly watching antenna, cable, or satellite broadcasts, delivered to their TVs at a fixed rate, and further limited by the speed of cathode ray tubes. You'd mix things up occasionally with a VCR, a laserdisc player, or an early game console, but even those weren't especially demanding.\n\nNow, of course, TVs are all-digital, and a window to a lot more than just movies and TV shows. You might, for example, jump from a movie shot at 24fps to a YouTube clip shot at 60fps. Some people connect computers to their TVs for both work and entertainment, and consoles have become so advanced that a lot of people mistake a game like Madden or NBA 2K26 for a sports channel.\n\nVRR adjusts refresh rates on the fly based on the details of an incoming video signal. If you're watching a 60fps YouTube video, for example, it'll set your TV's refresh rate to 60Hz. With games, this technology is almost constantly in action, since frame rates are in flux depending not just on content, but how well a PC or console's processor can keep up. A game with state-of-the-art visual effects might hit 60fps most of the time, but dip to 30fps or less during intense action scenes.\n\nWhen a signal is coming from an external peripheral like a PC, console, media streamer, or Blu-ray player, VRR requires that your TV port, the connecting cable, and the peripheral itself support HDMI 2.1 or later. Earlier versions of HDMI are stuck at fixed frame rates. For the best possible resolution and refresh rate options, you can choose HDMI 2.2, but nothing's really exploiting that yet.\n\nWhy are variable refresh rates such a big deal?\n\nIt's about the right speed, not the fastest\n\nA mismatch between frame rates and refresh rates doesn't automatically spell doom. In fact, even a TV with a fixed 60Hz refresh rate should look good most of the time, as long as it's not just simulating that with a motion smoothing option. Seriously, no matter how good your TV is, you should always disable smoothing immediately -- it's nice for sports and live news, but it can make something like Dune: Part Two or Lord of the Rings: The Return of the King look like it has the same budget as a '90s episode of General Hospital.\n\nThe problem with fixed refresh rates is that in some circumstances, they can lead to artifacts like stuttering, i.e. a choppy look when frame rate falls. Perhaps the worst effect is \"screen tearing,\" caused by multiple frames being displayed simultaneously. You're most likely to see stuttering and tearing in games, since it's easier for frame rates to fall out of sync with refresh rates.\n\nWith both games and other apps, TVs without VRR can feel laggier, which is a deal-killer for a lot of people. The effect stems from the fact that if a frame isn't rendered fast enough by a graphics processor (GPU), non-VRR sets display either the last frame or only part of the new one -- hence tearing. Ironically, by forcing the pipeline to wait until each frame is rendered properly, VRR creates a more fluid experience as far as your eyes are concerned.\n\nMaximum refresh rates are still important, don't get me wrong. If a connected source is capable of frame rates over 60, you won't be able to see those extra frames unless you've got a TV with 120Hz or better. Realistically, however, the further you go past 120Hz, the less likely you are to benefit. Even a top-of-the-line PC with Nvidia's GeForce RTX 5090 graphics card is unlikely to push most games past 120fps, not when they're rendered at 4K with maximum detail. There may be some benefit to a 144Hz TV when playing older games, or newer ones with lower resolutions or detail -- but even that's questionable. Anything over 144Hz is overkill, particularly given the limits of human anatomy. It's already difficult for many people to discern frame rates past the 60fps mark.\n\nSo why are some TV makers bothering with 144, 165, or even 240Hz TVs? Bragging rights enabled by new panel and processor technologies, essentially. It looks more impressive to shoppers who don't know any better, or who do know better but can afford the best of the best for its own sake.\n\nIf you're like most people and have a fixed budget, your money is much better spent on a 120Hz TV with VRR and other use-case amenities than hunting for a model with the highest possible specs in every category. Samsung's 240Hz TVs don't support Dolby Vision -- and that's going to matter a lot more in the long run than refresh rates your eye can't even register.",
  "medium": "Article",
  "links": [
    "https://www.pocket-lint.com/smart-tv-video-format-guide/",
    "https://www.pocket-lint.com/8k-gaming-isnt-worth-the-effort-yet/",
    "https://www.pocket-lint.com/peak-brightness-explained/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "frame rate",
      "weight": 0.11537008
    },
    {
      "name": "frame rates",
      "weight": 0.11537008
    },
    {
      "name": "fixed frame rates",
      "weight": 0.11246282
    },
    {
      "name": "refresh rates",
      "weight": 0.11246156
    },
    {
      "name": "fixed refresh rates",
      "weight": 0.10994895
    },
    {
      "name": "variable refresh rate",
      "weight": 0.10746088
    },
    {
      "name": "variable refresh rates",
      "weight": 0.10746088
    },
    {
      "name": "Maximum refresh rates",
      "weight": 0.10598517
    },
    {
      "name": "variable refresh rate tech",
      "weight": 0.1010514
    },
    {
      "name": "TVs",
      "weight": 0.089192584
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Shopping/Consumer Resources/Product Reviews & Price Comparisons",
      "score": 0.88427734375
    }
  ],
  "sentiment": {
    "positive": 0.09868115,
    "negative": 0.4609999,
    "neutral": 0.440319
  },
  "summary": "The article discusses the importance of variable refresh rate (VRR) in TVs, a feature that adjusts refresh rates based on an incoming video signal based on the details of an incoming signal. This feature, VRR, requires a TV port, the connecting cable, and the peripheral itself to support HDMI 2.1 or later, as well as a PC, console, media streamer, or Blu-ray player. Despite the increasing importance of motion smoothing on TVs, this feature is often overlooked due to the \"soap opera effect\" when watching movies or shows shot at 24 frames per second (fps). However, a mismatch between frame rates and refresh rates can lead to artifacts like stuttering and tearing, particularly in games.VRR can also be used to create a more fluid and fluid experience as far as possible. However, maximum refresh rates are still crucial, as long as a connected source is capable of rendering frames over 60.",
  "shortSummary": "Despite the prevalence of variable refresh rates in smart TVs, they remain crucial as they prevent stuttering and screen tearing, especially in gaming and entertainment.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "f79acfd1503f4e7baba13efba09339f6",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.pocket-lint.com/smart-tv-video-format-guide/",
      "text": "Summary\n- HDR improves color and contrast on smart TVs, making content more vibrant and detailed.\n- HDR10 provides static adjustments, while HDR10+ uses dynamic metadata to optimize the image on screen.\n- Dolby Vision is proprietary technology that rivals HDR10+, with divisions between TV manufacturers and streaming services.\nThere is no shortage of acronyms, abbreviations, and marketing terms that swirl around smart TVs, making an investment in one often a confusing and tedious process. Some terms may describe technical hardware features while other, seemingly similar terms, are actually about branding and marketing. An OLED screen is a technical consideration, but the Neo-QLED is marketing. Refresh rate is hardware, but motion rate is software.\nThe same complications arise when you look at video formatting, an important component of enjoying modern content on your smart TV. High Dynamic Range (HDR) is a crucial feature that offers more detail, color, and contrast, but the technology can get mighty confusing very quickly. Here's what you need to know.\nWhat is HDR, or High Dynamic Range?\nThis essential technology is found on most smart TVs\nHigh Dynamic Range is a televisual technology that improves both color and contrast. When watching a TV with HDR, you'll likely notice colors are more vibrant compared to TVs that don't have HDR. Not only do colors tend to 'pop' more, but they are also more accurate and realistic.\nIn terms of contrast, HDR allows for blacks to be darker and whites to be brighter, with more noticeable detail in both extremes. Darker scenes are a bit more nuanced, with shadowing allowing you to decipher details that otherwise blur together on TVs without HDR.\nJust about every new smart TV uses HDR to improve the image, and a lot of new content is shot in HDR. Most new titles from streaming services in particular are encoded in HDR, meaning in order to enjoy the content to its fullest, you'll need an HDR-comptaible TV. HDR significantly improves upon Standard Dynamic Range (SDR), which offers limited color and brightness ranges.\nWhat is HDR10 and HDR10+?\nKey differences separate these two video formats\nHere's where things start to get confusing. HDR is a technology, and there are different types of formats of said technology. Two of the most notable HDR formats are HDR10 and HDR10+, and despite the similar names, they boast some key differences.\nHDR10 is an open standard format and free to use, so it's found on many TVs. HDR10+ is also open to use, although it requires certification from its creators, which includes Samsung and Amazon. While HDR10 makes for a good image on screen, HDR10+ is better. That's because HDR10 is static and HDR10+ is dynamic.\nJust because a TV supports HDR doesn't necessarily mean it's great at it. Screen technologies and processing power influence the extent to which HDR formats look good.\nThat means that when you're watching content with HDR10, the brightness and color set will stay the same throughout the title as the format uses static metadata. HDR10+, meanwhile, employs dynamic metadata, which means that brightness and color settings are changing throughout the title based on the needs of any given scene. Simply, HDR10+ optimizes the visual settings and adapts as needed to what's happening on TV.\nWhat is Dolby Vision?\nThe top rival to HDR10+ offers some improvements\nAnother HDR format is Dolby Vision, which, like HDR10+, uses dynamic metadata to optimize the content on screen as you're watching. Dolby Vision and HDR10+ share a lot of similarities. Both offer similar peak brightness, and even though Dolby Vision can support a higher color bit depth (12-bit versus 10-bit), that slight difference doesn't really factor because most content only goes to 10-bit anyway.\nThe key difference comes down to availability. Dolby Vision is a proprietary technology made by Dolby, and requires manufacturers to pay a licensing fee to use it. Samsung, which pushed for the creation and implementation of HDR10+, does not use Dolby Vision. Competitors LG and Sony, however, do. Some models by Vizio, TCL, and Hisense will support both formats. In terms of streaming services, most popular options support both formats, though some lean in one direction more than the other. Amazon Prime Video has a lot of HDR10+ content, while Apple TV+, Paramount+ and Hulu are also boosting their HDR10+ titles. Netflix and Disney+, meanwhile, offer a lot of titles compatible with Dolby Vision.\nHow do video formats influence what TV I should buy?\nIf you watch a lot of content, the answer can get complicated\nVideo formats muddle up the process of buying a TV. The simplest approach is to make sure you buy a new smart TV that supports at least one of HDR10+ or Dolby Vision. These two formats are similar and offer the best way to watch modern content. If you're looking for a new TV to act as a main entertainment hub, and you frequently watch newer content from the top streaming services as well as major titles from big movie studios, then you'll want either of the top two formats and to avoid a TV that tops out at HDR10.\nMost new smart TVs will automatically recognize a supported video format when playing a title from physical media or a streaming service, so nothing is required from the viewer.\nDeciding between Dolby Vision and HDR10+ is more complicated. I don't know that either one should be the sole factor in determining whether you buy an LG smart TV or a Samsung one. Since most streaming services offer titles in both formats, it's also going to be hard to accurately assess which format you are likely to use more, especially if you use multiple services. It's important to know that streaming services don't necessarily want to align with one particular type of format, and many are trying to offer both as much as they can to appeal to all audiences. So, a catalog's compatibility today might change in the coming months and years.\nWhen shopping for a new smart TV, it's important to understand the differences between the main types of video formats, and the ways in which they can improve your picture. Once you know how they work, you can analyze the video formats of the content you frequently consume and find a TV that's most compatible in order to get the most out of your entertainment experience."
    },
    {
      "url": "https://www.pocket-lint.com/8k-gaming-isnt-worth-the-effort-yet/",
      "text": "Summary\n- The only console with practical 8K is the PS5 Pro, and even that has a minimal library.\n- 8K on a PC may require a video card that costs as much as an entire computer.\n- It may be years until hardware and the games industry catch up to the 8K standard.\nI've noticed that TV makers aren't pushing the 4K mantra as much as they have in the past, and considering everything, it makes perfect sense. 4K is no longer a selling point -- it's the new norm for TVs over 40 inches. A similar tone change happened when most sets started shipping with 1080p instead of 720p, or the atrocity that is 1080i.\n8K technology has been around for a few years now, and if you're here, you're probably wondering if you should be investing in a compatible system for console or PC gaming. The bad news is that 8K is far from being practical -- and probably won't be a big deal by the time it is. Here's why.\nPlayStation 5 Pro\n- 4K Capability\n- Yes\n- Brand\n- PlayStation\nPlayStation 5 Pro is the high-end console, featuring a brand-new GPU, AI-driven upscaling technology referred to as PSSR, and advanced ray tracing for more accurate reflections and lighting. The console is digital-only, requiring the seperate purchase of a disc-drive.\n- Storage\n- 2TB\n- Screen Resolution\n- VRR and 8K\nHardware limits are still in the way\nUnless you're rich, that is\nTechnically speaking, the PlayStation 5 and PlayStation 5 Pro are both 8K-capable consoles. Realistically, however, few PS5 games run in 8K unless they're upscaled from 4K. The only native 8K title for the base PS5 is The Touryst, and the Pro is limited to a handful of options at the moment, like Gran Turismo 7 and No Man's Sky. Some of those are capped at 30 frames per second unless you dial resolution back down.\nEffectively, 8K on a console means buying Sony's most expensive machine to play a small catalog. The base PS5 and Microsoft's Xbox Series X support 8K on paper only -- they sometimes struggle with detailed 3D in 4K, never mind pushing four times the number of pixels. In fact, part of the way the PS5 Pro gets around this is PlayStation Spectral Super Resolution, an AI-based upscaling technology. The output is true 8K -- but the games themselves are still in 4K.\nEffectively, 8K on a console means buying Sony's most expensive machine to play a small catalog.\nThings are arguably worse on the PC side, since you need a dedicated graphics card (GPU) with enough VRAM and performance overhead. As of this writing, that may mean Nvidia's RTX 4090, which costs a ridiculous $1,600 or more -- as much as some whole desktop configurations. Even most of the company's upcoming 50-series cards probably won't deliver 8K at a steady 60fps.\nA shared problem between PCs and consoles is the cost of 8K panels. Many \"cheap\" 8K TVs are close to $2,000, and the best are over $3,000. 8K monitors aren't even a thing for the most part, since there's not much point on a screen that close to your face, and they cost just as much as the hardware needed to drive them. You're better off buying an ultra-wide monitor when it comes to immersive gaming.\n8K doesn't offer a real advantage\nBe smarter with your gaming budget\nSimply put, 8K doesn't look that much sharper on the screen sizes people actually use for games. I own a 65-inch 4K TV, on which it's still sometimes difficult to tell the difference between native 4K and 1080p. For 8K to make an impact, you need a gigantic display relative to your point of view -- possibly so large that even if you can afford one, it might not fit. Personally, I'd choose a 4K OLED set over an 8K QLED any day of the week, since the colors and contrast of OLED are more impressive.\nWith gaming, refresh rates should be a higher priority than resolution, since a 120 or 144Hz panel translates into smoother, more responsive gameplay. Your existing TV or video card probably already supports that at 1080p or 1440p, if not 4K. Personally, I find 1440p and 120Hz to be more than enough for most games.\nSignificantly, there still isn't much 8K content worth upgrading for, in gaming or otherwise. Some YouTube videos are in 8K, but services like Netflix, Max, and Disney+ top out at 4K, even with their highest pay tiers. You won't find 8K movies on Blu-ray, either, since the format just doesn't have enough space to cram a film on a single disc. Game developers aren't optimizing their models and textures for 8K, given how much space that would take on your hard drive.\nIs there any situation in which 8K makes sense for gaming?\nYou know the short answer\nNot really. It might be essential if you're setting up a large-scale gaming experience, say at a cinema or entertainment center, but that's about it. Otherwise, heading into 2025, it's pure luxury.\nThere is another case, potentially: futureproofing. If you've got thousands of dollars to spare, an 8K setup is bound to last you a few years, since it's the cutting edge of what's available. But even then you may find yourself wishing you'd stuck with 4K OLED -- by the time there's a reasonable amount of 8K content available, the cost of displays and compatible game systems will have gone down, and the best ones will have even better technology -- imagine playing on an 8K micro-LED panel. There are more valuable things you could be spending money on these days -- including the actual games."
    },
    {
      "url": "https://www.pocket-lint.com/peak-brightness-explained/",
      "text": "Summary\nLED T\n- A brighter TV is important when watching in well-lit environments.\n- A high peak brightness of at least 1,000 nits is essential for HDR10+ and Dolby Vision enjoyment.\n- LED TVs can get much brighter than OLED models, but the latter offer superior contrast.\nAmong the many important features of a smart TV is brightness. Basically, you need to be able to see the screen clearly and vividly whenever and wherever you choose to watch. A dim screen doesn't do much good for anything, especially with so many shows, movies, and video games boasting bright, dynamic colors. Brightness also tends to be promoted as a means to help you discern everything that's going on in the darker scenes of whatever you're watching.\nEvery new smart TV boasts about brightness, throwing around random numbers and marketing terms to tempt you into a purchase. As a result, it's important to know what brightness is really about, and how to best navigate all the promotional material and technical terms that get talked about.\nWhat is brightness, and how do you measure it?\nA simple yet important setting\nBrightness is fairly straightforward when it comes to smart TVs, or really any screen for that matter. It's a measurement of how much light the screen can emit. With smart TVs, light can come from two different sources. LED TVs, including QLED models, feature a lighting panel behind the screen. That panel lights up in order to produce light on the screen in front of it. OLED TVs, meanwhile, do not have a back lighting panel. Instead, individual organic pixels on the screen turn on and off, producing light on their own.\nBrightness is measured in nits, a curious unit that is actually the ratio of the brightness of one candle per square meter. The specific measurement (or how it came to be) doesn't matter so much as the associated number. Most mid-range TVs offer a brightness of anywhere between 500 and 1,000 nits, with budget-friendly TVs often falling below that range. Newer and more advanced TVs regularly surpass 1,000 nits.\nWhy is brightness so important?\nBrightness influences the visual experience\nBrightness is crucial to watching TV in a well-lit room. Ideally, you want to enjoy your smart TV in a dark room, devoid of ambient light and perhaps just a bit of bias lighting in the form of a table lamp. You don't really want the sun coming in. However, that's not always possible or wanted, especially if you want to watch TV on a sunny afternoon. As a result, you need a TV with enough brightness to overcome the lighting in the room and stand out.\nBrightness as well as peak brightness, the top limit to just how bright your TV can get in small, fleeting doses, is also a big contributor to the quality and enjoyment of a lot of new shows and movies. Namely, peak brightness is key for enjoying HDR10+ and Dolby Vision, two of the top video formats that comprise plenty of new films, TV shows, and video games. These formats are designed to be compatible with smart TV boasting a high peak brightness, over 1,000, and because they are dynamic settings, the brightness will automatically adjust throughout in order to reproduce the most faithful image possible. It's another example of needing your smart TV's hardware, in this case peak brightness, matching the format of the source material.\nTV settings and software adjustments\nPlaying around with settings only goes so far\nYour TV features a roster of settings you can tinker with in order to change or enhance the picture depending on what you're watching, and how much light is in the room. This includes boasting the brightness. It's important to keep in mind that any such settings, from brightness to contrast to motion blur, are influenced both by your TV's hardware and software. Basically, the hardware puts a hard limit on any changes your TV can make, and software enhancements allow the perception of change on top of what your TV is actually capable of. Hardware specs are native and organic to an extent, while software specs are artificial enhancements.\nAdjusting your TV's brightness settings can definitely help you enjoy the image better, but there is a very small limit to what it can do. It basically adjusts the black level so that it appears as if your TV is brighter than it really is. Like most of your TV's settings, you'll want to avoid going too far in any one direction. Setting it too high will make blacks appear gray and washed out, while setting it too low will ruin contrast and mute the colors on screen.\nHow much brightness do you need?\nFind the TV that fits your viewing needs\nFor those looking to enjoy new shows, films, and video games to their fullest, you're best served by a new TV with a peak brightness of at least 1,000 nits. These will satisfy those who subscribe to the top streaming tiers that include HDR10+ or Dolby Vision, as well as anyone gaming on the latest-gen consoles. These TVs will boast a high peak brightness that can meet the needs of the top video formats, which means you can enjoy all the color and flashes of brilliance from what you're watching.\nMore casual viewers don't need to worry so much, especially if you tend to watch live sports, news, reality TV, or anything on the basic cable channels. These broadcasts tend to be created for mass consumption so that they look good on most TVs, new or old. If you watch some newer fare but aren't entirely concerned with fidelity and immersion, a smart TV with peak brightness from 800 to 1,000 nits should suffice.\nOLED models have lower peak brightness than comparable QLED models. However, OLED TVs have an infinite contrast ratio with perfect blacks due to their emissive pixels, and don't need peak brightness levels that are as high.\nIt's worth mentioning that brightness is at the forefront of TV innovation, with companies continuously trying to outdo each other for the brightness possible model. There are plenty available now with a peak brightness hitting 2,000 nits and up, with forthcoming models this year and in the years to come topping even 3,000 nits.\nLastly, brightness isn't everything. It's vital that your bright TV also has an effective anti-glare coating. All those extra nits won't matter too much if, when you look at the screen, you see reflections on the screen. Any light source, whether it's the sun or a lamp or an overhead light, can be reflected on screen if you have an inferior smart TV. Matte finishes, for example, are highly beneficial at combating glare, which is why they are frequently a component of lifestyle TVs. Brightness is important, but be sure what you're buying can fully support watching the screen in a well-lit environment."
    }
  ],
  "argos_summary": "The article discusses the significance of variable refresh rate (VRR) technology in modern smart TVs, emphasizing its ability to adjust refresh rates dynamically based on the content being viewed, which enhances the viewing experience by reducing issues like stuttering and screen tearing. It contrasts VRR with fixed refresh rates, noting that while maximum refresh rates are still relevant, features like VRR and HDR (high dynamic range) are more critical for a smoother and more visually appealing experience. The piece also touches on the importance of brightness in TVs, particularly for HDR content, and advises consumers to prioritize features that enhance overall performance rather than just high specifications.",
  "argos_id": "WDAEHNXNL"
}