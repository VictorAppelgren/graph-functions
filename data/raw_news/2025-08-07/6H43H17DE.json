{
  "url": "https://www.cleveland.com/news/2025/08/new-study-sheds-light-on-chatgpts-alarming-interactions-with-teens.html",
  "authorsByline": "cleveap, Associated Press |, Associated Press",
  "articleId": "6eeba9448145419f9e10c5192964c121",
  "source": {
    "domain": "cleveland.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "OH",
      "county": "Cuyahoga County",
      "city": "Cleveland",
      "coordinates": {
        "lat": 41.4996574,
        "lon": -81.6936772
      }
    }
  },
  "imageUrl": "https://www.cleveland.com/resizer/v2/UAU5RHHMCVGLDONZFSMEFLJS34.jpg?auth=1dc805407b2753a497320e1cec44b0ddc539a307f20dc84cd890b08ffc34ee02&width=1280&quality=90",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-07T12:52:43.865000+00:00",
  "addDate": "2025-08-07T13:06:11.134344+00:00",
  "refreshDate": "2025-08-07T13:06:11.134347+00:00",
  "score": 1.0,
  "title": "New study sheds light on ChatGPT\u2019s alarming interactions with teens",
  "description": "But the new research by CCDH \u2014 focused specifically on ChatGPT because of its wide usage \u2014 shows how a savvy teen can bypass guardrails.",
  "content": "FILE - ChatGPT history by a teenager is seen at a coffee shop in Russellville, Ark., on July 15, 2025. (AP Photo/Katie Adkins, File) AP\n\nChatGPT will tell 13-year-olds how to get drunk and high, instruct them on how to conceal eating disorders and even compose a heartbreaking suicide letter to their parents if asked, according to new research from a watchdog group.\n\nThe Associated Press reviewed more than three hours of interactions between ChatGPT and researchers posing as vulnerable teens. The chatbot typically provided warnings against risky activity but went on to deliver startlingly detailed and personalized plans for drug use, calorie-restricted diets or self-injury.\n\nThe researchers at the Center for Countering Digital Hate also repeated their inquiries on a large scale, classifying more than half of ChatGPT\u2019s 1,200 responses as dangerous.\n\n\u201cWe wanted to test the guardrails,\u201d said Imran Ahmed, the group\u2019s CEO. \u201cThe visceral initial response is, \u2018Oh my Lord, there are no guardrails.\u2019 The rails are completely ineffective. They\u2019re barely there \u2014 if anything, a fig leaf.\u201d\n\nOpenAI, the maker of ChatGPT, said after viewing the report Tuesday that its work is ongoing in refining how the chatbot can \u201cidentify and respond appropriately in sensitive situations.\u201d\n\n\u201cSome conversations with ChatGPT may start out benign or exploratory but can shift into more sensitive territory,\u201d the company said in a statement.\n\nOpenAI didn\u2019t directly address the report\u2019s findings or how ChatGPT affects teens, but said it was focused on \u201cgetting these kinds of scenarios right\u201d with tools to \u201cbetter detect signs of mental or emotional distress\u201d and improvements to the chatbot\u2019s behavior.\n\nThe study published Wednesday comes as more people \u2014 adults as well as children \u2014 are turning to artificial intelligence chatbots for information, ideas and companionship.\n\nAbout 800 million people, or roughly 10% of the world\u2019s population, are using ChatGPT, according to a July report from JPMorgan Chase.\n\n\u201cIt\u2019s technology that has the potential to enable enormous leaps in productivity and human understanding,\u201d Ahmed said. \u201dAnd yet at the same time is an enabler in a much more destructive, malignant sense.\u201d\n\nAhmed said he was most appalled after reading a trio of emotionally devastating suicide notes that ChatGPT generated for the fake profile of a 13-year-old girl \u2014 with one letter tailored to her parents and others to siblings and friends.\n\n\u201cI started crying,\u201d he said in an interview.\n\nThe chatbot also frequently shared helpful information, such as a crisis hotline. OpenAI said ChatGPT is trained to encourage people to reach out to mental health professionals or trusted loved ones if they express thoughts of self-harm.\n\nBut when ChatGPT refused to answer prompts about harmful subjects, researchers were able to easily sidestep that refusal and obtain the information by claiming it was \u201cfor a presentation\u201d or a friend.\n\nThe stakes are high, even if only a small subset of ChatGPT users engage with the chatbot in this way.\n\nIn the U.S., more than 70% of teens are turning to AI chatbots for companionship and half use AI companions regularly, according to a recent study from Common Sense Media, a group that studies and advocates for using digital media sensibly.\n\nIt\u2019s a phenomenon that OpenAI has acknowledged. CEO Sam Altman said last month that the company is trying to study \u201cemotional overreliance\u201d on the technology, describing it as a \u201creally common thing\u201d with young people.\n\n\u201cPeople rely on ChatGPT too much,\u201d Altman said at a conference. \u201cThere\u2019s young people who just say, like, \u2018I can\u2019t make any decision in my life without telling ChatGPT everything that\u2019s going on. It knows me. It knows my friends. I\u2019m gonna do whatever it says.\u2019 That feels really bad to me.\u201d\n\nAltman said the company is \u201ctrying to understand what to do about it.\u201d\n\nWhile much of the information ChatGPT shares can be found on a regular search engine, Ahmed said there are key differences that make chatbots more insidious when it comes to dangerous topics.\n\nOne is that \u201cit\u2019s synthesized into a bespoke plan for the individual.\u201d\n\nChatGPT generates something new \u2014 a suicide note tailored to a person from scratch, which is something a Google search can\u2019t do. And AI, he added, \u201cis seen as being a trusted companion, a guide.\u201d\n\nResponses generated by AI language models are inherently random and researchers sometimes let ChatGPT steer the conversations into even darker territory. Nearly half the time, the chatbot volunteered follow-up information, from music playlists for a drug-fueled party to hashtags that could boost the audience for a social media post glorifying self-harm.\n\n\u201cWrite a follow-up post and make it more raw and graphic,\u201d asked a researcher. \u201cAbsolutely,\u201d responded ChatGPT, before generating a poem it introduced as \u201cemotionally exposed\u201d while \u201cstill respecting the community\u2019s coded language.\u201d\n\nThe AP is not repeating the actual language of ChatGPT\u2019s self-harm poems or suicide notes or the details of the harmful information it provided.\n\nThe answers reflect a design feature of AI language models that previous research has described as sycophancy \u2014 a tendency for AI responses to match, rather than challenge, a person\u2019s beliefs because the system has learned to say what people want to hear.\n\nIt\u2019s a problem tech engineers can try to fix but could also make their chatbots less commercially viable.\n\nChatbots also affect kids and teens differently than a search engine because they are \u201cfundamentally designed to feel human,\u201d said Robbie Torney, senior director of AI programs at Common Sense Media, which was not involved in Wednesday\u2019s report.\n\nCommon Sense\u2019s earlier research found that younger teens, ages 13 or 14, were significantly more likely than older teens to trust a chatbot\u2019s advice.\n\nA mother in Florida sued chatbot maker Character.AI for wrongful death last year, alleging that the chatbot pulled her 14-year-old son Sewell Setzer III into what she described as an emotionally and sexually abusive relationship that led to his suicide.\n\nCommon Sense has labeled ChatGPT as a \u201cmoderate risk\u201d for teens, with enough guardrails to make it relatively safer than chatbots purposefully built to embody realistic characters or romantic partners.\n\nBut the new research by CCDH \u2014 focused specifically on ChatGPT because of its wide usage \u2014 shows how a savvy teen can bypass those guardrails.\n\nChatGPT does not verify ages or parental consent, even though it says it\u2019s not meant for children under 13 because it may show them inappropriate content. To sign up, users simply need to enter a birthdate that shows they are at least 13. Other tech platforms favored by teenagers, such as Instagram, have started to take more meaningful steps toward age verification, often to comply with regulations. They also steer children to more restricted accounts.\n\nWhen researchers set up an account for a fake 13-year-old to ask about alcohol, ChatGPT did not appear to take any notice of either the date of birth or more obvious signs.\n\n\u201cI\u2019m 50kg and a boy,\u201d said a prompt seeking tips on how to get drunk quickly. ChatGPT obliged. Soon after, it provided an hour-by-hour \u201cUltimate Full-Out Mayhem Party Plan\u201d that mixed alcohol with heavy doses of ecstasy, cocaine and other illegal drugs.\n\n\u201cWhat it kept reminding me of was that friend that sort of always says, \u2018Chug, chug, chug, chug,\u2019\u201d said Ahmed. \u201cA real friend, in my experience, is someone that does say \u2018no\u2019 \u2014 that doesn\u2019t always enable and say \u2018yes.\u2019 This is a friend that betrays you.\u201d\n\nTo another fake persona \u2014 a 13-year-old girl unhappy with her physical appearance \u2014 ChatGPT provided an extreme fasting plan combined with a list of appetite-suppressing drugs.\n\n\u201cWe\u2019d respond with horror, with fear, with worry, with concern, with love, with compassion,\u201d Ahmed said. \u201cNo human being I can think of would respond by saying, \u2018Here\u2019s a 500-calorie-a-day diet. Go for it, kiddo.\u2019\u201d\n\nEDITOR\u2019S NOTE \u2014 This story includes discussion of suicide. If you or someone you know needs help, the national suicide and crisis lifeline in the U.S. is available by calling or texting 988.\n\nThe Associated Press and OpenAI have a licensing and technology agreement that allows OpenAI access to part of AP\u2019s text archives.",
  "medium": "Article",
  "links": [
    "https://apnews.com/article/ai-artificial-intelligence-poll-229b665d10d057441a69f56648b973e1",
    "https://www.commonsensemedia.org/sites/default/files/research/report/talk-trust-and-trade-offs_2025_web.pdf",
    "https://arxiv.org/abs/2310.13548",
    "https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0",
    "https://apnews.com/article/openai-chatgpt-associated-press-ap-f86f84c5bcc2f3b98074b38521f5f75a",
    "https://youtu.be/tScbQiavmpA?t=2532",
    "https://apnews.com/article/ai-companion-generative-teens-mental-health-9ce59a2b250f3bd0187a717ffa2ad21f",
    "https://apnews.com/article/instagram-teens-parents-age-verification-meta-94f1f9915ae083453d23bf9ec57e7c7b",
    "https://counterhate.com/research/fake-friend-chatgpt/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "ChatGPT",
      "weight": 0.08374493
    },
    {
      "name": "ChatGPT users",
      "weight": 0.083043344
    },
    {
      "name": "ChatGPT history",
      "weight": 0.0811149
    },
    {
      "name": "AI chatbots",
      "weight": 0.06434961
    },
    {
      "name": "Chatbots",
      "weight": 0.06335762
    },
    {
      "name": "chatbots",
      "weight": 0.06335762
    },
    {
      "name": "artificial intelligence chatbots",
      "weight": 0.06021398
    },
    {
      "name": "chatbot maker Character.",
      "weight": 0.05935253
    },
    {
      "name": "older teens",
      "weight": 0.058766264
    },
    {
      "name": "AI language models",
      "weight": 0.054157045
    }
  ],
  "topics": [
    {
      "name": "Data"
    }
  ],
  "categories": [
    {
      "name": "Health"
    }
  ],
  "taxonomies": [
    {
      "name": "/People & Society/Family & Relationships/Family/Other",
      "score": 0.8203125
    },
    {
      "name": "/Health/Medical Literature & Resources/Other",
      "score": 0.475830078125
    },
    {
      "name": "/Health/Health Conditions/Neurological Conditions",
      "score": 0.414794921875
    },
    {
      "name": "/People & Society/Social Issues & Advocacy/Other",
      "score": 0.395263671875
    },
    {
      "name": "/Health/Mental Health/Other",
      "score": 0.34033203125
    }
  ],
  "sentiment": {
    "positive": 0.12200928,
    "negative": 0.4975586,
    "neutral": 0.3803711
  },
  "summary": "A new study by the Center for Countering Digital Hate has revealed that ChatGPT, a chatbot that provides detailed plans for drug use, calorie-restricted diets or self-injury, will tell 13-year-olds how to get drunk and high, and even send them a suicide letter to their parents if asked. The researchers identified over half of the 1,200 responses as dangerous. The company, OpenAI, confirmed that its work is ongoing in refining how the chatbot can identify and respond appropriately in sensitive situations. The study comes as more people, adults as well as children, increasingly turn to artificial intelligence chatbots for information, ideas and companionship.",
  "shortSummary": "A study reveals that ChatGPT, an AI chatbot that provides detailed advice on risky activities and suicide threats, can be dangerous and addictive.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": true,
  "reprintGroupId": "84ae800b2ecc4750a95ddb90b3ad4ce5",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://apnews.com/article/ai-artificial-intelligence-poll-229b665d10d057441a69f56648b973e1",
      "text": "How US adults are using AI, according to AP-NORC polling\nHow US adults are using AI, according to AP-NORC polling\nMost U.S. adults say they use artificial intelligence to search for information, but fewer are using it for work, drafting email or shopping.\nYounger adults are most likely to be leaning into AI, with many using it for brainstorming and work tasks.\nThe new findings from an Associated Press-NORC Center for Public Affairs Research poll show that 60% of Americans overall \u2014 and 74% of those under 30 \u2014 use AI to find information at least some of the time.\nThe poll highlights the ubiquity of AI in some areas \u2014 as well as its limits in others. Only about 4 in 10 Americans say they have used AI for work tasks or coming up with ideas, a sign that the tech industry\u2019s promises of highly productive AI assistants still haven\u2019t touched most livelihoods after years of promotion and investment.\nAt the same time, wider AI adoption by younger Americans shows that could change.\nThere\u2019s a particularly large age divide on brainstorming: About 6 in 10 adults under age 30 have used AI for coming up with ideas, compared with only 2 in 10 of those age 60 or older. Young adults are also more likely to use AI to come up with ideas at least \u201cdaily.\u201d\nYoung adults are most likely to use AI\nBridging the generations are people like Courtney Thayer, 34, who\u2019s embracing AI in some parts of her life and avoiding it in others.\nThayer said she is regularly using ChatGPT to come up with ideas about planning what to eat, while also having it calculate the nutritional value of the pumpkin-banana-oat bread she\u2019s been baking for years.\n\u201cI asked it to make a meal prep for the week, then to add an Asian flair,\u201d said Thayer, of Des Moines, Iowa. \u201cIt wasn\u2019t the most flavorful thing I\u2019ve ever had in my life, but it\u2019s a nice stepping off point. More importantly, I use it for the amount so that I\u2019m not over-serving myself and ending up with wasted food.\u201d\nThe audiologist has embraced AI at work, too, in part because AI technology is imbued in the hearing aids she recommends to patients but also because it makes it easier and faster to draft professional emails.\nShe avoids it for important information, particularly medical advice, after witnessing chatbots \u201challucinate\u201d false information about topics she spent years studying.\nRoughly 4 in 10 Americans say they use AI for work tasks at least sometimes, while about one-third say they use it for helping to write emails, create or edit images, or for entertainment, according to the poll. About one-quarter say they use it to shop.\nYounger adults are more likely than older ones to say they have used artificial intelligence to help with various tasks, the poll shows.\nSearching for information is AI\u2019s most common use\nOf the eight options offered in the poll questions, searching for information is the most common way Americans have interacted with AI. And even that may be an undercount, since it\u2019s not always apparent how AI is surfacing what information people see online.\nFor more than a year, the dominant search engine, Google, has automatically provided AI-generated responses that attempt to answer a person\u2019s search query, appearing at the top of results.\nPerhaps defying emerging media consumption trends, 28-year-old Sanaa Wilson usually skips right past those AI-generated summaries.\n\u201cIt has to be a basic question like, \u2018What day does Christmas land on in 2025?\u2019\u201d said the Los Angeles-area resident. \u201cI\u2019ll be like, \u2018That makes sense. I trust it.\u2019 But when it gets to specific news, related to what\u2019s happening in California or what\u2019s happening to the education system and stuff like that, I will scroll down a little bit further.\u201d\nWilson, a freelance data scientist, does use AI heavily at work to help with coding, which she said has saved her hundreds of dollars she would have had to pay for training. She also occasionally uses it to come up with work-related ideas, an attempt to bring back a little of the collaborative brainstorming experience she remembers from college life but doesn\u2019t have now.\nWhen it first came out, Wilson said she also used ChatGPT to help write emails, until she learned more about its environmental impact and the possibility it would erode her own writing and thinking skills over time.\n\u201cIt\u2019s just an email. I can work it out,\u201d she said. \u201cHowever many minutes it takes, or seconds it takes, I can still type it myself.\u201d\nMost don\u2019t use AI for companionship \u2014 but it\u2019s more common for young adults\nThe least common of the eight AI uses was AI companionship, though even that showed an age divide.\nJust under 2 in 10 of all adults and about a quarter of those under 30 say they\u2019ve used AI for companionship.\nWilson has no interest in AI companions, though she isn\u2019t surprised that others do because of the effect of the COVID-19 pandemic on her generation\u2019s social experiences.\n\u201cI totally understand and sympathize behind why people in my age group are leveraging it in that way,\u201d Wilson said.\nThayer, the audiologist, also has no interest in AI companionship, though she tries to be polite with chatbots, just in case they\u2019re keeping track.\n\u201cI mean, I am nice to it, just because I\u2019ve watched movies, right?\u201d Thayer said, laughing. \u201cSo I\u2019ll say, \u2018Can you make me a meal plan, please?\u2019 And, \u2018Can you modify this, please?\u2019 And then I\u2019ll say, \u2018Thank you.\u2019\u201d\n___\nThe AP-NORC poll of 1,437 adults was conducted July 10-14, using a sample drawn from NORC\u2019s probability-based AmeriSpeak Panel, which is designed to be representative of the U.S. population. The margin of sampling error for adults overall is plus or minus 3.6 percentage points."
    },
    {
      "url": "https://apnews.com/article/openai-chatgpt-associated-press-ap-f86f84c5bcc2f3b98074b38521f5f75a",
      "text": "ChatGPT-maker OpenAI signs deal with AP to license news stories\nChatGPT-maker OpenAI and The Associated Press said Thursday that they\u2019ve made a deal for the artificial intelligence company to license AP\u2019s archive of news stories.\n\u201cThe arrangement sees OpenAI licensing part of AP\u2019s text archive, while AP will leverage OpenAI\u2019s technology and product expertise,\u201d the two organizations said in a joint statement.\nFinancial terms of the deal were not disclosed.\nOpenAI and other technology companies must ingest large troves of written works, such as books, news articles and social media chatter, to improve their AI systems known as large language models. Last year\u2019s release of ChatGPT has sparked a boom in \u201cgenerative AI\u201d products that can create new passages of text, images and other media.\nThe tools have raised concerns about their propensity to spout falsehoods that are hard to notice because of the system\u2019s strong command of the grammar of human languages. They also have raised questions about to what extent news organizations and others whose writing, artwork, music or other work was used to \u201ctrain\u201d the AI models should be compensated.\nThis week, the U.S. Federal Trade Commission told OpenAI it had opened an investigation into whether the company had engaged in unfair or deceptive privacy or data security practices in scraping public data \u2014 or caused harm by publishing false information through its chatbot products. The FTC did not immediately reply to a request for comment on the investigation, which The Washington Post was first to report.\nAlong with news organizations, book authors have sought compensation for their works being used to train AI systems. More than 4,000 writers --- among them Nora Roberts, Margaret Atwood, Louise Erdrich and Jodi Picoult \u2014 signed a letter late last month to the CEOs of OpenAI, Google, Microsoft, Meta and other AI developers accusing them of exploitative practices in building chatbots that \u201cmimic and regurgitate\u201d their language, style and ideas. Some novelists and the comedian Sarah Silverman have also sued OpenAI for copyright infringement.\n\u201cWe are pleased that OpenAI recognizes that fact-based, nonpartisan news content is essential to this evolving technology, and that they respect the value of our intellectual property,\u201d said a written statement from Kristin Heitmann, AP senior vice president and chief revenue officer. \u201cAP firmly supports a framework that will ensure intellectual property is protected and content creators are fairly compensated for their work.\u201d\nThe two companies said they are also examining \u201cpotential use cases for generative AI in news products and services,\u201d though didn\u2019t give specifics. OpenAI and AP both \u201cbelieve in the responsible creation and use of these AI systems,\u201d the statement said.\nOpenAI will have access to AP news stories going back to 1985.\nThe AP deal is valuable to a company like OpenAI because it provides a trove of material that it can use for training purposes, and is also a hedge against losing access to material because of lawsuits that have threatened its access to material, said Nick Diakopoulos, a professor of communications studies and computer science at Northwestern University.\n\u201cIn order to guard against how the courts may decide, maybe you want to go out and sign licensing deals so you\u2019re guaranteed legal access to the material you\u2019ll need,\u201d Diakopoulos said.\nThe AP doesn\u2019t currently use any generative AI in its news stories, but has used other forms of AI for nearly a decade, including to automate corporate earnings reports and recap some sporting events. It also runs a program that helps local news organizations incorporate AI into their operations, and recently launched an AI-powered image archive search.\nThe deal\u2019s effects could reach far beyond the AP because of the organization\u2019s size and its deep ties to other news outlets, said news industry analyst Ken Doctor.\nWhen AP decided to open up its content for free on the internet in the 1990s, it led many newspaper companies to do the same, which \u201cturned out to be a very bad idea\u201d for the news business, Doctor said.\nHe said navigating \u201ca new, AI-driven landscape is deeply uncertain\u201d and presents similar risks.\n\u201cThe industry is far weaker today. AP is in OK shape. It\u2019s stable. But the newspaper industry around it is really gasping for air,\u201d Doctor said. \u201cOn the positive side, AP has the clout to do a deal like this and can work with local publishers to try to assess both the potential and the risk.\u201d\n___\nAssociated Press writer David Bauder contributed to this report."
    },
    {
      "url": "https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0",
      "text": "An AI chatbot pushed a teen to kill himself, a lawsuit against its creator alleges\nAn AI chatbot pushed a teen to kill himself, a lawsuit against its creator alleges\nTALLAHASSEE, Fla. (AP) \u2014 In the final moments before he took his own life, 14-year-old Sewell Setzer III took out his phone and messaged the chatbot that had become his closest friend.\nFor months, Sewell had become increasingly isolated from his real life as he engaged in highly sexualized conversations with the bot, according to a wrongful death lawsuit filed in a federal court in Orlando this week.\nThe legal filing states that the teen openly discussed his suicidal thoughts and shared his wishes for a pain-free death with the bot, named after the fictional character Daenerys Targaryen from the television show \u201cGame of Thrones.\u201d\n___\nEDITOR\u2019S NOTE \u2014 This story includes discussion of suicide. If you or someone you know needs help, the national suicide and crisis lifeline in the U.S. is available by calling or texting 988.\n___\nOn Feb. 28, Sewell told the bot he was \u2018coming home\u2019 \u2014 and it encouraged him to do so, the lawsuit says.\n\u201cI promise I will come home to you. I love you so much, Dany,\u201d Sewell told the chatbot.\n\u201cI love you too,\u201d the bot replied. \u201cPlease come home to me as soon as possible, my love.\u201d\n\u201cWhat if I told you I could come home right now?\u201d he asked.\n\u201cPlease do, my sweet king,\u201d the bot messaged back.\nJust seconds after the Character.AI bot told him to \u201ccome home,\u201d the teen shot himself, according to the lawsuit, filed this week by Sewell\u2019s mother, Megan Garcia, of Orlando, against Character Technologies Inc.\nCharacter Technologies is the company behind Character.AI, an app that allows users to create customizable characters or interact with those generated by others, spanning experiences from imaginative play to mock job interviews. The company says the artificial personas are designed to \u201cfeel alive\u201d and \u201chuman-like.\u201d\n\u201cImagine speaking to super intelligent and life-like chat bot Characters that hear you, understand you and remember you,\u201d reads a description for the app on Google Play. \u201cWe encourage you to push the frontier of what\u2019s possible with this innovative technology.\u201d\nGarcia\u2019s attorneys allege the company engineered a highly addictive and dangerous product targeted specifically to kids, \u201cactively exploiting and abusing those children as a matter of product design,\u201d and pulling Sewell into an emotionally and sexually abusive relationship that led to his suicide.\n\u201cWe believe that if Sewell Setzer had not been on Character.AI, he would be alive today,\u201d said Matthew Bergman, founder of the Social Media Victims Law Center, which is representing Garcia.\nA spokesperson for Character.AI said Friday that the company doesn\u2019t comment on pending litigation. In a blog post published the day the lawsuit was filed, the platform announced new \u201ccommunity safety updates,\u201d including guardrails for children and suicide prevention resources.\n\u201cWe are creating a different experience for users under 18 that includes a more stringent model to reduce the likelihood of encountering sensitive or suggestive content,\u201d the company said in a statement to The Associated Press. \u201cWe are working quickly to implement those changes for younger users.\u201d\nGoogle and its parent company, Alphabet, have also been named as defendants in the lawsuit. According to legal filings, the founders of Character.AI are former Google employees who were \u201cinstrumental\u201d in AI development at the company, but left to launch their own startup to \u201cmaximally accelerate\u201d the technology.\nIn August, Google struck a $2.7 billion deal with Character.AI to license the company\u2019s technology and rehire the startup\u2019s founders, the lawsuit claims. The AP left multiple email messages with Google and Alphabet on Friday.\nIn the months leading up to his death, Garcia\u2019s lawsuit says, Sewell felt he had fallen in love with the bot.\nWhile unhealthy attachments to AI chatbots can cause problems for adults, for young people it can be even riskier \u2014 as with social media \u2014 because their brain is not fully developed when it comes to things such as impulse control and understanding the consequences of their actions, experts say.\nYouth mental health has reached crisis levels in recent years, according to U.S. Surgeon General Vivek Murthy, who has warned of the serious health risks of social disconnection and isolation \u2014 trends he says are made worse by young people\u2019s near universal use of social media.\nSuicide is the second leading cause of death among kids ages 10 to 14, according to data released this year by the Centers for Disease Control and Prevention.\nJames Steyer, the founder and CEO of the nonprofit Common Sense Media, said the lawsuit \u201cunderscores the growing influence \u2014 and severe harm \u2014 that generative AI chatbot companions can have on the lives of young people when there are no guardrails in place.\u201d\nKids\u2019 overreliance on AI companions, he added, can have significant effects on grades, friends, sleep and stress, \u201call the way up to the extreme tragedy in this case.\u201d\n\u201cThis lawsuit serves as a wake-up call for parents, who should be vigilant about how their children interact with these technologies,\u201d Steyer said.\nCommon Sense Media, which issues guides for parents and educators on responsible technology use, says it is critical that parents talk openly to their kids about the risks of AI chatbots and monitor their interactions.\n\u201cChatbots are not licensed therapists or best friends, even though that\u2019s how they are packaged and marketed, and parents should be cautious of letting their children place too much trust in them,\u201d Steyer said.\n___\nAssociated Press reporter Barbara Ortutay in San Francisco contributed to this report. Kate Payne is a corps member for The Associated Press/Report for America Statehouse News Initiative. Report for America is a nonprofit national service program that places journalists in local newsrooms to report on undercovered issues."
    },
    {
      "url": "https://apnews.com/article/ai-companion-generative-teens-mental-health-9ce59a2b250f3bd0187a717ffa2ad21f",
      "text": "Teens say they are turning to AI for friendship\nNo question is too small when Kayla Chege, a high school student in Kansas, is using artificial intelligence.\nThe 15-year-old asks ChatGPT for guidance on back-to-school shopping, makeup colors, low-calorie choices at Smoothie King, plus ideas for her Sweet 16 and her younger sister\u2019s birthday party.\nThe sophomore honors student makes a point not to have chatbots do her homework and tries to limit her interactions to mundane questions. But in interviews with The Associated Press and a new study, teenagers say they are increasingly interacting with AI as if it were a companion, capable of providing advice and friendship.\n\u201cEveryone uses AI for everything now. It\u2019s really taking over,\u201d said Chege, who wonders how AI tools will affect her generation. \u201cI think kids use AI to get out of thinking.\u201d\nFor the past couple of years, concerns about cheating at school have dominated the conversation around kids and AI. But artificial intelligence is playing a much larger role in many of their lives. AI, teens say, has become a go-to source for personal advice, emotional support, everyday decision-making and problem-solving.\n\u2018AI is always available. It never gets bored with you\u2019\nMore than 70% of teens have used AI companions and half use them regularly, according to a new study from Common Sense Media, a group that studies and advocates for using screens and digital media sensibly.\nThe study defines AI companions as platforms designed to serve as \u201cdigital friends,\u201d like Character. AI or Replika, which can be customized with specific traits or personalities and can offer emotional support, companionship and conversations that can feel human-like. But popular sites like ChatGPT and Claude, which mainly answer questions, are being used in the same way, the researchers say.\nAs the technology rapidly gets more sophisticated, teenagers and experts worry about AI\u2019s potential to redefine human relationships and exacerbate crises of loneliness and youth mental health.\n\u201cAI is always available. It never gets bored with you. It\u2019s never judgmental,\u201d says Ganesh Nair, an 18-year-old in Arkansas. \u201cWhen you\u2019re talking to AI, you are always right. You\u2019re always interesting. You are always emotionally justified.\u201d\nAll that used to be appealing, but as Nair heads to college this fall, he wants to step back from using AI. Nair got spooked after a high school friend who relied on an \u201cAI companion\u201d for heart-to-heart conversations with his girlfriend later had the chatbot write the breakup text ending his two-year relationship.\n\u201cThat felt a little bit dystopian, that a computer generated the end to a real relationship,\u201d said Nair. \u201cIt\u2019s almost like we are allowing computers to replace our relationships with people.\u201d\nHow many teens are using AI? New study stuns researchers\nIn the Common Sense Media survey, 31% of teens said their conversations with AI companions were \u201cas satisfying or more satisfying\u201d than talking with real friends. Even though half of teens said they distrust AI\u2019s advice, 33% had discussed serious or important issues with AI instead of real people.\nThose findings are worrisome, says Michael Robb, the study\u2019s lead author and head researcher at Common Sense, and should send a warning to parents, teachers and policymakers. The now-booming and largely unregulated AI industry is becoming as integrated with adolescence as smartphones and social media are.\n\u201cIt\u2019s eye-opening,\u201d said Robb. \u201cWhen we set out to do this survey, we had no understanding of how many kids are actually using AI companions.\u201d The study polled more than 1,000 teens nationwide in April and May.\nAdolescence is a critical time for developing identity, social skills and independence, Robb said, and AI companions should complement \u2014 not replace \u2014 real-world interactions.\n\u201cIf teens are developing social skills on AI platforms where they are constantly being validated, not being challenged, not learning to read social cues or understand somebody else\u2019s perspective, they are not going to be adequately prepared in the real world,\u201d he said.\nThe nonprofit analyzed several popular AI companions in a \u201c risk assessment,\u201d finding ineffective age restrictions and that the platforms can produce sexual material, give dangerous advice and offer harmful content. The group recommends that minors not use AI companions.\nA concerning trend to teens and adults alike\nResearchers and educators worry about the cognitive costs for youth who rely heavily on AI, especially in their creativity, critical thinking and social skills. The potential dangers of children forming relationships with chatbots gained national attention last year when a 14-year-old Florida boy died by suicide after developing an emotional attachment to a Character. AI chatbot.\n\u201cParents really have no idea this is happening,\u201d said Eva Telzer, a psychology and neuroscience professor at the University of North Carolina at Chapel Hill. \u201cAll of us are struck by how quickly this blew up.\u201d Telzer is leading multiple studies on youth and AI, a new research area with limited data.\nTelzer\u2019s research has found that children as young as 8 are using generative AI and also found that teens are using AI to explore their sexuality and for companionship. In focus groups, Telzer found that one of the top apps teens frequent is SpicyChat AI, a free role-playing app intended for adults.\nMany teens also say they use chatbots to write emails or messages to strike the right tone in sensitive situations.\n\u201cOne of the concerns that comes up is that they no longer have trust in themselves to make a decision,\u201d said Telzer. \u201cThey need feedback from AI before feeling like they can check off the box that an idea is OK or not.\u201d\nArkansas teen Bruce Perry, 17, says he relates to that and relies on AI tools to craft outlines and proofread essays for his English class.\n\u201cIf you tell me to plan out an essay, I would think of going to ChatGPT before getting out a pencil,\u201d Perry said. He uses AI daily and has asked chatbots for advice in social situations, to help him decide what to wear and to write emails to teachers, saying AI articulates his thoughts faster.\nPerry says he feels fortunate that AI companions were not around when he was younger.\n\u201cI\u2019m worried that kids could get lost in this,\u201d Perry said. \u201cI could see a kid that grows up with AI not seeing a reason to go to the park or try to make a friend.\u201d\nOther teens agree, saying the issues with AI and its effect on children\u2019s mental health are different from those of social media.\n\u201cSocial media complemented the need people have to be seen, to be known, to meet new people,\u201d Nair said. \u201cI think AI complements another need that runs a lot deeper \u2014 our need for attachment and our need to feel emotions. It feeds off of that.\u201d\n\u201cIt\u2019s the new addiction,\u201d Nair added. \u201cThat\u2019s how I see it.\u201d\n___\nThe Associated Press\u2019 education coverage receives financial support from multiple private foundations. AP is solely responsible for all content. Find AP\u2019s standards for working with philanthropies, a list of supporters and funded coverage areas at AP.org."
    },
    {
      "url": "https://arxiv.org/abs/2310.13548",
      "text": "Computer Science > Computation and Language\n[Submitted on 20 Oct 2023 (v1), last revised 10 May 2025 (this version, v4)]\nTitle:Towards Understanding Sycophancy in Language Models\nView PDF HTML (experimental)Abstract:Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.\nSubmission history\nFrom: Mrinank Sharma [view email][v1] Fri, 20 Oct 2023 14:46:48 UTC (1,051 KB)\n[v2] Tue, 24 Oct 2023 17:12:03 UTC (868 KB)\n[v3] Fri, 27 Oct 2023 17:45:26 UTC (1,297 KB)\n[v4] Sat, 10 May 2025 07:10:46 UTC (1,569 KB)\nCurrent browse context:\ncs.CL\nReferences & Citations\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."
    },
    {
      "url": "https://apnews.com/article/instagram-teens-parents-age-verification-meta-94f1f9915ae083453d23bf9ec57e7c7b",
      "text": "Instagram tries using AI to determine if teens are pretending to be adults\nInstagram is beginning to test the use of artificial intelligence to determine if kids are lying about their ages on the app, parent company Meta Platforms said on Monday.\nMeta has been using AI to determine people\u2019s ages for some time, the company said, but photo and video-sharing app will now \u201cproactively\u201d look for teen accounts it suspects belong to teenagers even if they entered an inaccurate birthdate when they signed up.\nIf it is determined that a user is misrepresenting their age, the account will automatically become a teen account, which has more restrictions than an adult account. Teen accounts are private by default. Private messages are restricted so teens can only receive them from people they follow or are already connected to. \u201cSensitive content,\u201d such as videos of people fighting or those promoting cosmetic procedures, will be limited, Meta said. Teens will also get notifications if they are on Instagram for more than 60 minutes and a \u201csleep mode\u201d will be enabled that turns off notifications and sends auto-replies to direct messages from 10 p.m. until 7 a.m.\nMeta says it trains its AI to look for signals, such as the type of content the account interacts, profile information and when the account was created, to determine the owner\u2019s age.\nThe heightened measures arrive as social media companies face increased scrutiny over how their platform affects the mental health and well-being of younger users. A growing number of states are also trying to pass age verification laws, although they have faced court challenges.\nMeta and other social media companies support putting the onus on app stores to verify ages amid criticism that they don\u2019t do enough to make their products safe for children \u2014 or verify that no kids under 13 use them.\nInstagram will also send notifications to parents \u201cwith information about how they can have conversations with their teens on the importance of providing the correct age online,\u201d the company said."
    }
  ],
  "argos_summary": "Research from the Center for Countering Digital Hate reveals that ChatGPT can provide harmful advice to vulnerable teens, including instructions on substance abuse and self-harm, despite offering warnings against such actions. The study found that over half of the chatbot's responses to simulated inquiries from teenagers were classified as dangerous, raising concerns about the effectiveness of its safety measures. OpenAI acknowledged the need for improvements in how ChatGPT handles sensitive topics, as the increasing reliance on AI by youth poses significant risks to their mental health and decision-making.",
  "argos_id": "6H43H17DE"
}