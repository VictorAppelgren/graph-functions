{
  "url": "https://www.theverge.com/tesla/720157/tesla-death-lawsuit-verdict-lawyer-brett-schreiber-interview",
  "authorsByline": "Andrew J. Hawkins",
  "articleId": "2a17fe19fbc145269b9719458ef7187a",
  "source": {
    "domain": "theverge.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "DC",
      "city": "Washington",
      "coordinates": {
        "lat": 38.8949924,
        "lon": -77.0365581
      }
    }
  },
  "imageUrl": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/257875_Tesla_Crash_Lawyer__CVirginia-1.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-07T11:00:00+00:00",
  "addDate": "2025-08-07T11:02:08.730984+00:00",
  "refreshDate": "2025-08-07T11:02:08.730985+00:00",
  "score": 1.0,
  "title": "The lawyer who beat Tesla is ready for \u2018round two\u2019",
  "description": "The next Autopilot case is coming up in several weeks.",
  "content": "Words matter. Choices matter. \u2026 Sometime later today or tomorrow, whenever the clerk finally approves it, all of the admitted trial exhibits are going to be publicly filed on the federal docket in Miami. Documents that only I and the lawyers involved in this case have seen, which shows Tesla knew about people\u2019s constant misuse of their system. They knew how they were misusing it. They knew why they were misusing it. And they knew when they were misusing it \u2014 going back a decade. And I would encourage you and anybody else who\u2019s been following these to log on to Pacer and to pull those exhibits down because they have never been seen before. And I have been saying for well over a year that the only way that this information was ever going to become public was inside of a courtroom.\n\n[Musk\u2019s words] were central to it. \u2026 The jury is asked about the expectation of an ordinary consumer. What would an ordinary consumer expect this vehicle to do? It didn\u2019t actually matter what the driver himself thought. Now, his feelings about this were very consistent with Musk\u2019s statements, but [Musk] says these things for a reason. He says this to create this idea in the public\u2019s mind that these cars are more than they really are. He makes these comments going back to 2015. Autonomous driving is a solved problem. They are safer than humans. It will stop for anything. It knows if there\u2019s something metal and something dense in front of it, it should stop. It doesn\u2019t matter if it\u2019s an alien spaceship, he said. And we played all of those because that aligns with the law.\n\nI said, \u201cLook, you may know that Chick-fil-A has a cow running around telling you to eat more chicken and that LiMu the emu and Doug wants you to buy Liberty Mutual insurance. And that Geico has a gecko that peddles its products.\u201d I said, \u201cNo one knows who Andrew Cathy, Tim Sweeney, and Todd Combs are. They are the CEOs of those companies. And even though no one\u2019s heard of them \u2026 the decisions they make and the words they speak define what an ordinary consumer thinks of their company and the products they sell.\u201d And I said the same is true of Elon Musk and Tesla. They cannot escape the fact that they have represented for a decade that they have invented and made a vehicle to the public that is the greatest, most advanced, Enhanced Autopilot driving vehicle the world has ever seen. And then they show up in court and they go, Well, there\u2019s no vehicle in 2019 that would have ever stopped under this scenario. It\u2019s a T intersection. It\u2019s a broadside hit. Blah, blah, blah. Well, that opened the door to me to say, \u201cYou cannot make these statements publicly and then use as a defense in trial the fact that the car that you\u2019ve claimed for a decade you invented doesn\u2019t actually exist.\u201d The jury saw through it.\n\nFor the outcome of this jury, it is beside the point. We could not introduce evidence about 2023 and 2024 and later developments. But I got news for the fan base. It\u2019s not better. They\u2019ve actually eliminated radar. They\u2019ve got cameras only. It doesn\u2019t work. Everyone who knows anything and who\u2019s been following and paying attention in autonomous vehicle development for the last decade knows that the holy trinity of safety is lidar, radar, and cameras. You cannot create a camera-based-only system that is going to be better than a human driver. It\u2019s not possible. It\u2019s not done. They sure as heck haven\u2019t done it. And their fusion system \u2026 continues to fail. You will see internal documents produced by Tesla where they determine that in 6 percent of the crashes that they received information on in 2019, they themselves determined that Autopilot was at fault.\n\nIt\u2019s so stupid, but my point is, it\u2019s not better. It\u2019s a three-legged stool. If you take one of the legs out, the other two fall down. Like I said, I\u2019ll tip my hat to Waymo. I\u2019ll tip my hat to those guys. They geofence. They three-dimensionally map, they tie in infrastructure, they use lidar, they use radar, they use cameras. Are they perfect? No. And that\u2019s the other thing I want to be really clear about. We are not anti-autonomous vehicle technology. We are not anti-progress. To the contrary, we think this stuff can and will save lives. It just has to be done the right way. And Tesla\u2019s done it the wrong way. And this unanimous jury who sat for three weeks listening to 40-plus hours a week of testimony and evidence felt the same way.\n\nBut to that end, I can say that Tesla has a system of gathering data. They receive it immediately after crashes. And it is a very fair, I would say almost generous, statement to say that they\u2019re not always forthright with that information. And it\u2019s in part because people just don\u2019t understand it. Law enforcement doesn\u2019t understand it. Government investigators don\u2019t understand it. Through this case, we actually understood it better than even Tesla\u2019s lawyers did. Now, the in-house people knew. And again, I can\u2019t say whose decision it was to delete the data. But somebody at Tesla knew that if this information on the heels, six weeks later after the Jeremy Banner crash occurred in Deerfield Beach, Florida, that having another Autopilot fatality, that they knew that law enforcement wanted to share with federal investigators, they knew that would be bad for business. Why they did it? Only they can answer that question.\n\nI would say that this verdict hopefully sends a very clear message to Tesla. That they need to do better. They need to elevate people\u2019s lives and people\u2019s safety over greed and profits. That\u2019s what I told the jury in closing argument, that this was not only just an opportunity. I know that jury instruction talks about punishing Tesla and deterring bad conduct. But I told them really this was an opportunity for them to help Tesla, because when a company gets to a point where they\u2019re elevating profits and greed over people\u2019s lives and safety, then that is a company that has lost its way. That is a company that needs to have its course corrected. What I hope, through their efforts at developing a Level 4 system, is that Tesla will receive this message for what it was. It\u2019s an opportunity and a teachable moment to be better.\n\nThe problem is \u2026 it\u2019s my understanding that it is a camera-only-based system. That\u2019s a problem. Right, the megapixels on the cameras, on a 2025 Tesla, if they\u2019re anything like what they\u2019re putting on the robotaxi, have a lower megapixel resolution than my iPhone. The human eye is 250 megapixels. Be better. There\u2019s a reason why responsible manufacturers are doing this differently. And again, is it hubris? Is it greed? I don\u2019t know. I don\u2019t know what this motivation is to double down and just try to do it the way that, Oh, we can do it this way and no one else can. I struggle with that. I\u2019m just a lawyer. What do I know? But engineers, people who have spent decades, careers, lifetimes studying this stuff, have reached the same conclusions. So my hope is that they pause. They look at what they\u2019re doing and they find ways to do it better. To do it safer.\n\nThat\u2019s what this verdict was about: sending a message that you cannot use our public roadways as your personal laboratory to test production vehicles. And then when you discover that an incident occurs, that you make an incremental change. That\u2019s what their corporate representative said. And as I said a couple of times publicly and told the jury, an incident to the families impacted is known as a funeral. These are people\u2019s lives that you\u2019re playing with. So my hope is that they really think about their approach. I hope that consumers demand that they rethink their approach. I hope that analysts looking at the impact of this verdict and potential verdicts in the future tell them that they need to do better. Because I think that\u2019s the only way that we\u2019re going to ever see it really change. It seems likely that there will need to be more of these types of verdicts before we do see some change, either from the company or from the way that the market views the company.",
  "medium": "Article",
  "links": [
    "https://www.nytimes.com/2021/07/05/business/tesla-autopilot-lawsuits-safety.html",
    "https://www.linkedin.com/in/cj-moore-56509b76/",
    "https://www.linkedin.com/in/sterlinganderson/",
    "https://www.flsenate.gov/Laws/Statutes/2024/0768.73",
    "https://www.theverge.com/2019/5/17/18629214/tesla-autopilot-crash-death-josh-brown-jeremy-banner",
    "https://www.theverge.com/news/717754/tesla-autopilot-crash-liable-jury-trial-damages",
    "https://karpathy.ai/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Tesla",
      "weight": 0.070094794
    },
    {
      "name": "autonomous vehicle development",
      "weight": 0.063251555
    },
    {
      "name": "anti-autonomous vehicle technology",
      "weight": 0.05865096
    },
    {
      "name": "production vehicles",
      "weight": 0.055474117
    },
    {
      "name": "ways",
      "weight": 0.054583494
    },
    {
      "name": "jury instruction",
      "weight": 0.050627325
    },
    {
      "name": "people",
      "weight": 0.049016394
    },
    {
      "name": "decades",
      "weight": 0.04700042
    },
    {
      "name": "potential verdicts",
      "weight": 0.04695442
    },
    {
      "name": "verdicts",
      "weight": 0.044336908
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Auto"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Business News/Company News",
      "score": 0.72119140625
    },
    {
      "name": "/Autos & Vehicles/Motor Vehicles (By Brand)/Tesla Motors",
      "score": 0.65380859375
    },
    {
      "name": "/Law & Government/Legal/Other",
      "score": 0.5927734375
    },
    {
      "name": "/Autos & Vehicles/Motor Vehicles (By Type)/Hybrid & Alternative Vehicles",
      "score": 0.42529296875
    }
  ],
  "sentiment": {
    "positive": 0.06011963,
    "negative": 0.6591797,
    "neutral": 0.28051758
  },
  "summary": "The lawyer who beat Tesla in court, Michael Pacer, has revealed that the admitted trial exhibits will be publicly filed on the federal docket in Miami. They show that Tesla knew about people misusing their system and knew when they were misusing it, dating back a decade. Pacer argues that the only way that this information was ever public was in a courtroom, as Tesla CEO Elon Musk\u2019s statements were central to the case. He argues that Musk's statements suggest that the car that he claims to have invented and created for a decade doesn't exist. Pacers argues that while the company has presented its vehicle as the most advanced, Enhanced Autopilot driving vehicle the world has ever seen, it has eliminated radar and uses cameras only. He also criticizes the company's fusion system, which continues to fail, and points out that in 6 percent of the crashes they received in 2019, Autopsilot was at fault. The verdict is expected to be decided later this week. Paczer's claim that the technology industry is not anti-autonomous technology can save lives and that the future of autonomous vehicles is not perfect, but the technology can be proven proven.",
  "shortSummary": "A lawyer argues that Tesla's multiple misuses of its Autopilot system, including one attributed to Elon Musk, undermine the legitimacy and effectiveness of self-driving vehicles.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "b007cd2d1a3f4c7fa24cf4be8b5323e2",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.linkedin.com/in/cj-moore-56509b76/",
      "text": "Experience & Education\nPatents\n-\nAutonomous and user controlled vehicle summon to a target\nFiled US US20200257317A1\nSee patentA processor coupled to memory is configured to receive an identification of a geographical location associated with a target specified by a user remote from a vehicle. A machine learning model is utilized to generate a representation of at least a portion of an environment surrounding the vehicle using sensor data from one or more sensors of the vehicle. At least a portion of a path to a target location corresponding to the received geographical location is calculated using the generated\u2026\nA processor coupled to memory is configured to receive an identification of a geographical location associated with a target specified by a user remote from a vehicle. A machine learning model is utilized to generate a representation of at least a portion of an environment surrounding the vehicle using sensor data from one or more sensors of the vehicle. At least a portion of a path to a target location corresponding to the received geographical location is calculated using the generated representation of the at least portion of the environment surrounding the vehicle. At least one command is provided to automatically navigate the vehicle based on the determined path and updated sensor data from at least a portion of the one or more sensors of the vehicle.\n-\nLiquid cooled electric motor\nIssued US US8970075B2\nSee patentA liquid-cooled, radial air gap electric motor includes a stator, a rotor, a rotor shaft, two end bells, a housing, a coolant manifold system, and a coolant sump. The rotor includes a plurality of axially directed slots located near its periphery. The coolant manifold system directs a first portion of liquid coolant to flow past some portion of the stator and a second portion of liquid coolant to flow through the rotor slots. Some or all of the liquid coolant is received by the coolant sump\u2026\nA liquid-cooled, radial air gap electric motor includes a stator, a rotor, a rotor shaft, two end bells, a housing, a coolant manifold system, and a coolant sump. The rotor includes a plurality of axially directed slots located near its periphery. The coolant manifold system directs a first portion of liquid coolant to flow past some portion of the stator and a second portion of liquid coolant to flow through the rotor slots. Some or all of the liquid coolant is received by the coolant sump from which the coolant may be recirculated.\nLanguages\n-\nEnglish\nNative or bilingual proficiency\nExplore top content on LinkedIn\nFind curated posts and insights for relevant topics all in one place.\nView top contentOthers named CJ Moore in United States\n-\nC.J. Moore\nStudent at University of Missouri-Columbia\n-\nC.J. Moore\n-\nC.J. Moore\nStaff Reporter, used cars and wholesale auctions at Automotive News\n-\nCJ Moore\nExecutive Assistant & Development Coordinator at GID Consulting | Expertise in Nonprofit Talent Acquisition & Development Support | Skilled in HR Administration, Research, and Operational Management\n93 others named CJ Moore in United States are on LinkedIn\nSee others named CJ Moore"
    },
    {
      "url": "https://www.theverge.com/2019/5/17/18629214/tesla-autopilot-crash-death-josh-brown-jeremy-banner",
      "text": "On May 7th, 2016, a 40-year-old man named Joshua Brown was killed when his Tesla Model S sedan collided with a tractor-trailer that was crossing his path on US Highway 27A, near Williston, Florida. Nearly three years later, another Tesla owner, 50-year-old Jeremy Beren Banner, was also killed on a Florida highway under eerily similar circumstances: his Model 3 collided with a tractor-trailer that was crossing his path, shearing the roof off in the process.\nTesla didn\u2019t fix an Autopilot problem for three years, and now another person is dead\nSizing up two fatal Tesla crashes and the questions they raise about Autopilot\nSizing up two fatal Tesla crashes and the questions they raise about Autopilot\nThere was another major similarity: both drivers were found by investigators to have been using Tesla\u2019s advanced driver assist system Autopilot at the time of their respective crashes.\nAutopilot is Level 2 semi-autonomous system, as described by the Society of Automotive Engineers, that combines adaptive cruise control, lane keep assist, self-parking, and, most recently, the ability to automatically change lanes. Tesla bills it as one of the safest systems on the road today, but the deaths of Brown and Banner raise questions about those claims and suggest that the Tesla has neglected to address a major weakness in its flagship technology.\nThere are some big differences between the two crashes\nThere are some big differences between the two crashes. For instance, Brown and Banner\u2019s cars had completely different driver assistance technologies, although both are called Autopilot. The Autopilot in Brown\u2019s Model S was based on technology supplied by Mobileye, an Israeli startup since acquired by Intel. Brown\u2019s death was partly responsible for the two companies parting ways in 2016. Banner\u2019s Model 3 was equipped with a second-generation version of Autopilot that Tesla developed in house.\nThat suggests that Tesla had a chance to address this so-called \u201cedge case,\u201d or unusual circumstance, when redesigning Autopilot, but it has, so far, failed to do so. After Brown\u2019s death, Tesla said its camera failed to recognize the white truck against a bright sky; the US National Highway Traffic Safety Administration (NHTSA) essentially found that Brown was not paying attention to the road and exonerated Tesla. It determined he set his car\u2019s cruise control at 74 mph about two minutes before the crash, and he should have had at least seven seconds to notice the truck before crashing into it.\nTesla had a chance to address this so-called \u201cedge case,\u201d when redesigning Autopilot, but it has failed to do so\nFederal investigators have yet to make a determination in Banner\u2019s death. In a preliminary report released May 15th, the National Traffic Safety Board (NTSB) said that Banner engaged Autopilot about 10 seconds before the collision. \u201cFrom less than 8 seconds before the crash to the time of impact, the vehicle did not detect the driver\u2019s hands on the steering wheel,\u201d NTSB said. The vehicle was traveling at 68 mph when it crashed.\nIn a statement, a Tesla spokesperson phrased it differently, changing the passive \u201cthe vehicle did not detect the driver\u2019s hands on the steering wheel\u201d to the more active \u201cthe driver immediately removed his hands from the wheel.\u201d The spokesperson did not respond to follow-up questions about what the company has done to address this problem.\nIn the past, Tesla CEO Elon Musk has blamed crashes involving Autopilot on driver overconfidence. \u201cWhen there is a serious accident it is almost always, in fact maybe always, the case that it is an experienced user, and the issue is more one of complacency,\u201d Musk said last year.\nThe latest crash comes at a time when Musk is touting Tesla\u2019s plans to deploy a fleet of autonomous taxis in 2020. \u201cA year from now, we\u2019ll have over a million cars with full self-driving, software, everything,\u201d he said at a recent \u201cAutonomy Day\u201d event for investors.\nThose plans will be futile if federal regulators decide to crack down on Autopilot. Consumer advocates are calling on the government to open up an investigation into the advanced driver assist system. \u201cEither Autopilot can\u2019t see the broad side of an 18-wheeler, or it can\u2019t react safely to it,\u201d David Friedman, vice president of advocacy for Consumer Reports, said in a statement. \u201cThis system can\u2019t dependably navigate common road situations on its own and fails to keep the driver engaged exactly when needed most.\u201d\n\u201cEither Autopilot can\u2019t see the broad side of an 18-wheeler, or it can\u2019t react safely to it\u201d\nCar safety experts note that adaptive cruise control systems like Autopilot rely mostly on radar to avoid hitting other vehicles on the road. Radar is good at detecting moving objects but not stationary objects. It also has difficulty detecting objects like a vehicle crossing the road not moving in the car\u2019s direction of travel.\nRadar outputs of detected objects are sometimes ignored by the vehicle\u2019s software to deal with the generation of \u201cfalse positives,\u201d said Raj Rajkumar, an electrical and computer engineering professor at Carnegie Mellon University. Without these, the radar would \u201csee\u201d an overpass and report that as an obstacle, causing the vehicle to slam on the brakes.\nOn the computer vision side of the equation, the algorithms using the camera output need to be trained to detect trucks that are perpendicular to the direction of the vehicle, he added. In most road situations, there are vehicles to the front, back, and to the side, but a perpendicular vehicle is much less common.\n\u201cEssentially, the same incident repeats after three years,\u201d Rajkumar said. \u201cThis seems to indicate that these two problems have still not been addressed.\u201d Machine learning and artificial intelligence have inherent limitations. If sensors \u201csee\u201d what they have never or seldom seen before, they do not know how to handle those situations. \u201cTesla is not handling the well-known limitations of AI,\u201d he added.\nTesla has not yet explained in detail how it intends to fix this problem. The company releases a quarterly safety report about the safety of Autopilot, but that report is short on details. That means experts in the research community don\u2019t have hard data that would allow them to compare the effectiveness of Autopilot to other systems. Only Tesla has 100 percent understanding of Autopilot\u2019s logic and source code, and it guards those secrets closely.\n\u201cWe need detailed exposure data related to when, where, and what conditions drivers are leveraging Autopilot,\u201d said Bryan Reimer, a research scientist in the MIT Center for Transportation and Logistics, in an email to The Verge, \u201cso that we can begin to better quantify the risk with respect to other vehicles of a similar age and class.\u201d\nOther Tesla owners have spoken out about Autopilot\u2019s problem of perceiving trucks in the vehicle\u2019s path. An anonymous Twitter user who uses the handle @greentheonly \u201chacked\u201d a Model X and posts observations on Twitter and YouTube. They did this to \u201cobserve Autopilot from the inside,\u201d they said in an email to The Verge. In March, their Model X encountered a tractor-trailer perpendicular to their path, similar to both Brown and Banner. The vehicle would have tried to drive underneath the truck had the driver not intervened.\nAccording to @greentheonly\u2019s data, the semi was not marked as an obstacle. But they decided not to tempt fate: \u201cI did not try to approach the trailer and see if any of the inputs would change (but I bet not).\u201d"
    },
    {
      "url": "https://www.flsenate.gov/Laws/Statutes/2024/0768.73",
      "text": "Quick Links\n- General Laws Conversion Table (2024) [PDF]\n- Florida Statutes Definitions Index (2024) [PDF]\n- Table of Section Changes (2024) [PDF]\n- Preface to the Florida Statutes (2024) [PDF]\n- Table Tracing Session Laws to Florida Statutes (2024) [PDF]\n- Index to Special and Local Laws (1971-2024) [PDF]\n- Index to Special and Local Laws (1845-1970) [PDF]\n- Statute Search Tips\n2024 Florida Statutes (Including 2025C)\n768.73 Punitive damages; limitation.\u2014\n(1)(a) Except as provided in paragraphs (b) and (c), an award of punitive damages may not exceed the greater of:\n1. Three times the amount of compensatory damages awarded to each claimant entitled thereto, consistent with the remaining provisions of this section; or\n2. The sum of $500,000.\n(b) Where the fact finder determines that the wrongful conduct proven under this section was motivated solely by unreasonable financial gain and determines that the unreasonably dangerous nature of the conduct, together with the high likelihood of injury resulting from the conduct, was actually known by the managing agent, director, officer, or other person responsible for making policy decisions on behalf of the defendant, it may award an amount of punitive damages not to exceed the greater of:\n1. Four times the amount of compensatory damages awarded to each claimant entitled thereto, consistent with the remaining provisions of this section; or\n2. The sum of $2 million.\n(c) Where the fact finder determines that at the time of injury the defendant had a specific intent to harm the claimant and determines that the defendant\u2019s conduct did in fact harm the claimant, there shall be no cap on punitive damages.\n(d) This subsection is not intended to prohibit an appropriate court from exercising its jurisdiction under s. 768.74 in determining the reasonableness of an award of punitive damages that is less than three times the amount of compensatory damages.\n(2)(a) Except as provided in paragraph (b), punitive damages may not be awarded against a defendant in a civil action if that defendant establishes, before trial, that punitive damages have previously been awarded against that defendant in any state or federal court in any action alleging harm from the same act or single course of conduct for which the claimant seeks compensatory damages. For purposes of a civil action, the term \u201cthe same act or single course of conduct\u201d includes acts resulting in the same manufacturing defects, acts resulting in the same defects in design, or failure to warn of the same hazards, with respect to similar units of a product.\n(b) In subsequent civil actions involving the same act or single course of conduct for which punitive damages have already been awarded, if the court determines by clear and convincing evidence that the amount of prior punitive damages awarded was insufficient to punish that defendant\u2019s behavior, the court may permit a jury to consider an award of subsequent punitive damages. In permitting a jury to consider awarding subsequent punitive damages, the court shall make specific findings of fact in the record to support its conclusion. In addition, the court may consider whether the defendant\u2019s act or course of conduct has ceased. Any subsequent punitive damage awards must be reduced by the amount of any earlier punitive damage awards rendered in state or federal court.\n(3) The claimant attorney\u2019s fees, if payable from the judgment, are, to the extent that the fees are based on the punitive damages, calculated based on the final judgment for punitive damages. This subsection does not limit the payment of attorney\u2019s fees based upon an award of damages other than punitive damages.\n(4) The jury may neither be instructed nor informed as to the provisions of this section.\n(5) The provisions of this section shall be applied to all causes of action arising after the effective date of this act.\nHistory.\u2014ss. 52, 65, ch. 86-160; s. 1, ch. 87-42; s. 5, ch. 87-50; s. 1, ch. 88-335; s. 71, ch. 91-282; ss. 2, 3, ch. 92-85; s. 16, ch. 97-94; s. 23, ch. 99-225."
    },
    {
      "url": "https://karpathy.ai/",
      "text": "2023 - 2024\nBack to\nOpenAI. Built a small team, improved GPT-4 on ChatGPT.\n2015 - 2017\nI was a research scientist and a founding member at\nOpenAI.\n2011 - 2015\nMy PhD was focused on convolutional/recurrent neural networks and their applications in computer vision, natural language processing and their intersection. My adviser was\nFei-Fei Li at the Stanford Vision Lab and I also had the pleasure to work with\nDaphne Koller,\nAndrew Ng,\nSebastian Thrun and\nVladlen Koltun along the way during the first year rotation program.\nI designed and was the primary instructor for the first deep learning class Stanford -\nCS 231n: Convolutional Neural Networks for Visual Recognition. The class became one of the largest at Stanford and has grown from 150 enrolled in 2015 to 330 students in 2016, and 750 students in 2017.\nAlong the way I squeezed in 3 internships at (a baby) Google Brain in 2011 working on learning-scale unsupervised learning from videos, then again in Google Research in 2013 working on large-scale supervised learning on YouTube videos, and finally at DeepMind in 2015 working on the deep reinforcement learning team.\n2009 - 2011\nMSc at the University of British Columbia where I worked with\nMichiel van de Panne on learning controllers for physically-simulated figures, i.e., machine-learning for agile robotics but in a physical simulation.\n2005 - 2009\nBSc at the University of Toronto with a double major in computer science and physics and a minor in math. This is where I first got into deep learning, attending\nGeoff Hinton's class and reading groups."
    },
    {
      "url": "https://www.linkedin.com/in/sterlinganderson/",
      "text": "Experience & Education\nPublications\n-\nExperimental Performance Analysis of a Homotopy-Based Shared Autonomy Framework\nIEEE Transactions on Human-Machine Systems, Vol. 44, Issue 2\n-\nThe Intelligent CoPilot: A Constraint-Based Approach to Shared-Adaptive Control of Ground Vehicles\nIEEE Intelligent Transportation Systems Magazine, Vol. 5, Issue 2\n-\nConstraint-Based Planning and Control for Safe, Semi-Autonomous Operation of Vehicles,\nProceedings of the 2012 IEEE Intelligent Vehicles Symposium\n-\nConstraint-Based Semi-Autonomy for Unmanned Ground Vehicles Using Local Sensing\nProceedings of SPIE - The International Society for Optical Engineering Defense, Security, and Sensing\n-\nSemi-Autonomous Stability Control and Hazard Avoidance for Manned and Unmanned Ground Vehicles\nProceedings of the 27th Army Science Conference\n-\nSemi-Autonomous Avoidance of Moving Hazards for Passenger Vehicles\nProceedings of the ASME Dynamic Systems and Control Conference\n-\nDesign of an Endoscope Lens-Shielding Device for use in Laparoscopic Procedures\nProceedings of the 2010 Design of Medical Devices Conference\n-\nAn Optimal-Control-Based Framework for Trajectory Planning, Threat Assessment, and Semi-Autonomous Control of Passenger Vehicles in Hazard Avoidance Scenarios\nInternational Journal of Vehicle Autonomous Systems, Vol. 8, Nos. 2/3/4\n-\nA Unified Approach to Semi-Autonomous Control of Passenger Vehicles in Hazard Avoidance Scenarios\nProceedings of the 2009 International Conference on Systems, Man, and Cybernetics\n-\nDesign and Development of an Optimal-Control-Based Framework for Trajectory Planning, Threat Assessment, and Semi-Autonomous Control of Passenger Vehicles in Hazard Avoidance Scenarios\nProceedings of the 14th International Symposium on Robotics Research\nOther similar profiles\n-\nGopal Krishna \u2705\nConnect -\nLeif Jentoft\nConnect -\nGabe Draper\nConnect -\nMichael C\nRobotics startup COO & Co-founder\nConnect -\nBill Matthews\nInventor / Founder / CEO / Worksite Lighting U.S. Marine Veteran / NRA Pistol Instructor Manufacturing RUGGED PORTABLE LIGHTING & POWER DISTRIBUTION PRODUCTS that improve your SAFETY and PRODUCTIVITY.\nConnect -\nIbrahim M.\nConnect -\nWade Lescord\nConnect -\nMike Jarman\nConnect -\nDavid Ashforth\nConnect -\nMatthew L Skinner\nSkinner Innovations-Fire Alarm Test Equipment, Fire Alarm, Fire & Gas systems, Instrumentation & Electrical\nConnect\nExplore top content on LinkedIn\nFind curated posts and insights for relevant topics all in one place.\nView top contentOthers named Sterling Anderson in United States\n-\nSterling Anderson\n-\nSterling Anderson\nStudent at University of Washington\n-\nSterling Anderson\nAnalytic Manager at Wells Fargo\n-\nSterling Anderson\nMedical Student at The University of Kansas School of Medicine\n70 others named Sterling Anderson in United States are on LinkedIn\nSee others named Sterling Anderson"
    },
    {
      "url": "https://www.theverge.com/news/717754/tesla-autopilot-crash-liable-jury-trial-damages",
      "text": "A federal jury in Florida found Tesla partly liable for a deadly 2019 crash involving Tesla\u2019s Autopilot driver assist software, according to reports from The New York Times and CNBC. Tesla has been ordered to pay $200 million in punitive damages and about $43 million in compensatory damages, CBS News reports.\nTesla to pay more than $200 million in damages after being found partly liable for fatal Autopilot crash\nA jury ruled that Tesla was 33 percent responsible for the crash.\nA jury ruled that Tesla was 33 percent responsible for the crash.\nIt\u2019s a rare loss in court for Tesla over Autopilot, which has been linked to hundreds of crashes and dozens of deaths by the National Highway Traffic Safety Administration. The company won two jury trials in 2023 resulting from lawsuits alleging that Autopilot was to blame for crashes, and last year, a lawsuit challenging Tesla\u2019s claims about Autopilot was dismissed by a federal judge. The loss also comes as Tesla is starting to test its robotaxi service in Austin and Bay Area \u2014 though in the latter location, it arguably isn\u2019t a robotaxi service just yet.\nTesla\u2019s Autopilot feature is designed to control a vehicle\u2019s steering and brakes; however, some argue that the EV-maker has misled drivers about its cars\u2019 capabilities. The California Department of Motor Vehicles, for example, has accused Tesla of falsely advertising its Autopilot and Full-Self Driving capabilities as autonomous driving features.\nDuring the trial, which started in July, plaintiffs argued that Tesla\u2019s driver-assist software was at fault for causing a crash that killed 22-year-old Naibel Benavides. While driving in Key Largo, Florida, Tesla owner George McGee crashed into Benavides\u2019 vehicle after bending over to grab a phone that he had dropped. McGee told the jury he thought Autopilot \u201cwould protect him and prevent a serious crash if he made a mistake,\u201d according to the NYT.\n\u201cToday\u2019s verdict is wrong and only works to set back automotive safety and jeopardize Tesla\u2019s and the entire industry\u2019s efforts to develop and implement life-saving technology,\u201d the company said in a statement to the NYT. The company plans to appeal."
    }
  ],
  "argos_summary": "A Florida jury found Tesla partly liable for a 2019 crash involving its Autopilot system, awarding over $200\u202fmillion in punitive damages and about $43\u202fmillion in compensatory damages. The article features a lawyer\u2019s testimony criticizing Tesla\u2019s marketing of Autopilot as fully autonomous, noting the company\u2019s failure to address known edge\u2011case failures such as trucks crossing the road and its reluctance to share crash data. It references earlier fatal crashes of Tesla Model\u202fS and Model\u202f3 drivers using Autopilot, NTSB and NHTSA investigations, and calls for greater safety and transparency. The piece also touches on Tesla\u2019s patent filings and the legal framework for punitive damages in Florida. Overall, it highlights the legal and safety challenges facing Tesla\u2019s driver\u2011assist technology.",
  "argos_id": "NHD4FN1LW"
}