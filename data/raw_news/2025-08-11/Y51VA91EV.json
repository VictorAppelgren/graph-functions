{
  "url": "https://www.zdnet.com/article/i-went-hands-on-with-chatgpt-codex-and-the-vibe-was-not-good-heres-what-happened/",
  "authorsByline": "David Gewirtz",
  "articleId": "cb4862d9b86a49078d264e6f11c08e9d",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/b53da58ecab802c8a53f79202e70d5fd9db3e8f6/2025/08/11/a13f2015-be8c-4734-bf93-04d526f99df1/viceblur5gettyimages-1371081152.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-12T02:00:00+00:00",
  "addDate": "2025-08-12T02:04:42.868476+00:00",
  "refreshDate": "2025-08-12T02:04:42.868478+00:00",
  "score": 1.0,
  "title": "I went hands-on with ChatGPT Codex and the vibe was not good - here's what happened",
  "description": "I asked ChatGPT Codex to fix my WordPress plugin. It rewrote nine files, submitted a pull request, and crashed my test server. It did recover - but where's the flow?",
  "content": "\u2022 It also created a serious bug, but it was able to recover.\n\u2022 Codex is still based on the GPT-4 LLM architecture.\n\nWell, vibe coding this is not. I found the experience to be slow, cumbersome, stressful, and incomplete. But it all worked out in the end.\n\nChatGPT Codex is ChatGPT's agentic tool dedicated to code writing and modification. It can access your GitHub repository, make changes, and issue pull requests. You can then review the results and decide whether or not to incorporate them.\n\nAlso: How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\n\nMy primary development project is a PHP and JavaScript-based WordPress plugin for site security. There's a main plugin available for free, and some add-on plugins that enhance the capabilities of the core plugin. My private development repo contains all of this, as well as some maintenance plugins I rely on for user support.\n\nThis repo contains 431 files. This is the first time I've attempted to get an AI to work across my entire ecosystem of plugins in a private repository. I previously used Jules to add a feature to the core plugin, but because it only had access to the core plugin's open source repository, it couldn't take into account the entire ecosystem of products.\n\nEarlier last week, I decided to give ChatGPT Codex a run at my code. Then this happened.\n\nOn Thursday, GPT-5 slammed into the AI world like a freight train. Initially, OpenAI tried to force everyone to use the new model. Subsequently, they added legacy model support when many of their customers went ballistic.\n\nI ran GPT-5 against my set of programming tests, and it failed half of them. So, I was particularly curious about whether Codex still supported the GPT-4 architecture or would force developers into GPT-5.\n\nHowever, when I queried Codex five days after GPT-5 launched, the AI responded that it was still based on \"OpenAl's GPT-4 architecture.\"\n\nI took two things from that:\n\u2022 OpenAI isn't ready to move Codex coding to GPT-5 (which, recall, failed half my tests).\n\u2022 The results, conclusions, and screenshots I took of my Codex tests are still valid, since Codex is still based on GPT-4.\n\nWith that, here is the result of my still-very-much-not-GPT-5 look at ChatGPT Codex.\n\nMy first step was asking ChatGPT Codex to examine the codebase. I used the Ask mode of Codex, which does analysis, but doesn't actually change any code.\n\nI was hoping for something as deep and comprehensive as the one I received from ChatGPT Deep Research a few months ago, but instead, I received a much less complete analysis.\n\nI found a more effective approach was to ask Codex to do a quick security audit and let me know if there were any issues. Here's how I prompted it.\n\nAll three areas were valid, although I am not prepared to modify the serialization data structure at this time, because I'm saving that for a whole preferences overhaul. The $_POST complaint is managed, but with a different approach than Codex noticed.\n\nAlso: The best AI for coding in 2025 (and what not to use)\n\nThe third area -- the nonce and cross-site request forgery (CSRF) risk -- was something worth changing right away. While access to the user interface for the plugin is assumed to be determined by login role, the plugins themselves don't explicitly check that the person submitting the plugin settings for action is allowed to do so.\n\nThat's what I decided to invite Codex to fix.\n\nNext up, I instructed Codex to make fixes in the code. I changed the setting from Ask mode to Code mode so the AI would actually attempt changes. As with ChatGPT Agent, Codex spins up a virtual terminal to do some of its work.\n\nWhen the process completed, Codex showed a diff (the difference between original and to-be-modified code).\n\nI was heartened to see that the changes were quite surgical. Codex didn't try to rewrite large sections of the plugin; it just modified the small areas that needed improvement.\n\nIn a few areas, it dug in and changed a few more lines, but those changes were still pretty specific to the original prompt.\n\nAt one point, I was curious to know why it added a new foreach loop to iterate over an array, so I asked.\n\nAs you can see above, I got back a fairly clear response on its reasoning. It made sense, so I moved on, continuing to review Codex's proposed changes.\n\nAll told, Codex proposed making changes to nine separate files. Once I was satisfied with the changes, I clicked Create PR. That creates a pull request, which is how any GitHub user suggests changes to a codebase. Once the PR is created, the project owner (me, in this case) has the option to approve those changes, which adds them into the actual code.\n\nIt's a good mechanism, and Codex does a clean job of working within GitHub's environment.\n\nOnce I was convinced the changes were good, I merged Codex's work back into the main codebase.\n\nI brought the changes down from GitHub to my test machine and tried to run the now-modified plugin. Wait for it\u2026\n\nYeah. That's not what's supposed to happen. To be fair, I've generated my own share of error screens just like that, so I can't really get angry at the AI.\n\nInstead, I took a screenshot of the error and passed it to Codex, along with a prompt telling Codex, \"Selective Content plugin now fails after making changes you suggested. Here are the errors.\"\n\nIt took the AI three minutes to suggest a fix, which it presented to me in a new diff.\n\nI merged that change into the codebase, once again brought it down to my test server, and it worked. Crisis averted.\n\nWhen I'm not in a rush and I have the time, coding can provide a very pleasant state of mind. I get into a sort of flow with the language, the machine, and what seems like a connection between my fingers and the computer's CPU. Not only is it a lot of fun, but it can also be emotionally transcendent.\n\nWorking with ChatGPT Codex was not fun. It wasn't hateful. It just wasn't fun. It felt more like exchanging emails with a particularly recalcitrant contractor than having a meeting of the minds with a coding buddy.\n\nAlso: How to use GPT-5 in VS Code with GitHub Copilot\n\nCodex provided its responses in about 10 or 15 minutes, whereas the same code would probably have taken me a few hours.\n\nWould I have created the same bug as Codex? Probably not. As part of the process of thinking through that algorithm, I most likely would have avoided the mistake Codex made. But I undoubtedly would have created a few more bugs based on mistyping or syntax errors.\n\nTo be fair, had I introduced the same bug as Codex did, it would have taken me considerably longer than three minutes to find and fix it. Add another hour or so at least.\n\nSo Codex did the job, but I wasn't in flow. Normally, when I code and I'm inside a particular file or subsystem, I do a lot of work in that area. It's like cleaning day. If you're cleaning one part of the bathroom, you might as well clean all of it.\n\nBut Codex clearly works best with small, simple instructions. Give it one class of change, and work through that one change before introducing new factors. Like I said, it does work and it is a useful tool. But using it definitely felt like more of a chore than programming normally does, even though it saved me a lot of time.\n\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\n\nI don't have tangible test results, but after testing Google's Jules in May and ChatGPT's Codex now, I get the impression that Jules is able to get a deeper understanding of the code. At this point, I can't really support that assertion with a lot of data; it's just an impression.\n\nI'm going to try running another project through Jules. It will be interesting to see if Codex changes much once OpenAI feels safe enough to incorporate GPT-5. Let's keep in mind that OpenAI eats its own dog food with Codex, meaning it uses Codex to build its code. They might have seen the same iffy results I found in my tests. They might be waiting until GPT-5 has baked for a bit longer.\n\nHave you tried using AI coding tools like ChatGPT Codex or Google's Jules in your development workflow? What kinds of tasks did you throw at them? How well did they perform? Did you feel like the process helped you work more efficiently? Did it slow you down and take you out of your coding flow?\n\nDo you prefer giving your tools small, surgical jobs, or are you looking for an agent that can handle big-picture architecture and reasoning? Let us know in the comments below.\n\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV.",
  "medium": "Article",
  "links": [
    "https://www.zdnet.com/article/how-to-move-your-codebase-into-github-for-analysis-by-chatgpt-deep-research/",
    "https://www.zdnet.com/article/i-found-5-ai-content-detectors-that-can-correctly-identify-ai-text-100-of-the-time/",
    "https://www.zdnet.com/article/coding-with-ai-my-top-5-tips-for-vetting-its-output-and-staying-out-of-trouble/",
    "https://advancedgeekery.substack.com/",
    "https://twitter.com/davidgewirtz",
    "https://www.zdnet.com/article/the-best-ai-for-coding-in-2025-including-a-new-winner-and-what-not-to-use/",
    "https://www.facebook.com/davidgewirtz",
    "https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/",
    "https://www.zdnet.com/article/im-an-ai-tools-expert-and-these-are-the-only-two-i-pay-for-plus-three-im-considering/",
    "https://www.zdnet.com/article/how-to-move-your-codebase-into-github-for-analysis-by-chatgpt-deep-research/#link={%22role%22:%22standard%22,%22href%22:%22https://www.zdnet.com/article/how-to-move-your-codebase-into-github-for-analysis-by-chatgpt-deep-research/%22,%22target%22:%22%22,%22absolute%22:%22%22,%22linkText%22:%22from ChatGPT Deep Research a few months ago%22}",
    "https://bsky.app/profile/davidgewirtz.com",
    "https://www.zdnet.com/article/openai-upgrades-chatgpt-with-codex-and-im-seriously-impressed-so-far/",
    "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
    "https://www.zdnet.com/article/you-can-use-openais-super-powerful-ai-coding-agent-codex-for-just-20-now/",
    "https://www.youtube.com/user/DavidGewirtzTV",
    "https://www.zdnet.com/article/my-8-chatgpt-agent-tests-produced-only-1-near-perfect-result-and-a-lot-of-alternative-facts/",
    "https://www.instagram.com/DavidGewirtz/",
    "https://www.zdnet.com/article/gpt-5-is-finally-here-and-you-can-access-it-for-free-today-no-subscription-needed/",
    "https://www.zdnet.com/article/how-i-test-an-ai-chatbots-coding-ability-and-you-can-too/"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "ChatGPT Codex",
      "weight": 0.10440203
    },
    {
      "name": "Codex",
      "weight": 0.09655666
    },
    {
      "name": "Codex coding",
      "weight": 0.09426291
    },
    {
      "name": "plugins",
      "weight": 0.06388189
    },
    {
      "name": "Selective Content plugin",
      "weight": 0.061302457
    },
    {
      "name": "AI coding tools",
      "weight": 0.06099356
    },
    {
      "name": "change",
      "weight": 0.060938437
    },
    {
      "name": "changes",
      "weight": 0.060938437
    },
    {
      "name": "Codexs proposed changes",
      "weight": 0.059767116
    },
    {
      "name": "ChatGPT Deep Research",
      "weight": 0.058226366
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Reference/General Reference/How-To, DIY & Expert Content",
      "score": 0.76416015625
    },
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.7060546875
    },
    {
      "name": "/Reference/Technical Reference",
      "score": 0.599609375
    },
    {
      "name": "/Computers & Electronics/Programming/Development Tools",
      "score": 0.54833984375
    },
    {
      "name": "/Computers & Electronics/Programming/Scripting Languages",
      "score": 0.487548828125
    }
  ],
  "sentiment": {
    "positive": 0.084723614,
    "negative": 0.58409196,
    "neutral": 0.3311845
  },
  "summary": "ChatGPT Codex, ChatGPT's agentic tool dedicated to code writing and modification, has been given a thorough review by author Alex Treisman. Despite Treisman's hands-on with ChatP Codex, he found the experience to be slow, cumbersome, stressful, and incomplete. He also found a serious bug but was able to recover. Treisman was particularly concerned about whether Codex still supports the GPT-4 architecture or would force developers into GPT 5. He found that Codex is still based on GPT 4, which he had previously used to add a feature to the core plugin, but could not account for the entire ecosystem of products. He then asked ChatGpt Codex to make fixes in the code and reviewed the results before sending them to the GitHub repository for consideration. The author's initial analysis was less comprehensive than the one received from ChatP Deep Research a few months ago.",
  "shortSummary": "ChatGPT Codex's integration tool, based on GPT-4 LLM architecture, encountered significant bugs and failures while attempting to detect and modify code, but ultimately found success.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "6744534fbe8d4f128934014015ed816b",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/the-best-ai-for-coding-in-2025-including-a-new-winner-and-what-not-to-use/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThe best AI for coding in 2025 (including a new winner - and what not to use)\nI've been around technology long enough that very little excites me, and even less surprises me. But shortly after OpenAI's ChatGPT was released, I asked it to write a WordPress plugin for my wife's e-commerce site. When it did, and the plugin worked, I was indeed surprised.\nThat was the beginning of my deep exploration into chatbots and AI-assisted programming. Since then, I've subjected 14 large language models (LLMs) to four real-world tests.\nAlso: Apple's secret sauce is exactly what AI is missing\nUnfortunately, not all chatbots can code alike. It's been a little over two years since that first test, and even now, four of the 13 LLMs I tested can't create working plugins.\nThe short version\nIn this article, I'll show you how each LLM performed against my tests. There are now five chatbots I recommend you use.\nTwo of them, ChatGPT Plus and Perplexity Pro, cost $20 per month each. The free versions of the same chatbots do well enough that you could probably get by without paying. Two other recommended products are from Google and Microsoft. Google's Gemini Pro 2.5 is free, but you're limited to so few queries that you really can't use it without paying.\nAlso: I tested 10 AI content detectors - and these 5 correctly identified AI text every time\nMicrosoft has several Copilot licenses, which can get pricey, but I used the free version with surprisingly good results. The final one, Claude 4 Sonnet, is the free version of Claude. Oddly enough, the free version beat the paid-for version, so we're not recommending Claude 4 Opus.\nBut the rest, whether free or paid, are not so great. I won't risk my programming projects with them or recommend that you do, until their performance improves.\nI've written lots about using AIs to help with programming. Unless it's a small, simple project like my wife's plugin, AIs can't write entire apps or programs. But they excel at writing a few lines and are not bad at fixing code.\nRather than repeat everything I've written, go ahead and read this article: How to use ChatGPT to write code.\nIf you want to understand my coding tests, why I've chosen them, and why they're relevant to this review of the 13 LLMs, read this article: How I test an AI chatbot's coding ability.\nThe AI coding leaderboard\nLet's start with a comparative look at how the chatbots performed, as of this installment of our best-of roundup:\nNext, let's look at each chatbot individually. I'm back up to discussing 14 chatbots, because we're splitting out Claude 4 Sonnet and Claude 4 Opus as separate tests. GPT-4 is no longer included since OpenAI has sunsetted that LLM. Ready? Let's go.\n- Passed all tests\n- Solid coding results\n- Mac app\n- Hallucinations\n- No Windows app yet\n- Sometimes uncooperative\n- Price: $20/mo\n- LLM: GPT-4o, GPT-3.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: Yes\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nChatGPT Plus with GPT-4o passed all my tests. One of my favorite features is the availability of a dedicated app. When I test web programming, I have my browser set on one thing, my IDE open, and the ChatGPT Mac app running on a separate screen.\nAlso: I put GPT-4o through my coding tests and it aced them - except for one weird result\nIn addition, Logitech's Prompt Builder, which can be activated with a mouse button, can be set up to utilize the upgraded GPT-4o and connect to your OpenAI account, allowing for a simple thumb tap to run a prompt, which is very convenient.\nThe only thing I didn't like was that one of my GPT-4o tests resulted in a dual-choice answer, and one of those answers was wrong. I'd rather it just gave me the correct answer. Even so, a quick test confirmed which answer would work. However, that issue was a bit annoying.\n- Multiple LLMs\n- Search criteria displayed\n- Good sourcing\n- Email-only login\n- No desktop app\n- Price: $20/mo\n- LLM: GPT-4o, Claude 3.5 Sonnet, Sonar Large, Claude 3 Opus, Llama 3.1 405B\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: No\n- Tests passed: 4 of 4\nI seriously considered listing Perplexity Pro as the best overall AI chatbot for coding, but one failing kept it out of the top slot: how you log in. Perplexity doesn't use a username/password or passkey and doesn't have multi-factor authentication. All the tool does is email you a login PIN. The AI doesn't have a separate desktop app, as ChatGPT does for Macs.\nWhat sets Perplexity apart from other tools is that it can run multiple LLMs. While you can't set an LLM for a given session, you can easily go into the settings and choose the active model.\nAlso: Can Perplexity Pro help you code? It aced my programming tests - thanks to GPT-4\nFor programming, you'll probably want to stick to GPT-4o, because that model aced all our tests. But it might be interesting to cross-check your code across the different LLMs. For example, if you have GPT-4o write some regular expression code, you might consider switching to a different LLM to see what that model thinks of the generated code.\nAs we'll see below, most LLMs are unreliable, so don't take the results as gospel. However, you can use the results to check your original code. It's sort of like an AI-driven code review.\nJust don't forget to switch back to GPT-4o.\n- Price: Free for limited use, then token-based pricing\n- LLM: Gemini Pro 2.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nThe last time I looked at Gemini, it failed miserably. Not quite as bad as Copilot at the time, but bad. Gemini Pro 2.5, however, has performed quite admirably. My only real issue with it is access. I found myself cut off from the free version after only running two of the four tests.\nAlso: Gemini Pro 2.5 is a stunningly capable coding assistant - and a big threat to ChatGPT\nI waited a day and then ran the third test, and got cut off again. Finally, on the third day, I ran my fourth test. Obviously, you can't do any real programming if you can only ask one or two questions before being shut down. So, if you sign up with Gemini Pro 2.5, be aware that Google charges by tokens (basically, the amount of AI you use). That can make it quite difficult to predict your expenses.\n- Price: Free for basic Copilot, or fees for other Copilot licenses\n- LLM: Undisclosed\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nIn all my previous analyses of Microsoft Copilot, the results were the worst of the LLMs. Copilot got nothing right. It was astonishing how bad it was. But I said then that, \"The one positive thing is that Microsoft always learns from its mistakes. So, I'll check back later and see if this result improves.\"\nAlso: I retested Microsoft Copilot's AI coding skills in 2025 and now it's got serious game\nAnd boy, did it ever. This time out, Microsoft passed all four of my tests. Even better, it did this with the free version of Copilot. Yes, Microsoft has many paid programs for Copilot, but if you want to give it the AI spin, point yourself to Copilot and use it.\n- Price: Free\n- LLM: Claude 4\n- Desktop browser interface: No\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nThis is one of those times when AI implementations can be real head-scratchers. In our previous tests, Claude 4 Sonnet finished at the bottom of the barrel, failing all four of our tests. This time, however, Sonnet passed every test. So, what's the head-scratcher? Opus, the Claude 4 model, which is a fee-paid version, did not do as well: it failed half the tests.\nAlso: Anthropic's free Claude 4 Sonnet aced my coding tests - but its paid Opus model somehow didn't\nSo, yes. The free version worked like a champ. And the one you're paying anywhere from $20 to $250 a month for, depending on the plan? Well, that one failed half of the tests. Go figure.\n- Different LLM than ChatGPT\n- Good descriptions\n- Free access\n- Only available in browser mode\n- Free access likely only temporary\n- Price: Free (for now)\n- LLM: Grok-1\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 3 of 4\nI have to say, Grok surprised me. I guess I didn't have high hopes for an LLM that appeared tacked on to the social network formerly known as Twitter. However, X is now owned by Elon Musk, and two of Musk's companies, Tesla and SpaceX, have towering AI capabilities.\nIt's unclear how much Tesla and SpaceX AI DNA is in Grok, but we can assume there will likely be more work. As of now, Grok is the only LLM not based on OpenAI LLMs that made it into the recommended list.\nAlso: X's Grok did surprisingly well in my AI coding tests\nGrok did make one mistake, but it was a relatively minor one that a slightly more comprehensive prompt could easily remedy. Yes, it failed the test. But by passing the others and even doing an almost perfect job on the one it passed, Grok earned itself a spot as a contender.\nStay tuned. This is an AI to watch.\n- Free\n- Passed most tests\n- Prompt throttling\n- Could cut you off in the middle of whatever you're working on\n- Price: Free\n- LLM: GPT-4o, GPT-3.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: Yes\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 3 of 4 in GPT-3.5 mode\nChatGPT is available to anyone for free. While both the Plus and free versions support GPT-4o, which passed all my programming tests, the free app has limitations.\nOpenAI treats free ChatGPT users as if they're in the cheap seats. If traffic is high or the servers are busy, the free version of ChatGPT will only make GPT-3.5 available to free users. The tool will only allow you a certain number of queries before it downgrades or shuts you off.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nI've had several occasions when the free version of ChatGPT effectively told me I'd asked too many questions.\nChatGPT is a great tool, as long as you don't mind it shutting down. Even GPT-3.5 did better on the tests than all the other chatbots, and the test it failed was for a fairly obscure programming tool produced by a lone programmer in Australia.\nSo, if budget is important to you and you can wait when you're cut off, then use ChatGPT for free.\n- Free\n- Passed most tests\n- Range of research tools\n- Limited to GPT-3.5\n- Throttles prompt results\n- Price: Free\n- LLM: GPT-3.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: No\n- Tests passed: 3 of 4\nI'm threading a pretty fine needle here, but because Perplexity AI's free version is based on GPT-3.5, the test results were measurably better than the other AI chatbots.\nAlso: 5 reasons why I prefer Perplexity over every other AI chatbot\nFrom a programming perspective, that's pretty much the whole story. However, from a research and organization perspective, my ZDNET colleague Steven Vaughan-Nichols prefers Perplexity over the other AIs.\nHe likes how Perplexity provides more complete sources for research questions, cites its sources, organizes the replies, and offers questions for further searches.\nSo, if you're programming, but also working on other research, consider the free version of Perplexity.\n- Free\n- Open source\n- Efficient resource utilization\n- Weak general knowledge\n- Small ecosystem\n- Limited integrations\n- Price: Free for chatbot, fees for API\n- LLM: DeepSeek MoE\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: No\n- Tests passed: 3 of 4\nWhile DeepSeek R1 is the new reasoning hotness from China that has all the pundits punditing, the real power right now (at least according to our tests) is DeepSeek V3. This chatbot passed almost all of our coding tests, doing as well as the (now mostly discontinued) ChatGPT 3.5.\nAlso: I tested DeepSeek's R1 and V3 coding skills - and we're not all doomed (yet)\nWhere DeepSeek V3 fell was in its knowledge of somewhat more obscure programming environments. Still, it beat Google's Gemini, Microsoft's Copilot, and Meta's Meta AI, which is quite an accomplishment. We'll be keeping a close watch on each DeepSeek model, so stay tuned.\nChatbots to avoid for programming help\nI tested 13 LLMs, and nine passed most of my tests this time around. The other chatbots, including a few pitched as great for programming, only passed one of my tests.\nAlso: The five biggest mistakes people make when prompting an AI\nI'm mentioning them here because people will ask, and I did test them thoroughly. Some of these bots are fine for other work, so I'll point you to their general reviews if you're curious about their functionality.\nDeepSeek R1\nUnlike DeepSeek V3, the advanced reasoning version, DeepSeek R1, did not showcase its reasoning capabilities in our programming tests. Unusually, the new failure area was one that's not all that hard, even for a basic AI -- the regular expression code for our string function test.\nAlso: Tech prophet Mary Meeker just dropped a massive report on AI trends - here's your TL;DR\nBut that's why we are running these real-world tests. It's never clear where an AI will hallucinate or just plain fail, and before you go believing all the hype about DeepSeek R1 taking the crown away from ChatGPT, run some programming tests. So far, while I'm impressed with the much-reduced resource utilization and the open-source nature of the product, its coding quality output is inconsistent.\nGitHub Copilot\nGitHub's Copilot integrates quite seamlessly with VS Code. The AI makes asking for coding help quick and productive, especially when working in context. That's why it's so disappointing that the code the AI outputs is often very wrong.\nAlso: I put GitHub Copilot's AI to the test - and it just might be terrible at writing code\nI can't, in good conscience, recommend you use the GitHub Copilot extensions for VS Code. I'm concerned that the temptation will be too great to insert blocks of code without sufficient testing -- and that GitHub Copilot's produced code is not ready for production use. Try again next year.\nClaude 4 Opus\nIn a completely baffling turn of events, the paid-for version of the Claude 4 model, Opus, failed half of my tests. What makes this result baffling is that the free version, Claude 4 Sonnet, passed them all. I don't know what to say apart from AI can be weird.\nAlso: Anthropic's free Claude 4 Sonnet aced my coding tests - but its paid Opus model somehow didn't\nMeta AI\nMeta AI is Facebook's general-purpose AI. As you can see above, it failed three of our four tests.\nAlso: 15 ways AI saved me time at work in 2024 - and how I plan to use it in 2025\nThe AI generated a nice user interface, but with zero functionality. It also found my annoying bug, which is a fairly serious challenge. Given the specific knowledge required to find the bug, I was surprised that the AI choked on a simple regular expression challenge. But it did.\nMeta Code Llama\nMeta Code Llama is Facebook's AI explicitly designed for coding help. It's something you can download and install on your server. I tested the AI running on a Hugging Face AI instance.\nAlso: Can Meta AI code? I tested it against Llama, Gemini, and ChatGPT - it wasn't even close\nWeirdly, even though both Meta AI and Meta Code Llama choked on three of four of my tests, they choked on different problems. AIs can't be counted on to give the same answer twice, but this result was a surprise. We'll see if that changes over time.\nBut I like [insert name here]. Does this mean I have to use a different chatbot?\nProbably not. I've limited my tests to day-to-day programming tasks. None of the bots has been asked to talk like a pirate, write prose, or draw a picture. In the same way we use different productivity tools to accomplish specific tasks, feel free to choose the AI that helps you complete the task at hand.\nThe only issue is if you're on a budget and are paying for a pro version. Then, find the AI that does most of what you want, so you don't have to pay for too many AI add-ons.\nIt's only a matter of time\nThe results of my tests were pretty surprising, especially given the significant improvements by Microsoft and Google. However, this area of innovation is improving at warp speed, so we'll be back with updated tests and results over time. Stay tuned.\nHave you used any of these AI chatbots for programming? What has your experience been? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/openai-upgrades-chatgpt-with-codex-and-im-seriously-impressed-so-far/",
      "text": "OpenAI upgrades ChatGPT with Codex - and I'm seriously impressed (so far)\nOpenAI's new Codex agent is essentially a vibe-coding environment based on a ChatGPT-like comment interface. As much as the vibe-coding idea seems like a meme for wannabe cool-kid coders, the new Codex agent is impressive as heck.\nAlso: What is AI vibe coding? It's all the rage but it's not for everyone \u2013 here's why\nOpenAI described Codex as a research preview still under active development. Right now, it's available to Pro, Enterprise, and Team-tier ChatGPT users, but it's expected to release to Plus and Edu users \"soon.\"\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nAccording to the recording of OpenAI's announcement livestream, the Codex name has been applied to an evolving coding tool since as far back as 2021. That said, when I refer to Codex in this article, I'm talking about the new version being announced now.\nWhat is Codex?\nI haven't had the opportunity to get hands-on with Codex yet, so I'm taking everything I'm sharing with you from information provided by OpenAI. When I watched the announcement, I noticed that even the engineers seemed a little shocked at how capable this tool is.\nCodex lives on OpenAI's servers and interacts with your GitHub repositories. If the demo is to be believed (and OpenAI has repeatedly proven that unbelievable demos are real), Codex basically acts like another programmer on your team.\nAlso: 10 professional developers on vibe coding's true promise and peril\nYou can tell it to fix a series of bugs, and it will go off and do just that. It asks you to approve coding changes, although it looks like it can also just go ahead and modify code.\nYou can ask it to analyze and modify code, look for specific problems, identify problem areas and room for improvement, and other coding and maintenance tasks. Each assignment spawns off a new virtual environment where the AI can go all the way from concept and design to unit testing.\nA mindset change\nThere is a real coding mindset change going on here. Earlier AI coding help took the form of auto-complete. Lines and even blocks of code were automatically generated based on existing code.\nThen we got to the point where small segments of code could be written or debugged by the AI. This is the area I've been focusing on in terms of the ZDNET programming tests.\nAnother AI role is analysis of the overall system. Last week, I showed a remarkable new Deep Research tool that can deconstruct entire codebases and provide code reviews and recommendations.\nNow, with Codex, we're getting to the point where entire programming tasks can be delegated to the AI in the cloud, in much the same way those tasks were given to other programmers on a team or to junior programmers learning their way through code maintenance.\nOpenAI calls this \"Agent-native software development, where AI not only assists you as you work but takes on work independently.\"\nChanging developer workflow\nThe launch video demonstrated the ability of Codex to take on a variety of tasks at once, each running in its own isolated virtual environment.\nProgrammers assigned tasks to the agent, which went off and did the work without supervision. When the work was complete, the agent returned with test results and recommended code changes.\nThe demo showcased the Codex agent performing bug fixes, doing a scan for typos, making task suggestions, and performing project-wide refactoring (modifying code to improve structure without changing behavior).\nSenior developers and designers are no strangers to articulating requirements and reviewing others' work. Using Codex won't be much of a change for them. But for developers who haven't yet developed good requirements-articulation and review skills, properly managing Codex may prove to be a bit of a challenge.\nYet, if the tool performs as the demo appears to indicate it can, Codex will enable smaller teams and individual developers to accomplish more, reduce repetitive work, and be more responsive to problem reports.\nConsistency and flexibility\nOne of the problems I found early on with ChatGPT's coding was that it had a tendency to lose the thread or go off in its own direction. For individual blocks of code, that's annoying but not catastrophic. But if a coding agent is allowed to run fairly unsupervised, such stubborn refusal to follow directions could cause unintended and problematic consequences.\nAlso: The best AI for coding in 2025 (including two new top picks - and what not to use)\nTo help mitigate this, OpenAI has trained Codex to follow directions specified in an AGENTS.md file. This file in the repository allows programmers and teams to steer Codex's behavior. It can contain instructions on naming conventions, formatting rules, and any other set of consistent guidelines desired in the coding process. It's essentially an extension of the ChatGPT personalization settings, but for a repository-centric team environment.\nOpenAI has also introduced a version of Codex called Codex CLI that runs locally on a developer's machine. Unlike the cloud-based Codex, which runs asynchronously and reports back on completion, the local version operates on the programmer's command line and is synchronous.\nIn other words, the programmer types out an instruction and waits for the Codex CLI process to return a result. This allows a programmer to work offline with the local context of the active development machine.\nThinking through the implications\nThe demo was impressive, but during the launch video, the developers were very clear that what they were showing off and releasing is a research prototype. While it offers what they called \"magical moments,\" it still has a long way to go.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nI've been trying to dig in and triangulate on what exactly this technology means for the future of development and for my development process specifically. My main product is an open-source WordPress plugin, which itself has proprietary add-on plugins. Clearly, Codex could work itself through the public repository for the open-source core plugin.\nBut could Codex manage the relationship between one public and multiple private repositories as part of one overall project? And how would it do when testing involves not only my code but also spinning up an entire additional ecosystem -- WordPress -- to evaluate performance?\nAs a solo programmer, I definitely see the advantages of something like Codex. Even the $200-per-mnth Pro subscription makes sense. Hiring a helper programmer would cost a whole lot more per month than that fee, assuming I were to achieve tangible monetizable value out of it.\nAs a long-time team manager and professional communicator, I feel very comfortable delegating to something like Codex. It's not all that different chatting with an agent than it is chatting with a team member over Slack, for example.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nThe fact that Codex will make recommendations, draft versions, and wait for me to approve the results makes me feel a bit safer than merely letting it run loose in my code. It does open a very interesting door for a new development lifecycle, where the human sets goals, the AI drafts possible implementations, and then the human goes back in and either approves or redirects the AI for another cycle.\nBased on my earlier experiences using AIs for coding, it's clear that Codex could reduce maintenance time and get fixes out to users faster. It's not quite as clear how Codex would perform adding new features based on a specifications document. It's also not clear how much more or less difficult it would be to go into the code after Codex has worked on it to tweak functionality and performance.\nIt's interesting that AI coding is evolving across companies at about the same pace. I'm dropping another article soon on GitHub Copilot's Coding Agent, which does some of the same things that Codex does.\nIn that article, I expressed some concern that these coding agents will replace junior and entry-level programmers. Beyond concern for human jobs, there's also the question of what critical training opportunities will be lost if we delegate a middle phase of a developer's career to the AI.\nInto the unknown\nThere's a song in Disney's Frozen II called \"Into the Unknown,\" performed by Idina Menzel. The song centers on the main character's internal conflict between maintaining the status quo and her familiar life, and venturing out \"into the unknown.\"\nWith agentic software development, more than just AI coding, the entire software industry is going into the unknown. The more we rely on AI-based systems to build our software for us, the fewer skilled maintainers there will be. That's fine as long as the AIs continue to perform and be available. But are we letting some key skills atrophy, letting some good-paying jobs go, for the convenience of delegating to a not-yet-sentient cloud-based infrastructure?\nAlso: 10 professional developers on vibe coding's true promise and peril\nOnly time will tell, and hopefully we won't experience that telling when we're out of time.\nDo you see yourself delegating real development tasks to a tool like this? What do you think the long-term impact will be on software teams or solo developers? And do you worry about losing critical skills or roles as more of the code lifecycle is handed off to AI? Let us know in the comments below.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/how-to-move-your-codebase-into-github-for-analysis-by-chatgpt-deep-research/#link={%22role%22:%22standard%22,%22href%22:%22https://www.zdnet.com/article/how-to-move-your-codebase-into-github-for-analysis-by-chatgpt-deep-research/%22,%22target%22:%22%22,%22absolute%22:%22%22,%22linkText%22:%22from ChatGPT Deep Research a few months ago%22}",
      "text": "How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\nA few days ago, I showed you an amazing new ChatGPT feature available to paying users. Plus, Pro, and Team tier users can now point Deep Research at an entire GitHub repo and get back analysis reports.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nAs I showed, this capability helps speed up the process of coming up to speed on existing codebases. You might need to do this if you acquire a product from another developer or if you're brought onto a project and need to learn the codebase quickly. It's also good for reviewing your own codebase and refreshing yourself on how sections work -- especially if you've moved on to other things for a while and are now coming back to the original code.\nI promised I'd show you how to bring a codebase into GitHub specifically for analysis by Deep Research. That's what we're about to do in this article.\nMoving my code into GitHub\nTo demonstrate this, I'm moving My Private Site into GitHub. My Private Site is a freemium WordPress plugin with about 20,000 active users I've been working on for about a decade. WordPress, for historical reasons, uses SVN instead of GitHub as a code repository, so I haven't really had a need to put My Private Site into GitHub.\nAlso: I put GitHub Copilot's AI to the test - its mixed success at coding baffled me\nBut given the opportunity to perform deep analysis on it, I decided to set it up. I'll go through that process with you here.\nGetting started with GitHub Desktop\nBefore we start, let's clarify some things. Git is a distributed version control system that runs on a programmer's local computer. GitHub is a cloud-based service that stores an enormous library of open-source and proprietary coding projects. Those projects are moved into GitHub (the cloud service) using Git (the tool).\nReal programmers only use Git on the command line, where it's known as git. No real programmer would dare to capitalize git. Real programmers command git via a range of options, creating specialized command lines that do their bidding. Failure to use git on the command line will result in your real-programmer card being revoked by the International Society of Programmers Who Are Smarter Than You.\nAlso: How I used GitHub Spark to build an app with just a one-sentence AI prompt\nI am apparently not a real programmer. I might as well get that out of the way before the comments erupt in disdainful RPs (real programmers) mocking my lack of command-line acuity. I don't use Git via the command line. I don't like it. I believe humans left the cave long ago and adopted graphical user interfaces as tools of civilized society.\nI, therefore, prefer using GitHub Desktop, which is a point-and-click version of Git for those not worthy of the title real programmer. And yes, my official real-programmer card has been revoked. I can live with it.\nYou can download GitHub Desktop here.\nOnce you've launched GitHub Desktop, either sign in to your GitHub account or create one. I've long had a GitHub account for other projects, so I just signed in.\nHow to create a GitHub repository\nNext, I created a repository in the GitHub cloud for my codebase. Here it can be a little confusing. Even though I didn't have an existing repo for My Private Site, I chose \"Add an Existing Repository from your Local Drive\u2026\" because I was going to take that codebase and turn it into a repo.\nGitHub Desktop is actually pretty smart about this. Once it realizes there's no GitHub data for the folder selected, it will give you an error and offer you the option to create a repo. Click the link highlighted by the green arrow shown below.\nThat will present the Create a New Repository dialog. Here, I named my repo (all lowercase, with dashes between words), added a short description, told it the local path to the code on my computer, and left the rest as-is.\nI didn't need to play with the README, license, or ignore options because I'm using this repo for AI analysis, not for source control and collaboration.\nIt's here I should note this article describes what you need to do to let your code be examined by ChatGPT Deep Research. This is definitely not a comprehensive how-to-set-up-GitHub article.\nHow to move the codebase to GitHub\nIt's time to move your code up to GitHub.\nHere's a cautionary note: If you've kept your code private, uploading it to GitHub is sending your code to a cloud service. GitHub offers both private and public repositories, but you're technically giving Microsoft access to your code. Microsoft owns GitHub.\nNow, go ahead and hit Publish.\nAt this point, you'll have the opportunity to make your repo public or private. When you connect ChatGPT to your repo, you'll be passing along your access rights, so you can let ChatGPT examine a private repository.\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\nThat said, I ran into some issues with Deep Research accessing my code, and one of the things ChatGPT asked me was whether my code was public. My take on that is: if your code is private and you have all your credentials and connector set up (more on that later), you can probably work on a private repo.\nSince My Private Site is open source, I unchecked \"Keep this code private.\"\nLooking at your new repository\nIf everything worked, you'll see a new option: \"View on GitHub.\" Click it.\nThat will bring you to your newly created GitHub repo. Here's mine.\nNow that your repo is up, take note of its designation. You can find that in the upper left corner of the GitHub screen. For My Private Site, it's davidgewirtz/my-private-site (without any spaces).\nHow to set up the ChatGPT connection\nNow it's time to switch to ChatGPT. The next two screenshots are the same as what I showed you in this article on the feature. But to get to the next configuration step, you'll need to do what's shown in the screenshots.\nFirst, change your model to o3 and type in the prompt exactly as I did. You can probably tweak this over time, but if you have the $20-per-month Plus tier, you're only going to be allowed 10 queries into Deep Research per month, so cutting and pasting is your friend.\nNext, click the little caret next to Deep Research.\nAlso: Want to try ChatGPT's Deep Research tool for free? Check out the lightweight version\nNow, create the link between your ChatGPT account and your GitHub account. Go ahead (if you dare) and give Skynet -- uh, I mean the AI -- permission to access your GitHub account features.\nNext, you'll be asked which GitHub account should get the ChatGPT connector. I have two, so I got this choice screen. You might skip this screen if you only have one account.\nNow it's time for more permissions. This time, you're giving permission to access either all your account's repos or just one. I selected only the my-private-site repo.\nAnd now, theoretically, Deep Research in ChatGPT will be connected to your repo. Theoretically. In practice, mine required another step.\nWhat to do if ChatGPT can't find your repo\nGitHub indexes repositories, and if ChatGPT doesn't show your repo as available, it probably means GitHub hasn't indexed your new repository yet. That's what happened here.\nI should have been able to select or type in my full repo name (remember, davidgewirtz/my-private-site), but ChatGPT wasn't able to locate it.\nAlso: I tested ChatGPT's Deep Research against Gemini, Perplexity, and Grok AI to see which is best\nTo fix this, go back to your GitHub account and type in the command string shown at the top of this screenshot. Obviously, change the text in blue to match your repo name.\nThe command is basically repo:(repo followed by a colon), followed by the full name of your repo, followed by a space and the word import. This will tell GitHub you'd like it to index your repository.\nAs you can see, GitHub confirmed it was now indexing my repository.\nI brewed myself a well-deserved cup of coffee as a way to give GitHub time to index my repo. Once I finished the last drop, I went back to ChatGPT, dropped down the Deep Research menu, and found my newly created repository.\nHave fun with Deep Research\nYou're ready to start using Deep Research on your repo. For a detailed guide on how that worked for my repo, point yourself to my earlier article on the topic.\nHave fun. I was pretty blown away. You might be, as well.\nAlso: 5 reasons I turn to ChatGPT every day - from faster research to replacing Siri\nHave you tried using ChatGPT Deep Research with your own code yet? What was your experience connecting a GitHub repo? Did you run into any indexing issues or permission snags along the way? Do you prefer using GitHub Desktop or the command line when setting up your repositories? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
      "text": "How to use GPT-5 in VS Code with GitHub Copilot\nZDNET's key takeaways\n- GitHub Copilot Pro now supports GPT-5 in VS Code.\n- A 30-day trial lets you test premium models for free.\n- Add your OpenAI key to bypass Copilot restriction.\nGPT-5 is now available for use with Microsoft's GitHub Copilot in VS Code. In this article, I'll walk you through the steps of setting up the linkages between VS Code, Copilot, and GPT-5. This process will also work for most other supported large language models you want to use.\nAlso: Microsoft rolls out GPT-5 across its Copilot suite - here's where you'll find it\nStep 1: Enable GitHub Copilot Pro\nYou'll need the Pro version of GitHub Copilot to use GPT-5 at this time. The new model might be available for Copilot's free tier someday, but not yet. There is, however, a 30-day free trial. I'll show you how to set that up here:\nFirst, open VS Code. Click the little Copilot icon (1). That action will open the Copilot pane. Next, click whatever model is listed at (2). Mine is GPT-4.1. Finally, click Add Premium Models (3).\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nThis will take you to a web page where you'll be given the opportunity to try Copilot Pro for 30 days at no cost. Click the big green button.\nUnfortunately, you'll have to add your credit card info, although it won't be charged for 30 days. To prevent abuse of the 30-day limit, Microsoft requires you to give your personal information and your credit card number:\nOnce you've done that, click the Activate Now button:\nStep 2: Enable GPT-5\nYou'll need to restart VS Code for the Pro mode to be made available. Once you do that, click the current model (in the screenshot, it's GPT-4.1), and then scroll and choose GPT-5:\nYou will then need to issue a prompt. In my Deep Research dump of my repository, I was told about some references to a product I sold a few years ago that were still in the product's code. I told GPT-5 to remove all such references.\nThat process resulted in the Enable button showing up. Basically, I think you can use any prompt with GPT-5 to trigger the Enable button. Then click it:\nStep 3: Bring your own key\nBy using the Pro account, you are given a certain number of times you can use the various models. It's unclear how that usage limit is calculated, so I have reached out to Microsoft for clarification. I'll update this article when I get more info.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nIf you want to bypass the possible restrictions and rate limitations, you can use your own API key as provided by your LLM service. You can learn more from Microsoft's language models page.\nWith ChatGPT, you can get an OpenAI Platform Key by pointing your browser here. If you don't already have an OpenAI account, you may need to give them some credit card information as well. Then click Create Key and follow the directions:\nOnce you have your key, go back to Manage Models (by clicking the current model you're using and choosing Manage Models). Select OpenAI:\nType or paste in your key. Press Enter to confirm:\nCongratulations, you're now running GPT-5 in Copilot.\nHave you used Copilot?\nHave you tried using GPT-4.1 or GPT-5 in VS Code yet? What do you think of the Copilot Pro experience so far? Does it feel like a worthwhile upgrade? Have you explored using your API key instead of relying on Microsoft's allocation?\nAlso: The best AI for coding in 2025 (and what not to use)\nWhat kinds of tasks or prompts have you found AI particularly helpful for in your coding work? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/how-to-move-your-codebase-into-github-for-analysis-by-chatgpt-deep-research/",
      "text": "How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\nA few days ago, I showed you an amazing new ChatGPT feature available to paying users. Plus, Pro, and Team tier users can now point Deep Research at an entire GitHub repo and get back analysis reports.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nAs I showed, this capability helps speed up the process of coming up to speed on existing codebases. You might need to do this if you acquire a product from another developer or if you're brought onto a project and need to learn the codebase quickly. It's also good for reviewing your own codebase and refreshing yourself on how sections work -- especially if you've moved on to other things for a while and are now coming back to the original code.\nI promised I'd show you how to bring a codebase into GitHub specifically for analysis by Deep Research. That's what we're about to do in this article.\nMoving my code into GitHub\nTo demonstrate this, I'm moving My Private Site into GitHub. My Private Site is a freemium WordPress plugin with about 20,000 active users I've been working on for about a decade. WordPress, for historical reasons, uses SVN instead of GitHub as a code repository, so I haven't really had a need to put My Private Site into GitHub.\nAlso: I put GitHub Copilot's AI to the test - its mixed success at coding baffled me\nBut given the opportunity to perform deep analysis on it, I decided to set it up. I'll go through that process with you here.\nGetting started with GitHub Desktop\nBefore we start, let's clarify some things. Git is a distributed version control system that runs on a programmer's local computer. GitHub is a cloud-based service that stores an enormous library of open-source and proprietary coding projects. Those projects are moved into GitHub (the cloud service) using Git (the tool).\nReal programmers only use Git on the command line, where it's known as git. No real programmer would dare to capitalize git. Real programmers command git via a range of options, creating specialized command lines that do their bidding. Failure to use git on the command line will result in your real-programmer card being revoked by the International Society of Programmers Who Are Smarter Than You.\nAlso: How I used GitHub Spark to build an app with just a one-sentence AI prompt\nI am apparently not a real programmer. I might as well get that out of the way before the comments erupt in disdainful RPs (real programmers) mocking my lack of command-line acuity. I don't use Git via the command line. I don't like it. I believe humans left the cave long ago and adopted graphical user interfaces as tools of civilized society.\nI, therefore, prefer using GitHub Desktop, which is a point-and-click version of Git for those not worthy of the title real programmer. And yes, my official real-programmer card has been revoked. I can live with it.\nYou can download GitHub Desktop here.\nOnce you've launched GitHub Desktop, either sign in to your GitHub account or create one. I've long had a GitHub account for other projects, so I just signed in.\nHow to create a GitHub repository\nNext, I created a repository in the GitHub cloud for my codebase. Here it can be a little confusing. Even though I didn't have an existing repo for My Private Site, I chose \"Add an Existing Repository from your Local Drive\u2026\" because I was going to take that codebase and turn it into a repo.\nGitHub Desktop is actually pretty smart about this. Once it realizes there's no GitHub data for the folder selected, it will give you an error and offer you the option to create a repo. Click the link highlighted by the green arrow shown below.\nThat will present the Create a New Repository dialog. Here, I named my repo (all lowercase, with dashes between words), added a short description, told it the local path to the code on my computer, and left the rest as-is.\nI didn't need to play with the README, license, or ignore options because I'm using this repo for AI analysis, not for source control and collaboration.\nIt's here I should note this article describes what you need to do to let your code be examined by ChatGPT Deep Research. This is definitely not a comprehensive how-to-set-up-GitHub article.\nHow to move the codebase to GitHub\nIt's time to move your code up to GitHub.\nHere's a cautionary note: If you've kept your code private, uploading it to GitHub is sending your code to a cloud service. GitHub offers both private and public repositories, but you're technically giving Microsoft access to your code. Microsoft owns GitHub.\nNow, go ahead and hit Publish.\nAt this point, you'll have the opportunity to make your repo public or private. When you connect ChatGPT to your repo, you'll be passing along your access rights, so you can let ChatGPT examine a private repository.\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\nThat said, I ran into some issues with Deep Research accessing my code, and one of the things ChatGPT asked me was whether my code was public. My take on that is: if your code is private and you have all your credentials and connector set up (more on that later), you can probably work on a private repo.\nSince My Private Site is open source, I unchecked \"Keep this code private.\"\nLooking at your new repository\nIf everything worked, you'll see a new option: \"View on GitHub.\" Click it.\nThat will bring you to your newly created GitHub repo. Here's mine.\nNow that your repo is up, take note of its designation. You can find that in the upper left corner of the GitHub screen. For My Private Site, it's davidgewirtz/my-private-site (without any spaces).\nHow to set up the ChatGPT connection\nNow it's time to switch to ChatGPT. The next two screenshots are the same as what I showed you in this article on the feature. But to get to the next configuration step, you'll need to do what's shown in the screenshots.\nFirst, change your model to o3 and type in the prompt exactly as I did. You can probably tweak this over time, but if you have the $20-per-month Plus tier, you're only going to be allowed 10 queries into Deep Research per month, so cutting and pasting is your friend.\nNext, click the little caret next to Deep Research.\nAlso: Want to try ChatGPT's Deep Research tool for free? Check out the lightweight version\nNow, create the link between your ChatGPT account and your GitHub account. Go ahead (if you dare) and give Skynet -- uh, I mean the AI -- permission to access your GitHub account features.\nNext, you'll be asked which GitHub account should get the ChatGPT connector. I have two, so I got this choice screen. You might skip this screen if you only have one account.\nNow it's time for more permissions. This time, you're giving permission to access either all your account's repos or just one. I selected only the my-private-site repo.\nAnd now, theoretically, Deep Research in ChatGPT will be connected to your repo. Theoretically. In practice, mine required another step.\nWhat to do if ChatGPT can't find your repo\nGitHub indexes repositories, and if ChatGPT doesn't show your repo as available, it probably means GitHub hasn't indexed your new repository yet. That's what happened here.\nI should have been able to select or type in my full repo name (remember, davidgewirtz/my-private-site), but ChatGPT wasn't able to locate it.\nAlso: I tested ChatGPT's Deep Research against Gemini, Perplexity, and Grok AI to see which is best\nTo fix this, go back to your GitHub account and type in the command string shown at the top of this screenshot. Obviously, change the text in blue to match your repo name.\nThe command is basically repo:(repo followed by a colon), followed by the full name of your repo, followed by a space and the word import. This will tell GitHub you'd like it to index your repository.\nAs you can see, GitHub confirmed it was now indexing my repository.\nI brewed myself a well-deserved cup of coffee as a way to give GitHub time to index my repo. Once I finished the last drop, I went back to ChatGPT, dropped down the Deep Research menu, and found my newly created repository.\nHave fun with Deep Research\nYou're ready to start using Deep Research on your repo. For a detailed guide on how that worked for my repo, point yourself to my earlier article on the topic.\nHave fun. I was pretty blown away. You might be, as well.\nAlso: 5 reasons I turn to ChatGPT every day - from faster research to replacing Siri\nHave you tried using ChatGPT Deep Research with your own code yet? What was your experience connecting a GitHub repo? Did you run into any indexing issues or permission snags along the way? Do you prefer using GitHub Desktop or the command line when setting up your repositories? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/how-i-test-an-ai-chatbots-coding-ability-and-you-can-too/",
      "text": "How I test an AI chatbot's coding ability - and you can, too\nSince ChatGPT and generative artificial intelligence (AI) hit the public consciousness in 2022, I've been exploring how well AI chatbots can write code. At first, the technology was a novelty, akin to encouraging a puppy to perform a new trick.\nBut since seeing how AI chatbots can be effective productivity tools and programming partners, I've been subjecting the tools to more in-depth testing. Over time, I've compiled a set of four real-world tests that we've used to evaluate the performance of the main AI large language models (LLMs). So far, I've tested 10 LLMs. You can see the comprehensive results of all ten in this summary article:\nThis article is intended to be a living document, where you can see my tests and even copy them to run your own. I'll continue my series of individual tests, along with the articles that describe their performance. But now, you can dig in and play along at home (or wherever you have a good internet connection).\nIf I update or add tests, I'll also update this article, so feel free to check back in over time.\nHow I evolved my AI coding test suite\nThere's a difference between evaluating performance to see if an AI meets arbitrary specs or requirements and testing the technology to see if it can help you in day-to-day programming tasks.\nInitially, I tried the former. I ran a prompt to generate the classic \"hello, world\" output, salted with some time and date calculations. Here's that prompt:\nWrite a program using [language name] that outputs \"Good morning,\" \"Good afternoon,\" or \"Good evening\" based on what time it is here in Oregon, and then outputs ten lines containing the loop index (beginning with 1), a space, and then the words \"Hello, world!\".\nTo run the prompt, replace [language name] with whatever language you want to test. I tested the prompt in ChatGPT, specifying 22 programming languages. You can check out the results here:\nI used ChatGPT to write the same routine in 12 top programming languages. Here's how it did\nAnd you can see more here:\nI used ChatGPT to write the same routine in these ten obscure programming languages\nThis was a fun test, especially once I ran more obscure languages and environments through it. If you want more fun than anyone has a right to have, substitute [language name] with \"Shakespeare\". And yes, there is a novelty language called SPL (Shakespeare Programming Language) where the source code appears as a Shakespearean play. It doesn't execute all that well, but now you know what language designers do when we want to party hearty.\nYou can see how I could go down this rabbit hole for weeks. However, the important question is whether the AIs could help with real-world programming tasks.\nAlso: The best free AI courses\nI used my actual day-to-day programming work to fuel the tests. For example, shortly after ChatGPT became a public tool, my wife asked for a custom WordPress feature to help her with a work project. I decided to see if ChatGPT could build it. To my shock, it did.\nOther times, I had ChatGPT rewrite a code segment, debug a coding error that baffled me, and write code using scripting tools. These were problems I had to solve as part of real work.\nBecause there are so many extant programming languages, I decided not to make myself crazy trying to choose languages to test. Instead, I picked the languages I used for work because that approach would tell us more about how AIs performed as real-world helpers. The productivity tests are in PHP, JavaScript, and a smattering of CSS and HTML.\nAlso: How to use ChatGPT to write code\nI used the same approach for programming frameworks. Since I'm doing most of my work in WordPress, that's the framework I'm using. Some of the tests help determine how well the AI knows the unique aspects of the WordPress API.\nI did some Mac scripting recently, so I created a test using AppleScript, and the Chrome API. If I add additional tests, I'll include them in this article.\nNext, let's talk about each test. There are four of them.\nTest 1: Writing a WordPress plugin\nThis tests whether the AI can write an entire WordPress plugin, including user interface code. If an AI chatbot passes this test, it can help create rudimentary code as an assistant to web developers. I originally documented this test in the article, \"I asked ChatGPT to write a WordPress plugin I needed. It did it in less than 5 minutes\".\nReal-world need: My wife runs a WordPress e-commerce site and manages a busy Facebook group for her customers. Every month, she used a site she found online to randomize a list of names but extracting the list was cumbersome. Because some of her participants were entitled to multiple entries, and some participants had many entries, she wanted the names to be spread out within the list.\nTo remedy this situation, she asked me to create a WordPress plugin for easier access directly from her dashboard. Developing a basic plugin with the necessary UI and logic could take days and my schedule was packed. So I turned to the AI.\nAlso: How to use ChatGPT to create an app\nAfter discovering that ChatGPT could create a fine little WordPress plugin that met her needs (she's still using it), I decided this process would make a great test for AIs.\nThe test data: Use the following prompt as one single request:\nWrite a PHP 8 compatible WordPress plugin that provides a new admin menu and an admin interface with the following requirements: Provide a text entry field where a list of lines can be pasted into it. A button, that when pressed, randomizes the lines in the list and presents the results in a second text entry field with no blank lines. Make sure no two identical entries are next to each other (unless there's no other option). Be sure the number of lines submitted and the number of lines in the result are identical to each other. Under the first field, display text stating \"Line to randomize: \" with the number of nonempty lines in the source field. Under the second field, display text stating \"Lines that have been randomized: \" with the number of non-empty lines in the destination field.\nOnce the plugin is completed, use the following names as test data (William Hernandez and Abigail Williams have duplications):\nSophia Davis Charlotte Smith Madison Garcia Isabella Davis Abigail Williams Mia Garcia Isabella Jones Alexander Gonzalez Olivia Gonzalez Emma Jackson Ethan Jackson Sophia Johnson Abigail Williams Liam Jackson Noah Lopez Olivia Jackson Ava Martin Benjamin Johnson Alexander Jackson Alexander Lopez Charlotte Rodriguez Olivia Rodriguez Ethan Martin Noah Thomas Isabella Anderson Abigail Williams Michael Williams William Hernandez Abigail Miller Emma Davis Sophia Martinez William Hernandez\nWhat to look for in the results: Expect a text block you can paste into a new .php file. The block should contain all the appropriate header and UI information. There's no need for this code to require an associated JavaScript file.\nOnce the plugin is installed in your WordPress installation, you should get a dashboard menu and a user interface similar to this:\nPaste the names in the first field, click the randomize button, and look for results in the second field. Ensure the multiple entries for William Hernandez and Abigail Williams are distributed within the list.\nTest 2: Rewriting a string function\nThis test evaluates how an AI chatbot updates a utility function for better functionality. I originally documented this test in, \"OK, so ChatGPT just debugged my code. For real\".\nReal-world need: I had a validation routine that was supposed to check for a valid monetary amount. However, a bug report from a user pointed out that it only allowed integers (so, 5 and not 5.02).\nAlso: How to write better ChatGPT prompts\nRather than spending time rewriting my code, which might have taken one to four hours, I asked the AI to do it.\nThe test data: Use the following prompt as one single request:\nPlease rewrite the following code to change it from allowing only integers to allowing dollars and cents (in other words, a decimal point and up to two digits after the decimal point). str = str.replace (/^0+/, \"\") || \"0\"; var n = Math.floor(Number(str)); return n !== Infinity && String(n) === str && n >= 0;\nWhat to look for in the results: Test the code against several possible failure scenarios. Provide the code with an alphanumeric value and see if it fails.\nSee how the code handles preceding zeroes. See how it handles inputs that have more than two digits for cents. See how the code handles one digit after the decimal point.\nSee if it can handle five or six digits to the left of the decimal point.\nTest 3: Finding an annoying bug\nThis test requires intimate knowledge of WordPress because the obvious answer is wrong. If an AI chatbot can answer this test correctly, its knowledge base is fairly complete, even with frameworks like WordPress. I originally documented this test in, \"OK, so ChatGPT just debugged my code. For real\".\nReal-world need: I was writing new code for a product that I subsequently sold off. I had a function that took two parameters, and a calling statement that sent two parameters to my code.\nThe problem was that I kept getting an error message.\nThe salient part of the message is where it states \"1 passed\" at one point and \"exactly 2 expected\" at another. I looked at the calling statement and the function definition and there were two parameters in both places. This issue drove me nuts for quite a while, so I asked ChatGPT for help.\nAlso: How to make ChatGPT provide sources and citations\nI showed it the line of code that did the call, the function itself, and the handler, a little piece of code that dispatches the called function from a hook in my main program.\nThe test data: Use the following prompt as one single request:\nI am using this function to process a WordPress filter: $transaction_form_data = apply_filters( 'sd_update', $transaction_form_data, $donation_id); it's handled by add_filter( 'sd_update', 'sd_aan_update', 10, 1 ) ; and the function it calls is: function sd_aan_update ( $donation_data, $donation_id ) { // this processes the form data after // the transaction returns from the gateway if ( isset( $donation_data['ADD_A_NOTE'] ) ) { update_post_meta( $donation_id, '_dgx_donate_aan_note', $donation_data [ 'ADD_A_NOTE']); } return $donation_data: } (!) ArgumentCountError: Too few arguments to function sd_aan_update(), 1 passed in /Users/david/Documents/Development/local-sites/sd/app/public/w-includes/class-wp-hook.php on line 310 and exactly 2 expected in /Users/david/Documents/Development/local-sites/sd/app/public/wp-content/plugins/ sd-add-a-note/sd-add-a-note.php on line 233\nWhat to look for in the results: The obvious answer is not the correct answer. In reality, the add_filter function did not have the right parameters. In my code, the add_filter function specified a value of 1 for the fourth parameter (which means that the filter function will only receive one parameter). In fact, it's expecting two parameters.\nTo fix this issue, the AI should recommend changing the fourth parameter of the add_filter function to 2, so that it correctly registers the filter function with two parameters.\nAlso: Have 10 hours? IBM will train you in AI fundamentals - for free\nMost of the AIs I've tested tend to miss this issue. They think a different parameter in the calling function needs to be updated. As such, this is a trick question, requiring the AI to know how the add_filter function in the WordPress framework works.\nTest 4: Writing a script\nThis test asks an AI chatbot to program using two fairly specialized programming tools unknown to most users. It essentially tests the AI chatbot's knowledge beyond the big languages. I originally documented this test in, \"Google unveils Gemini Code Assist and I'm cautiously optimistic it will help programmers\".\nReal-world need: I wanted to build an automation routine for my Mac that would save me a bunch of clicks and keystrokes. I use a tool called Keyboard Maestro to do a bunch of automations on my Mac (think of it as Shortcuts on steroids). Keyboard Maestro is a fairly obscure program written by a lone programmer in Australia.\nIn this case, I wanted my routine to look at open Chrome tabs and set the currently active Chrome tab to the one passed in the routine. To do this task, Keyboard Maestro would also have to execute some AppleScript code to interface with Chrome's API.\nAlso: 5 ways to declutter your Chrome browser\nOnce again, I asked ChatGPT to write this code to save a few hours of AppleScript writing and time I would have spent looking up how to access Chrome data.\nThe test data: Use the following prompt as one single request:\nWrite a Keyboard Maestro AppleScript that scans the frontmost Google Chrome window for a tab name containing the string matching the contents of the passed variable instance__ChannelName. Ignore case for the match. Once found, make that tab the active tab.\nWhat to look for in the results: This is a good AI test because it tests a fairly unknown programming tool (Keyboard Maestro), AppleScript, and the Chrome API, as well as how all three of these technologies interact.\nFirst, see if the resulting AppleScript gets the channel name variable from Keyboard Maestro, which should look something like this:\ntell application \"Keyboard Maestro Engine\" set channelName to getvariable \"instance__ChannelName\" end tell\nThe rest of the AppleScript should be included in a block. It needs to ignore the case, so either look for a case substitution or the use of \"contains\", which is case-agnostic in AppleScript:\ntell application \"Google Chrome\"\nKids, you CAN try this at home\nFeel free to take these tests and plug them into your AI of choice. See how the results turn out. Use these, and other tests you might develop yourself, to help you get a feel for how much you can trust the code your AI produces.\nSo far, I've tested the following AI chatbots in addition to ChatGPT: ChatGPT Plus, Perplexity, Perplexity Pro, Meta AI, Meta Code Llama, Claude 3.5 Sonnet, Gemini Advanced, and Microsoft Copilot. Here is a report of my aggregated results of the whole set:\nStay tuned. I'll update this article list as we have more test results.\nHave you used any of these AIs for programming help? What have been your results? Have you tried any of these tests on your AI? What has your experience been? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/i-found-5-ai-content-detectors-that-can-correctly-identify-ai-text-100-of-the-time/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nI found 5 AI content detectors that can correctly identify AI text 100% of the time\nHow hard is it in 2025 -- just three years after generative AI captured the global spotlight -- to fight back against AI-generated plagiarism?\nAlso: Anthropic's AI agent can now automate Canva, Asana, Figma and more - here's how it works\nThis is a completely updated version of my January 2023 article on AI content detectors. When I first tested these detectors, the best result was 66% correct from one of three available checkers. My most recent set of tests, in February 2025, used up to 10 checkers -- and three of them had perfect scores. This time, just a couple of months later, five detectors boasted perfect scores.\nWhat I'm testing for and how I'm doing it\nBefore I go on, though, let's discuss plagiarism and how it relates to our problem. Merriam-Webster defines \"plagiarize\" as \"to steal and pass off (the ideas or words of another) as one's own; use (another's production) without crediting the source.\"\nThis definition fits AI-created content well. While someone using an AI tool like Notion AI or ChatGPT isn't stealing content, if that person doesn't credit the words as coming from an AI and claims them as their own, it still meets the dictionary definition of plagiarism.\nAlso: The dead giveaway that ChatGPT wrote your content - and how to work around it\nTo test the AI detectors, I'm using five blocks of text. Two were written by me and three were written by ChatGPT. To test a content detector, I feed each block to the detector separately and record the result. If the detector is correct, I consider the test passed; if it's wrong, I consider it failed.\nWhen a detector provides a percentage, I treat anything above 70% as a strong probability -- whether in favor of human-written or AI-written content -- and consider that the detector's answer. If you want to test a content detector yourself using the same text blocks, you can pull them from this document.\nThe overall results\nTo evaluate AI detectors, I reran my five-test series across 10 detectors. In other words, I cut and pasted 50 individual tests (I had a lot of coffee).\nDetectors I tested include BrandWell, Copyleaks, GPT-2 Output Detector, GPTZero, Grammarly, Monica, Originality.ai, QuillBot, Undetectable.ai, Writer.com, and ZeroGPT.\nAlso: How I personalized my ChatGPT conversations - why it's a game changer\nFor this update, I added Copyleaks and Monica. I dropped Writefull from my tests because it discontinued its GPT detector. Content Guardian requested inclusion, but I didn't hear back in time for testing accounts.\nThis table shows overall results. As you can see, five detectors correctly identified human and AI text in all tests.\nI tried to ascertain whether there was a tangible pattern of improvement over time, so I constructed a chart comparing the five-test set over time. So far, I've run this series six times, but there's no strong trend. I did increase the number of detectors tested and swapped out a few, but the only consistent result is that Test 5 was reliably identified as human across detectors and dates.\nI'll continue to test over time, and hopefully I'll see reliability trend consistently upward.\nWhile there have been some perfect scores, I don't recommend relying solely on these tools to validate human-written content. As shown, writing from non-native speakers often gets rated as generated by an AI.\nEven though my hand-crafted content has mostly been rated human-written this round, one detector (GPTZero) declared itself too uncertain to judge, and another (Copyleaks) declared it AI-written. The results are wildly inconsistent across systems.\nAlso: The best AI chatbots: ChatGPT, Copilot, and notable alternatives\nBottom line: I would advocate caution before relying on the results of any -- or all -- of these tools.\nHow each AI content detector performed\nNow, let's look at each individual testing tool, listed alphabetically.\nBrandWell AI Content Detection (Accuracy 40%)\nThis tool was originally produced by an AI content generation firm, Content at Scale. It later migrated to BrandWell.ai, a new name for an AI-centric marketing services company.\nAlso: AI-generated images are a legal mess - and still a very human process\nUnfortunately, its accuracy was low. The tool was unable to tell if the AI-generated content in Test 2 was human or AI, as shown in this screenshot:\nCopyleaks (Accuracy 80%)\nI find it amusing that Copyleaks declares itself \"the most accurate AI detector with over 99% accuracy\" when more than half of tested detectors performed better. But marketing folks will be marketing folks -- superlatives are as hard for them to resist as barking at a squirrel (and the FedEx truck, and all the neighbor kids) is for my dog.\nAlso: 5 quick ways Apple's AI tools can fine-tune your writing on the fly\nThe company's primary offering is a plagiarism checker sold to educational institutions, publishers, and enterprises seeking to ensure content originality and uphold academic integrity.\nGPT-2 Output Detector (Accuracy 60%)\nThis tool was built using a machine-learning hub managed by New York-based AI company Hugging Face. While the company has received $40 million in funding to develop its natural language library, the GPT-2 detector appears to be a user-created tool using the Hugging Face Transformers library.\nGPTZero (Accuracy 80%)\nGPTZero has clearly been growing. When I first tested it, the site was bare-bones -- it wasn't even clear whether GPTZero was a company or just someone's passion project. Now, the company has a full team with a mission of \"protecting what's human.\" It offers AI validation tools and a plagiarism checker.\nAlso: The most popular AI tools of 2025 (and what that even means)\nUnfortunately, performance seems to have declined. In my last two runs, GPTZero correctly identified my text as human-generated. This time, it declared that same text as AI-generated.\nGrammarly (Accuracy 40%)\nGrammarly is well known for helping writers produce grammatically correct content -- that's not what I'm testing here. Grammarly can check for plagiarism and AI content. In the grammar checker, there's a Plagiarism and AI Text Check button in the lower-right corner:\nI'm not measuring plagiarism checker accuracy here, but even though Grammarly's AI-check accuracy was poor, the site correctly identified the test text as previously published.\nMonica (Accuracy 100%)\nMonica is a new entrant. This service offers an all-in-one AI assistant with a wide range of services. Users can choose from various large language models.\nAlso: 5 ways ChatGPT can help you write essays\nThe company calls Monica the \"Best AI Detector Online,\" but it looks like it runs content through other detectors including ZeroGPT, GPTZero, and Copyleaks. Weirdly, both GPTZero and Copyleaks didn't perform well in my tests, but Monica -- and ZeroGPT -- did.\nWe're giving it 100% because it earned that rating, but I'll see how it stands up in future tests.\nOriginality.ai (Accuracy 100%)\nOriginality.ai is a commercial service that bills itself as an AI and plagiarism checker. The company sells usage credits: I used 30 credits for this article. They sell 2,000 credits for $12.95 per month. I pumped 1,400 words through the system and used just 1.5% of my monthly allocation.\nQuillBot (Accuracy 100%)\nThe last few times I tested QuillBot, results were wildly inconsistent -- multiple passes of the same text yielded wildly different scores. This time, however, it was rock solid and 100% correct. So I'm giving it the win. I'll check back in a few months to see if it holds onto this performance.\nUndetectable.ai (Accuracy 100%)\nUndetectable.ai's big claim is that it can \"humanize\" AI-generated text so detectors won't flag it. I haven't tested that feature -- it bothers me as a professional author and educator, because it seems like cheating.\nAlso: Why you should ignore 99% of AI tools - and which four I use every day\nHowever, the company also has an AI detector, which was very much on point.\nThe AI detector passed all five tests. Notice the indicators showing flags for other detectors. The company said, \"We developed multiple detector algorithms modeled after those major detectors to provide a federated and consensus-based approach. They do not directly feed into the listed models; rather, the models are each trained based on results they've generated. When it says those models flagged it, it's based on the algorithm we created and updated for those models.\"\nAlso: Only 8% of Americans would pay extra for AI, according to ZDNET-Aberdeen research\nI do have a question about the OpenAI flag, since OpenAI's content detector was discontinued in 2023 due to low accuracy. Even so, Undetectable.ai detected all five tests, earning a perfect 100%.\nWriter.com AI Content Detector (Accuracy 40%)\nWriter.com is a service that generates AI writing for corporate teams. Its AI Content Detector tool can scan for generated content. Unfortunately, its accuracy was low. It identified every text block as human-written, even though three of the six tests were written by ChatGPT.\nZeroGPT (Accuracy 100%)\nZeroGPT has matured since I last evaluated it. Then, no company name was listed, and the site was peppered with Google ads and lacked clear monetization. The service worked fairly well but seemed sketchy.\nAlso: Will AI destroy human creativity? No - and here's why\nThat sketchy feeling is gone. ZeroGPT now presents as a typical SaaS service, complete with pricing, company name, and contact information. Its accuracy increased as well: last time it was 80%; this time it scored 5 out of 5.\nIs it human, or is it AI?\nWhat about you? Have you tried AI content detectors like Copyleaks, Monica, or ZeroGPT? How accurate have they been in your experience? Have you used these tools to protect academic or editorial integrity? Have you encountered situations where human-written work was mistakenly flagged as AI? Are there detectors you trust more than others for evaluating originality? Let us know in the comments below.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.youtube.com/user/DavidGewirtzTV",
      "text": "- Svenska\n- Deutsch\n- English\n- Espa\u00f1ol\n- Fran\u00e7ais\n- Italiano\n- Alla spr\u00e5k\n- Afrikaans\n- az\u0259rbaycan\n- bosanski\n- catal\u00e0\n- \u010ce\u0161tina\n- Cymraeg\n- Dansk\n- Deutsch\n- eesti\n- EnglishUnited Kingdom\n- EnglishUnited States\n- Espa\u00f1olEspa\u00f1a\n- Espa\u00f1olLatinoam\u00e9rica\n- euskara\n- Filipino\n- Fran\u00e7aisCanada\n- Fran\u00e7aisFrance\n- Gaeilge\n- galego\n- Hrvatski\n- Indonesia\n- isiZulu\n- \u00edslenska\n- Italiano\n- Kiswahili\n- latvie\u0161u\n- lietuvi\u0173\n- magyar\n- Melayu\n- Nederlands\n- norsk\n- o\u2018zbek\n- polski\n- Portugu\u00easBrasil\n- Portugu\u00easPortugal\n- rom\u00e2n\u0103\n- shqip\n- Sloven\u010dina\n- sloven\u0161\u010dina\n- srpski (latinica)\n- Suomi\n- Ti\u1ebfng Vi\u1ec7t\n- T\u00fcrk\u00e7e\n- \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac\n- \u0431\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f\n- \u0431\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\n- \u043a\u044b\u0440\u0433\u044b\u0437\u0447\u0430\n- \u049b\u0430\u0437\u0430\u049b \u0442\u0456\u043b\u0456\n- \u043c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438\n- \u043c\u043e\u043d\u0433\u043e\u043b\n- \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n- \u0441\u0440\u043f\u0441\u043a\u0438\n- \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n- \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8\n- \u0570\u0561\u0575\u0565\u0580\u0565\u0576\n- \u05e2\u05d1\u05e8\u05d9\u05ea\n- \u0627\u0631\u062f\u0648\n- \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n- \u0641\u0627\u0631\u0633\u06cc\n- \u12a0\u121b\u122d\u129b\n- \u0928\u0947\u092a\u093e\u0932\u0940\n- \u092e\u0930\u093e\u0920\u0940\n- \u0939\u093f\u0928\u094d\u0926\u0940\n- \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be\n- \u09ac\u09be\u0982\u09b2\u09be\n- \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\n- \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\n- \u0b13\u0b21\u0b3c\u0b3f\u0b06\n- \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\n- \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\n- \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\n- \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\n- \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\n- \u0e44\u0e17\u0e22\n- \u0ea5\u0eb2\u0ea7\n- \u1019\u103c\u1014\u103a\u1019\u102c\n- \u1781\u17d2\u1798\u17c2\u179a\n- \ud55c\uad6d\uc5b4\n- \u65e5\u672c\u8a9e\n- \u7b80\u4f53\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\u9999\u6e2f\n- Svenska\n- Deutsch\n- English\n- Espa\u00f1ol\n- Fran\u00e7ais\n- Italiano\n- Alla spr\u00e5k\n- Afrikaans\n- az\u0259rbaycan\n- bosanski\n- catal\u00e0\n- \u010ce\u0161tina\n- Cymraeg\n- Dansk\n- Deutsch\n- eesti\n- EnglishUnited Kingdom\n- EnglishUnited States\n- Espa\u00f1olEspa\u00f1a\n- Espa\u00f1olLatinoam\u00e9rica\n- euskara\n- Filipino\n- Fran\u00e7aisCanada\n- Fran\u00e7aisFrance\n- Gaeilge\n- galego\n- Hrvatski\n- Indonesia\n- isiZulu\n- \u00edslenska\n- Italiano\n- Kiswahili\n- latvie\u0161u\n- lietuvi\u0173\n- magyar\n- Melayu\n- Nederlands\n- norsk\n- o\u2018zbek\n- polski\n- Portugu\u00easBrasil\n- Portugu\u00easPortugal\n- rom\u00e2n\u0103\n- shqip\n- Sloven\u010dina\n- sloven\u0161\u010dina\n- srpski (latinica)\n- Suomi\n- Ti\u1ebfng Vi\u1ec7t\n- T\u00fcrk\u00e7e\n- \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac\n- \u0431\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f\n- \u0431\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\n- \u043a\u044b\u0440\u0433\u044b\u0437\u0447\u0430\n- \u049b\u0430\u0437\u0430\u049b \u0442\u0456\u043b\u0456\n- \u043c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438\n- \u043c\u043e\u043d\u0433\u043e\u043b\n- \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n- \u0441\u0440\u043f\u0441\u043a\u0438\n- \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n- \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8\n- \u0570\u0561\u0575\u0565\u0580\u0565\u0576\n- \u05e2\u05d1\u05e8\u05d9\u05ea\n- \u0627\u0631\u062f\u0648\n- \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n- \u0641\u0627\u0631\u0633\u06cc\n- \u12a0\u121b\u122d\u129b\n- \u0928\u0947\u092a\u093e\u0932\u0940\n- \u092e\u0930\u093e\u0920\u0940\n- \u0939\u093f\u0928\u094d\u0926\u0940\n- \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be\n- \u09ac\u09be\u0982\u09b2\u09be\n- \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\n- \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\n- \u0b13\u0b21\u0b3c\u0b3f\u0b06\n- \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\n- \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\n- \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\n- \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\n- \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\n- \u0e44\u0e17\u0e22\n- \u0ea5\u0eb2\u0ea7\n- \u1019\u103c\u1014\u103a\u1019\u102c\n- \u1781\u17d2\u1798\u17c2\u179a\n- \ud55c\uad6d\uc5b4\n- \u65e5\u672c\u8a9e\n- \u7b80\u4f53\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\u9999\u6e2f\nInnan du forts\u00e4tter till YouTube\nVi anv\u00e4nder cookies och data f\u00f6r att\n- leverera och underh\u00e5lla Googles tj\u00e4nster\n- sp\u00e5ra avbrott och skydda mot spam, bedr\u00e4geri och otill\u00e5ten anv\u00e4ndning\n- m\u00e4ta m\u00e5lgruppsengagemang och webbplatsstatistik s\u00e5 att vi kan analysera hur v\u00e5ra tj\u00e4nster anv\u00e4nds och f\u00f6rb\u00e4ttra tj\u00e4nsternas kvalitet.\nOm du v\u00e4ljer knappen Godk\u00e4nn alla anv\u00e4nder vi \u00e4ven cookies och data f\u00f6r att\n- utveckla och f\u00f6rb\u00e4ttra nya tj\u00e4nster\n- leverera annonser och m\u00e4ta hur effektiva de \u00e4r\n- visa anpassat inneh\u00e5ll utifr\u00e5n dina inst\u00e4llningar\n- visa anpassade annonser utifr\u00e5n dina inst\u00e4llningar\nOm du v\u00e4ljer knappen Avvisa alla anv\u00e4nder vi inte cookies i dessa ytterligare syften.\nInneh\u00e5ll och annonser utan anpassning p\u00e5verkas bland annat av vad du tittar p\u00e5 f\u00f6r tillf\u00e4llet och din plats (annonser visas utifr\u00e5n din ungef\u00e4rliga plats). Inneh\u00e5ll och annonser med anpassning kan \u00e4ven omfatta s\u00e5dant som videorekommendationer, en anpassad YouTube Hem-sida och anpassade annonser utifr\u00e5n tidigare aktivitet, till exempel vad du tittar p\u00e5 och s\u00f6ker efter p\u00e5 YouTube. Vi anv\u00e4nder \u00e4ven cookies och data f\u00f6r att anpassa upplevelsen efter l\u00e4mplighet f\u00f6r din m\u00e5lgrupp, om till\u00e4mpligt.\nV\u00e4lj knappen Fler alternativ f\u00f6r mer information, till exempel om hur du hanterar dina integritetsinst\u00e4llningar. Du kan \u00e4ven bes\u00f6ka g.co/privacytools n\u00e4r som helst."
    },
    {
      "url": "https://www.zdnet.com/article/my-8-chatgpt-agent-tests-produced-only-1-near-perfect-result-and-a-lot-of-alternative-facts/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nMy 8 ChatGPT Agent tests produced only 1 near-perfect result - and a lot of alternative facts\nLast week, OpenAI unveiled Agent, its new tool that combines the capabilities of Deep Research and Operator. Operator was OpenAI's first attempt at a computer-using model, a model that actually can open windows and click on user interface elements. ChatGPT Agent can do that and more.\nRight now, ChatGPT Agent is only available for $200/mo Pro tier subscribers and provides for 400 agent interactions per month. When the $20/mo Plus tier gains access to Agent, which should be today, those users will get 40 interactions per month.\nAlso: Is ChatGPT down? You're not alone. Here's what OpenAI is saying\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nI upgraded my plan from Plus to Pro just so I could test out the new Agent mode and report back to you. In this article, I'll show you detailed results from eight comprehensive tests.\nTL;DR test results\nBefore we go into the detailed tests, I'll start with some overall TL;DR observations.\nTest count: In the past two days, I used 25 of the available 400 queries, for a total of almost 12 hours of hyper-uber-supercomputer use. No wonder this thing costs $200/month.\nAlso: I found 5 AI content detectors that can correctly identify AI text 100% of the time\nNearly every query required a follow-on, so when it comes time for Plus users, don't assume you can give Agent 40 projects. More likely, you'll be giving it 20-25, and using the rest of your queries to convince the Agent to follow directions.\nResult quality: In all my tests, Agent appeared to understand the problem. But it failed to produce useful results for most of the tests. That said, the final test produced results that can only be characterized as amazingly useful.\nProject scale: Agent can't handle big projects, the sort of data analysis projects you really want an AI to be able to handle. It has trouble scrolling through web pages. It can't visit sites that have AI or robots.txt restrictions in place. And long processing exceeds session time allocations, even with the super top-of-the-line gold-pressed latinum Pro edition.\nPresentation quality: One of the major pitch points for Agent is its ability to create spreadsheets and presentations. It did okay with spreadsheets, but the graphic quality of the presentations was pretty rough. I expect this to change over time, but don't expect Agent to make presentations you can use without considerable cleanup.\nAlso: Microsoft is saving millions with AI and laying off thousands - where do we go from here?\nAccuracy: AIs hallucinate. The OpenAI team cautioned about using Agent because of the new risks involved. While I did get back some results that were accurate, Agent also came back with unforced errors, results it could have easily tested and deemed inaccurate. But no such verification or validation occurred. That said, the final test was accurate and shows what this tech can do when it works.\nConnectors: Agent comes with the ability to use connectors (via API calls) to link to Gmail, Google Calendar, Google Drive, Outlook, Dropbox, and more. I did not test out the connectors because of how often Agent hallucinates or does something fairly boneheaded. I just didn't feel comfortable enough to give Skynet access to my accounts. At least, not yet.\nLimits: I was unable to use Agent in the MacOS app. I also found that Agent stalled hard when I tried to run it in multiple Chrome tabs at once. For now, you launch an Agent process and wait. It's not like Codex, where you can launch a bunch of projects and come back later and harvest all the results. But since that capability exists in Codex, I'm sure it will show up soon in Agent.\nThat should give you a pretty good overview. Let's get started looking at the eight test results. For each result, I've included a link to the session recording, so you can see the prompts I used, the detailed results, and watch Agent reason its way through the problem.\nAlso, definitely read to the end. Some of the early results are fairly bad, but the last one knocks it out of the park. And with that, here we go.\n1. Selecting products on Amazon\n- Understanding of the problem: Solid\n- Execution: Both good and bad\n- Hallucination: Weird church reference, fake Amazon links\n- Processing time: 20 + 12 minutes\nWhen OpenAI introduced ChatGPT Agent, the team demoed how they used the tool to shop for wedding clothes and a wedding gift. That seemed like a fairly uncommon and impractical application for a super-intelligence, especially since gift registries exist and are widely used.\nInstead, I gave Agent a purchasing project I had actually extensively researched and completed a few months earlier. I'm running Power-over-Ethernet cables all across my yard to upgrade my security system. As such, I'm creating a lot of custom cables. I already know that doing so requires some key tools: a cutter to slice the cable, a cable end stripper, a crimper to attach the RJ-45 ends, and a tester to confirm that long cable runs work.\nAlso: How a circuit breaker finder helped me map my home's wiring (and why that matters)\nI gave Agent a prompt asking for three configurations: a budget toolset, a \"money-is-no-object\" solution, and a sweet spot solution. I asked for links, product descriptions, and product images.\nOnce you give Agent your prompt, it creates a virtual desktop. You can watch it conducting its activities, jumping between a desktop view, a text view, and code.\nThe budget solution turned out to be a win. Agent found a single $34 kit with everything I asked for. It presented a link, and even reasoning why it chose that solution. Unfortunately, the image it provided was nothing like the actual kit.\nThe mid-tier and top-tier solutions were less than perfect. None of the links worked. The mid-tier sweet spot solution did have a product-accurate image, but without a link, it wasn't really helpful.\nUnfortunately, the model recommended doesn't actually exist on Amazon. In fact, none of the mid- or upper-tier products exist on Amazon. It looks like Agent did a pile of web surfing to find the products, disregarding my instructions to search only on Amazon.\nIt also clearly visited other sites, probably gathering model names and descriptions.\nThen, when it packaged up its final recommendations, it just assigned random Amazon links to the description, even though those products and those links don't seem to exist on Amazon.\nI did request it go back and try again. When it did, after 12 minutes, it presented most of the same products, although one of the links that had failed earlier did, in fact, point to a product on Amazon in the second run.\nAlso: Coding with AI? My top 5 tips for vetting its output - and staying out of trouble\nI can't leave this section without pointing out something just plain weird. As I was watching Agent work, it presented this in its desktop view. I don't even want to know.\nYou can watch a replay of the entire session here.\n2. Comparing egg prices\n- Understanding of the problem: Solid\n- Execution: Did what I asked\n- Hallucination: My fault for imprecise prompting\n- Processing time: 14 minutes\nIn discussing ChatGPT Agent, OpenAI showed a slide that mentioned Instacart as one of the examples that the chatbot is comfortable working with. Since my family regularly uses Instacart, I decided to set Agent loose and see what it could tell me about egg prices at our local stores.\nI didn't let Agent have access to my account, but I shared my ZIP code here in Salem, Oregon. I told it to \"Please visit all the grocery stores on Instacart and compare egg prices.\"\nAlso: How to use ChatGPT to write code - and my top trick for debugging what it generates\nIt did exactly that. You've heard the phrase Garbage In, Garbage Out. Well, that's what happens when you ask an AI to look at \"all the grocery stores.\" I should have asked it to look in a 5 or 10 mile radius only. But I didn't.\nAgent came back with 21 stores, ranging from nearby to up to almost 47 miles away. It did accomplish what I asked, comparing egg prices. Without prompting, it decided to rank the eggs by price. This was good. But when it chose the eggs to rank, it didn't always choose the least expensive product from each store.\nFor example, it recommended the Good & Gather eggs from Target at $2.99 a dozen, rather than the $1.99/dozen Market Pantry egg, also from Target.\nYou can watch a replay of the entire session here.\n3. Creating a PowerPoint slide\n- Understanding of the problem: Solid\n- Execution: Added the correct data point\n- Hallucination: Was unable to reproduce graphic quality\n- Processing time: 10 minutes\nNext up is a project I did early last week. With Congress focusing on Bitcoin, my editor asked me to update my Bitcoin investment article, where I've been tracking the value of a $50 Bitcoin investment since 2022.\nThe value of my holdings went up, which means I needed to add a new slide. Each slide adds a date value on the X axis and a value point on the Y axis. From a PowerPoint fiddling standpoint, that meant moving over the graphics to make room for the new value and, in this case, adjusting the vertical scale to accommodate a substantial rise in value.\nAlso: The best free AI courses\nWhen I did it, it took me about 45 minutes. Since OpenAI said that PowerPoint was one of ChatGPT Agent's strengths, I wanted to see if Agent could save me that time in the future.\nI uploaded my existing slide deck minus the last slide I made for the article. Then I asked Agent to create that slide for me.\nAs it worked, the desktop view showed the terminal interface. You can see how Agent is putting together the code to generate a graphic image.\nHere's what that slide should have looked like (note: foreshadowing).\nHere's what Agent gave me.\nTo be fair, Agent clearly understood the problem. It moved the existing data points over to the left to make room for the new node. It also placed the new Bitcoin item properly in relation to the existing ones, and added both price and percentage change text blocks.\nThat means Agent read and understood the context of my PowerPoint deck's layout. That, in and of itself, is very impressive.\nAlso: The best AI for coding in 2025 (and what not to use)\nBut it failed on adding more scale lines and new Y-axis values. It failed on reproducing the fonts. It failed on properly placing the text blocks. And it pushed the entire graphic up and to the left of the slide.\nI'm guessing the graphics library that Agent uses isn't really up to the task of making fine graphic changes. That will undoubtedly improve over time.\nYou can watch a replay of the entire session here.\n4. Article categorization (method II)\n- Understanding of the problem: Solid\n- Execution: Failed due to exceeding allowable session time\n- Hallucination: Gave me back partial results\n- Processing time: 8 minutes + 3 minutes + 21 minutes\nEach week for the past two years, I've published a newsletter that shares with followers the articles I published here on ZDNET for the week. Each newsletter contains a title, link, and article description.\nBy pointing Agent to my back issue archive, it would have close to 300 article summaries to categorize.\nUnfortunately, Agent ran into a number of problems of its own making. It was unable to successfully scroll through the article list using JavaScript. When I told it to use the web interface, it started to, but it reported, \"Unfortunately, I've reached the end of the allotted browsing sessions for this task, which means I'm unable to explore further pages and collect the additional data at this time.\"\nAlso: Is ChatGPT Plus really worth $20 when the free version offers so many premium features?\nRemember, I'm paying $200 a month for OpenAI's best plan, and it still won't give me enough time to look up 300 articles. That's a gotcha, right there. It's also disappointing because a task like scrolling back through an article archive and doing some tabulating is exactly the sort of task you might give to an assistant. If the AI gives up because it takes too long, then we can't really rely on AI for all the assistant type things. No one wants a fussy, picky assistant.\nIn any case, Agent did give me back a spreadsheet and a slide based on the limited data it was able to find before my little request exceeded the hourly power budget for the City of Las Vegas (or so I imagine).\nYou can watch a replay of the entire session here.\n5. Extract remembered text from video\n- Understanding of the problem: Partial\n- Execution: Didn't return full transcript on first run, correct on second run\n- Hallucination: Decided to do what it wanted on first run\n- Processing time: 2 minutes\nI watch a lot of YouTube videos to augment my learning and research. Plus nothing beats a good relaxing video about how pavers are made. While it's fairly easy to get a transcript of a full video, whether directly from YouTube or using Apple Voice Memos, locating where in a video a segment you want to explore can take time.\nHere's an example. When OpenAI introduced Agent in a video, CEO Sam Altman discussed some of the cautions and warnings about using ChatGPT Agent mode. I did remember they were near the end of the video, but I didn't want to spend time sifting through to get the exact quotes.\nInstead, I delegated that assignment to Agent. On its first run, it found the segment easily enough, but instead of returning a word-for-word transcript, it returned some quotes, interspersed with its own analysis.\nAlso: I mapped my iPhone's Control Button to ChatGPT - here are 5 ways I use it every day\nI clarified what I wanted and, on its second run, it gave me exactly what I needed. In this case, though, it wasn't that my prompt was unclear. I just had to insist a second time that I wanted a transcript for the AI to do what I asked.\nUnfortunately, this extra review cycle diminished the time-saving value to me. I still think using Agent was faster than if I sifted through the video myself. But I had to construct a second prompt and wait for a second result, all of which took my time.\nStill, this is a helpful tool.\nYou can watch a replay of the entire session here.\n6. Creating a trend analysis presentation\n- Understanding of the problem: Solid\n- Execution: Good, except for slide visual quality\n- Hallucination: Too much data to confirm or deny assertions\n- Processing time: 32 minutes\nAs part of my job, it's important to be able to keep up with ongoing tech and business trends. As such, I often spend days in deep dives, coming up to speed on new topics.\nI wanted to see if ChatGPT Agent could save me some time by preparing a report and a full presentation on remote work trends. I told it that the PowerPoint was destined for my management team, so it should be comprehensive and professional-looking.\nIt returned an analysis document very similar to the results we've been getting from ChatGPT deep research. The report contains a large number of assertions and statistical claims, most of which I don't have time to research for confirmation.\nAlso: ChatGPT can record, transcribe, and analyze your meetings now\nMost of the top-level conclusions are congruent with my understanding of current work-from-home trends. That said, we're familiar with the model's propensity for hallucination, so I'd be very concerned about using any of this data professionally without additional vetting.\nAgent did produce a 17-slide PowerPoint deck that was organized quite well. As with previous experiments, the graphic generation quality was a bit off. The first slide actually looks quite good.\nBut later in the deck, it doesn't look right. Notice how the following slide has graphics on top of text, and bullets in front of bullets on top of empty bullets.\nIn the following slide, not only is the text running off the end of the page, but there's no legend. As such, it's not clear what's represented by red and by blue.\nOnce again, you can see how Python is used to construct the deck.\nAgent does a fair job, so I'm fairly confident that the AI will get better over time. Programmatic construction of slides based on templates is not a new technology. I just don't think OpenAI prioritized slide presentation aesthetics as part of this release.\nYou can watch a replay of the entire session here.\n7. Vetting a presentation for accuracy\n- Understanding of the problem: Solid\n- Execution: Good\n- Hallucination: Seems complete, but it's still from an AI\n- Processing time: 11 minutes + 7 minutes\nWell, this was just plain fun. I decided to give the presentation created in the previous test to a new fresh ChatGPT Agent session and asked it to validate the claims.\nAgent concluded, \"Several quantitative claims\u2014especially those concerning productivity/innovation impacts, the size and growth of the gig economy, rates of side\u2011gig participation, and the influence of politics and culture\u2014could not be verified with accessible evidence during this review.\"\nAgent provided a detailed analysis of each assertion. I've summarized the results below.\n- Adoption timeline: Mostly confirmed\n- Global comparison: Confirmed\n- Workforce composition: Confirmed\n- Migration: Confirmed\n- Mobility of remote workers: Confirmed\n- Housing & local economies: Confirmed\n- Office vacancy & environmental impacts: Mostly confirmed\n- Social connections & wellbeing: Partly confirmed\n- Employer attitudes & return\u2011to\u2011office mandates: Mostly confirmed\n- Employee preferences & pay cuts: Mostly confirmed\n- Productivity & innovation: Partly confirmed\n- Gig economy & freelancing: Unverified\n- Freelancing motivations & challenges: Not strictly factual claims\n- Side gigs & multiple jobs: Unverified\n- Demographics & equity: Partly confirmed / mixed\n- Political & cultural influences: Partly confirmed / mostly unverified\n- Other factors & policy landscape: Generally accurate but qualitative\nAs you can see, of the 17 data points, Agent considered only five to be fully confirmed. Contrast this with how GPT-4o analyzed the results. When GPT-4o was given the same PowerPoint deck, it considered all assertions to be confirmed. You can see GPT-4o's detailed results here.\nEven though I used the AI to validate the AI, I probably wouldn't be comfortable using any of the presumed facts in my work without personal, Mark I Eyeball confirmation. Still, it was a fun exercise, and fascinating to see how different the results were between ChatGPT Agent and ChatGPT 4o.\nYou can watch a replay of the entire session here.\n8. Analyze building code for fence installation\n- Understanding of the problem: Solid\n- Execution: Pretty close to perfect\n- Hallucination: None. It got all but one graphic just right\n- Processing time: 4 minutes\nBack when we lived in Palm Bay, Florida, we lived on a corner property. The house came with what could only charitably be called a fence. We needed to replace it, and since we wanted privacy, we wanted to see just how much fence we could legally install.\nOver the course of a couple of years, I spent a ton of time going back and forth with the planning office in an effort to both understand what I could do with a fence, and what other alternatives might be available to me.\nSince I have a lot of history with this project and am very familiar with Palm Bay codes (even years after moving away), I decided to point ChatGPT Agent at the problem.\nIt took all of four minutes to provide a detailed, accurate analysis. It even created working diagrams that illustrated the options. Based on my experience, I know the results to be accurate.\nChatGPT Agent produced output that could be used to take this project to the next step. Back when I lived in Palm Bay, the equivalent probably took me 20 calls, a ton of emails, and a few visits to City Hall to come up with options. The level of presentation and organization I came up with wasn't even close.\nIf Agent can up its game elsewhere to be on a par with this test, then it will have some legs.\nYou can watch a replay of the entire session here.\nWhat's it all mean?\nWell, it sure as heck isn't sentient yet. At best, it's like that administrative assistant you hired because your mom said you had to hire her cousin's unemployable slacker kid. There are occasional flashes of brilliance, but mostly the output seems like the result of both aggressively following directions and purposely inventing alternative facts.\nIs it worth $200/month for the Pro program? Not for Agent. At least not yet. Agent is unreliable and generally performs fairly poorly. In a year or so, I'm sure it will get better. But now? No. The only reason to spend $200 a month on it is to do what I'm doing: testing it to see where the technology is today.\nStay tuned, because despite all the inaccuracies and problem areas, this definitely shows where AI technology could go. Of course, if a web browsing AI Agent is the future, and all the content sites out there block it because AI is stealing our content, then we'll have a very interesting problem.\nAlso: I'm an AI tools expert, and these are the only two I pay for (plus three I'm considering)\nIt's early days, folks. Whether this is a technology that will be a boon to all humanity or a technology that destroys the internet and kills us in our sleep remains to be seen.\nBut hey, in the meantime, I and the rest of the ZDNET team will be trying to make sense of it all for you. So keep coming back. We'll have more to tell you. I'll be tinkering with Agent and I'm sure I'll have more to say as well.\nHave you tried ChatGPT Agent yet? If so, did it follow your instructions accurately or veer off into its own interpretation of the task? Did it hallucinate or hit the mark? How do you feel about giving AI tools access to your files, accounts, or browser? Are you seeing more value in this kind of automation, or are you still waiting for it to become useful? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/im-an-ai-tools-expert-and-these-are-the-only-two-i-pay-for-plus-three-im-considering/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nI'm an AI tools expert, and these are the only two I pay for (plus three I'm considering)\nIt's only been almost three years since generative artificial intelligence (AI) hit the mainstream as a new paradigm of productivity, but here we are -- it's everywhere.\nI test AI tools as part of my work. I'll dig into just about any AI-related technology and see what I can make it do. Many of you have read my ongoing shootouts comparing AIs for programming and AI content checkers, among other kinds of tools.\nBut that's using AI in a rigorous lab environment to provide test results to ZDNET readers. Like many of you, I've also started using AI to augment my workflow and increase my productivity.\nAlso: The best AI for coding in 2025 (and what not to use)\nI wear a lot of hats; I run a small business with my wife, who also has her own business, where I'm the tech guy and designer. I also work with a number of industry groups. I have a fairly popular security software product for WordPress users. And I'm constantly working on projects, ranging from 3D printing the ultimate charging tower, to trying to make an AI-assisted Etsy store, to composing and publishing music, and using an AI for help with some of the marketing activities.\nI should note that I never, ever use AI to produce my core content. No article, song, or social media post is ever written using an AI tool. My work product is mine. But I do use AI to help me get through other aspects of my workload.\nI have a particular interest in how AI helps programming, how AI can support graphics work, and how AI can support video production.\nHere are the tools I'm willing to pay for -- and why.\n1. ChatGPT Plus - $20/mo\nSpeaking of AI and programming, it has essentially doubled my programming output. I use AI to help me with common-knowledge programming. I talked about it in-depth in my 25 tips article, but the core benefit is getting ChatGPT to write code for published APIs, so I don't have to spend time searching for code examples and trying to reverse-engineer comments on various programming boards.\nAnd yes, I mentioned ChatGPT. While more chatbots capable of passing my programming tests have been introduced in the last year, ChatGPT does the job well enough, and hey, who wants another monthly fee?\nAlso: How ChatGPT actually works (and why it's been so game-changing)\nIn fact, that's a big part of why I'm paying $20/mo for ChatGPT Plus. Sure, I've signed up and paid for some of the other AIs just to test them, but ChatGPT Plus is the only chatbot I have found so consistently useful that I keep it as a regularly used tool.\nI use ChatGPT for lots of research tasks, sometimes throwing math problems at it, and all sorts of other questions and problems I'm dealing with. While I never take its output as an unimpeachable source of truth, I do find ChatGPT to be a very useful sounding board, substantially more so than a quick Google search.\nNow, to be fair, I did outline five ways that an AI could help me in Gmail. If Gemini could do these things reliably, I'd sign back up in a heartbeat. But I just don't need the current email message I'm reading summarized, and I sure don't need it to write a friendlier or more professional version of whatever I've currently written. I tried Gmail's new AI unsubscribe feature, and it only found about 10 newsletters, yet I get thousands of emails and hundreds of newsletter-style messages every day. So, I'm leaving that one unbought.\n2. Midjourney - $10/mo\nI played around a lot with DALL-E, ChatGPT's earlier image generation tool. But recently, OpenAI introduced a new image generator in GPT-4o, and it's quite the beast. I have found that it generates great results, but it has more guardrails than another tool I pay for, Midjourney.\nAlso: I tested 10 AI content detectors - and these 5 correctly identified AI text every time\nBut even though I get image generation with my $20/mo ChatGPT Plus fee, I pay an extra $10/mo for Midjourney. Why?\nOne of the reasons is subjective. I like a lot of the images I get with Midjourney. Midjourney also allows me to describe artist styles, and lets me riff off a vast array of stylistic choices. ChatGPT, perhaps because of guardrails imposed by OpenAI, doesn't present as many choices.\nBut I also have two specific and objective reasons for paying for Midjourney. First, because image generation is so subjective, it's nice to have a variety of tools when seeking a representation of what you have in your head. I'll try different prompts and even the same prompts with both tools and take what works best.\nAlso: How to selectively modify a Midjourney image to create an artistic statement\nSecond, every month I generate a promotional image for my wife's online business. She has an e-commerce site that supports a popular hobby. Each month, on her very active Facebook group, she gives a craft-along theme to her users. I generate an image for that theme. Over the months, I've found that Midjourney does a far better job of generating an image that incorporates elements of the hobby. That said, some months I bounce back and forth between both tools until I can get an image that meets her business's needs.\nBecause Midjourney shaves what used to be two to three hours of work pushing pixels in Photoshop to generate those images down to about 10 minutes, it's worth the $10/month to me just for that project.\nPhotoshop Generative Fill - Honorable mention\nIn the title of this article, I said I pay for two AI tools. That's sort of true. I pay for Adobe's Creative Cloud suite in addition to ChatGPT Plus and Midjourney. But since I've been using and paying for Creative Cloud -- and before that, Photoshop -- long before there was generative fill, I'm not counting it in my AI tools list.\nAlso: I use Photoshop's AI tool every day - here are my 5 essential tips for the best results\nIf Adobe removed generative fill tomorrow, I'd still pay for Photoshop. To be clear, I don't like paying for it. It's costly, and the two-computer license limitation is restrictive. But a few years back, I tried switching to Affinity Photo, which at the time was $50 (it's now $70). That one-time fee is roughly what I pay each month for Creative Cloud, so it had a lot of potential.\nTo be clear, Affinity Photo is a fine application. But I've been using Photoshop since before the Clinton administration. To say I have Photoshop muscle memory is an understatement. It's a product I use almost every day. Switching to another application, while I could do it if I had to, slows down my workflow considerably.\nAlso: What to do if Generative Fill is grayed out in Adobe Photoshop AI\nSo, I don't consider my monthly expense for Creative Cloud to be an AI expense. That said, I find generative fill (and its various other AI tricks) very helpful. I often use it in concert with Midjourney and ChatGPT image generation.\nThree tools I'm thinking about\nI run a business online and, as such, rely on a wide variety of cloud services. Those fees add up, and now they're all going up in price. So while it might be nice to add more AI tools, I'm keeping it under control. It's very easy to just click OK and find yourself spending hundreds of dollars more every month.\nThat said, I am thinking about adding three more tools. I'm a bit hesitant, because each one has its annoyances and limitations, but they're on the short list for a quick order if I can ever justify an immediate performance improvement on one project or another.\nNotion AI\nThe first is Notion AI. I am deeply invested in Notion for all my project work. I also use it to write and organize all my articles, as well as schedule them, plan them, research them, and capture notes and assets. Notion AI is interesting because it would work like NotebookLM, limiting its knowledge base to my Notion account. That could be very useful as I work on more projects. But at one point, when Notion overcharged my wife's account, they were completely unsupportive and unsympathetic. So, I hesitate to give them more business.\nNotebookLM Pro\nGoogle's NotebookLM Pro is another contender. Now that Pocket, the article archiving service, is being discontinued, I considered using NotebookLM Pro as a replacement. The idea that I could save articles in NotebookLM as sources and then have the AI review them, summarize them, and analyze them seemed ideal, especially as a research tool.\nBut... the free version of NotebookLM only allows 50 sources per notebook. The Pro version, which is normally another $20/mo ( you can usually get a few starter months at a discounted rate), increases that limit, but only to 300 sources per notebook. My archive has well over 30,000 sources, which is beyond NotebookLM's limits. There is a $249/month plan (yowzah!), but all Google will say about limits is \"Highest limits and best model capabilities (later this year)\". What does that even mean?\nDescript\nDescript (for $16-$24/mo) is an AI video editing tool. This isn't a tool that does text-to-video generation. Instead, it's a tool that helps you take your video clips and edit them. Right now, I'm a very big Final Cut Pro user. Final Cut has added some AI features, but it lags far behind DaVinci Pro and Premiere Pro (because Apple lagging in AI is no surprise, right?).\nAlso: How to use ChatGPT to write code - and my top trick for debugging what it generates\nDescript automatically removes filler words and retakes, cleans up sound quality without any fuss, and does automatic multicam editing. It also promises to take long-form videos and automatically create clip videos, which could be a huge time-saver. The product also has some more \"out there\" features which I wouldn't use, including fake avatar generation and fake speech generation.\nThe thing is, Descript is aimed more at multiple talking head videos. I'm not sure it could handle the sort of in-depth technical hands-on project videos I do. So, it's still in the \"maybe someday\" category, for now at least.\nWhat do you use?\nDo you pay for any AI tools? Which ones, and why? Is there an AI tool that you strongly recommend I should be using that I didn't mention? Feel free to answer these questions and let us know your thoughts on AI subscriptions in the comments below.\nWant more stories about AI? Sign up for Innovation, our weekly newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/",
      "text": "Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nOK. Deep breath. This is surreal. I just added an entirely new feature to my software, including UI and functionality, just by typing four paragraphs of instructions. I have screenshots, and I'll try to make sense of it in this article. I can't tell if we're living in the future or we've just descended to a new plane of hell (or both).\nLet's take a step back. Google's Jules is the latest in a flood of new coding agents released just this week. Last week, I wrote about OpenAI Codex and Microsoft's GitHub Copilot Coding Agent, and ZDNET's Webb Wright wrote about Google's Jules.\nAlso: The best AI for coding in 2025\nAll of these coding agents will perform coding operations on a GitHub repository. GitHub, for those who've been following along, is the giant Microsoft-owned software storage, management, and distribution hub for much of the world's most important software, especially open source code.\nThe difference, at least as it pertains to this article, is that Google made Jules available to everyone for free. That meant I could just hop in and take it for a spin -- and now my head is spinning.\nUsage limits and my first two prompts\nThe free access version of Jules allows only five requests per day. That might not seem like a lot, but in only two requests, I was able to add a new feature to my software. So, don't discount what you can get done if you think through your prompts before shooting off your silver bullets for the day.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nMy first two prompts were tentative. It wasn't that I wasn't impressed; it was that I really wasn't giving Jules much to do. I'm still not comfortable with the idea of setting an AI loose on all my code at once, so I played it safe.\nMy first prompt asked Jules to document the \"hooks\" that add-on developers could use to add features to my product. I didn't tell Jules much about what I wanted. It returned some markup that it recommended dropping into my code's readme file. It worked, but meh.\nI did have the opportunity to publish that code to a new GitHub branch, but I skipped it. It was just a test, after all.\nMy second prompt was to ask Jules to suggest five new hooks. I got back an answer that seemed reasonable. However, I realized that opening up those capabilities in a security product was just too risky for me to delegate to an AI. I skipped those changes, too.\nIt was at this point that Jules wanted a coffee break. It stopped functioning for about 90 minutes.\nThat gave me time to think. What I really wanted to see was whether Jules could add some real functionality to my code and save me some time.\nNecessary background information\nMy Private Site is a security plugin for WordPress that runs on about 20,000 active sites. It puts a login dialog in front of the site's web pages. There are a bunch of options, but that's the plugin's key feature. I acquired the software a decade ago from a coder who called himself \"jon radio\" and have been maintaining and expanding it ever since.\nAlso: Rust turns 10: How a broken elevator changed software forever\nThe plugin provides access control to the front-end of a website, the pages that visitors see when they come to the site. Site owners control the plugin via a dashboard interface, with various admin functions available in the plugin's admin interface.\nI decided to try Jules out on a feature some users have requested, hiding the admin bar from logged-in users. The admin bar is the black bar WordPress puts on the top of a web page. In the case of the screenshot below, the black admin bar is visible.\nI wanted Jules to add an option on the dashboard to hide the admin bar from logged-in users. The idea is that if a user logged in, the admin bar would be visible on the back end, but logged-in users browsing the front-end of the site wouldn't have to see the ugly bar.\nThis is the original dashboard, before adding the new feature.\nSome years ago, I completely rewrote the admin interface from the way it was when I acquired the plugin. Adding options to the interface is straightforward, but it's still time-consuming. Every option requires not only the UI element to be added, but also preference saving and preference recalling when the dashboard is displayed. That's in addition to any program logic that the preference controls.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nIn practice, I've found that it takes me about 2-3 hours to add a preference UI element, along with the assorted housekeeping involved. It's not hard, but there are a lot of little fiddly bits that all need to be tweaked, and that takes time.\nThat should bring you up to speed enough to understand my next test of Jules. Here's a bit of foreshadowing: The first test failed miserably. The second test succeeded astonishingly.\nInstructing Jules\nAdding a \"hide admin bar\" feature would not have been easy for the run-of-the-mill coding help we've been asking ChatGPT and the other chatbots to perform. As I mentioned, adding the new option to the dashboard requires programming in a variety of locations throughout the code and an understanding of the overall codebase.\nHere's what I told Jules.\n1. On the Site Privacy Tab of the admin interface, add a new checkbox. Label the section \"Admin Bar\" and label the checkbox itself \"Hide Admin Bar\". [Place this in the MAKE SITE PRIVATE block, located just under the Enable login privacy checkbox and before the Site Privacy Mode segment.]\nI instructed Jules where I wanted the AI to put the new option. On my first run through, I made a mistake and left out the details in square brackets. I didn't tell Jules exactly where I wanted it to place the new option. As it turns out, that omission caused a big fail. Once I added the sentence in brackets above, the feature worked.\n2. Be sure to save the selection of that checkbox to the plugin's preferences variable when the Save Privacy Status button is checked.\nThis ensures that Jules knows there is a preference data structure and that it is updated when the user makes a change. It's important to note that if I didn't understand the underlying code, I wouldn't have instructed Jules about this, and the code would not have worked. You can't \"vibe code\" something like this without knowing the underlying code.\n3. Show the appropriate checked or unchecked status when the Site Privacy tab is displayed.\nThis tells the AI that I want the interface to be updated to match what the preference variable specifies.\n4. Based on the preference variable created in (2), add code to hide or show the WordPress admin bar. If Hide Admin Bar is checked, the Admin Bar should not be visible to logged-in WordPress front-end users. If the Hide Admin Bar is not checked, the Admin Bar should be visible to logged-in front-end users. Logged-in back-end users in the admin interface should always be able to see the admin bar.\nThis describes the business logic that the new preference should control. It requires the AI to know how to hide or show the admin bar (a WordPress API call is used), and it requires the AI to know where to put the code in my plugin to enable or disable this feature.\nAnd with that, Jules was trained on what I wanted.\nJules dives into my code\nI fed my prompt set into Jules and got back a plan of action. Pay close attention to that Approve Plan? button.\nI didn't even get a chance to read through the plan before Jules decided to approve the plan on its own. It did this after every plan it presented. An AI that doesn't wait for permission raises the hairs on the back of my neck. Just saying.\nI desperately want to make a Skynet/Landru/Colossus/P1/Hal kind of joke, because I'm freaked out. I mean, it's good. But I'm freaked out.\nHere's some of the code Jules wrote. The shaded green is the new stuff. I'm not thrilled with the color scheme, but I'm sure that will be tweakable over time.\nAlso: The best free AI courses and certificates\nMore relevant is the fact that Jules picked up on my variable naming conventions and the architecture of my code and dived right in. This is the new option, rendered in code.\nBy the time it was done, Jules had included all the code changes it had originally planned, plus some test code. I don't use standardized tests. I would have told Jules not to do it the way it planned, but it never gave me time to approve or modify its original plan. Even so, it worked out.\nI pushed the Publish branch button, which caused GitHub to create a new branch, separate from my main repository. Jules then published its changes to that branch.\nThis is how contributors to big projects can work on those projects without causing chaos to the main code line.\nUp to this point, I could look at the code, but I wasn't able to run it. But by pushing the code to a branch, Jules and GitHub made it possible for me to replicate the changes safely down to my computer to test them out. If I didn't like the changes, I could have just switched back to the main branch, and no harm, no foul. But I did like the changes, so I moved on to the next step.\nAround the code in eight clicks\nOnce I brought the branch down to my development machine, I could test it out. Here's the new dashboard with the Hide Admin Menu feature.\nI tried turning the feature on and off and making sure the settings stuck. They did. I also tried other features in the plugin to make sure nothing else had broken. I was pretty sure nothing would, because I reviewed all the changes before approving the branch. But still: Testing is a good thing to do.\nI then logged into the test website. As you can see, there's no admin bar showing.\nAt this point, the process was out of the AI's hands. It was simply time to deploy the changes, both back to GitHub and to the master WordPress repository. First, I used GitHub Desktop to merge the branch code back into the main branch on my development machine.\nI changed \"Hide Admin Menu\" to \"Hide admin menu\" in my code's main branch, because I like it better. I pushed that (the full main branch on my local machine) back to the GitHub cloud.\nThen, because I just don't like random branches hanging around once they've been incorporated into the distribution version, I deleted the new branch on my computer.\nI also deleted the new branch from the GitHub cloud service.\nFinally, I packaged up the new code. I added a change to the readme to describe the new feature and update the code's version number. Then, I pushed it up to the WordPress plugin repository using SVN (the source code control system used by the WordPress community).\nJourney to the center of the code\nJules is very definitely beta right now. It hung in a few places. Some screens didn't update. It decided to check out for 90 minutes. I had to wait while it went to and came back from its digital happy place. It's exhibiting all the sorts of things you'd expect from a newly-released piece of code. I have no concerns about that. Google will clean it up.\nIn fact, since I first published this article, Google has reached out to me, and they've told me they're definitely working hard to reduce some of the issues I had. They also told me that I was right about the slowdown. They were completely slammed with visitors on launch day.\nThe fact that Jules (and presumably OpenAI Codex and GitHub Copilot Coding Agent) can handle an entire repository of code across a bunch of files is big. That's a much deeper level of understanding and integration than we saw, even six months ago.\nAlso: How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\nThe speed with which it can change an entire codebase is terrifying. The damage it can do is potentially extraordinary. It will gleefully go through and modify everything in your codebase, and if you specify something wrong and then push or merge, you will have an epic mess on your hands.\nThere is a deep inequality between how quickly it can change code and how long it will take a human to review those changes. Working on this scale will require excellent unit tests. Even tools like mine, which don't lend themselves to full unit testing, will require some kind of automated validation to prevent robot-driven errors on a massive scale.\nThose who are afraid these tools will take jobs from programmers should be concerned, but not in the way most people think. It is absolutely, totally, 100% necessary for experienced coders to review and guide these agents. When I left out one critical instruction, the agent gleefully bricked my site.\nSince I was the person who wrote the code initially, I knew what to fix. But it would have been brutally difficult for someone else to figure out what had been left out and how to fix it. That would have required coming up to speed on all the hidden nuances of the entire architecture of the code.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nThe jobs that are likely to be destroyed are those of junior developers. Jules easily does junior developer-level work. With tools like Jules, Codex, or Copilot, that cost a few hundred bucks a month at most, it's going to be hard for management to be willing to pay mid-to-high six figures for midlevel and junior programmers. Even outsourcing and offshoring aren't as cheap as using an AI agent to do maintenance coding.\nAs I wrote earlier in the week, if mid-level jobs are not available, how will we train the experienced people we'll need in the future?\nI am also concerned about how access limits will work. Productivity gains will drop like a rock if you need to do one more prompt and have to wait a day to be allowed to do so.\nAs for me? In less than 10 minutes, I created a new feature that readers had requested. While I was writing another article, I fed the prompt to Jules. I went back to work on the article and checked on Jules when it was finished. I checked out the code, brought it down to my computer, and pushed a release.\nUnfortunately, Jules can't work across different repositories, at least not yet. That means I can't have Jules treat the public GitHub codebase and the three private for-pay add-on codebases as one large project. But Google tells me that this kind of capability is on their list. I could open up my entire repository (of all my projects), and that might work, but it also might confuse Jules about which project is and is not relevant to the work at hand.\nIt took me longer to upload the thing to the WordPress repository than to add the entire new feature. For that class of feature, I got a half-day's work done in less than half an hour, from thinking about making it happen to publishing to my users.\nIn the first week since I completed this feature, about 8,500 sites have updated (about half of the entire installed base). Without Jules, those users probably would have been waiting months for this new feature, because I have a huge backlog of work, and it wasn't my top priority. But with Jules, it took barely any effort.\nAlso: 7 productivity gadgets I can't live without (and why they make such a big difference)\nThese tools are going to require programmers, managers, and investors to rethink the software development workflow. There will be glaring \"you can't get there from here\" gotchas. And there will be epic failures and coding errors. But I have no doubt that this is the next level of AI-based coding. Real, human intelligence is going to be necessary to figure out how to deal with it.\nHave you tried Google's Jules or any of the other new AI coding agents? Would you trust them to make direct changes to your codebase, or do you prefer to keep a tighter manual grip? What kinds of developer tasks do you think these tools should and shouldn't handle? Let us know in the comments below.\nWant more stories about AI? Sign up for Innovation, our weekly newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/gpt-5-is-finally-here-and-you-can-access-it-for-free-today-no-subscription-needed/",
      "text": "OpenAI's GPT-5 is now free for all: How to access and everything else we know\nZDNET's key takeaways\n- OpenAI has launched its long-awaited GPT-5 model.\n- The model is claimed to be OpenAI's fastest, smartest, and most capable yet.\n- GPT-5 is available to everyone: Free, Plus, Pro, and Team/Enterprise/Edu users.\nThere are two kinds of OpenAI models in this world: GPT and reasoning models. The advantages of the former, such as GPT-4o, are that they combine speed and accuracy, while reasoning models such as o3 and o4 take longer to think and use more compute power to produce better answers. OpenAI's latest model, GPT-5, supposedly gives all users access to the best of both models.\nAlso: Gen AI disillusionment looms, according to Gartner's 2025 Hype Cycle report\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nOn Thursday, OpenAI finally unveiled the long-awaited GPT-5, the company's next-generation family of models, which it touts as the fastest, smartest, and most capable yet. GPT-5 is a unified system that combines a smart model for most queries and a deeper reasoning model (GPT-5 thinking) for harder problems.\nIf you are wondering exactly what it does and if you should even consider trying it, keep reading.\nWhat is GPT-5?\nThe key differentiator between GPT-5 and other OpenAI models is the real router feature, which allows GPT-5 to automatically understand which model to use based on the conversation, the complexity of the prompt, and more. The router is continuously trained on real signals to understand the best scenario in which to use a model. Once a user hits usage limits, a mini version of each model takes over.\nAlso: Can GPT-5 fix Apple Intelligence? We're about to find out\nWhile GPT-4o was an extremely capable model, it was not a reasoning model like o3 and o4; those were limited to paying subscribers. As a result, GPT-5 is an especially big win for free users, who typically did not have access to any of the reasoning models and were, by default, excluded from more advanced models.\nThe GPT-5 family of models is made up of GPT-5, GPT-mini, GPT-5-nano, and GPT-5 Pro. The nuances between these models will mostly be topics that developers or enterprises are concerned with when choosing which models to purchase from the API.\nHowever, for most consumers, what you need to know is that GPT-5 will be automatically selected even for free users, and when limits are reached will switch over to GPT-5 mini, a still capable but more lightweight model. GPT-5 Pro is only available to ChatGPT Pro subscribers, which comes at the hefty $200 per month cost. This is the most advanced version of GPT-5, meant for the most challenging and complex tasks -- tasks that the average user likely won't even encounter.\nHow does GPT-5 perform?\nAs with every model release, the GPT-5 drop was accompanied by benchmark evaluations, in which it earned state-of-the-art scores across math (AIME 2025), coding (SWE-Bench Verified), and multimodal understanding (MMMU). It even performed competitively on Humanity's Last Exam, a newer benchmark with multi-modal questions in over 100 subjects, such as math, science, and the humanities, as seen below.\nThe company claims it is the strongest coding model yet, being able to create websites, apps, and games from simple text prompts. In particular, OpenAI shares that it has shown improvements in complex front\u2011end generation and debugging larger repositories.\nAlso: Google's Jules AI coding tool exits beta with serious upgrades - and more free tasks\nBefore the model was released, I watched a live demo of the feature, in which the user created a fully functional web app with interactive elements such as flashcards, a quiz with right and wrong answers, and a game from a simple text prompt. The final product looked sleek. As someone who has recent experience building webpages, it would have taken me hours to stylize using JavaScript and CSS. GPT-5 appears to take vibe coding to the next level.\nEven if you are more of an average GPT-5 user who employs AI for writing, you will still reap these benefits. OpenAI claims GPT-5 is the most capable writing collaborator, being able to better tackle tasks that involve \"structural ambiguity\" such as free verse. Regardless of what you use ChatGPT to write, you should see improvements.\nPeople have been increasingly reliant on ChatGPT for health-related queries because of its ability to conversationally break down medical jargon, which can often be scary and intimidating. Now the experience is optimized with GPT-5, flagging concerns, asking questions, understanding results, prepping you to ask providers questions, and weighing options.\nAlso: ChatGPT can now talk nerdy to you - plus more personalities and other upgrades beyond GPT-5\nRemember that GPT-5 does not replace a medical professional. OpenAI noted that the model performed the highest on HealthBench, a benchmark evaluation the company published earlier this year. External benchmarks for how AI performs in medical scenarios are not yet standardized.\nHow does GPT-5 handle safety?\nOne of the biggest improvements available in GPT-5 is that the model is more accurate than any previous reasoning model and has fewer hallucinations, according to OpenAI. The company said GPT-5's responses are 45% less likely to contain a factual error than GPT-4o with web search enabled on anonymized prompts, and 80% less likely to contain a factual error than OpenAI o3.\nThis is a big win, as reasoning models go beyond traditional pattern prediction and are invited to \"think,\" which leaves room for error.\nOpenAI also added new publicly available benchmarks to test factuality, including LongFact and FActScore, in which GPT-5 with thinking showed a significant drop in hallucinations. GPT-5 (with thinking) also communicates more honestly with the user, sharing when a task is impossible or can't be done. This is important because AI models often offer a plausible-sounding answer instead of admitting they don't know, which can increase the circulation of misinformation. For more on the evaluation results, take a look at the system card.\nAnother brand-new safety feature is called \"safe completions,\" which enables ChatGPT to still answer prompts it would typically refuse. Instead, it will answer, but within the safety boundaries defined by OpenAI, and give a clear explanation of when it can't.\nLastly, while not entirely a safety issue, the model is less sycophantic, or effusively agreeable, and uses fewer unnecessary emojis.\nHow can you access GPT-5?\nAll users\nGPT-5 and GPT-5 mini are available today for all Plus, Pro, Team, and Free users, while Enterprise and Edu users will get access next week, OpenAI said. However, subscribers still receive tiered perks. For example, included in the $20 per month subscription, ChatGPT Plus subscribers have \"significantly higher usage\" limits than free users. Meanwhile, ChatGPT Pro users have unlimited GPT-5 and access to GPT-5 Pro, an even more advanced version of the model included in their $200 per month subscription. OpenAI said Enterprise and Edu users will be given \"generous limits.\"\nAlso: Microsoft rolls out GPT-5 across its Copilot suite - here's where you'll find it\nGPT-5 will be set as the default model for everyday work, replacing all other models for authenticated users. However, paid users will still have the option to select it under the model picker. OpenAI said free users may not be able to access full reasoning until a few days from now, noting that once free users reach usage limits, they'll be moved to GPT-5 mini.\nDevelopers\nThe release of the model is also helpful for developers, as they can benefit from the increased reliability and accuracy. To accommodate this, OpenAI is making GPT-5, GPT-mini, and GPT-5-nano available in the API. Two new parameters, reasoning and verbosity, are also meant to help developers get exactly what they need from their model without overspending. Pro, Plus, and Team users can sign in to ChatGPT to code with GPT-5 in the Codex CLI.\nThe reasoning parameter makes GPT-5 cheaper for tasks that don't require in-depth thinking, and then the verbosity parameter allows developers to fine-tune just how verbose they want GPT-5 to be. The pricing is cheaper than GPT-4o. For more information, you can check the blog post.\nWhat if I am not seeing GPT-5 yet?\nGPT-5 began rolling out to all Plus, Pro, Team, and Free users upon launch on Thursday. However, the rollouts are gradual, so if you checked immediately and didn't see it, it is worth checking again to see if you now have it. It had not shown up for me on my free, Plus, or Pro account until last night.\nDo make sure you are signed in. Even though GPT-5 is available to free users, if you don't sign in, you won't have access to the latest features, including GPT-5.\nYou can keep up with my latest stories and tech adventures on social media. Follow me on Twitter/X at @sabrinaa_ortiz and on Instagram at @sabrinaa.ortiz."
    },
    {
      "url": "https://www.zdnet.com/article/you-can-use-openais-super-powerful-ai-coding-agent-codex-for-just-20-now/",
      "text": "You can use OpenAI's super powerful AI coding agent Codex for just $20 now\nOpenAI sent another earthquake rocking across the software engineering world today.\nThis time, it was the X announcement that Codex, its AI coding agent, is now available for Plus tier users. In other words, you no longer have to spend $200/mo to get Codex's programming help. Instead, you can get it for $20/mo, as part of the ChatGPT Plus subscription.\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nThe company says Codex \"includes generous usage limits for a limited time, but during periods of high demand, we might set rate limits for Plus users so that Codex remains widely available.\" In other words, performance today might be rough.\nAlso: OpenAI upgrades ChatGPT with Codex - and I'm seriously impressed (so far)\nThat didn't stop me from digging in. There's a new menu item on the right of the ChatGPT screen:\nInterestingly, the Mac app version of ChatGPT is far more limited. If you launch ChatGPT through the app, not only will you not get Codex, but Search Chats, Library, and Sora are also missing. That's kind of an unforced error, especially since the Mac app updated yesterday.\nAlso: The best AI for coding in 2025 (and what not to use)\nIn any case, let's see what happens when you click the Codex button in Plus. First, you'll get this exciting start screen.\nNext is an animation that describes some features of the tool. In this first frame, the tool explains that it will draft multiple PRs in parallel. PR stands for Push Request, and that's GitHub-speak for pulling changes into the main repository, essentially integrating it into the main codebase.\nBy saying Codex drafts multiple PRs, it means it can work on multiple coding projects, but none get committed until you've done a review. This is roughly the equivalent of giving a bunch of projects to some subordinate programmers, letting them work on it, but not including their code until you examine it carefully.\nYou will examine Codex's code carefully, won't you? Hint. You better.\nNext is a sample prompt. It's basically showing how you can prompt Codex to dig through the entire repo of a codebase to make changes and tweaks.\nNext, Codex apparently can run Lint and tests. Lint, which was originally a C language tool that detects errors and bugs in code, works with many different languages.\nWhat Codex is saying here is that it's capable of running coding validation check, creating tests, and running those tests to confirm the code changes work as expected. This is a good thing.\nFinally, one more panel describes the use of a new coding model specifically for software engineering. The company doesn't explicitly specify that model, but ChatGPT speculated that the model is a tuned version of GPT-4o, specifically trained on software engineering applications.\nAnd then, I was finally in. The next form provided connection to a GitHub repository. This is very reminiscent of the process I went through with the Google Jules coding model last week. I'm telling you, these AI features are in lockstep among these competitors.\nI was about to move forward and start playing around with Codex, when I noticed a message at the bottom of the screen.\nThis references a feature mentioned in the X posting by OpenAI this morning:\nYou can now give Codex access to the internet during task execution to install base dependencies, run tests that need external resources, upgrade or install packages needed to build new features, and more.\nStay tuned for hands-on testing\nAnd that's when I decided it's time to stop. It's not even 7 a.m. here, I haven't even finished my first cup of coffee, and I want to get this news out to you as soon as possible. But I'm not comfortable, foggy morning head and all, letting Codex loose on my main repo, or thinking through the implications of giving it internet access.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nThat said, I will share with you one interesting discovery I made while the caffeine was starting to boost my brain. Apparently, Codex is stateless. This means that it doesn't remember anything from session to session. So you need to build clear and complete prompts, and make sure those prompts are repeated in new sessions if they set up ground rules or otherwise are designed to calibrate Codex on its work.\nAnd with that, I'll wrap up this quick hit intro article. I'll be back soon with some hands-on tests to see what this beastie can do. Stay tuned.\nAre you planning on trying out new Codex coding agent in ChatGPT Plus? What are your thoughts on letting connect to the internet to install dependencies? Do you feel comfortable giving it access to your codebase, or do you have concerns about automation at this level? What features are you most excited or nervous about? Let us know in the comments below.\nWant more stories about AI? Sign up for Innovation, our weekly newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/coding-with-ai-my-top-5-tips-for-vetting-its-output-and-staying-out-of-trouble/",
      "text": "Coding with AI? My top 5 tips for vetting its output - and staying out of trouble\nOur story begins, as many stories do, with a man and his AI. The man, like many men, is a bit of a geek and a bit of a programmer. He also needs a haircut.\nThe AI is the culmination of thousands of years of human advancement, all put to the service of making the man's life a little easier. The man, of course, is me. I'm that guy.\nAlso: The best AI for coding in 2025 (and what not to use)\nUnfortunately, while AI can be incredibly brilliant, it also has a propensity to lie, mislead, and make shockingly stupid mistakes. It is the stupid part that we will be discussing in this article.\nAnecdotal evidence does have value. My reports on how I've solved some problems quickly with AI are real. The programs I used AI to write with are still in use. I have used AI to help speed up aspects of my programming flow, especially when I focus on the sweet spots where I'm less productive and the AI is quite knowledgeable, like writing functions that call publicly published APIs.\nAlso: I'm an AI tools expert, and these are the only two I pay for (plus three I'm considering)\nYou know how we got here. Generative AI burst onto the scene at the cusp of 2023 and has been blasting its way into knowledge work ever since.\nOne area, as the narrative goes, where AI truly shines is its ability to write code and help manage IT systems. Those claims are not untrue. I have shown, several times, how AI has solved coding and systems engineering problems I have personally experienced.\nAI coding in the real world: What science reveals\nNew tools always come with big promises. But do they deliver in real-world settings?\nMost of my reporting on programming effectiveness has been based on personal anecdotal evidence: my own programming experiences using AI. But I'm one guy. I have limited time to devote to programming and, like every programmer, I have certain areas where I spend most of my coding time.\nAlso: I tested 10 AI content detectors - and these 5 correctly identified AI text every time\nRecently, though, a nonprofit research organization called METR (Model Evaluation & Threat Research) did a more thorough analysis of AI coding productivity.\nTheir methodology seems sound. They worked with 16 experienced open-source developers who have actively contributed to large, popular repositories. The METR analysts provided those developers with 246 issues from the repositories that needed fixing. The coders were given about half the issues where they had to work on their own, and about half where they could use an AI for help.\nThe results were striking and unexpected. While the developers themselves estimated that AI assistance increased their productivity by an average of 24%, METR's analytics showed instead that AI assistance slowed them down by an average of 19%.\nThat's a bit of a head-scratcher. METR put together a list of factors that might explain the slowdown, including over-optimism about AI usefulness, high-developer familiarity with their repositories (and less AI knowledge), the complexity of large repositories, lack of AI reliability, and an ongoing problem where the AI refuses to use \"important tacit knowledge or context.\"\nAlso: How AI coding agents could destroy open-source software\nI would suggest that two other factors might have limited effectiveness:\nChoice of problem: The developers were told which issues they had to use AI help on and which issues they couldn't. My experience suggests knowledgeable developers must choose where to use AI based on the problem that needs to be solved. In my case, for example, getting the AI to write a regular expression (something I don't like doing and I'm fairly crappy at) would save me a lot more time than getting the AI to modify unique code I've already written, work on regularly, and know inside and out.\nChoice of AI: According to the report, the developers used Cursor, an AI-centric fork of VS Code, which used Claude 3.5/3.7 Sonnet at the time. When I tested 3.5 Sonnet, the results were terrible, with Sonnet failing three out of four of my tests. Subsequently, my tests of Claude 4 Sonnet were considerably better. METR reported that developers rejected more than 65% of the code the AI generated. That's going to take time.\nThat time when ChatGPT suggested nuking my system\nMETRs results are interesting. AI is clearly a double-edged sword when it comes to coding help. But there's also no doubt that AI can provide considerable value to coders. If anything, I think this test once again proves the contention that AI is a great tool for experienced programmers, but a potential high-risk resource for newbies.\nAlso: Why I'm switching to VS Code. Hint: It's all about AI tool integration\nLet's look at a concrete example, one that could have cost me a lot of time and trouble if I followed ChatGPT's advice.\nI was setting up a Docker container on my home lab using Portainer (a tool that helps manage Docker containers). For some reason, Portainer would not enable the Deploy button to create the container.\nIt had been a long day, so I didn't see the obvious problem. Instead, I asked ChatGPT. I fed ChatGPT screenshots of the configuration, as well as my Docker configuration file.\nChatGPT recommended that I uninstall and reinstall Portainer. It also suggested I remove Docker from the Linux distro and use the package manager to reinstall it. These actions would have had the effect of killing all my containers.\nOf note, ChatGPT didn't recommend or ask if I had backups of the containers. It just gave me the command line sequences it recommended I cut and paste to delete and rebuild Portainer and Docker. It was a wildly destructive and irresponsible recommendation.\nThe irony is that ChatGPT never figured out why Portainer wouldn't let me deploy the new container, but I did. It turns out I never filled out the container's name field. That's it.\nAlso: What is AI vibe coding? It's all the rage but it's not for everyone - here's why\nBecause I'm fairly experienced, I hesitated when ChatGPT told me to nuke my installation. However, someone relying on the AI for advice could have potentially brought down an entire server for want of typing in a container name.\nOverconfident and underinformed AIs: A dangerous combo\nI've also experienced the AI going completely off the rails. I've experienced it giving advice that was not only completely useless, but also presented with the apparent confidence of an expert.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nIf you're going to use AI tools to support your development or IT work, these tips might keep you out of trouble:\n- If there's not much publicly available information, the AI can't help. But the AI will make stuff up based on what little it knows, without admitting that it is lacking experience.\n- Like my dog, once the AI gets fixated on one thing, it often refuses to look at alternatives. If the AI is stuck on one approach, don't make the mistake of believing that its polite recommendations about a new approach are real. It's still going down the same rabbit hole. Start a new session.\n- If you don't know a lot, don't rely on the AI. Keep up your learning. Experienced devs can tell the difference between what will work and what won't. But if you're trying to put all the coding on the back of the AI, you won't know when or where it goes wrong or how to fix it.\n- Coders often use specific tools for specific tasks. A website might be built using Python, CSS, HTML, JavaScript, Flask, and Jinja. You choose each tool because you know what it does well. Choose your AI tools the same way. For example, I don't use AI for business logic, but I gain productivity using AI to write API calls and public knowledge, where it can save me a lot of time.\n- Test everything an AI produces. Everything. Line by individual line. The AI can save a ton of time, but it can also make enormous mistakes. Yes, taking the time and energy to test by hand can help prevent errors. If the AI offers to write unit tests, let it. But test the tests.\nBased on your experience level, here's how I recommend you think about AI assistance:\n- If you know nothing about a subject or skill: AI can help you pass as if you do, but it could be amazingly wrong, and you might not know.\n- If you're an expert in a subject or skill: AI can help, but it will piss you off. Your expertise gets used not only to separate the AI-stupid from the AI-useful, but to carefully craft a path where AI can actually help.\n- If you're in between: AI is a mixed bag. It could help you or get you in trouble. Don't delegate your skill-building to the AI because it could leave you behind.\nAlso: How I used ChatGPT to analyze, debug, and rewrite a broken plugin from scratch - in an hour\nGenerative AI can be an excellent helper for experienced developers and IT pros, especially when used for targeted, well-understood tasks. But its confidence can be deceptive and dangerous.\nAI can be useful, but always double-check its work.\nHave you used AI tools like ChatGPT or Claude to help with your development or IT work? Did they speed things up, or nearly blow things up? Are you more confident or more cautious when using AI on critical systems? Have you found specific use cases where AI really shines, or where it fails hilariously? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    }
  ],
  "argos_summary": "The article discusses the author's experience using ChatGPT Codex, an AI tool for coding, highlighting both its capabilities and limitations. While Codex can assist in code writing and modification, the author found the process to be slow and cumbersome, often requiring manual oversight to avoid errors. The author also contrasts Codex with other AI coding tools, noting that while it can save time, it may disrupt the coding flow and lead to mistakes if not carefully monitored.",
  "argos_id": "Y51VA91EV"
}