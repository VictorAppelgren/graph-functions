{
  "url": "https://www.howtogeek.com/how-i-moved-my-docker-server-to-a-new-system/",
  "authorsByline": "Patrick Campanale",
  "articleId": "84e3cd38faeb41b78501d77c4e60c3d5",
  "source": {
    "domain": "howtogeek.com",
    "location": {
      "country": "us",
      "state": "VA",
      "county": "Fairfax County",
      "city": "McNair",
      "coordinates": {
        "lat": 38.9695316,
        "lon": -77.3859479
      }
    }
  },
  "imageUrl": "https://static1.howtogeekimages.com/wordpress/wp-content/uploads/2024/08/shutterstock_2428013233.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-24T14:15:15+00:00",
  "addDate": "2025-08-24T14:20:08.574271+00:00",
  "refreshDate": "2025-08-24T14:20:08.574273+00:00",
  "score": 1.0,
  "title": "How I Moved My Docker Server to a New System",
  "description": "It wasn't actually as hard as I thought it'd be.",
  "content": "Have you ever considered moving your Docker containers from one server to another? I recently got the itch to upgrade my homelab, retiring an old rack-mount server in favor of a newer, more powerful system\u2014but moving my Docker containers was a beast I wasn\u2019t sure how to tackle.\n\nIt actually wasn\u2019t that difficult to do, and was more time consuming than scary or frustrating. The move only requires a few commands and a lot of patience (depending on how many Docker containers you\u2019re running). So, here\u2019s how I migrated my Docker server to an entirely new system (without moving the virtual machine itself).\n\nMoving Docker Hosts Doesn\u2019t Have to Be Overwhelming\n\nThis past January was the first time I migrated Docker containers. I moved from Unraid to a proper Docker install in an Ubuntu virtual machine. This was honestly a pretty straightforward move, and I used the built-in backup methods for some apps, and started others from scratch.\n\nHowever, I recently decided to move my virtual Docker host from one server to another. While I thought about moving the virtual machine itself, I decided to go a different route and migrate the Docker install to a brand-new virtual machine.\n\nThis was quite overwhelming to think about. I\u2019d have to have my Docker containers down for quite some time to do the migration, and there\u2019s a lot that could go wrong during the transfer.\n\nHowever, once I dug into the process it really wasn\u2019t as bad as I thought it would be. Sure, it\u2019d take a few hours (and I definitely ran into a few snags), but overall it was pretty painless and not that overwhelming.\n\nMake Sure the New System Is Properly Prepared\n\nI started by making sure my new virtual machine was ready to go. I first made sure I was using the same version of Ubuntu: Server 24.04 LTS. Ubuntu Server has been my go-to for over a decade now whenever I need a Linux server. I\u2019m just comfortable in it.\n\nOnce Ubuntu Server 24.04 was installed on the new virtual machine host, I made sure to get Docker and any other dependencies I\u2019d need configured and ready to go. This includes any extra drivers for NVIDIA or Intel, and making sure all the folder paths were set up that I\u2019d need with Docker.\n\nAfter I had all that configured, I stopped both the Docker service and socket with the following commands:\n\nYou might not need to run both commands, but I did. This essentially shut off Docker so that way I could make changes to the core Docker filesystem and not mess anything up.\n\nOnce this was done, I ran the exact same commands on the old system. I had to make sure Docker was turned off on the old system before gathering files, because that would ensure that nothing would change from when I started the migration to when I finished it.\n\nThis also meant that all of my Docker containers would be off. In all honesty, this was the worst part for me. My homelab runs my entire house using Docker, from Home Assistant to Audiobookshelf, Plex, and many other services. Turning Docker off meant shutting these systems down, so I had to wait for the right time to do it.\n\nEventually, I found a time when the server was idle and I was able to proceed by shutting Docker down and then going onto the next step: getting all of the files ready to transfer.\n\nThis was the longest part of the process for me. I ended up deleting quite a few files and folders from the folder where I bind mount my Docker volumes to try and make this as fast of a process as possible, but it still took a few hours.\n\nEssentially, you want to tar up your Docker folder and then, if you use a different area for your volume mounts, that folder too. If you just use Docker volumes then they\u2019ll be caught by the first command.\n\nTo start with, I ran the following command on my old server:\n\nWhat this does is backs up the /var/lib/docker folder into a tar.gz file. Depending on how many containers you\u2019re running, and whether you are using Docker Volumes or not, this could take a while. For me, it took about 30 to 45 minutes.\n\nNext, I ran the following command on my old server:\n\nSince I run Portainer as my Docker manager, I just put all of my Docker files under /portainer on the server. So, I have /portainer/audiobookshelf, /portainer/plex, etc. The above command takes the /portainer folder and tars it up into a nice little file, making it easy to move. This took about an hour to back up because of how much data I have in my Docker Volume folders.\n\nOnce both folders finished tarring up, I was ready to move onto the next step: moving the compressed archives to the new server.\n\nMoving Everything to the New System Is Straightforward\n\nI have a 2.5Gb/s network at home, and I like to take full advantage of it whenever I can. So, I dug up a command to move the file from one server to the other while showing me transfer speeds in the terminal:\n\nUsing this command, I could monitor the transfer and make sure everything went according to plan. Thankfully, it did. Within a few minutes, the files were moved from the old server to the new server and I was ready to keep on going.\n\nOf course, if you would prefer, you could use USB to move files from one server to the other. My old server (a Dell R720 rack-mount system) only has USB 2.0, so it was much faster for me to move the files over the network.\n\nOnce the files are on the new server, simply untar them into place. I started with the Docker folder:\n\nThis put all of my old Docker files and my volume mounts back where they should go and kept permissions properly set, which is the bulk of the move.\n\nReally, tarring up Docker, moving the tarfiles, and then untarring the files is what took the bulk of the time for the move. In all, those three actions took around three to four hours to complete on my server.\n\nHowever, once I had that done, I was one step from the move being complete. Now, it was time for me to finally start Docker up on the new server.\n\nStart Docker on the New System and Your Containers Should Spin Up\n\nWith all the files in place, I turned Docker on for the first time with all the containers in place. This was simple, and required just two commands:\n\nI expected a ton of things to break when turning Docker on after the move. Instead, most things just spun right up and were ready to go. Really though, that\u2019s all there is to starting up Docker on your new machine.\n\nIf the configuration isn\u2019t much different between your old system and new, then everything should just work. If you\u2019re like me, however, and changed a lot between the two systems, then there\u2019s a few things to watch out for.\n\nThings to Look Out for in the Move\n\nOne of the first things I had to fix was changing a few containers that were using the NVIDIA runtime to using an Intel runtime. On my old server, I had a GTX 1650 graphics card to help with AI processing and transcoding. The new server had an i9-13900K with Intel\u2019s integrated graphics that\u2019s capable of handling the same workloads.\n\nThis took some finagling, and I\u2019m still not fully switched over yet (I gave up with Scrypted and am letting it just run on the processor instead of the iGPU), but overall it was a pretty simple switch.\n\nI also ran into a pretty big snag with Home Assistant. Home Assistant\u2019s Docker container, for some reason, didn\u2019t move any of its settings over. There\u2019s some config info that I have it store in /portainer/homeassistant, but it started up as if I had never run Home Assistant before.\n\nThis was an easy remedy, though. I simply turned the old virtual machine back on, entered Home Assistant on it, downloaded the backup file, and restored that on the new virtual machine. That immediately got me up and running.\n\nSome of Docker\u2019s networking might also need a once-over. Scrypted, for instance, I had to change back to host networking from a new scrypted-default network that was created for some reason.\n\nI had to get the static IP set on the new virtual machine as well. Then, I had to update a few other little odds and ends around the server to get it all working smoothly with the integrated graphics that I was passing through.\n\nOverall, it was a pretty smooth move and I\u2019m very happy with the new server. It\u2019s faster, more responsive, and much more capable than the old server\u2014all while using less electricity.\n\nThe move was relatively simple, and I\u2019m glad I got to learn how to move Docker containers from one system to another. I\u2019m sure that\u2019ll come in handy sometime down the road!\n\nIf you\u2019ve never run Docker containers before, or are just starting out, Docker is a wonderful tool. It singlehandedly runs just about my entire homelab.\n\nHere\u2019s a handful of my favorite Docker containers that I think every homelabber should run. Most are simple to set up and can add a lot of value to your setup. In fact, my homelab has become one of the most useful things in my house.",
  "medium": "Article",
  "links": [
    "https://www.shutterstock.com/image-vector/realistic-3d-orange-container-cargo-shipping-2507765753",
    "https://www.howtogeek.com/how-my-homelab-became-the-most-useful-thing-in-my-home/",
    "https://www.howtogeek.com/how-i-run-my-entire-homelab-on-docker-and-why-you-should-too/",
    "https://www.howtogeek.com/733522/docker-for-beginners-everything-you-need-to-know/",
    "https://www.howtogeek.com/what-are-virtual-machines-and-how-do-they-work/",
    "https://www.howtogeek.com/248780/how-to-compress-and-extract-files-using-the-tar-command-on-linux/",
    "https://www.howtogeek.com/why-25gbs-networking-is-the-sweet-spot-for-your-homelab/",
    "https://www.howtogeek.com/i-use-docker-for-almost-everything-and-im-not-even-a-developer/",
    "https://www.howtogeek.com/why-i-chose-unraid-over-truenas-scale-in-2025/",
    "https://www.howtogeek.com/10-docker-containers-every-homelabber-should-run/"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Docker containers",
      "weight": 0.10589994
    },
    {
      "name": "Docker volumes",
      "weight": 0.09960656
    },
    {
      "name": "Docker Hosts",
      "weight": 0.09811296
    },
    {
      "name": "Docker",
      "weight": 0.09720409
    },
    {
      "name": "Docker Volumes",
      "weight": 0.09394028
    },
    {
      "name": "files",
      "weight": 0.06916724
    },
    {
      "name": "my old Docker files",
      "weight": 0.068309754
    },
    {
      "name": "Home Assistant\u2019s Docker container",
      "weight": 0.0639297
    },
    {
      "name": "My Docker Server",
      "weight": 0.062755354
    },
    {
      "name": "my Docker server",
      "weight": 0.062755354
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Reference/General Reference/How-To, DIY & Expert Content",
      "score": 0.82568359375
    },
    {
      "name": "/Reference/Technical Reference",
      "score": 0.568359375
    },
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.505859375
    },
    {
      "name": "/Computers & Electronics/Programming/Development Tools",
      "score": 0.40380859375
    },
    {
      "name": "/Computers & Electronics/Programming/Scripting Languages",
      "score": 0.30029296875
    }
  ],
  "sentiment": {
    "positive": 0.13764653,
    "negative": 0.44748545,
    "neutral": 0.41486803
  },
  "summary": "The author recently moved his Docker server from one server to another, without moving the virtual machine itself. This process was a daunting task, but was surprisingly time-consuming and less intimidating than initially thought it would be. The author demonstrates how to migrate Docker containers from an old rack-mount server to a new, more powerful system. The process requires a few commands and patience, depending on how many Docker containers are running. The first time I migrated Docker containers was in January, when I moved from Unraid to a proper Docker install in an Ubuntu virtual machine. However, I decided to migrate the Docker install to a brand-new virtual machine and instead of having my Docker containers down for several hours, it was a painless process that took 45 to 45 minutes.",
  "shortSummary": "Moving a Docker server to a new Ubuntu virtual machine, using simple commands and patience, was surprisingly easy and time consuming but challenging.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "716d4f45564d4cb2a2072d1d8906b309",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.howtogeek.com/why-25gbs-networking-is-the-sweet-spot-for-your-homelab/",
      "text": "Quick Links\nAre you tired of data transfers taking forever on your local network? I was, so I set out to figure out what networking upgrade made the most sense for both me and most people. I started by thinking about 10 gigabit, but quickly realized that 2.5 gigabit networking is the true sweet spot in 2025.\nGigabit Networking Is Becoming Slow for Local Networks\nBack in the mid-1990s, 100Mb/s networking, also known as Fast Ethernet, was the fastest networking available. However, it only stayed around for about three years before Gigabit Ethernet (1000Mb/s) took over as the standard.\nSince 1999, Gigabit Ethernet has been the industry standard for home networking. Many internet service providers offer internet speeds of gigabit, too. It has become ubiquitous in our modern age.\nGigabit Ethernet is now 26 years old, as hard as that might be to believe.\nIf you think about how much data we had to move back in 1999 when Gigabit Ethernet was released compared to now, it\u2019s honestly mind-blowing that we\u2019re still using it as a standard. The average hard drive size in 1999 was somewhere between 4GB to 20GB. By 2005, the average hard drive size was between 160GB to 500GB.\nNow? My desktop has 8TB of NVMe storage (which has transfer speeds many magnitudes greater than Gigabit Ethernet is capable of). My home server sitting next to me has 60TB of storage.\nWi-Fi 7 is capable of multi-gigabit transfer speeds. In fact, I have a Wi-Fi 7 router that\u2019s plugged into my server over 2.5 gigabit Ethernet, and I was able to achieve faster than two gigabit transfer speeds to the server\u2014all without a single wire.\nOur storage (and storage moving) needs have grown exponentially, and wireless networking is advancing so fast, so why hasn\u2019t our wired networking grown too? Well, it\u2019s starting to, but it\u2019s a slow transition\u2014almost as slow as Gigabit Ethernet in 2025.\n10 Gigabit Networking Is Quite Expensive to Dive Into (and Overkill)\nGiven my home networking setup, earlier this year, I was ready to make the move away from Gigabit Ethernet to something faster for my internal computers. For my home\u2019s internet plan, I am still locked at gigabit, so there\u2019s no need to worry about a faster outside connection just yet.\nMy search began when I picked up some servers that had 10-gigabit networking cards already installed. I thought to myself, \u201cWe jumped from 100Mb/s to 1000Mb/s so that the next natural jump would be to 10000Mb/s, another magnitude of 10.\u201d\nHowever, that\u2019s just not feasible in standard home environments. Yes, there are 10-gigabit networking switches out there, but they\u2019re just extremely expensive. Designed more for enterprise setups, most 10-gigabit switches are also fairly complicated to set up and configure for the home user.\nHonestly, though, 10 gigabit networking is a bit overkill. While NVMe drives have reached insane speeds of over 14GB/s, a 10-gigabit network connection tops out at around 1.25GB/s. This might seem like even 10 gigabit wouldn\u2019t be enough, but it\u2019s rare to transfer from one NVMe drive to another over a network.\nUnifi Pro XG 8 PoE\nThe Unifi Pro XG 8 PoE switch provides 155W of PoE++ power across its eight 10Gb/s Ethernet ports. There are also two 10Gb/s SFP+ ports for uplinking the switch to other networking gear.\nMore often than not, you\u2019ll be transferring files from your computer to a network attached storage server of some sort. Sometimes there\u2019s a SSD cache there for faster speeds, other times there\u2019s not.\nEither way, 10 gigabit is far too expensive for normal installations for little-to-no massive gain. Let\u2019s say you\u2019re moving 50GB of files from your desktop to a network storage server with an NVMe drive. On a Gigabit connection, with perfect transfer rates and no throttling, the file would move in six minutes and 40 seconds. At 10 gigabits, that file would move in just 40 seconds.\nHowever, there\u2019s a stepping stone that\u2019s faster than gigabit, but not nearly as expensive as 10 gigabit: 2.5 gigabit, which would move that same 50GB file in just under three minutes.\n2.5 Gigabit Networking Is Affordable, Easy to Do, and Provides 2.5x the Transfer Speeds\nAt first, I was a little leery of the benefits of 2.5 gigabit networking. It\u2019s only 2.5x faster than gigabit, not 10x, so how good could it really be? I was actually quite surprised.\nAfter finding out that 10 gigabit networking was simply out of reach for my setup, I turned to 2.5 gigabit and found it to be much more affordable.\nI settled on the Unifi Switch Flex Mini 2.5G, which offers five total 2.5 gigabit Ethernet ports (one of which can power the unit with Power over Ethernet). The 2.5-gigabit switch was actually affordable compared to the 10-gigabit switches I was looking at from Unifi, and it had just the right number of ports for me.\nUnifi Flex Mini 2.5G Switch\nThe Unifi Flex Mini 2.5G Ethernet Switch is a fully-managed network switch delivering multi-gig speeds. It works both standalone or with a Unifi Network Controller, making it a versatile option for your network setup. You get an included USB-C power adapter, though the switch can be powered over PoE+ from the upstream switch.\nEthernet cards are also relatively affordable at 2.5 gigabit speeds, too. My desktop already had 2.5 gigabit Ethernet on it, as many motherboards come with that as a standard now, so I only had to outfit my servers themselves with 2.5 gigabit cards, which cost me about $20 each, buying them used on Amazon.\nTP-Link 2.5G PCIe Network Card (TX201)\nThe TP-Link TX201 is a 2.5Gb/s network card that installs in a PCIe slot on your computer. Designed to deliver multi-gig networking, this add-in NIC is perfect for upgrading your desktop, server, or NAS for faster networking. The slim profile suits even 1U servers, and you'll get both a normal and low-profile mounting bracket in the box. It supports networking speeds of 2500/1000/100, meaning it's backward compatible with standard Gigabit and Fast Ethernet connections too.\nAfter moving my servers and desktop to 2.5 gigabit networking, I was actually very impressed with the speed improvement. As I mentioned, a 50GB file moves in just under three minutes, compared to nearly seven minutes at gigabit speeds.\nRecently, I migrated server hardware, and that caused me to have to move around 70GB of data from one machine to another. Since I have both systems linked over 2.5 gigabit, I was able to complete this data transfer in about five minutes or so. That\u2019s shy of the \u201cideal\u201d transfer speed of 2.5 gigabits, which would move that data in just under four minutes.\nHowever, given the many variables there and the fact that it was virtual machine to virtual machine, I was very happy with five minutes. At standard gigabit, that data transfer would have taken around 10 minutes in a perfect scenario, meaning that realistically, it would have probably taken 12 to 15 minutes.\nAll in all, 2.5 gigabit networking is the upgrade that I think most people should make the jump to. 10 gigabit is nice and all, but it\u2019s just unobtainable for the masses. Whether you have a homelab setup or are just wanting to make backups faster on your home network, it\u2019s time to leave Gigabit Ethernet in the past and fully embrace multi-gigabit networking.\nIf you really want to get the most out of your network-attached storage system and 2.5 gigabit networking, then you\u2019ll want to make sure your NAS has a cache set up.\nI actually went a non-traditional way when I built my own NAS. Instead of buying a pre-configured system, I turned to eBay and Unraid to build my own storage server. My powerhouse of a system can house 12 3.5-inch drives alongside as much SSD storage as I can throw at it, fully taking advantage of 2.5 gigabit networking right now, and eventually 10 gigabit when I get to that point."
    },
    {
      "url": "https://www.howtogeek.com/what-are-virtual-machines-and-how-do-they-work/",
      "text": "Summary\n- Virtual machines emulate physical computers; they're easy to create and configure, and you can experiment without harming the host computer.\n- Your host computer needs sufficient RAM, CPU power, and hard drive space to service your VMs.\n- VMs are perfect for test-driving operating systems, and trying out configuration changes in a safe space before rolling them out on your physical computer.\nYou\u2019ve probably heard phrases like \u201cSpin up a virtual machine\u201d or \u201cTry it out in a virtual machine.\u201d But what are virtual machines, and how can you make use of them?\nWhat Is a Virtual Machine?\nA virtual machine (VM) is a software-based emulation of a computer. A program called a hypervisor runs on your computer. It allows you to create virtual machines\u2013virtual computers\u2014and configure them with an amount of RAM, hard drive space, CPU cores, and other details that would normally be provisioned in hardware. Once you\u2019ve configured your VM, you can turn it on and install an operating system.\nThe hypervisor must emulate the hardware of a physical computer, so that the operating system believes it is running on a normal, physical, computer. It must also pass requests that need to be serviced by the hardware of your actual computer (known as the host computer) through to the host. For example, if an application in your VM tries to access a web site, it interacts with the virtual network card in the VM, but the actual network requests are handled by the network card of the host.\nBecause VMs are easy to create, are disposable, and just like using a real computer, they are commonly used for testing, development, and training. You can shut down and restart VMs very quickly, and save snapshots of their current state.\nAnd, because VMs can\u2019t harm the host computer, you\u2019re safe to experiment. They\u2019re often suggested as a great way to investigate a Linux distribution you\u2019re curious about, and this is a great use case for a VM.\nYou can use VMs on macOS, Windows, and Unix-like operating systems such as Linux, BSD, and OpenIndiana. On all platforms, you\u2019ll find free versions of once-commercial hypervisors, such as VMware Workstation, and open-source offerings such as Oracle VirtualBox.\nI\u2019ve used VMs on Windows and Linux, with the majority of them being on Linux. But actually, using a VM on any platform is the same. Install your hypervisor, configure a virtual machine, and install an operating system in it. Apart from a few Linux-specific observations, everything here applies to all platforms.\nHow to Install Linux in VirtualBox\nWant to try out Linux but don't want to commit to a full install? Use VirtualBox.\nYour Computer Has to Be Up to the Task\nVMs might be virtual, but they require real hardware to operate. Sadly, we can\u2019t conjure hardware out of thin air. So if your computer has 16GB of RAM you can\u2019t allocate 32GB of RAM to a VM. Nor could you allocate all the RAM you have to a VM and leave nothing for the host to operate on.\nYou also need sufficient free hard drive space for the hard drives of your VMs to be stored on. Modern hypervisors let you specify a maximum size of hard drive for a VM. That way the VM only stores the used portion of the VM hard drive.\nIf you want to store snapshots of your VMs, you\u2019ll need hard drive space for this too. A snapshot captures the entire state of the virtual machine, allowing you to close it down with applications open and running, and to have it restored to the exact same state when you reactivate it.\n6 Best Virtual Machine Programs for Windows, macOS, and Linux\nWant to run Windows on a Mac, or boot into Linux without having to restart Windows?\nSnapshots provide a safety net for your experiments. If you\u2019re about to do something that could adversely affect your VM, take a snapshot first. If your experiment goes badly wrong, restore your saved snapshot and your VM will be in the state it was before your ill-fated changes.\nYour CPU must support virtualization, and sometimes you have to turn on virtualization in your BIOS. Most modern CPUs support virtualization, including the Intel VT-x and AMD AMD-V families of CPU.\nYou also need enough CPU power to allow your VMs and your host to run with sufficient speed. If you ever need to set up small networks of VMs and have them running on the same host at the same time, you\u2019ll need a fast CPU and plenty of RAM.\nVirtual Machines on Linux\nSince 2007, the Linux kernel has supported Kernel-based Virtual Machines (KVM). This means the kernel can behave as a hypervisor. The creation and control of KVM VMs is done with a Virtual Machine Monitor, such as QEMU.\nQEMU allows you to emulate specific hardware, including CPUs of a different architecture than the physical one in your host computer. QEMU is a command line program. Beginners might feel more at home with a graphical front end such as GNOME Boxes. GNOME Boxes wraps the complications of QEMU in an accessible and intuitive graphical display.\nFor me, VirtualBox hits the sweet spot of having enough functionality and configurability without being too fussy and long-winded to set up.\nHow to Use QEMU to Boot Another Operating System\nThat old legacy system is gone, but you can get it back.\nNetworking and Accessing Devices\nSometimes you may want your VM to interact with another device on your network, such as a printer. There are several ways to do this with networking, and your hypervisor will probably have a menu of options to let you choose a networking paradigm for your VM.\nThe default is often Network Address Translation (NAT), which allows your VM to access the internet, download files, and so on. It cannot see any of the networks your host computer is attached to. If isolating your VMs is most important to you, use this mode.\nThe simplest way to access one of the networks your host computer is attached to is to use bridged networking. If having access to other network devices on your physical network is important to you, configure your VM with bridged networking. This puts your VM on the same network as your host.\nIt can be useful to let your VM have access to USB devices that are connected to your host. This usually requires you to turn on USB functionality in your VM. You may get a choice of using USB 1.0, 2.0, or 3.0.\nWhen you plug a USB device into your host, your VM will have the opportunity to claim it. When you do, your host computer will think the device has been unplugged. To your VM, it appears as though the physical USB device has been plugged in to one of its virtual USB ports.\nThe Virtues of the Virtual\nA virtual machine is a fantastic way to evaluate other operating systems, as long as you understand a couple of points.\nAn operating system in a VM always runs slightly slower than it does on bare metal. You\u2019ll get a good feel for its speed of performance, but it\u2019ll not be an exact measurement. And your VM won\u2019t be equipped with the same graphics card you have in your host, so you won\u2019t be able to test how super-duper graphics applications or high-end games will work on that operating system on a physical computer with a gaming graphics card.\nBut, as a way to investigate the ease of installation of a Linux distribution and to check whether a particular desktop environment or tiling window manager fits with your preferences and workflow, VMs are perfect. Run Linux but need to have access to Windows once in a while? Spin it up in a VM.\nAnd, best of all, you can thoroughly kick the tires of a new operating system and leave your host computer untouched."
    },
    {
      "url": "https://www.howtogeek.com/i-use-docker-for-almost-everything-and-im-not-even-a-developer/",
      "text": "Do you think Docker is only for developers? I\u2019ve heard that saying, but it couldn\u2019t be more wrong. I\u2019m not a developer, and I use Docker to run dozens of services and apps in my homelab.\nWhat Is Docker Anyway?\nDocker is a containerization platform that allows you to run services, programs, and mini operating systems on your computer or server. It can also be used to run various development environments, tools, and other things needed for building applications.\nOne of the main reasons to use Docker is to run various services segregated from both each other and your host operating system. This allows you to have greater privacy and protection for your system. If one container gets compromised, it only has access to itself and whatever else you grant it access to.\nSo, Am I a Developer?\nI wouldn\u2019t necessarily call myself a \u201cdeveloper.\u201d While I used to do some programming and application development, I haven\u2019t in many years. I do know my way around a Linux terminal, and still dabble in HTML or CSS when it comes to my personal websites, but I am in no way a full-blown developer.\nAt most, I\u2019m a tinkerer. I love messing with things, changing an app\u2019s code to make it do what I want, and poking around the various services that I run in my homelab.\nBecause I don\u2019t do development tasks, I don\u2019t actually run any development services in Docker. There are some awesome developer tools available in Docker, I just don\u2019t have a use for them personally.\nIf I\u2019m Not a Developer, Why Use Docker?\nWhile Docker does a fantastic job of helping with development tasks, that\u2019s just not my use case for it. Instead, I leverage the containerization of Docker for something entirely different: homelabbing.\nThere are tens of thousands of Docker containers available. Many of the available containers focus on running services or programs, not necessarily development.\nIn my homelab, I have nearly 30 Docker containers deployed on my primary Docker host, and another half dozen on my secondary Docker host. These services range from Pi-hole to Plex, OpenSpeedTest, Calibre, paperless-ngx, Home Assistant, Tautulli, Homepage, and many others.\nThe question is: why do I use Docker for this instead of just installing the apps themselves? The answer is simple: I love containers.\nWith Docker, my Plex container cannot access anything on my network unless I grant it access. So, I give Plex access to my media library and that\u2019s it. Home Assistant actually has zero access to my server at all; it has its own IP and runs on my network, but if it gets compromised, it can\u2019t get to the other hardware on my network.\nThe same goes for all the other apps I run on Docker. They\u2019re all segregated away from each other. Docker containers only have access to what I grant, meaning I have fine-tuned control over my security risks in the homelab.\nOn top of that, Docker containers come pre-configured for the app you\u2019re wanting to run, so there\u2019s little setup required to get a service up and running. All dependencies are already there, and they are kept up-to-date by the container developer. If a dependency requires a reboot, all I have to reboot is the Docker container, not my host system.\nDocker is also portable in that I can easily change the host system without reinstalling or reconfiguring services. I recently moved my Docker server from one computer to another, and the only container that didn\u2019t actually bring its configuration with it was Home Assistant (which I easily restored from a backup).\nWhen I first started using Docker, I didn\u2019t realize how useful of a feature this would be. However, I\u2019ve used it so many times. While the migration function is nice, my favorite part of Docker is that I can reinstall a container without losing any settings or configurations.\nSo, why do I use Docker? It\u2019s simply the easiest way for me to install, test, and use services in my homelab. Deploying a Docker container takes just a few seconds. When I\u2019m done, I just remove the container and the service (with all dependencies) disappear.\nDocker Runs My Entire Homelab and I Couldn\u2019t Imagine Otherwise\nAt this point in my homelab journey, I couldn\u2019t imagine not using Docker. I can\u2019t even begin to count the number of services I\u2019ve tried out in Docker, only to remove a few minutes later because it didn\u2019t do what I wanted.\nWithout Docker, I\u2019d have to install the software plus any dependencies it had. Uninstallation would then require the removal of those dependencies, and there\u2019s almost always stray configuration files left behind.\nDocker just makes managing my homelab simple, and it\u2019s definitely the best tool for the job.\nIn the future, I plan to deploy Docker in a high-availability environment. Essentially, I\u2019ll have three similarly-specced servers that all run Docker. Portainer, my preferred Docker manager, would then distribute those services between the various servers.\nIf I needed to work on one server, all of those services would migrate to another server and stay running. When the server that underwent maintenance was back up, then it\u2019d be available for containers to deploy to it again.\nWithout using some form of application management system, like Docker, this wouldn\u2019t be possible. Because of how Docker handles its volumes and storage, when those services migrate, so do all their preferences, configuration files, and any local storage used by the app.\nDocker is absolutely essential to my setup, and I don\u2019t see it going anywhere anytime soon.\nSo, no, I am not a developer, but I absolutely love Docker. If you\u2019re not a developer, you should totally give Docker a try\u2014it\u2019ll change your setup for the better.\nThere are a lot of Docker containers out there to try. Tens of thousands, actually. If that\u2019s overwhelming to you, you\u2019re not alone. Here are a handful of my favorite Docker containers that I think should be in every homelab, ranging from simple web pages to full-blown services."
    },
    {
      "url": "https://www.howtogeek.com/how-my-homelab-became-the-most-useful-thing-in-my-home/",
      "text": "Are you on the fence with whether you should run a homelab? Well, what started as a small journey has now become the cornerstone of my house. I\u2019m not kidding when I say that my homelab is the most useful and functional part of my home.\nMy Homelab Runs My Entire Media Center\nMy homelab does a lot in my household, but one of the main functions of it is my media servers. I have a video server, audiobook server, and ebook server even\u2014all of which make accessing my self-hosted content a breeze.\nPlex powers my video server, allowing me to access my media library both at home and when I\u2019m out and about. I\u2019ve used my Plex server to download movies to watch on airplanes before, or even just enjoy a movie in the car while riding down the road.\nAudiobookshelf handles all of my audiobook needs. Not all audiobooks are available on Audible, and I like to go directly to the publisher when I can. So, I have my own audiobook server that allows me to listen to my entire library anywhere I am.\nFor ebooks, I\u2019m still getting the hang of things. Calibre and Calibre-Web both work fantastically, but my Kindle is old enough that it doesn\u2019t like EPUB files that much. Either way, there are ebooks that I either get directly from the publisher or through other marketplaces that I want to read\u2014so I host my own ebook server. This also works with PDFs guides that I purchase for my wife and me (like recipes), and Calibre-Web works great for reading them on any machine too.\nThe great thing about hosting my own media servers is the media that I host won\u2019t go anywhere unless I want it to. Movies never leave Plex unless I remove them. Audiobooks take up hardly any space and once I purchase and upload them, they\u2019ll never disappear. Ebooks are the same way: once they\u2019re hosted on my own server, I never worry whether they\u2019ll be there when I want to read them or not.\nThis can\u2019t be said about using services that are run by third-party websites like Netflix, Audible, or Kindle. The media center part of my homelab is enough reason for me to make it the most useful thing in my house, but that\u2019s just really scratching the surface.\nPlex\nWith Plex, you can keep a single, unified Watchlist for any movie or TV show you hear about, on any service\u2014even theater releases! You can finally stop hopping between watchlists on all your other streaming services, and add it all on Plex instead.\nWithout My Homelab, I\u2019d Be Paying for Way More Services\nMy homelab doesn\u2019t just house a media server. It also houses my game servers, photo storage, security camera, and so much more.\nAll of these services are things that I either was paying for before, or would have eventually paid for as the needs arose.\nFor starters, I can host just about any game server I could ever want. Minecraft? You bet. Satisfactory? Of course. ARK: Survival Evolved for a buddy? Absolutely!\nIn all, I typically have somewhere between three and five (or more) game servers running at any time, with the ability to spin up dozens more if needed.\nI also keep all of my photos backed up to my server. It allows me to have two copies of all pictures I take\u2014one copy on my portable SSD and one on the server. I\u2019m about to deploy a third server off-site, and then I\u2019ll even back up to that remote server so that way I can have three copies of every photo I take.\nAn area that I didn\u2019t expect that my homelab would come into play for is security camera recording. I never expected to host that locally\u2014cloud storage is so ubiquitous and easy to use, right? Well, it is, until you find yourself paying a few hours wage per month just to store security camera footage.\nThat\u2019s why I recently spun up a network video recorder (NVR) in my homelab. It brings in camera feeds from various manufacturers and allows me to easily record all the footage locally. I picked up an 8TB HDD on Amazon for $100, and now I can store months of recordings at one time before it starts to overwrite the oldest footage.\nHGST Ultrastar He (8TB)\n- Storage Capacity\n- 8TB\n- Brand\n- HGST\n- Spindle speed\n- 7200\n- Cache\n- 128MB\n- Warranty\n- 5-years\nThe HGST / WD Ultrastar 8TB Helium NAS hard drive offers 128MB of cache, spins at 7200RPM, and is data center-grade to deliver optimal performance and longevity to your network storage system.\nThis just scratches the surface of the services that I can self-host in my homelab to save me money every month. At some point, I\u2019ll likely move my cloud storage local (which I currently pay for Google Drive).\nI might even self-host my own AI large language model instead of paying for ChatGPT Plus in the future. The opportunities are endless when it comes to self-hosting services in a homelab.\nMy Homelab Keeps My Information Private\nOne of my favorite parts of my homelab is that my documents and information stay private. While storing my home security footage on a third party\u2019s server gives them access to it to do what they please with the recordings, keeping those same files locally means only I have access to them.\nSame goes for my audiobooks, movies, and ebooks. You name it, if I host it on my own server, I control who has access to it.\nYes, I do open up some services and files to the World Wide Web (with proper login credentials), but I get to choose what services are open and who can access it. If there\u2019s a folder on my server that I don\u2019t want anyone to access, like my tax documents or anything else, then I can absolutely do that.\nThis is one of the reasons I spun up an Immich server a few months ago when I was exploring completely ditching all Google Photos ties and self-hosting all of my pictures. It\u2019s also why I\u2019m debating spinning up a Nextcloud server and ditching Google Drive.\nThe amount of control you have over the data that you host yourself is infinite. The amount of control you have over the data that other people host is, well, very limited.\nRunning My Own Server Lets Me Use Any Smart Home Gear I Want\nLast, but certainly not least, my homelab opens up my smart home to use any device I want. I\u2019m an Apple person through and through, and that means I rely on HomeKit to power my smart home. The downside to HomeKit? There are not a lot of certified devices.\nSo, to remedy that, I have Home Assistant deployed in my homelab. This allows me to bring unsupported devices into HomeKit as if they\u2019re native, which has totally transformed my smart home setup.\nI can now find dirt-cheap smart plugs at yard sales or thrift stores and use them in HomeKit. Plus, Home Assistant handles most of its communication locally, meaning you could even remove external network access from your smart plugs to make your home\u2019s network more secure, while retaining control over using smart devices.\nThis Self-Hosted App Made My HomeKit Setup Far Less Restrictive\nWho knew HomeKit could be so welcoming of unofficial accessories?\nAt the end of the day, I couldn\u2019t imagine my house without my homelab. The hardware will change with time, but the homelab will remain there. In fact, I\u2019m in the process of swapping out hardware in the homelab right now, but all of my services are still going to be running without question.\nReady to get started with your own homelab? There are several services I highly recommend everyone run. From Plex to Tautulli, Nginx Proxy Manager, audiobookshelf, and much more, my list will get your homelab started in a flash."
    },
    {
      "url": "https://www.howtogeek.com/10-docker-containers-every-homelabber-should-run/",
      "text": "There are thousands of Docker containers that you could potentially run on your server. While I haven't run anywhere near all of them, here are 10 containers that I simply can't live without in my homelab, and why I think you should run them too.\nDocker for Beginners: Everything You Need to Know\nLearn to use this incredibly popular development tool.\n10 Homepage (A Dashboard of All Your Home Services)\nA homelab dashboard is something that I never thought I needed\u2014until I set one up.\nWith Homepage, you'll get a simple, easy-to-use homelab dashboard. It's configured with YAML and is pretty straightforward to add new services, backgrounds, categories, and more. Homepage is now something that I use daily, if not multiple times per day to access my self-hosted services.\nI'm able to set icons based on the service, alongside a name and description. Some services even support widgets so I can see if my Plex server is in use, or if there are any of my Pterodactyl game servers down at the time. With the categories, I can even separate the services by which system they're running on, though I do plan to categorize them by actual app category in the future.\n9 Home Assistant (Smart Home Manager)\nHome Assistant is pretty well-known in the smart home and homelab worlds. However, recently I migrated my Home Assistant install to Docker, and it's been one of the best decisions ever for me.\nWith Home Assistant, you're able to unify your entire smart home setup under one roof. It supports a vast array of manufacturers, which is why I love it so much. Home Assistant also has an extremely robust automations system, and even can integrate non-HomeKit devices into HomeKit.\nWhat Is Home Assistant? And Why Isn't Everyone Using It?\nGet to know this open-source locally-controlled smart home platform.\n8 OpenSpeedTest (Self-Hosted Speed Test Server)\nAt first, I didn't really know why I'd want to host my own speed test server, but I've actually found quite a bit of use with it.\nWhile you can use things like iPerf to test your network speed between computers or computer and server, testing phones isn't quite that easy. Sometimes I simply want to see what my wireless speed is from my phone to my server, and sometimes I want to test my speed from outside the house.\nEither way I go, OpenSpeedTest is up to the task. When I run it through my reverse proxy, I get WAN speeds. When running via the internal IP, I get LAN speeds. With Wi-Fi 7 and a 2.5G uplink from the server to the router, I can achieve multi-gig speeds over a wireless connection to my server. OpenSpeedTest helps me see just how fast that connection is.\nHonestly, this is more for bragging rights than anything else, but it's still a fun Docker container to run none-the-less.\n7 Plex (Personal Media Server)\nWhere do I start with Plex? It's a staple of the homelab community and something that I think everyone should run on one of their computers. Whether you have a vast DVD and Blu-ray library that needs to be digitized, or simply want to access your home movies from years gone past, Plex is the best way to organize that.\nI've tried Jellyfin too, and Plex still wins out for me. The user management is fantastic, remote access is easier, and overall, it's just prettier. Even if you choose not to run Plex, every homelab should have some form of a media server running somewhere.\nWhat Is Plex and How Does It Work?\nStreaming services are just as expensive as cable. So, why not make your own streaming service with Plex?\n6 Portainer (Docker Management Web GUI)\nSince you're already running Docker containers, why not make them easier to manage? When I used Unraid as my primary Docker host, it had a nice web interface that let me easily manage my containers and update them. Without Unraid, Docker typically just runs in a command line.\nDon't get me wrong, I love command lines as much as the next guy. However, to maintain 30+ Docker containers, their ports, settings, and other related things, I want a nice GUI. That's where Portainer comes in.\nYou can spin up Portainer as a Docker container on your Docker host, and it'll let you manage everything about your other containers. It supports both traditional Docker launches and Docker Compose, and it's ran flawlessly for me for multiple months since I spun up the first instance.\n5 Tautulli (Plex Info Tracker)\nWhile I love Plex, the information gathered by the stock Plex app just isn't all that great. That's where Tautulli comes in.\nWith Tautulli, I'm able to see not only what other users are watching, but also very detailed information about the stream. What media container it's using, exact quality profile it's using, and more. That's not really where Tautulli shines though.\nTautulli is a great way to see what's being watched, how many times a specific show or movie has been watched, and which of your users uses the server the most. While a lot of this information isn't crucial, it's nice to have some detailed information as to the usage of your Plex server.\n4 Nginx Proxy Manager (Reverse Proxy)\nI couldn't run my homelab without Nginx Proxy Manager. NPM, as it's called, handles all of my reverse proxy needs.\nThink of NPM as a web GUI for nginx, but with extra features. It handles all of your config settings, generating and using SSL certificates, and choosing ports. Instead of me having to navigate to 192.168.0.6:13378 for AudioBookshelf, I just go to audiobooks.mydomain.com and NPM handles the rest on the backend.\nIf you're wanting to make any services accessible outside your local network, then check out NPM. It's simple to set up, easy to use, and delivers tons of features.\n3 Pi-Hole (Content Blocker)\nIs a homelab really complete without Pi-Hole running somewhere on the network? Mine isn't, that's for sure.\nPi-hole handles content blocking and filtering at a DNS level. Once set up and configured, you simply point your router's DNS settings to the Pi-hole's IP, and you'll be off to the races. Pi-hole handles all kinds of content blocking and filtering. Yes, it works with ads, but it also allows you to block adult sites on the network with no easy way around it. It'll also let you use local DNS entries similar to NPM, but without having to be accessed from outside the network.\nPi-hole has many benefits, and is so easy to use. It deploys on Docker with a quick command and just sits there, running in the background, ready to handle your content filtering needs.\n2 audiobookshelf (Self-Hosted Audible Alternative)\naudiobookshelf is a newer addition to my self-hosted stack, but one that I've quickly learned I can't live without.\nOne of my 2025 goals is to listen to more audiobooks. While I'm not a big reader, I don't mind listening to a book, especially when it's dramatized. That's where audiobookshelf comes in.\nIt is a simple-to-use audiobook server, similar to how Plex is a media server. Yes, Plex supports audiobooks, but it's nowhere near as robust as audiobookshelf's implementation.\nYou can upload MP3 or M4B files to audiobookshelf, and it handles the rest from there. It has an interface for adding chapters, converting multi-file MP3 audiobooks to single-file M4B with all the info embedded, and much more.\nI use the Plappa app on iOS to access my audiobookshelf server (through NPM for remote access) and it's completely replaced Audible for me.\n1 Immich (Self-Hosted Google Photos Alternative)\nNot everyone wants to let Google spy on their photos, using them to train AI algorithms on visual content. I recently migrated all of my Google Photos to Immich, and I'm not looking back.\nImmich is a self-hosted photos server that has a lot of features very similar to Google Photos and iCloud Photos. With location lookup, history, memories, albums, AI machine learning, and more, Immich is pretty much a drop-in replacement for Google Photos in most workflows. There's a mobile app for both iOS and Android, letting you back up either remotely or only when on your local network.\nOverall, Immich is an extremely powerful Google Photos replacement, and it's not even that difficult to spin up once you have the hang of Docker."
    },
    {
      "url": "https://www.howtogeek.com/how-i-run-my-entire-homelab-on-docker-and-why-you-should-too/",
      "text": "Summary\n- Docker containers are lightweight, self-contained virtual system for running services in minimal space & resources.\n- Docker simplifies setting up, managing, updating, and uninstalling a variety of services with ease.\n- I run over 30 services in my homelab, which fully operates on Docker, showing just how flexible it is and its ease of use.\nWhen it comes to homelabbing, there are a lot of services and apps you might want to run. While installing them individually is an option, I chose to go the Docker route instead\u2014and there's no going back.\nWhat Exactly Is Docker?\nIf you've never heard of Docker before, the easiest way to think about it is that it's a bunch of ultra-small and lightweight virtual systems running within one environment.\nDocker for Beginners: Everything You Need to Know\nLearn to use this incredibly popular development tool.\nDocker's \"virtual systems\" are called containers, and each one is only the bare essentials of whatever Linux distro is needed for the service itself to run.\nContainers take up very little resources and space on your computer. This allows you to deploy a lot of services that are self-contained without hogging resources. It also means that, when you remove a service from Docker, it completely removes itself without leaving any traces behind.\nAll dependencies and everything are gone when it's removed. A small volume might be left behind, but it's very easy to prune unused volumes, too.\nWhat Does Docker Do, and When Should You Use It?\nDocker is a tool for running your applications inside containers.\nDocker, I Choose You\nThere are many options out there when it comes to containerized services. For the longest time, TrueNAS used jails (though it recently switched to Docker), there's Kubernetes, virtual machines, LXC containers, and likely more. Docker is just one service of many. Why'd I choose it?\nDocker is, in my opinion, simply the easiest containerization software to set up, and also among the most robust. There are over 10,000 Docker containers in the Docker Hub, and that's just one place to source apps from. No matter what you're wanting to run, chances are, there's a Docker container for it.\nJails are more limited in their function and availability. Kubernetes, while based around Docker, and runs Docker containers, is just more complicated to set up for a homelab environment as it's more geared toward enterprise use. Virtual machines are nice and all, but that's just installing software bare metal into an operating system, and doesn't accomplish what I want. LXC containers are very similar to Docker containers, but aren't quite as simple to manage as Docker is.\nI originally started using Docker in 2021 when I spun up my first Unraid server. It was a requirement of mine even back then, as I was starting to really dive into the world of homelabbing. To this day, it's a requirement of mine on any server I run. Whether it's an Ubuntu virtual machine or a bare-metal NAS, it has to have Docker capabilities.\nDocker is also just extremely easy to use and understand. While I've run web servers for many years, installing dependencies, configuring JSON files, and trying to debug services was never simple for me. Uninstalling things was even more complex half the time. Docker simplifies that.\nI love how easy I can spin a service up in Docker, test it out, evaluate it, and decide whether I'm going to keep it.\nJust the other day, I had Uptime-Kuma running on my main Docker host. I liked the service but wanted to run it on a machine that has a better uptime than my main Docker instance. So, I just spun another Uptime-Kuma container up on another Docker host I have and, within minutes, I was ready to go again.\nI Run Over 30 Services, All On Docker\nMy entire homelab runs on Docker. I can only think of a single service that doesn't run on Docker, and I'm actually thinking of moving it to a Docker container.\nA quick list of just some of the services I run in my homelab:\n- AudioBookshelf\n- Calibre\n- Calibre-Web\n- Home Assistant\n- Immich\n- Nginx Proxy Manager\n- Plex\n- Scrypted\n- Pi-Hole\nThese are services that run on my primary Docker instance. I also have Docker running on my Raspberry Pi, which handles Uptime-Kuma and will soon run my backup Pi-Hole instance.\nI do have two secondary virtual machines that run Docker. These VMs use Ubuntu 24.04 as a base, and Docker as the container host. The reason they're two separate VMs from my primary Docker VM is because the services simply run better when separated on different machines. Those two VMs run my Pterodactyl game server and management panel.\nDocker Makes Homelabbing Easy\nWhether you're wanting to get into homelabbing, or you're already doing it, Docker is something that I truly think everyone should be using.\nIf you've not experienced the life of a containerized homelab, then you're missing out. There's so much Docker is capable of, including moving into the realm of high availability, something I plan to explore later this year.\nHaving all of your services as self-contained containers simply makes them easier to manage. Whether it's updating one service and not another, removing something you're no longer using, or just spinning up multiple containers that all talk to each other but are segregated from your main OS, Docker is the tool for the job.\nMy homelab couldn't run as it does without Docker. I think if you give it a try, you'll see just how useful of a tool it is for your setup too.\nAre you read to dive head-first into Docker containers? Here are 10 Docker containers I think everyone should run. This list will get you started with some of the core services that run my homelab through Docker, and they're all pretty easy to spin up, too!"
    },
    {
      "url": "https://www.howtogeek.com/733522/docker-for-beginners-everything-you-need-to-know/",
      "text": "Summary\n- Docker containers create isolated environments similar to VMs without running a full operating system, enhancing portability and convenience.\n- Docker is easy to use and popular for launching applications quickly without impacting your system.\n- To use Docker, install it on your platform and build your first Docker image by writing a Dockerfile.\nDocker creates packaged applications called containers. Each container provides an isolated environment similar to a virtual machine (VM). Unlike VMs, Docker containers don't run a full operating system. They share your host's kernel and virtualize at a software level.\nDocker Basics\nDocker has become a standard tool for software developers and system administrators. It's a neat way to quickly launch applications without impacting the rest of your system. You can spin up a new service with a single docker run command.\nContainers encapsulate everything needed to run an application, from OS package dependencies to your own source code. You define a container's creation steps as instructions in a Dockerfile. Docker uses the Dockerfile to construct an image.\nImages define the software available in containers. This is loosely equivalent to starting a VM with an operating system ISO. If you create an image, any Docker user will be able to launch your app with docker run.\nHow Does Docker Work?\nContainers utilize operating system kernel features to provide partially virtualized environments. It's possible to create containers from scratch with commands like chroot. This starts a process with a specified root directory instead of the system root. But using kernel features directly is fiddly, insecure, and error-prone.\nDocker is a complete solution for the production, distribution, and use of containers. Modern Docker releases are comprised of several independent components. First, there's the Docker CLI, which is what you interact with in your terminal. The CLI sends commands to a Docker daemon. This can run locally or on a remote host. The daemon is responsible for managing containers and the images they're created from.\nThe final component is called the container runtime. The runtime invokes kernel features to actually launch containers. Docker is compatible with runtimes that adhere to the OCI specification. This open standard allows for interoperability between different containerization tools.\nYou don't need to worry too much about Docker's inner workings when you're first getting started. Installing docker\non your system will give you everything you need to build and run containers.\nWhy Do So Many People Use Docker?\nContainers have become so popular because they solve many common challenges in software development. The ability to containerize once and run everywhere reduces the gap between your development environment and your production servers.\nUsing containers gives you confidence that every environment is identical. If you have a new team member, they only need to docker run\nto set up their own development instance. When you launch your service, you can use your Docker image to deploy to production. The live environment will exactly match your local instance, avoiding \"it works on my machine\" scenarios.\nDocker is more convenient than a full-blown virtual machine. VMs are general-purpose tools designed to support every possible workload. By contrast, containers are lightweight, self-sufficient, and better suited to throwaway use cases. As Docker shares the host's kernel, containers have a negligible impact on system performance. Container launch time is almost instantaneous, as you're only starting processes, not an entire operating system.\nGetting Started\nDocker is available on all popular Linux distributions. It also runs on Windows and macOS. Follow the Docker setup instructions for your platform to get it up and running.\nYou can check that your installation is working by starting a simple container:\ndocker run hello-world\nThis will start a new container with the basic \"hello-world\" image. The image emits some output explaining how to use Docker. The container then exits, dropping you back to your terminal.\nCreating Images\nOnce you've run hello-world, you're ready to create your own Docker images. A Dockerfile describes how to run your service by installing required software and copying in files. Here's a simple example using the Apache web server:\nFROM httpd:latest\nRUN echo \"LoadModule headers_module modules/mod_headers.so\" >> /usr/local/apache2/conf/httpd.conf\nCOPY .htaccess /var/www/html/.htaccess\nCOPY index.html /var/www/html/index.html\nCOPY css/ /var/www/html/css\nThe FROM\nline defines the base image. In this case, we're starting from the official Apache image. Docker applies the remaining instructions in your Dockerfile on top of the base image.\nThe RUN\nstage runs a command within the container. This can be any command available in the container's environment. We're enabling the headers\nApache module, which could be used by the .htaccess\nfile to set up routing rules.\nThe final lines copy the HTML and CSS files in your working directory into the container image. Your image now contains everything you need to run your website.\nNow, you can build the image:\ndocker build -t my-website:v1 .\nDocker will use your Dockerfile to construct the image. You'll see output in your terminal as Docker runs each of your instructions.\nThe -t\nin the command tags your image with a given name (my-website:v1\n). This makes it easier to refer to in the future. Tags have two components, separated by a colon. The first part sets the image name, while the second usually denotes its version. If you omit the colon, Docker will default to using latest\nas the tag version.\nThe .\nat the end of the command tells Docker to use the Dockerfile in your local working directory. This also sets the build context, allowing you to use files and folders in your working directory with COPY\ninstructions in your Dockerfile.\nOnce you've created your image, you can start a container using docker run\n:\ndocker run -d -p 8080:80 my-website:v1\nWe're using a few extra flags with docker run\nhere. The -d\nflag makes the Docker CLI detach from the container, allowing it to run in the background. A port mapping is defined with -p\n, so port 8080 on your host maps to port 80 in the container. You should see your web page if you visit localhost:8080\nin your browser.\nDocker images are formed from layers. Each instruction in your Dockerfile creates a new layer. You can use advanced building features to reference multiple base images, discarding intermediary layers from earlier images.\nImage Registries\nOnce you have an image, you can push it to a registry. Registries provide centralized storage so that you can share containers with others. The default registry is Docker Hub.\nWhen you run a command that references an image, Docker first checks whether it's available locally. If it isn't, it will try to pull it from Docker Hub. You can manually pull images with the docker pull\ncommand:\ndocker pull httpd:latest\nIf you want to publish an image, create a Docker Hub account. Run docker login\nand enter your username and password.\nNext, tag your image using your Docker Hub username:\ndocker tag my-image:latest docker-hub-username/my-image:latest\nNow, you can push your image:\ndocker push docker-hub-username/my-image:latest\nOther users will be able to pull your image and start containers with it.\nYou can run your own registry if you need private image storage. Several third-party services also offer Docker registries as alternatives to Docker Hub.\nManaging Your Containers\nThe Docker CLI has several commands to let you manage your running containers. Here are some of the most useful ones to know:\nListing Containers\ndocker ps\nshows you all your running containers. Adding the -a\nflag will show stopped containers, too.\nStopping and Starting Containers\nTo stop a container, run docker stop my-container\n. Replace my-container\nwith the container's name or ID. You can get this information from the ps\ncommand. A stopped container is restarted with docker start my-container\n.\nContainers usually run for as long as their main process stays alive. Restart policies control what happens when a container stops or your host restarts. Pass --restart always\nto docker run\nto make a container restart immediately after it stops.\nGetting a Shell\nYou can run a command in a container using docker exec my-container my-command\n. This is useful when you want to manually invoke an executable that's separate to the container's main process.\nAdd the -it\nflag if you need interactive access. This lets you drop into a shell by running docker exec -it my-container sh\n.\nMonitoring Logs\nDocker automatically collects output emitted to a container's standard input and output streams. The docker logs my-container\ncommand will show a container's logs inside your terminal. The --follow\nflag sets up a continuous stream so that you can view logs in real time.\nCleaning Up Resources\nOld containers and images can quickly pile up on your system. Use docker rm my-container\nto delete a container by its ID or name.\nThe command for images is docker rmi my-image:latest\n. Pass the image's ID or full tag name. If you specify a tag, the image won't be deleted until it has no more tags assigned. Otherwise, the given tag will be removed but the image's other tags will remain usable.\nBulk clean-ups are possible using the docker prune\ncommand. This gives you an easy way to remove all stopped containers and redundant images.\nGraphical Management\nIf the terminal's not your thing, you can use third-party tools to set up a graphical interface for Docker. Web dashboards let you quickly monitor and manage your installation. They also help you take remote control of your containers.\nPersistent Data Storage\nDocker containers are ephemeral by default. Changes made to a container's filesystem won't persist after the container stops. It's not safe to run any form of file storage system in a container started with a basic docker run\ncommand.\nThere are a few different approaches to managing persistent data. The most common is to use a Docker Volume. Volumes are storage units that are mounted into container filesystems. Any data in a volume will remain intact after its linked container stops, letting you connect another container in the future.\nMaintaining Security\nDockerized workloads can be more secure than their bare metal counterparts, as Docker provides some separation between the operating system and your services. Nonetheless, Docker is a potential security issue, as it normally runs as root\nand could be exploited to run malicious software.\nIf you're only running Docker as a development tool, the default installation is generally safe to use. Production servers and machines with a network-exposed daemon socket should be hardened before you go live.\nAudit your Docker installation to identify potential security issues. There are automated tools available that can help you find weaknesses and suggest resolutions. You can also scan individual container images for issues that could be exploited from within.\nWorking with Multiple Containers\nThe docker\ncommand only works with one container at a time. You'll often want to use containers in aggregate. Docker Compose is a tool that lets you define your containers declaratively in a YAML file. You can start them all up with a single command.\nThis is helpful when your project depends on other services, such as a web backend that relies on a database server. You can define both containers in your docker-compose.yml\nand benefit from streamlined management with automatic networking.\nHere's a simple docker-compose.yml\nfile:\nversion: \"3\"\nservices:\napp:\nimage: app-server:latest\nports:\n- 8000:80\ndatabase:\nimage: database-server:latest\nvolumes:\n- database-data:/data\nvolumes:\ndatabase-data:\nThis defines two containers (app\nand database\n). A volume is created for the database. This gets mounted to /data\nin the container. The app server's port 80 is exposed as 8000 on the host. Run docker-compose up -d\nto spin up both services, including the network and volume.\nThe use of Docker Compose lets you write reusable container definitions that you can share with others. You could commit a docker-compose.yml\ninto your version control instead of having developers memorize docker run\ncommands.\nThere are other approaches to running multiple containers, too. Docker App is an emerging solution that provides another level of abstraction. Elsewhere in the ecosystem, Podman is a Docker alternative that lets you create \"pods\" of containers within your terminal.\nContainer Orchestration\nDocker isn't normally run as-is in production. It's now more common to use an orchestration platform such as Kubernetes or Docker Swarm mode. These tools are designed to handle multiple container replicas, which improves scalability and reliability.\nDocker is only one component in the broader containerization movement. Orchestrators utilize the same container runtime technologies to provide an environment that's a better fit for production. Using multiple container instances allows for rolling updates as well as distribution across machines, making your deployment more resilient to change and outage. The regular docker\nCLI targets one host and works with individual containers.\nA Powerful Platform for Containers\nDocker gives you everything you need to work with containers. It has become a key tool for software development and system administration. The principal benefits are increased isolation and portability for individual services.\nGetting acquainted with Docker requires an understanding of the basic container and image concepts. You can apply these to create your specialized images and environments that containerize your workloads."
    },
    {
      "url": "https://www.howtogeek.com/248780/how-to-compress-and-extract-files-using-the-tar-command-on-linux/",
      "text": "Quick Links\nSummary\n- The tar command on Linux is used to create and extract TAR archive files.\n- Run \"tar -czvf (archive name).tar.gz (pathtofile)\u201d in the Terminal to compress a file or folder. To extract an archive to the current folder, run the command \u201ctar -xzvf (archive file)\".\n- You can compress multiple directories or files at once by providing a list of files or directories, and you can exclude certain files or directories using the --exclude switch.\nThe tar command on Linux is often used to create .tar.gz or .tgz archive files, also called \"tarballs.\" This command has a large number of options, but you just need to remember a few letters to quickly create archives with tar. The tar command can extract the resulting archives, too.\nThe GNU tar command included with Linux distributions has integrated compression. It can create a .tar archive and then compress it with gzip or bzip2 compression in a single command. That's why the resulting file is a .tar.gz file or .tar.bz2 file.\nCompress an Entire Directory or a Single File\nUse the following command to compress an entire directory or a single file on Linux. It'll also compress every other directory inside a directory you specify \u2014 in other words, it works recursively.\ntar -czvf name-of-archive.tar.gz /path/to/directory-or-file\nHere's what those switches actually mean:\n- -c: Create an archive.\n- -z: Compress the archive with gzip.\n- -v: Display progress in the terminal while creating the archive, also known as \"verbose\" mode. The v is always optional in these commands, but it's helpful.\n- -f: Allows you to specify the filename of the archive.\nLet's say you have a directory named \"stuff\" in the current directory and you want to save it to a file named archive.tar.gz. You'd run the following command:\ntar -czvf archive.tar.gz stuff\nOr, let's say there's a directory at /usr/local/something on the current system and you want to compress it to a file named archive.tar.gz. You'd run the following command:\ntar -czvf archive.tar.gz /usr/local/something\nCompress Multiple Directories or Files at Once\nWhile tar is frequently used to compress a single directory, you could also use it to compress multiple directories, multiple individual files, or both. Just provide a list of files or directories instead of a single one. For example, let's say you want to compress the /home/ubuntu/Downloads directory, the /usr/local/stuff directory, and the /home/ubuntu/Documents/notes.txt file. You'd just run the following command:\ntar -czvf archive.tar.gz /home/ubuntu/Downloads /usr/local/stuff /home/ubuntu/Documents/notes.txt\nJust list as many directories or files as you want to back up.\nExclude Directories and Files\nIn some cases, you may wish to compress an entire directory, but not include certain files and directories. You can do so by appending an --exclude\nswitch for each directory or file you want to exclude.\nFor example, let's say you want to compress /home/ubuntu, but you don't want to compress the /home/ubuntu/Downloads and /home/ubuntu/.cache directories. Here's how you'd do it:\ntar -czvf archive.tar.gz /home/ubuntu --exclude=/home/ubuntu/Downloads --exclude=/home/ubuntu/.cache\nThe --exclude\nswitch is very powerful. It doesn't take names of directories and files \u2014 it actually accepts patterns. There's a lot more you can do with it. For example, you could archive an entire directory and exclude all .mp4 files with the following command:\ntar -czvf archive.tar.gz /home/ubuntu --exclude=*.mp4\nUse bzip2 Compression Instead\nWhile gzip compression is most frequently used to create .tar.gz or .tgz files, tar also supports bzip2 compression. This allows you to create bzip2-compressed files, often named .tar.bz2, .tar.bz, or .tbz files. To do so, just replace the -z for gzip in the commands here with a -j for bzip2.\nGzip is faster, but it generally compresses a bit less, so you get a somewhat larger file. Bzip2 is slower, but it compresses a bit more, so you get a somewhat smaller file. Gzip is also more common, with some stripped-down Linux systems including gzip support by default, but not bzip2 support. In general, though, gzip and bzip2 are practically the same thing, and both will work similarly.\nFor example, instead of the first example we provided for compressing the stuff directory, you'd run the following command:\ntar -cjvf archive.tar.bz2 stuff\nExtract a Tar File\nOnce you have an archive, you can extract it with the tar command. The following command will extract the contents of archive.tar.gz to the current directory.\ntar -xzvf archive.tar.gz\nIt's the same as the archive creation command we used above, except the -x\nswitch replaces the -c\nswitch. This specifies you want to extract an archive instead of create one.\nYou may want to extract the contents of the archive to a specific directory. You can do so by appending the -C\nswitch to the end of the command. For example, the following command will extract the contents of the archive.tar.gz file to the /tmp directory.\ntar -xzvf archive.tar.gz -C /tmp\nIf the file is a bzip2-compressed file, replace the \"z\" in the above commands with a \"j\".\nThis is the simplest possible usage of the tar command. The command includes a large number of additional options, so we can't possibly list them all here. For more information. run the info tar command at the shell to view the tar command's detailed information page. Press the q key to quit the information page when you're done. You can also read tar's manual online.\nIf you're using a graphical Linux desktop, you could also use the file-compression utility or file manager included with your desktop to create or extract .tar files. On Windows, you can extract and create .tar archives with the free 7-Zip utility."
    },
    {
      "url": "https://www.howtogeek.com/why-i-chose-unraid-over-truenas-scale-in-2025/",
      "text": "Quick Links\nI recently got a new storage server and decided to give TrueNAS Scale a try after four years of not touching it. In the end, Unraid still won the day. Here's why.\nTrueNAS Scale Has Come a Long Way From the Days of FreeNAS\nTrueNAS (which used to be FreeNAS) has come quite a long way in recent years. Originally based on FreeBSD, TrueNAS Scale, the latest version of the operating system, offers a lot more modern features than it used to be.\nBeing based on a standard Linux kernel now, TrueNAS Scale offers proper Docker support now instead of the previous Jails that FreeNAS used. This is a huge upgrade for TrueNAS and was one of the reasons I was considering using it in my server stack.\nTrueNAS Scale also just received the ability to expand VDEVs, which is pretty great. Previously, upgrading your storage pool on TrueNAS was somewhat cumbersome. Being based around the ZFS file system, you'd use traditional RAID styles (typically RAIDZ1 or RAIDZ2, depending on whether you want to be redundant for up to one or two drive failures). In the past, you'd have to rebuild the array to expand, more or less.\nNow, you can just add an extra drive (or multiple) to an existing RAID, and it'll expand it. You're still limited to only using drives of the same size (or having the RAID be built against the drive with the lowest amount of storage in the pool). However, it is nice that you can expand it easily now.\nThe other thing that TrueNAS has held onto all these years is its price: free. Yep, TrueNAS is completely free (unless you want to go with their enterprise service or their pre-built systems). But, for the average user, TrueNAS is completely and utterly free. This was also another feature that drew me to give it a try again as I spun up a new server.\nSpeaking of, let's talk about that for a moment. The entire reason I dove down the \"TrueNAS vs Unraid\" rabbit hole again is because I recently picked up another storage server. I came across a deal I couldn't pass up on r/homelabsales for a storage server that came with 10 3TB SAS drives (plus a ton of RAM and other features). So, I picked it up to use for my photo storage.\nI really wanted to love and use TrueNAS on the server. It is free, after all. However, after spending a week or so with TrueNAS, I just couldn't. The UI was a bit more clunky than I liked, Docker containers were way more complicated than I was used to, and setting up shares sometimes worked but not always.\nI could have likely spent some time figuring it all out, but at the end of the day, time is also money. And I had already spent a good while trying to get it to work. So, I decided to go with the storage server solution I've been using since 2021: Unraid.\nUnraid Might Cost, but It\u2019s Worth It\nUnraid is very untraditional when it comes to how it functions as a storage server. Going with Unraid instead of TrueNAS had a lot of benefits for me. One of which is that I'm familiar with it since I've been using it for nearly four years. The other is the way it handles storage.\nWhile TrueNAS Scale uses a more traditional RAID style, Unraid uses parity. In short, instead of having to have all the same sized drives, Unraid allows you to use any size drives you want within your array. It simply reserves the largest drive for parity.\nI won't get into how parity works. Unraid has a great explainer of how it works if you want to dive deeper. The main thing to know is that you lose the largest drive (in my primary Unraid server, that's 12TB), and then your storage is made up of all the drives below that. You can have drives the same size as your parity drive or smaller.\nWhile the new server I picked up currently has 10 3TB drives in it, expanding it in the future is what I was mainly thinking about. With TrueNAS, I'd have to keep buying 3TB drives, and if I wanted to move away from 3TB as the storage size, I'd have to replace all the drives. On Unraid? I just simply slot in a larger drive, and away I go.\nRight now, I have a 3TB drive as my parity, which gives me approximately 27TB of available storage. But, if I want to up that, I can easily slot in a 12TB and gain that other 3TB drive back, and now I can put larger drives (up to 12TB) in the array and quickly grow and expand the server. I can remove a 3TB drive and replace it with a 6TB, no problem, for instance.\nI also prefer how Unraid handles caching and moving files around. TrueNAS does offer caching, but it's not done in the same way Unraid does. With Unraid, you can set up a drive as a cache drive where all incoming files go to the drive, and then a mover will move them to the array. I typically will go with 1-2TB NVMe drives on a PCIe adapter for cache, so as files ingress into the server, they hit NVMe first and then eventually migrate to the slower spinning drives.\nUnraid is also much easier for me to use with Docker. While I'm planning to migrate all of my apps and services off of both my Unraid servers and onto a separate apps server, I still wanted the easy ability to spin up something on the server if necessary. TrueNAS has quite a complicated setup when it comes to Docker containers and is also relatively limited in the available containers unless you use third-party repos.\nUnraid, on the other hand, has thousands of containers ready to go out of the box. Most of the containers are pre-configured for you, outside of maybe changing some folder paths or ports. Unraid is much more user-friendly when it comes to just about everything, in my experience.\nHowever, Unraid isn't free like TrueNAS. There's a cost for the license to help with maintaining and updating the operating system. If you only plan to attach six storage devices, then it can be had for as low as $49. You get a year of updates at that price, and it's $36 per year thereafter if you want to continue getting updates. Or, you can just stick with whichever the latest version was when your license ends in perpetuity.\nPersonally, I ended up with the Unleashed license for $109. It lets you have unlimited attached storage devices, but still only comes with a year of updates. I'd like to get the Lifetime license, but at $249, I'm just not ready to spend that right now. I can upgrade in the future from my Unleased license to Lifetime for $149, so it's only $10 more than buying it upfront.\nUnraid Is the Best Solution for Home Servers and NAS Systems\nAt the end of the day, Unraid is just more user-friendly than TrueNAS. There's no doubt that TrueNAS offers greater flexibility with its standard RAID options. And, some might argue (rightfully so), that TrueNAS offers better data protection with things like RAIDZ2 and RAIDZ3.\nHowever, for the most user-friendly home NAS operating system, it's hard not to go with Unraid. The small upfront cost is worth it in the long run. This will be my second Unraid server, and if I ever spun up another storage server, I'd go with Unraid again.\nWith its better docker support, ability to expand drives freely (without having to worry about keeping the same sized drives together), and just its overall easier usability, Unraid wins hands down as my pick for best home storage server operating system in 2025, just like it did back in 2021.\nPlus, Unraid makes it easy to start out small and grow. Most people aren't going to start with 10 drives in an array. If you have six or fewer, it's just $49 for the software, and you'll be good to go. Upgrade from there."
    }
  ],
  "argos_summary": "The article explains how the author migrated a Docker host from an old rack\u2011mount server to a new Ubuntu VM by stopping Docker, tarring the /var/lib/docker directory and any bind\u2011mounted volumes, transferring the archives over a fast 2.5\u202fGb/s network, untarring them on the new machine, and restarting Docker. The process took about three to four hours and required careful handling of GPU runtimes, Home Assistant configuration, and networking settings. The author notes that while the migration was time\u2011consuming, it was straightforward and largely painless, and highlights Docker\u2019s portability and ease of use for running a homelab of over 30 services. The piece also touches on Docker\u2019s broader benefits, such as isolation, quick deployment, and the ability to move containers between hosts without reinstalling applications.",
  "argos_id": "BBWOWYGLT"
}