{
  "url": "https://lifehacker.com/tech/openai-open-weight-model",
  "authorsByline": "Jake Peterson",
  "articleId": "b3c7b846602142619019f70e74963aea",
  "source": {
    "domain": "lifehacker.com",
    "paywall": false,
    "location": null
  },
  "imageUrl": "https://lifehacker.com/imagery/articles/01K25JBX930BREXP3Q5DXZWMCY/hero-image.fill.size_1200x675.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-08T22:00:00+00:00",
  "addDate": "2025-08-08T22:23:36.007473+00:00",
  "refreshDate": "2025-08-08T22:23:36.007475+00:00",
  "score": 1.0,
  "title": "I Ran OpenAI's 'Open-Weight' Model on My Laptop (but I Wouldn't Recommend It)",
  "description": "Before GPT-5, OpenAI made a splash with its open-weight models that could run on a laptop. They technically can run, but I wouldn't recommend it.",
  "content": "All AI eyes might be on GPT-5 this week, OpenAI's latest large language model. But looking past the hype (and the disappointment), there was another big OpenAI announcement this week: gpt-oss, a new AI model you can run locally on your own device. I got it working on my laptop and my iMac, though I'm not so sure I'd recommend you do the same.\n\ngpt-oss is, like GPT-5, an AI model. However, unlike OpenAI's latest and greatest LLM, gpt-oss is \"open-weight.\" That allows developers to customize and fine-tune the model to their specific use cases. It's different from open source, however: OpenAI would have had to include both the underlying code for the model as well as the data the model is trained on. Instead, the company is simply giving developers access to the \"weights,\" or, in other words, the controls for how the model understands the relationships between data.\n\nI am not a developer, so I can't take advantage of that perk. What I can do with gpt-oss that I can't do with GPT-5, however, is run the model locally on my Mac. The big advantage there, at least for a general user like myself, is that I can run an LLM without an internet connection. That makes this perhaps the most private way to use an OpenAI model, considering the company hoovers up all of the data I generate when I use ChatGPT.\n\nThe model comes in two forms: gpt-oss-20b and gpt-oss-120b. The latter is the more powerful LLM by far, and, as such, is designed to run on machines with at least 80GB of system memory. I don't have any computers with nearly that amount of RAM, so no 120b for me. Luckily, gpt-oss-20b's memory minimum is 16GB: That's exactly how much memory my M1 iMac has, and two gigabytes less than my M3 Pro MacBook Pro.\n\nInstalling gpt-oss is surprisingly simple on a Mac: You just need a program called Ollama, which allows you run to LLMs locally on your machine. Once you download Ollama to your Mac, open it. The app looks essentially like any other chatbot you may have used before, only you can pick from a number of different LLMs to download to your machine first. Click the model picker next to the send button, then find \"gpt-oss:20b.\" Choose it, then send any message you like to trigger a download. You'll need a little more than 12GB for the download, in my experience.\n\nAlternatively, you can use your Mac's Terminal app to download the LLM by running the following command: ollama run gpt-oss:20b. Once the download is complete, you're ready to go.\n\nWith gpt-oss-20b on both my Macs, I was ready to put them to the test. I quit almost all of my active programs to put as many resources as possible towards running the model. The only active apps were Ollama, of course, but also Activity Monitor, so I could keep tabs on how hard my Macs were running.\n\nI started with a simple one: \u201cwhat is 2+2?\u201d After hitting return on both keywords, I saw chat bubbles processing the request, as if Ollama was typing. I could also see that the memory of both of my machines were being pushed to the max.\n\nOllama on my MacBook thought about the request for 5.9 seconds, writing \u201cThe user asks: 'what is 2+2'. It's a simple arithmetic question. The answer is 4. Should answer simply. No further elaboration needed, but might respond politely. No need for additional context.\u201d It then answered the question. The entire process took about 12 seconds. My iMac, on the other hand, thought for nearly 60 seconds, writing: \u201cThe user asks: 'what is 2+2'. It's a simple arithmetic question. The answer is 4. Should answer simply. No further elaboration needed, but might respond politely. No need for additional context.\u201d It took about 90 seconds in total after answering the question. That's a long time to find out the answer to 2+2.\n\nNext, I tried something I had seen GPT-5 struggling with: \u201chow many bs in blueberry?\u201d Once again, my MacBook started generating an answer much faster than my iMac, which isn\u2019t unexpected. While still slow, it was coming up with text at a reasonable rate, while my iMac was struggling to get each word out. It took my MacBook roughly 90 seconds in total, while my iMac took roughly 4 minutes and 10 seconds. Both programs were able to correctly answer that there are, indeed, two bs in blueberry.\n\nFinally, I asked both who the first king of England was. I am admittedly not familiar with this part of English history, so I assumed this would be a simple answer. But apparently it\u2019s a complicated one, so it really got the model thinking. My MacBook Pro took two minutes to fully answer the question\u2014it's either \u00c6thelstan or Alfred the Great, depending on who you ask\u2014while my iMac took a whopping 10 minutes. To be fair, it took extra time to name kings of other kingdoms before England had unified under one flag. Points for added effort.\n\nIt's evident from these three simple tests that my MacBook's M3 Pro chip and additional 2GB of RAM crushed my iMac's M1 chip with 16GB of RAM. But that shouldn't give the MacBook Pro too much credit. Some of these answers are still painfully slow, especially when compared to the full ChatGPT experience. Here's what happened when I plugged these same three queries into my ChatGPT app, which is now running GPT-5.\n\u2022 None When asked \"what is 2+2,\" ChatGPT answered almost instantly.\n\u2022 None When asked \"how many bs in blueberry,\" ChatGPT answered in around 10 seconds. (It seems OpenAI has fixed GPT-5's issue here.)\n\u2022 None When asked \"who was the first king of England,\" ChatGPT answered in about 6 seconds.\n\nIt took the bot longer to think through the blueberry question than it did to consider the complex history of the royal family of England.\n\nI'm probably not going to use gpt-oss much\n\nI'm not someone who uses ChatGPT all that much in my daily life, so maybe I'm not the best test subject for this experience. But even if I was an avid LLM user, gpt-oss runs too slow on my personal hardware for me to ever consider using it full-time.\n\nCompared to my iMac, gpt-oss on my MacBook Pro feels fast. But compared to the ChatGPT app, gpt-oss crawls. There's really only one area where gpt-oss shines above the full ChatGPT experience: privacy. I can't help but appreciate that, even though it's slow, none of my queries are being sent to OpenAI, or anyone for that matter. All the processing happens locally on my Mac, so I can rest assured anything I use the bot for remains private.\n\nThat in and of itself might be a good reason to turn to Ollama on my MacBook Pro any time I feel the inkling to use AI. I really don't think I can bother with it on my iMac, except for perhaps reliving the experience of using the internet in the '90s. But if your personal machine is quite powerful\u2014say, a Mac with a Pro or Max chip and 32GB of RAM or more\u2014this might be the best of both worlds. I'd love to see how gpt-oss-20b scales on that type of hardware. For now, I'll have to deal with slow and private.\n\nDisclosure: Ziff Davis, Lifehacker\u2019s parent company, in April filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
  "medium": "Article",
  "links": [
    "https://bsky.app/profile/edzitron.com/post/3lvu7bxaxlk2k",
    "https://lifehacker.com/tech/you-might-have-sent-your-chatgpt-conversations-to-google",
    "https://lifehacker.com/tech/everything-we-know-about-gpt-5-openais-latest-model",
    "https://openai.com/index/introducing-gpt-oss/",
    "https://ollama.com/",
    "https://lifehacker.com/tech/openai-gpt-5-rollout"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "M3 Pro MacBook Pro",
      "weight": 0.0788529
    },
    {
      "name": "MacBook Pro",
      "weight": 0.07188163
    },
    {
      "name": "OpenAI",
      "weight": 0.06159334
    },
    {
      "name": "iMac",
      "weight": 0.061415613
    },
    {
      "name": "gpt",
      "weight": 0.059404586
    },
    {
      "name": "M3 Pro",
      "weight": 0.058896825
    },
    {
      "name": "ChatGPT",
      "weight": 0.05701588
    },
    {
      "name": "Pro",
      "weight": 0.05416774
    },
    {
      "name": "MacBook",
      "weight": 0.053653482
    },
    {
      "name": "my M3 Pro MacBook Pro",
      "weight": 0.050299313
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.6630859375
    },
    {
      "name": "/Reference/General Reference/How-To, DIY & Expert Content",
      "score": 0.52685546875
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.52685546875
    },
    {
      "name": "/Computers & Electronics/Programming/Development Tools",
      "score": 0.390869140625
    },
    {
      "name": "/News/Technology News",
      "score": 0.3759765625
    },
    {
      "name": "/Reference/Technical Reference",
      "score": 0.331298828125
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.322998046875
    }
  ],
  "sentiment": {
    "positive": 0.22546387,
    "negative": 0.20959473,
    "neutral": 0.5649414
  },
  "summary": "OpenAI's 'Open-Weight' AI model, gpt-oss, has been announced as a new AI model that can be run locally on your own device. Unlike its predecessor, OpenAI's latest large language model, this model is \"open-weight\" and allows developers to customize and fine-tune the model for specific use cases. The model comes in two forms: gPT-oss-20b and gpt -oss-120b, which are designed to run on machines with at least 80GB of system memory. However, this allows general users to run an LLM without an internet connection, which is considered the most private way to use an OpenAI model. Installation and use a program called Ollama to download the model, which requires a minimum of 16GB for the download. The author tested the model on both Macs and MacBook, but doubts that it would be recommended for others.",
  "shortSummary": "OpenAI's new \"open-weight\" AI model, gpt-oss-20b, can be run locally on Macs, though it's recommended for other models like GPT-5.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": true,
  "reprintGroupId": "19266183329d44a78ee7de38b24717bd",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://lifehacker.com/tech/openai-gpt-5-rollout",
      "text": "On Thursday, OpenAI officially revealed GPT-5 to the world. The much-hyped presentation was sparse on many specific benchmarks comparing GPT-5 to its past models, but OpenAI's staff was adamant: this model is the best, most knowledgeable, and most powerful one to date.\nGPT-5 has its haters\nMany of the users who have been test driving GPT-5 in the 24 hours since, however, disagree. A visit to r/ChatGPT is enough to see the scope of the situation: The front page is full of posts complaining about the current state of the model, including: \"GPT-5 is the biggest [piece] of garbage even as a paid user,\" \"OpenAI just pulled the biggest bait-and-switch in AI history and I'm done,\" and \"ChatGPT-5 rollout is an unmitigated disaster.\"\nOne of the most prominent complaints concerns OpenAI's decision to deprecate previous models, something the company announced unceremoniously during the GPT-5 presentation. GPT-4o, o3, 4.5, and other models are no longer available to use. Going forward, users will only have access to GPT-5 and its subsequent models (e.g. GPT-5 mini). Many users are upset that OpenAI took away previous models overnight with zero warning, especially when they feel the replacement doesn't offer the same experience. Some have even canceled their subscriptions as a result.\nI know people use ChatGPT for therapy, and I'm aware that people have formed deep attachments to the technology, but I'll admit, I was a bit shocked to read some of the emotional reactions to losing access to these models. In one post, a user detailed how they relied on individual models for different tasks: They'd use 4o for creative ideas, o3 for logic problems, o3-Pro for deep research, and 4.5 for tasks related to writing. Another user talked about how they used 4o to help with their anxiety and depression, as, in their view, the model felt \"human.\" They believe people are grieving the loss of 4o, which tracks, at least with some other 4o-specific posts. There are people out there who really like these models, and are distraught following their removal.\nBut beyond mourning, some users just think GPT-5 isn't very good. If you ask the model how many times the letter \"b\" occurs in the word \"blueberry,\" it reportedly says \"three\": once at the beginning, once in the word \"blue,\" and once in \"berry.\" This isn't necessarily a new problem\u2014LLMs have had trouble spelling \"strawberry\" as well\u2014but its not a great look for OpenAI's \"best\" model ever. One X user highlighted an example of GPT-5's inability to solve a \"simple linear equation,\" versus Google's Gemini 2.5's ability to solve it without issue, while this user posted GPT-5's generation of a map of the United States, with most of the states labeled with gibberish.\nSome users teased OpenAI over its vague benchmarking data. Rhys on X sarcastically posted \"these gpt-5 numbers are insane,\" and attached a graph that charted each GPT version by number (GPT-1 lands at \"1\" on the Y axis, GPT-2 at \"2,\" and so on until you reach GPT-5 at \"5.\"\nThis Tweet is currently unavailable. It might be loading or has been removed.\nThere are also criticisms of auto-switching, one of GPT-5's core features. Free and Plus ChatGPT users aren't able to choose the specific model, but in OpenAI's view, that's a good thing. GPT-5 is supposed to be intelligent enough to pick the right model for you based on your query: simple questions use weaker models, while more complex requests use most powerful models. But if OpenAI is so sure that's a good thing, why does it still offer the ability to manually switch models, so long as you pay $200 per month for a Pro plan?\nNot everyone agrees that GPT-5 is bad, mind you. There are users who appear to be enjoying the model, appreciating the concise responses and fast performance. But the majority of discourse I'm seeing on social media and forums is neutral to negative. Even posts that at first seem positive end up criticizing the model:\nThis Tweet is currently unavailable. It might be loading or has been removed.\n4o lives on, for now\nSince starting this piece, OpenAI has responded to the backlash. CEO Sam Altman posted a series of updates on X that seem to backtrack a bit on the decisions users have criticized most severely: Rate limits will double for ChatGPT Plus users for now; GPT-5 should seem smarter starting today; it will be easy to see which model is answering a given query; and manually choosing the thinking model will be more simple. Altman also acknowledged the initial rollout is going slower than expected, which makes sense since I still don't have access to the new model.\nBut the biggest announcement of the bunch should come as welcome news to many users: 4o is back, at least for Plus users. If you pay $20 a month for ChatGPT, you can keep using 4o for the time being. Altman says the company is watching usage, and will make a decision on how long it will offer legacy models for in the future.\nI'm curious how users respond going forward: Will those who canceled resubscribe to keep using 4o? Then again, why bother, if OpenAI is planning on taking away that model again sometime in the future? One thing's for sure: This likely isn't how OpenAI expected GPT-5's rollout to go.\nDisclosure: Ziff Davis, Lifehacker\u2019s parent company, in April filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems."
    },
    {
      "url": "https://lifehacker.com/tech/you-might-have-sent-your-chatgpt-conversations-to-google",
      "text": "When you start a conversation with ChatGPT, you probably don't expect that chat to end up discoverable in a Google search\u2014but that's exactly what happened for some users. As TechCrunch highlights, if you filtered your search engine results by \"site:https://chatgpt.com/share,\" you could find the transcripts for real conversations people were having with OpenAI's bot\u2014chats you'd think remain private to the account they're associated with were as easy to find as a recipe or tech hack.\nAs one might assume from conversations that weren't meant to be shared publicly, some of these chats contained embarrassing or questionable discourse. TechCrunch said it found a user asking ChatGPT for help rewriting a resume for a specific job application, a job which TechCrunch was able to deduce based on the conversation. Another user asked ChatGPT questions that, according to TechCrunch, \"sound like they came out of an incel forum,\" though the outlet didn't elaborate on the contents of the chat.\nOpenAI's experimental feature\nBefore you panic, there are a couple of caveats to this particular situation. First, OpenAI has since removed the ability to make chats public to search engines, and, from what I can tell, any new searches return zero results for ChatGPT conversations. Any chats you start now do not come with the risk of exposure\u2014at least, not in this capacity. To that point, the exposed chats in question were only discoverable on Google because the users had explicitly opted into that feature. You would have needed to click the \"share\" button on a chat, choose a \"create link\" option, skim past an alert letting you know your name, chat instructions, and messages you add after the fact remain private, then hit a toggle to make that chat discoverable in search.\nSome chats were archived by the Wayback Machine\nFollowing the initial story, sleuths like those from Digital Digging discovered that some of these shared conversations even ended up archived on the Wayback Machine. By their count, 110,000 ChatGPT threads were accessible through this tool. Despite OpenAI's efforts to erase the chats from search, many were still available to skim for anyone in the know and interested. Digital Digging's article dropped on Aug. 1, and, at the time, OpenAI had not issued a take down request to the Wayback Machine concerning the chatgpt.com/share URLs. However, as of Aug. 4, the Wayback Machine says this domain is excluded, so it appears OpenAI did eventually make the request.\nWhy even make this a feature in the first place? OpenAI had this to say to TechCrunch: \"We\u2019ve been testing ways to make it easier to share helpful conversations, while keeping users in control, and we recently ended an experiment to have chats appear in search engine results if you explicitly opted in when sharing.\u201d\nThat's not overly clear, though it's not hard to assume the benefit to OpenAI. The more exposure ChatGPT has, the better it is for the company. And as the internet increasingly moves toward both AI (think AI Overviews) and forum-based answers (think Reddit), I could see OpenAI thinking they have an opportunity to capitalize on the market here. If a user asks ChatGPT a question they think was answered well, perhaps they share it with search engines, so that other users benefit as well. Now, when someone googles that same question, maybe that ChatGPT conversation floats to the top of the search results, right next to the AI Overview or relevant Reddit threads.\nOpenAI also isn't the only company to experiment with public AI conversations. Back in June, we learned that Meta AI also had a function that would allow users to post their questions and generations\u2014not to search, mind you, but to the public Meta AI feed. It seems AI companies are increasingly interested in publicizing AI-generated content, whether that's a conversation you had with a chatbot, or an AI artwork that bot produced.\nChatbots are not private\nYou can now rest easy knowing your ChatGPT conversations won't end up on the front page of Google. However, don't assume that your chats with any bot are generally private. In fact, there's a good chance the company that owns your bot is using your conversations to train their models, or that human reviews will even be able to see your chats.\nDepending on the bot, there are some privacy settings you can enable to protect yourself. ChatGPT's \"Improve the model for everyone\" setting controls whether or not ChatGPT can take your conversations to train their model\u2014though disabling it won't stop the company from storing your chats. Even temporary chats, which don't appear in your history, remain on ChatGPT servers for up to 30 days.\nAs such, you really shouldn't use chatbots for anything sensitive or personal. OpenAI's Sam Altman offered a good reminder of this last month: During an interview with Theo Von, Altman discussed how so many of their users, especially young people, use ChatGPT as a therapist or life coach. Altman said: \"Right now, if you talk to a therapist or a lawyer or a doctor about those problems, there\u2019s legal privilege for it ... We haven\u2019t figured that out yet for when you talk to ChatGPT.\""
    },
    {
      "url": "https://lifehacker.com/tech/everything-we-know-about-gpt-5-openais-latest-model",
      "text": "OpenAI's latest model, GPT-5, is officially here. The company announced the model in a \"longer than usual\" presentation today. The event, which spanned nearly an hour and a half, ran through some of the specific changes that come with GPT-5, as well as some practical use cases, though some of this information was spoiled in a Thursday morning leak.\nIntroducing GPT-5\nFirst things first: OpenAI is clearly hyped about GPT-5, as any company with a new product would be. But some of the language here is interesting. OpenAI CEO Sam Altman likened the different AI models, or GPTs, to people of different ages and skill sets: Altman said GPT-3 is like talking to a high school student; GPT-4 is like talking to a college student; and GPT-5 is like talking to an \"expert,\" a keyword thrown around quite a bit during this presentation.\nAs with any new model, OpenAI says GPT-5 out performs previous GPTs and models on the market across a number of benchmarks, including SWE-bench, Aider Polyglot, MMMU, and AIME 2025. What piqued my interest in particular are OpenAI's claims that GPT-5 has a far lower hallucination rate than previous models. AI models have a bad habit of hallucinating, or, in plain terms, making things up. OpenAI says GPT-5 makes up information less than other models, which will be interesting to put to the test.\nSpeaking of which, the model will be available to test starting today, Aug. 7. What's more, OpenAI is making the full GPT-5 model available to all users\u2014including those who use ChatGPT for free. Free users do have the lowest use limit when it comes to GPT-5, and when they hit it, they'll be bumped to GPT-5-mini, a lightweight version of the model. Plus users, those who pay $20 per month for ChatGPT, have higher limits than free users, but will still be kicked to the mini model when they hit their limits, while Pro users ($200 per month) have no limits, as well as access to GPT-5 Pro. OpenAI didn't share what those limits are in the presentation, though they may reveal that on their website in a separate post.\nOne key feature of GPT-5 is its ability to switch between a thinking and non-thinking model depending on the prompt. If you ask ChatGPT something simple, GPT-5 will respond with a quick, but detailed, answer. Ask it something more complex\u2014say, a request to create something\u2014and the thinking model kicks in. You'll then be able to see the model break down the request step by step, and watch as it reasons through its response. If you pay for ChatGPT, you can access the thinking model from the model picker, but all users can request that GPT-5 think during the prompt.\nOpenAI demonstrators casually announced that GPT-5 deprecates older GPT models, which means you likely won't be able to use older models like GPT-4o or GPT-4.1 going forward. As such, demonstrators asked both GPT-4o and GPT-5 to write a eulogy for older models, showing how GPT-5's writing is less cookie cutter than 4o and earlier. I disagree, though. While GPT-5 might have a bit more personality and flourish than GPT-4o, its writing had plenty of the telltale signs of AI-generated text, including flowery language and awkward similes.\nModels\nAs with previous GPTs, GPT-5 comes in three different models. These include:\nGPT-5: This model is for logic and multi-step tasks.\nGPT-5-mini: This model is not as powerful as GPT-5, and, as such, is designed for \"cost-sensitive\" applications.\nGPT-5-nano: This model is designed to be fast for \"low latency\" applications.\nThis isn't a huge surprise, since the news was leaked in an accidental GitHub post early Thursday morning.\nCoding\nMy take is that OpenAI is particularly happy with GPT-5's coding abilities. Much of the presentation was spent on demoing how the model can help write code, craft programs from scratch, and hunt for bugs. In the first example, a demonstrator asked the bot to create a website to help their partner learn French. They wanted GPT-5 to create three elements: a flashcard section; a quiz; and a game; specifically a recreation of Snake, but with a mouse hunting cheese, and anytime the mouse reached the cheese, the game would speak a word in French out loud.\nSure enough, GPT-5 generated the site. In fact, the demonstrator had it generate three different versions of the site, as a way of showing the variety of GPT-5 outputs. The sites were fine, if not a big plain\u2014though don't get me wrong, it's incredible an LLM can do this at all, in just minutes. I'm not sure we're anywhere close to being able to describe a high-quality website and expect a chatbot to spit it out in a minute, but this doesn't make me want to invest much time in learning how to code.\nIn another example, the demonstrator asked ChatGPT to build a balloon popping game set around a castle on a mountain. Instead of waiting for the bot to generate the game, they pulled up a previously generated game, which means you should take this with a grain of salt. But sure enough, OpenAI demoed a crude 3D game which let you shoot balloons around a castle on a hill. You could even \"chat\" with the soldiers and characters around the castle, though in practice it was more of a simple chatbot function with different personalities.\nDeception and safety\nOpenAI touched on GPT-5's deceptions, or, in other words, its ability to lie. This is different from hallucinations, as deceptions seem more deliberate\u2014rather than simply making up information, the bot actively attempts to deceive you, perhaps in response to news that you're shutting it down. Scary stuff.\nOpenAI claims that GPT-5 is less deceptive than both o3 and o4-mini. Plus, if you ask it a question that would normally be refused by a previous model due to safety concerns, it'll try to answer the question in a impartial or objective way\u2014in an attempt to service those who are asking legitimate questions. If it must refuse, it'll give you a detailed answer as to why.\nHealth\nSam Altman was particularly keen on hyping up GPT-5's health capabilities. Altman called GPT-5 the \"best model ever for health,\" and that it scores higher than any previous model on OpenAI's HealthBench benchmark.\nAltman then brought out a couple to discuss their use of ChatGPT in navigating a difficult and complex cancer diagnosis. While most of their experience was using previous ChatGPT models, the couple did note that GPT-5 was particularly helpful in going deeper on their questions\u2014giving them additional information, including questions they should ask the doctor or what to expect next.\nOther new features\nDemonstrators also touched on Voice Mode, but not for long. The major demo was of Study and Learn Mode, which supposedly helps you learn subjects using Voice Mode. They demoed this by asking the bot to help them learn Korean, specifically to slow down its speech so they could hear each individual word clearly. The bot did, though it didn't slow down quite enough for me, someone who does not speak Korean, to understand. Once GPT-5 hits my ChatGPT account, I'll be curious to see if you can ask the bot to speak even slower.\nAdditional, smaller features announced include the ability to customize the color of your ChatGPT conversations, as well as Gmail and Google Calendar integration. The latter gives me privacy concerns, but I imagine any Google Workspace users who also rely on ChatGPT will enjoy this collaboration.\nDisclosure: Lifehacker\u2019s parent company, Ziff Davis, filed a lawsuit against OpenAI in April, alleging it infringed Ziff Davis copyrights in training and operating its AI systems."
    }
  ],
  "argos_summary": "OpenAI has announced gpt-oss, a new AI model that can be run locally on personal devices, offering users greater privacy compared to its latest model, GPT-5. While gpt-oss allows for customization through open weights, it is slower than GPT-5, which has received mixed reviews since its launch, with many users expressing disappointment over its performance and the removal of previous models. Despite the criticisms, OpenAI's CEO has acknowledged user feedback and reinstated access to older models for Plus subscribers, indicating ongoing adjustments in response to user concerns.",
  "argos_id": "U708PBONI"
}