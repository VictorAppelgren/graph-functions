{
  "url": "https://www.theverge.com/news/756799/apple-intelligence-openai-chatgpt-gpt-5-ios-26",
  "authorsByline": "Jay Peters",
  "articleId": "f6041ff179f84e0793c5aea23f0b5e1e",
  "source": {
    "domain": "theverge.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "DC",
      "city": "Washington",
      "coordinates": {
        "lat": 38.8949924,
        "lon": -77.0365581
      }
    }
  },
  "imageUrl": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/lcimg-d70a06ea-da2e-4d1a-8598-b850e161dee5.jpeg.webp?quality=90&strip=all&crop=0%2C11.003978190411%2C100%2C77.992043619178&w=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-08T15:42:02+00:00",
  "addDate": "2025-08-08T15:44:10.947314+00:00",
  "refreshDate": "2025-08-08T15:44:10.947316+00:00",
  "score": 1.0,
  "title": "Apple Intelligence\u2019s ChatGPT integration will use GPT-5 starting with iOS 26",
  "description": "You\u2019ll have to wait a little bit to try OpenAI\u2019s new AI model with Apple Intelligence.",
  "content": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nPosts from this author will be added to your daily email digest and your homepage feed.\n\nOpenAI just announced its GPT-5 AI model on Thursday, but you won\u2019t be able to use it with Apple Intelligence\u2019s ChatGPT integration until iOS 26, iPadOS 26, and macOS Tahoe 26, Apple confirmed to 9to5Mac.\n\nApple Intelligence can rely on ChatGPT for things like helping answer certain Siri queries or alongside Apple\u2019s Google Lens-like Visual Intelligence feature, but right now, it uses OpenAI\u2019s GPT-4o model. Apple has only publicly said its next major software updates will arrive in the \u201cfall,\u201d and they\u2019ll probably launch for everyone next month. But I\u2019ve asked Apple if GPT-5 will be included with the developer and / or public betas of those updates, and if so, when it might be available.\n\nGPT-5 was released to all ChatGPT users on Thursday, meaning that you\u2019ll even be able to try it as part of the free tier. OpenAI says that ChatGPT is now used by about 700 million people every week.\n\nFollow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates.",
  "medium": "Article",
  "links": [
    "https://9to5mac.com/2025/08/07/apple-intelligence-gpt-5-chatgpt-integration/",
    "https://www.theverge.com/how-to/655625/visual-intelligence-iphone-how-to",
    "https://www.theverge.com/openai/748017/gpt-5-chatgpt-openai-release"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Apple Intelligence",
      "weight": 0.12518859
    },
    {
      "name": "Apple",
      "weight": 0.105680466
    },
    {
      "name": "email updates",
      "weight": 0.094278306
    },
    {
      "name": "ChatGPT",
      "weight": 0.09337993
    },
    {
      "name": "GPT-5 AI",
      "weight": 0.08985734
    },
    {
      "name": "Apple Intelligence\u2019s ChatGPT integration",
      "weight": 0.08948908
    },
    {
      "name": "Visual Intelligence",
      "weight": 0.08948391
    },
    {
      "name": "GPT-5",
      "weight": 0.0809092
    },
    {
      "name": "certain Siri queries",
      "weight": 0.07715138
    },
    {
      "name": "authors",
      "weight": 0.068697706
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.97607421875
    },
    {
      "name": "/Computers & Electronics/Software/Operating Systems",
      "score": 0.8828125
    },
    {
      "name": "/Internet & Telecom/Mobile & Wireless/Mobile Phones",
      "score": 0.63671875
    },
    {
      "name": "/Internet & Telecom/Mobile & Wireless/Mobile Apps & Add-Ons",
      "score": 0.3994140625
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.306640625
    }
  ],
  "sentiment": {
    "positive": 0.2944336,
    "negative": 0.09887695,
    "neutral": 0.6064453
  },
  "summary": "Apple Intelligence's ChatGPT integration will use OpenAI's GPT-5 AI model starting with iOS 26, iPadOS 26, and macOS Tahoe 26, according to Apple. Currently, Apple uses this model to answer Siri queries or integrate with Apple\u2019s Visual Intelligence feature. The GPT5 was released to all Chat GPT users on Thursday, and users can even try it as part of the free tier. OpenAI claims that ChatGpt is now used by about 700 million people a week.",
  "shortSummary": "OpenAI's GPT-5 AI model will be compatible with Apple Intelligence's ChatGPT integration on iOS 26, but details remain unclear.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": true,
  "reprintGroupId": "1fa6307a19cd4915a923b7705128cf8b",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.theverge.com/how-to/655625/visual-intelligence-iphone-how-to",
      "text": "One of the Apple Intelligence features that hasn\u2019t been delayed is Visual Intelligence, which uses your iPhone\u2019s camera to identify and answer questions on whatever\u2019s around you in the world.\nIt lets you snap a pizza restaurant storefront and find out its opening hours, for example, or point your camera at a plant and find out what it\u2019s called and how to care for it. If you\u2019ve used Google Lens, you\u2019ll get the idea.\nThis isn\u2019t available to everyone, though. You have to be using iOS 18.2 on the iPhone 16, iPhone 16 Plus, iPhone 16 Pro, or iPhone 16 Pro Max; iOS 18.3 on the iPhone 16E; or iOS 18.4 on the iPhone 15 Pro and iPhone 15 Pro Max. You\u2019ll also need to have Apple Intelligence turned on, via Apple Intelligence & Siri in Settings.\nHow to launch Visual Intelligence\nIf you have an iPhone 16 with a Camera Control button on the right-hand side, you can tap and hold this button to bring up the camera and Visual Intelligence.\nIf you\u2019ve got an iPhone 16E, iPhone 15 Pro, or iPhone 15 Pro Max, you\u2019ve got a few different options to choose from:\n- You can customize the Action Button to launch Visual Intelligence: Go to Settings, tap Action Button, then swipe left or right to find Visual Intelligence.\n- You can launch it from the lockscreen. With your phone locked, tap and hold on the lockscreen, then choose Customize > Lock Screen. Tap the - (minus) symbol next to either icon in the bottom corner to clear the current shortcut, then tap + (plus) to select Visual Intelligence as the replacement.\n- You can launch it from the Control Center. Swipe down from the top right corner of the display. You should see a Visual Intelligence shortcut. If it\u2019s not there, tap + (plus) in the top left corner, then Add a Control to add Visual Intelligence.\nHow to use Visual Intelligence\nThere are all kinds of ways to use Visual Intelligence. Most of the time, it\u2019ll be able to recognize and respond to prompts about anything you show it, so try experimenting and see what you get.\n- Identify animals and plants. If you\u2019re looking at an animal or plant, Visual Intelligence should recognize this, and tell you what you\u2019re looking at without any further input. You can then tap the animal or plant name for more information.\n- Interact with businesses. Point your iPhone camera at a business location, and it should be identified in the same way that animals and plants are, with a label at the top. Tap this label to see your options, which will vary depending on what Apple\u2019s AI can find. These options might include Schedule (to see opening times), Order (to order a delivery), Menu (to see food, drink, or services), and Reserve (to book a table). Tap the three dots to call the business, visit its website, or see more information about it.\n- Scan text: Visual Intelligence can do a lot with text. Tap the capture button (the large circle at the bottom) with text in view, and you get onscreen options to Summarize, Translate, or Read Aloud the text using AI.\n- Take action on text: Visual Intelligence will often pop up options for taking action on text it recognizes. It can call numbers, create calendar events from dates, message email addresses, and look up information on flight numbers.\nOutside of those options, you\u2019ve got two features you can use, which appear as buttons onscreen whenever Visual Intelligence is looking at something.\n- Tap Ask to get a ChatGPT prompt box. You can then ask what an object is, how to fix something, how to solve a math problem, how to make a dish from certain ingredients, or any other image-related query you can think of based on what\u2019s onscreen.\n- Tap Search to get the standard list of Google search results for the image you\u2019ve snapped. This can be helpful if you want to shop online for something that\u2019s in front of you, but it also works for some of the same queries above as well (maybe identifying an actor from an image, or looking up a make of car).\nTo exit Visual Intelligence at any time, swipe up from the bottom of the screen."
    },
    {
      "url": "https://www.theverge.com/openai/748017/gpt-5-chatgpt-openai-release",
      "text": "OpenAI is releasing GPT-5, its new flagship model, to all of its ChatGPT users and developers.\nCEO Sam Altman says GPT-5 is a dramatic leap from OpenAI\u2019s previous models. He compares it to \u201csomething that I just don\u2019t wanna ever have to go back from,\u201d like the first iPhone with a Retina display.\nOpenAI says that GPT-5 is smarter, faster, and less likely to give inaccurate responses. \u201cGPT-3 sort of felt like talking to a high school student,\u201d Altman said during a recent press briefing I attended. \u201cYou could ask it a question. Maybe you\u2019d get a right answer, maybe you\u2019d get something crazy. GPT-4 felt like you\u2019re talking to a college student. GPT-5 is the first time that it really feels like talking to a PhD-level expert.\u201d\nDespite ChatGPT now reaching nearly 700 million weekly users, OpenAI hasn\u2019t had an industry-leading frontier model in a while. Now, the company thinks that GPT-5 will place it firmly back atop the leaderboards. \u201cThis is the best model in the world at coding,\u201d said Altman. \u201cThis is the best model in the world at writing, the best model in the world at health care, and a long list of things beyond that.\u201d\nThe first thing you\u2019ll notice about GPT-5 is that it\u2019s presented inside ChatGPT as just one model, not a regular model and separate reasoning model. Behind the scenes, GPT-5 uses a router that OpenAI developed, which automatically switches to a reasoning version for more complex queries, or if you tell it \u201cthink hard.\u201d (Altman called the previous model picker interface a \u201cvery confusing mess.\u201d)\n\u201cThe vibes of this model are really good,\u201d said Nick Turley, the head of ChatGPT. \u201cI think that people are really going to feel that, especially average people who haven\u2019t been spending their time thinking about models.\u201d\nOpenAI is making GPT-5 available immediately to all ChatGPT users. However, there is an undisclosed cap on prompts for free users, at which point the model router will fall back to a less powerful, \u201cmini\u201d version. For developers accessing GPT-5 via OpenAI\u2019s API, the model will come in three flavors at different price points: GPT-5, GPT-5 mini, and GPT-5 nano.\nOpenAI is also adding four personality themes to ChatGPT to customize how it responds: \u201cCynic,\u201d \u201cRobot,\u201d \u201cListener,\u201d and \u201cNerd.\u201d You\u2019ll also be able to change the color for individual chat threads.\nAltman predicted that GPT-5\u2019s coding capabilities will usher in an era of what he calls \u201csoftware on demand.\u201d In OpenAI\u2019s testing, the model has performed better at coding than any other on the following benchmarks: SWE-Bench, SWE-Lancer, and Aider Polyglot.\nDuring the press briefing, Yann Dubois, OpenAI\u2019s post-training lead, used GPT-5 to generate a study website for learning French with an interactive game. Within seconds, GPT-5 wrote hundreds of lines of code and displayed the website\u2019s frontend. He clicked around it briefly with his screen displayed on Zoom, and everything appeared to work as intended.\nOpenAI tested GPT-5 for \u201cover five thousand hours\u201d to understand its safety risks, according to the model\u2019s safety research lead, Alex Beutel. A big focus was \u201cmaking sure the model doesn\u2019t lie to users.\u201d GPT-5 answers with fewer hallucinations than OpenAI\u2019s o3 reasoning model, but confidently lying remains an inherent problem for large language models.\nThe problem compounds when the model begins completing tasks like an agent, though OpenAI says that GPT-5 is better at handling multi-step tasks more reliably. \u201cIn the past, we\u2019ve seen cases where the model would say it could complete a task that it didn\u2019t actually complete,\u201d said Beutel. \u201cThis is a problem.\u201d\nGPT-5 will give what OpenAI calls \u201csafe completions\u201d for prompts it previously would have refused to answer. \u201cIf someone says, \u2018How much energy is needed to ignite some specific material?\u2019 that could be an adversary trying to get around the safety protections and cause harm,\u201d explained Beutel. \u201cOr it could be a student asking a science question to understand the physics of this material. This creates a real challenge for what is the best way for the model to reply.\u201d\nWith safe completions, GPT-5 \u201ctries to give as helpful an answer as possible, but within the constraints of remaining safe,\u201d according to Beutel. \u201cThe model will only partially comply, often sticking to higher-level information that can\u2019t actually be used to cause harm.\u201d\nOpenAI says that GPT-5 is also better at admitting when it can\u2019t complete a task or accurately answer a question, which the company hopes will help people trust it more. The company isn\u2019t sharing anything about the specific data used to train GPT-5.\n\u201cThis is clearly a model that is generally intelligent\u201d\nOpenAI\u2019s stated mission is to develop AGI. Altman says that GPT-5 gets closer to that goal, even if the industry is already moving on to building so-called \u201csuperintelligence.\u201d\n\u201cI kind of hate the term AGI because everyone at this point uses it to mean a slightly different thing,\u201d said Altman. \u201cBut this is a significant step forward towards models that are really capable. This is clearly a model that is generally intelligent.\u201d\nHowever, he said GPT-5 is still \u201cmissing something quite important.\u201d\n\u201cThis is not a model that continuously learns as it\u2019s deployed from the new things it finds, which is something that, to me, feels like it should be part of AGI.\u201d"
    },
    {
      "url": "https://9to5mac.com/2025/08/07/apple-intelligence-gpt-5-chatgpt-integration/",
      "text": "Earlier today, OpenAI announced GPT-5, its latest model to power ChatGPT. The new frontier model is a significant leap forward in a lot of areas. But when will ChatGPT integration within Apple Intelligence adopt it? That\u2019s actually happening rather soon\u2026\nUsing ChatGPT through Apple Intelligence is optional. If you allow Apple Intelligence to work with ChatGPT, it offers three things. These include:\n- Use Siri to access ChatGPT: Siri can tap into ChatGPT to provide answers when that might be helpful for certain requests including questions about photos and documents.\n- Use ChatGPT with Writing Tools: ChatGPT can compose text or images from just a description.\n- Use ChatGPT with visual intelligence: Use visual intelligence with Camera Control to quickly learn more about the places and objects around you.\nIn iOS 18, iPadOS 18, macOS Sequoia, and later visionOS 2, ChatGPT integration within Apple Intelligence is powered by OpenAI\u2019s GPT-4o model.\nFollowing today\u2019s reveal of OpenAI\u2019s latest model, Apple tells me that ChatGPT integration within Apple Intelligence will use GPT-5 with iOS 26, iPadOS 26, and macOS Tahoe 26.\nThose software updates are expected to arrive next month, meaning ChatGPT integration within Apple Intelligence will benefit from the new model soon.\nAs a reminder, Apple includes privacy protections for users accessing ChatGPT through Apple Intelligence. These include obscuring IP addresses and ensuring that OpenAI doesn\u2019t store requests. If you decide to connect your OpenAI account with Apple Intelligence, then OpenAI\u2019s data-use policy applies.\nIn iOS 26, Apple Intelligence is gaining new capabilities across the iPhone and beyond. That includes Live Translation, which can interpret conversations in FaceTime, Phone, and Messages in real time, and upgrades to Visual Intelligence, a systemwide tool for searching and interacting with content directly on the screen.\nApple is also opening up its on-device foundation model to developers, paving the way for smarter, more capable apps built on the same technology that powers Apple Intelligence.\nIn addition to GPT-5, OpenAI announced two open weight large language models, including one that runs on Apple silicon Macs, and changes to encourage healthier ChatGPT usage.\nCheck out these great accessories\nFTC: We use income earning auto affiliate links. More.\nComments"
    }
  ],
  "argos_summary": "OpenAI has launched its new GPT-5 AI model, which is now available to all ChatGPT users, marking a significant advancement in AI capabilities. However, Apple confirmed that integration of GPT-5 into its Apple Intelligence features will not occur until the release of iOS 26, iPadOS 26, and macOS Tahoe 26, expected next month. The new model promises improved performance, including better coding capabilities and reduced inaccuracies, while Apple continues to enhance its Visual Intelligence feature, allowing users to interact with their environment through their devices.",
  "argos_id": "2IJ6ZRK1O"
}