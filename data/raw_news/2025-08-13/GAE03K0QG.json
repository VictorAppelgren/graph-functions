{
  "url": "https://appleinsider.com/articles/25/08/13/apple-machine-learning-privacy-workshop-talks-help-promote-ai-security",
  "authorsByline": "Malcolm Owen",
  "articleId": "c52fff2b09f94127b29bada0dbc593e1",
  "source": {
    "domain": "appleinsider.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "CA",
      "county": "Santa Clara County",
      "city": "Cupertino",
      "coordinates": {
        "lat": 37.3228934,
        "lon": -122.0322895
      }
    }
  },
  "imageUrl": "https://photos5.appleinsider.com/gallery/64708-134814-applemlprivacyworkshop-xl.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-13T12:17:44+00:00",
  "addDate": "2025-08-13T12:30:13.005247+00:00",
  "refreshDate": "2025-08-13T12:30:13.005249+00:00",
  "score": 1.0,
  "title": "Apple machine learning privacy workshop talks help promote AI security",
  "description": "Apple has shared recordings of talks from its workshop about privacy and machine learning, demonstrating how it is considering how to protect user data while it is processed using AI.",
  "content": "Apple has shared recordings of talks from its workshop about privacy and machine learning, demonstrating how it is considering how to protect user data while it is processed using AI.\n\nApple has repeatedly insisted that it is a privacy-forward company, including in its artificial intelligence and machine learning efforts. Following a workshop on privacy preservation in machine learning, Apple has shared details and published work that was presented at the event.\n\nThe workshop on Privacy-Preserving Machine Learning (PPML) was a two-day event earlier in 2025. It played host to researchers both within and outside of Apple, to discuss PPML in general.\n\nThe presentations list participants from various universities, as well as Google Research, Google DeepMind, and Microsoft Research.\n\nThe workshop focused on four areas: Private Learning and Statistics, Attacks and Security, Differential Privacy Foundations, and Foundation Models and Privacy.\n\nApple explains that the presentations and discussions were to explore the intersection of privacy, security, and the AI landscape as it evolves. There were discussions about the challenges of building AI systems with privacy protections.\n\nThe privacy discussions aim to \"foster innovation while safeguarding user privacy,\" writes Apple.\n\nApple's published presentations and work from the event cover quite a number of areas in the field of AI and privacy.\n\nOne topic that was discussed multiple times was ways to protect users more directly, such as creating privacy-conscious conversational agents. Due to the potential threat of malicious actors taking advantage of the contextual knowledge skills of chatbots, the AirGapAgent is proposed as a way to prevent leaks via limited data access.\n\nThe paper claims that it is powerful than currently available agents. A \"single-query context hijacking attack\" on a Gemini Ultra agent reduced its protectiveness of user data from 94% to 45%, while AirGapAgent maintained a 97% rate.\n\nSimilar in concept, \"User Inference Attacks on Large Language Models\" discusses how a malicious actor can determine if the responses from an LLM are fine-tuned using a user's data. If so, the paper asks what can be discovered and how such an attack could be defended.\n\nAnother presented a scalable private search system called Wally, which supports efficient semantic and keyword queries. The paper discusses how the system can do better at scale than others, which can get bogged down due to the processing-intensive cryptographic operations used for each database entry.\n\nOther talks include \"A Generalized Binary Tree Mechanism for Differentially Private Approximation of All-Pair Distances,\" \"Nearly Tight Black-Box Auditing of Differentially Private Machine Learning,\" and \"Elephants Do Not Forget: Differential Privacy with State Continuity for Privacy Budget.\"\n\nThis is not the first workshop Apple has held dedicated to machine learning subjects. In 2024, it held workshops on \"Human-Centred Machine Learning,\" and released talks from it in July 2025.\n\nThe release of published papers from a privacy-focused workshop is also quite apt, considering the constant criticism the machine learning industry has to deal with.\n\nIn July, Apple had to insist that its AI training is ethical, in that it won't scrape data from sources if the publisher doesn't agree to the practice.\n\nHowever, in August, AI startup Perplexity was revealed to be actively working around restrictions like robots.txt. A report determined that it used a second browser agent to crawl webpages, even if robots.txt said it could not.\n\nApple's own efforts in machine learning are seemingly faltering in public, with extended delays affecting the long-awaited upgrade of Siri under Apple Intelligence.\n\nBy continuing to push its message that privacy is paramount and demonstrating that it is walking the walk, Apple at least shows its work in the field is at least as ethical as possible compared to rivals.\n\nEven if it's quite late in comparison.",
  "medium": "Article",
  "links": [
    "https://appleinsider.com/inside/siri",
    "https://appleinsider.com/articles/25/07/21/how-apple-made-ai-in-ios-26-more-helpful-more-private",
    "https://machinelearning.apple.com/research/wally-search",
    "https://machinelearning.apple.com/updates/ppml-2025",
    "https://appleinsider.com/articles/25/08/05/perplexity-defensive-over-ignoring-robotstxt-and-stealing-data",
    "https://appleinsider.com/inside/apple-intelligence",
    "https://arxiv.org/abs/2405.05175",
    "https://appleinsider.com/articles/25/07/21/apple-insists-its-ai-training-is-ethical-and-respects-publishers",
    "https://appleinsider.com/articles/25/07/29/apples-delayed-siri-update-spawns-another-securities-lawsuit",
    "https://arxiv.org/pdf/2310.09266",
    "https://appleinsider.com/articles/25/07/25/apple-researchers-take-aim-at-ai-hallucinations-and-true-conversations"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Apple machine learning privacy workshop talks",
      "weight": 0.11495489
    },
    {
      "name": "user privacy",
      "weight": 0.099298425
    },
    {
      "name": "machine learning",
      "weight": 0.087679505
    },
    {
      "name": "Differential Privacy",
      "weight": 0.087658174
    },
    {
      "name": "privacy",
      "weight": 0.084255755
    },
    {
      "name": "Privacy Budget",
      "weight": 0.083940394
    },
    {
      "name": "privacy preservation",
      "weight": 0.08375154
    },
    {
      "name": "machine learning subjects",
      "weight": 0.08270259
    },
    {
      "name": "privacy protections",
      "weight": 0.082172446
    },
    {
      "name": "user data",
      "weight": 0.079872
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.9892578125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.87158203125
    },
    {
      "name": "/Computers & Electronics/Computer Security/Network Security",
      "score": 0.54541015625
    },
    {
      "name": "/People & Society/Social Issues & Advocacy/Other",
      "score": 0.501953125
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.427978515625
    }
  ],
  "sentiment": {
    "positive": 0.15229739,
    "negative": 0.30802497,
    "neutral": 0.5396776
  },
  "summary": "Apple has shared recordings of talks from a workshop about privacy and machine learning, which is part of its efforts to protect user data while it is processed using AI. The event, which included researchers from various universities, Google Research, Google DeepMind, and Microsoft Research, focused on four areas: Private Learning and Statistics, Attacks and Security, Differential Privacy Foundations, and Foundation Models and Privacy. The discussions also included discussions about the challenges of building AI systems with privacy protections. One topic was discussed multiple times: ways to protect users more directly, such as creating privacy-conscious conversational agents. Other talks included a scalable private search system called Wally, which can do better at scale than others due to the processing-intensive cryptographic operations used for each database entry. This is not the first workshop dedicated to machine learning subjects.",
  "shortSummary": "Apple shared details and published work from a privacy workshop on AI, aiming to foster innovation while safeguarding user data.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "9ffb55aeea254c8f90791ad838083250",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://machinelearning.apple.com/research/wally-search",
      "text": "Scalable Private Search with Wally\nAuthorsHilal Asi, Fabian Boemer, Nicholas Genise, Muhammad Haris Mughees, Tabitha Ogilvie, Rehan Rishi, Guy N. Rothblum, Kunal Talwar, Karl Tarbe, Ruiyu Zhu, Marco Zuliani\nAuthorsHilal Asi, Fabian Boemer, Nicholas Genise, Muhammad Haris Mughees, Tabitha Ogilvie, Rehan Rishi, Guy N. Rothblum, Kunal Talwar, Karl Tarbe, Ruiyu Zhu, Marco Zuliani\nThis paper presents Wally, a private search system that supports efficient semantic and keyword search queries against large databases. When sufficiently many clients are making queries, Wally\u2019s performance is significantly better than previous systems. In previous private search systems, for each client query, the server must perform at least one expensive cryptographic operation per database entry. As a result, performance degraded proportionally with the number of entries in the database. In Wally, we remove this limitation. Specifically, for each query the server performs cryptographic operations against only a few database entries. We achieve these results by requiring each client to add a few fake queries, and send each query via an anonymous network to the server at independently chosen random instants. Additionally, each client also uses somewhat homomorphic encryption (SHE) to hide whether a query is real or fake. Wally provides (\u03b5, \u03b4) -differential privacy guarantee, which is an accepted standard for strong privacy. The number of fake queries each client makes depends inversely on the number of clients making queries. Therefore, the fake queries\u2019 overhead vanishes as the number of clients increases, enabling scalability to millions of queries and large databases. Concretely, Wally can process eight million queries in 117 minutes, or just under two hours. That is around four orders of magnitude faster than the state of the art.\nOctober 24, 2024research area Privacy\nAt Apple, we believe privacy is a fundamental human right. Our work to protect user privacy is informed by a set of privacy principles, and one of those principles is to prioritize using on-device processing. By performing computations locally on a user\u2019s device, we help minimize the amount of data that is shared with Apple or other entities. Of course, a user may request on-device experiences powered by machine learning (ML) that can be enriched...\nJune 17, 2024research area Knowledge Bases and Search, research area Speech and Natural Language Processingconference SIGIR\nThis paper was accepted in the Industry Track at SIGIR 2024.\nVirtual Assistants (VAs) are important Information Retrieval platforms that help users accomplish various tasks through spoken commands. The speech recognition system (speech-to-text) uses query priors, trained solely on text, to distinguish between phonetically confusing alternatives. Hence, the generation of synthetic queries that are similar to existing VA usage can greatly improve..."
    },
    {
      "url": "https://appleinsider.com/articles/25/07/21/apple-insists-its-ai-training-is-ethical-and-respects-publishers",
      "text": "In a new research paper, Apple doubles down on its claim of not training its Apple Intelligence models on anything scraped illegally from the web.\nIt's a fair bet that Artificial Intelligence systems have been scraping every part of the web they can access, whether or not they should. In 2023, both OpenAI and Microsoft were sued by the New York Times for copyright infringement, and that was far from the only suit.\nWhereas also in 2023, Apple was reported to have attempted to buy the rights to train its large language models (LLMs) on work from publishers including Conde Nast, and NBC News. Apple was said to have offered publishers millions of dollars, although it was not clear at the time which had agreed or disagreed.\nNow in a newly published research paper, Apple says that if a publisher does not agree to its data being scraped for training, Apple won't scrape it.\nApple details its ethics\n\"We believe in training our models using diverse and high-quality data,\" says Apple. \"This includes data that we've licensed from publishers, curated from publicly available or open-sourced datasets, and publicly available information crawled by our web-crawler, Applebot.\"\n\"We do not use our users' private personal data or user interactions when training our foundation models, it continues. \"Additionally, we take steps to apply filters to remove certain categories of personally identifiable information and to exclude profanity and unsafe material. \"\nMost of the research paper is concerned with how Apple goes about doing this scraping, and specifically how its internal Applebot system ensures getting useful information despite \"the noisy nature of the web.\" But it does return to the overall issues regarding copyright, and each time insists that Apple is respecting rights holders.\n\"[We] continue to follow best practices for ethical web crawling, including following widely-adopted robots. txt protocols to allow web publishers to opt out of their content being used to train Apple's generative foundation models,\" says Apple. \"Web publishers have fine-grained controls over which pages Applebot can see and how they are used while still appearing in search results within Siri and Spotlight.\"\nThe \"fine-grained controls\" appear to be based around the long-standing robots.txt system. That is not any kind of standard privacy system, but it is widely adopted and involves publishers including a text file called robots.txt on their sites.\nIf an AI system sees that file, it is supposed to not scrape the site or specific pages that the file details. It's as simple as that.\nWhat companies say and what they do\nIt's easy to say that a company's AI systems will respect robots.txt, and OpenAI implies \u2014 but only implies \u2014 that it does too.\n\"Decades ago, the robots.txt standard was introduced and voluntarily adopted by the Internet ecosystem for web publishers to indicate what portions of websites web crawlers could access,\" said OpenAI in a May 2024 blog post called \"Our approach to data and AI.\"\n\"Last summer,\" it continued, \"OpenAI pioneered the use of web crawler permissions for AI, enabling web publishers to express their preferences about the use of their content in AI. We take these signals into account each time we train a new model.\"\nEven that last part about taking signals into account is not the same as saying OpenAI respects these signals. Then that key paragraph about signals directly follows the one about robots.txt, but does not explicitly say it pays any attention.\nAnd seemingly a great many AI companies do not adhere to any robots.txt instructions. Market analysis firm TollBit said that in March 2025, there were over 26 million disallowed scrapes where AI firms ignored robots.txt entirely.\nThe same firm also reports that the number is rising. In Q4 2024, 3.3% of AI scrapes ignored robots.txt, and in Q1 2025 it was around 13%.\nWhile TollBit does not speculate on the reasons for this, it's likely that the entire available internet has already been scraped. So the companies are pressing on, and in June 2025, a US District Court said they could.\nRobots.txt is more than a simple no\nWhen any AI system attempts to scrape a website, it identifies itself. So when Google does it, the site registers that Googlebot is accessing it, and returns a comprehensive list of permissions.\nThat list comprises which sections of the site the bot is not allowed to access. When Apple's system, Applebot, was revealed in 2015, Apple said that if a site doesn't recognize it, Applebot would follow any guidelines included for Googlebot.\nThe BBC said in 2023 that \"we have taken steps to prevent web crawlers like those from OpenAI and Common Crawl from accessing BBC websites.\" Around the same time, a study of 1,156 news publishers found that 626 had blocked AI scraping, including that by OpenAI and Google AI.\nBut a company changed the name of its scraping tool, and it can just ignore blocks \u2014 or at least be accused of doing so.\nPerplexity.ai \u2014 which Apple is repeatedly rumored to be buying \u2014 marketed itself as an ethical AI too, with a detailed blog post about why ethics are so necessary.\nBut that was published in November 2024, and in the June before it, Forbes threatened Perplexity over it having scraped anyway. Perplexity CEO Aravind Srinivas later admitted to its search and scraping having some \"rough edges.\"\nApple stands out in AI\nUnless Apple's claims on ethical AI training are challenged legally, as Forbes at least started to do with Perplexity.ai, we will never know if they are true.\nBut OpenAI has been sued over this, Microsoft has, and Perplexity has been called out for doing it. So far, no one has claimed Apple has done anything unethical.\nThat's not the same thing as publishers being happy with any firm training its LLMs on the data, but so far, Apple may be the only one doing it all legally."
    },
    {
      "url": "https://appleinsider.com/articles/25/07/21/how-apple-made-ai-in-ios-26-more-helpful-more-private",
      "text": "A recent machine learning update from Apple reveals how iOS 26 brings faster, safer AI that was trained without your texts, your photos, or your permission.\nApple has overhauled its foundation models for iOS 26 to be faster, safer, and more private. The system behind features like Genmoji, Writing Tools, and image understanding is now trained with a sharper focus on data quality, cultural awareness, and user protection.\nThe company recently published a detailed Machine Learning Journal report outlining how it trains and upgrades the models behind Apple Intelligence. Dive into the paper for nuanced details and specs on the artificial intelligence training.\nThese are the key takeaways from that report, focusing on privacy, data sourcing, and responsible AI behavior in iOS 26.\nApplebot powers smarter AI\nApple's training pipeline starts with Applebot, the company's web crawler. It collects data from sites that allow it, pulling in pages from across the internet in multiple languages. But it's not scraping everything it finds.\n\"We continue to source a significant portion of the pre-training data for our models from web content crawled by Applebot,\" Apple wrote in the report. \"These data span hundreds of billions of pages and cover an extensive range of languages, locales, and topics.\"\nApplebot prioritizes clean, structured web pages and uses signals like language detection and topic analysis to filter out junk. It also handles complex websites by simulating full-page loading and running JavaScript.\nThat allows it to gather content from modern pages that rely on interactive design. The goal is to collect useful, high-quality material without ever touching your private information.\nThis is Apple's answer to the data challenge facing AI developers. Instead of gathering more data at any cost, the company is focused on building smarter datasets from cleaner, publicly available sources.\nIt's still a tricky situation for Apple, that while it didn't just take every bit of data it could find, it still scrapped the web before telling anyone it did so. Websites can ask to not be scraped now, but the original crawling of Applebot was already complete and can't be undone.\nThat said, Apple is one of the only companies seemingly taking an ethical approach to AI and its training.\nTraining for safety and cultural context\nOnce the data is collected, Apple trains the models in stages. It starts with supervised examples that show the model how to respond in different situations. Then it uses reinforcement learning, with real people rating model responses, to fine-tune the results.\n\"We fine-tune our models to behave helpfully, respectfully, and safely in accordance with Apple's AI principles,\" the company said. \"This includes instruction tuning, reinforcement learning from human feedback, and evaluations across a variety of safety and responsibility axes.\"\nApple also built a safety system that identifies categories like hate speech, misinformation, and stereotypes. The models are trained to refuse requests when necessary and to adapt their tone depending on where the user lives.\nThat includes using the right regional terms and avoiding content that might feel unnatural or insensitive in a local context. The extra training helps the system behave more reliably in the real world.\nWhat this means for users\nFeatures powered by Apple Intelligence now respond faster, support more languages, and stay on track when given complex prompts. For example, Genmoji knows how to avoid inappropriate results even when different languages are involved.\nThe Writing Tools can follow specific instructions without drifting off-topic. The image parser can turn a photo of a flyer into a calendar event, even if the design is cluttered.\nAnd all of that happens without Apple seeing what you type or share. If the model needs help from the cloud, Private Cloud Compute handles the request in encrypted memory, on servers Apple cannot access.\nFor users, the big shift is that Apple Intelligence feels more useful without giving up control. For developers, the new Foundation Models framework offers structured outputs, safer tool integration, and Swift-native design.\nDeveloper access and why 3 billion parameters matter\nAt WWDC 2025, Apple announced that developers can now use its on-device foundation model through the new Foundation Models framework. That gives third-party apps direct access to the same model that powers Apple Intelligence across iOS 26.\nThe on-device model has about 3 billion parameters, making it one of the largest language models ever deployed entirely on a phone or tablet. It's roughly the same size as Google's GeminiNano2 (about 3.25 billion) and not far off from Microsoft's Phi3mini (about 3.8 billion).\nApple isn't just matching competitors in model size. Its 3 billion-parameter model is optimized for Apple Silicon using 2-bit quantization and KV-cache sharing. That gives it a performance and efficiency edge without relying on the cloud.\nDevelopers get faster results, lower costs, and tighter user privacy. Instead of relying on external APIs or background network calls, apps can now integrate powerful AI locally and privately."
    },
    {
      "url": "https://appleinsider.com/articles/25/08/05/perplexity-defensive-over-ignoring-robotstxt-and-stealing-data",
      "text": "Perplexity was discovered to be actively bypassing blocks from websites to scrape content in 2024, and a new report shows that it has continued with increasing sophistication as the company defends the practice.\nApple received some significant blowback when it was discovered that Applebot had been crawling the web for years to get data to train Apple Intelligence. Websites immediately blocked the bot, and others, which sparked some interesting discoveries about how AI companies are operating.\nA year on, and at least one company is still doing everything in its power to ignore robots.txt and scrape webpages anyway \u2014 Perplexity. According to a report from Cloudflare, Perplexity is using several techniques to undermine the trust expected on the web and access data to train its large language models.\nTesting was conducted by creating new websites that had never been scraped before, then asking Perplexity AI about them. When the crawling bot encountered a robots.txt file that told it not to crawl, a new bot with a different browser agent, IP address, and even a new ASN appeared.\nThen, Perplexity was able to provide information that was available only on the website. It was clear that Perplexity was operating this new bot, even though it was unlabeled and its IP didn't appear in Perplexity's official IP range.\nThe methodology showed that data was most accurate when the new bots could get through. If the new bots were also blocked on a new webpage, the Perplexity AI results would be less specific or completely hallucinated \u2014 which indicates the new bots did indeed feed information to Perplexity.\nOld news, new details\nCloudflare's reporting helps reignite the attention around chatbots and how they get their data. That said, their findings, except for details around the new ASNs, are nearly identical to what was covered by Wired and Robb Knight in June 2024.\nPerplexity hasn't changed its tune, and in fact, seems to be trying to find new ways to avoid robots.txt. The document is an exercise in trust that is meant to stop any reputable company from accessing a website and scraping its data.\nApple, Google, ChatGPT, and others honor robots.txt while Perplexity has not and does not. While there's no legal backing to robots.txt, it colors the company as shady and untrustworthy versus its competitors.\nAt the least, it damages Perplexity's reputation and may jeopardize any talks it may have had with Apple about an acquisition. It seems that Apple is confident in its foundation models team and won't be looking for an acquisition to \"save\" Apple Intelligence, anyway.\nWe reached out to the Perplexity AI chatbot about the situation, and it faithfully regurgitated Cloudflare's reporting that it scraped from its website. However, Perplexity's blog has a surprising new post published Monday, curiously defending the company's approach.\nPerplexity fires back at Cloudflare\nIn an unsurprising turn of events, Perplexity has taken a defensive tack on its actions, claiming its web scraper and AI agents are two different entities. It blames Cloudflare for being unable to distinguish between the two and calls it a threat to the open web.\nThis controversy reveals that Cloudflare's systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats. If you can't tell a helpful digital assistant from a malicious scraper, then you probably shouldn't be making decisions about what constitutes legitimate web traffic.\nThese claims are ludicrous, of course. Humans navigate the free and open web, and websites not wanting their content stolen by an AI chatbot is a perfectly legitimate concern.\nA recent report from 404 Media shows how AI data scrapers have ruined the internet thanks to Google no longer directing user traffic to the source. Ars Technica also published a similar report, suggesting human web traffic is way down.\nThe problem with Perplexity's claims is that it assumes we've all mistakenly labeled its agents as scrapers that absorb data for AI training, which isn't the problem. While Perplexity says agents accessing websites aren't using the data for training, it misses the entire point of robots.txt.\nWebsites that tell automated web crawlers of any kind to ignore their page aren't doing it just because of potential ethical training issues; they're doing it to protect their livelihoods. If a user never has to see a website to gather information, then the human-run website will wither and die.\nWhat Perplexity doesn't understand is that without the human-run web, its AI will be useless. If all the humans go out of business, there will be nothing left to scrape.\nIt doesn't matter that it isn't stored or used for training, the AI agent isn't creating revenue or respecting the website's business model. Perplexity is actively, aggressively, and proudly building bots that are systematically tearing down the open web in the name of justice and freedom.\nThe blog post attempts to undermine Cloudflare's authority, suggesting it was either malicious clickbait or incompetence that resulted in the report. In the end, the company's public response is an embarrassment and goes against everything it claims to want to preserve.\nApple's part in all this\nWhen Apple revealed Apple Intelligence, it also shared that Applebot had played a part in scraping the web for freely available information that could train its foundation models. Apple was clear that it abided by robots.txt, though that was an empty promise considering websites thought it was indexing data for Siri and Spotlight.\nThe reaction was immediate \u2014 many websites updated their robots.txt to block Apple and other AI scrapers. The result of that and threatened legal action from Forbes was increased attention around AI data collection.\nApple has repeated consistently that it only uses ethically sourced data. While the Applebot situation was unfortunate, those horses are out of the barn, and Apple has shown considerable restraint in a world full of ethically questionable AI companies.\nApple's unique approach brings a combination of local models, private cloud models running on servers powered by renewable energy, and a promise to never train on user data or prompts. If Apple is to continue acting as a kind of ethical beacon in artificial intelligence, it's going to need to steer clear of Perplexity."
    },
    {
      "url": "https://machinelearning.apple.com/updates/ppml-2025",
      "text": "Apple believes that privacy is a fundamental human right. As AI experiences become increasingly personal and a part of people's daily lives, it's important that novel privacy-preserving techniques are created in parallel to advancing AI capabilities.\nApple's fundamental research has consistently pushed the state-of-the-art in using differential privacy with machine learning, and earlier this year, we hosted the Workshop on Privacy-Preserving Machine Learning (PPML). This two-day hybrid event brought together Apple and members of the broader research community to discuss the state of the art in PPML, focusing on four key areas: Private Learning and Statistics, Attacks and Security, Differential Privacy Foundations, and Foundation Models and Privacy.\nThe presentations and discussions of these topics explored the intersection of privacy, security, and the rapidly evolving landscape of artificial intelligence. Workshop participants discussed the theoretical underpinnings and practical challenges of building AI systems that protect privacy. By addressing privacy and security concerns from both theoretical and practical perspectives, we aim to foster innovation while safeguarding user privacy.\nIn this post, we share recordings of selected talks and a recap of the publications discussed at the workshop.\nPublished Work Presented at the Workshop\nAirGapAgent: Protecting Privacy-Conscious Conversational Agents by Eugene Bagdasarian (Google Research), Peter Kairouz (Google Research), Ren Yi (Google Research), Marco Gruteser (Google Research), Sahra Ghalebikesabi (Google DeepMind), Sewoong Oh (Google Research), Borja Balle (Google DeepMind), and Daniel Ramage (Google Research)\nA Generalized Binary Tree Mechanism for Differentially Private Approximation of All-Pair Distances by Michael Dinitz (Johns Hopkins University), Chenglin Fan (Seoul National University), Jingcheng Liu (Nanjing University), Jalaj Upadhyay (Rutgers University), and Zongrui Zou (Nanjing University)\nDifferentially Private Synthetic Data via Foundation Model APIs 1: Images by Zinan Lin (Microsoft Research), Sivakanth Gopi (Microsoft Research), Janardhan Kulkarni (Microsoft Research), Harsha Nori (Microsoft Research), and Sergey Yekhanin (Microsoft Research)\nDifferentially Private Synthetic Data via Foundation Model APIs 2: Text by Chulin Xie (University of Illinois Urbana-Champaign), Zinan Lin (Microsoft Research), Arturs Backurs (Microsoft Research), Sivakanth Gopi (Microsoft Research), Da Yu (Sun Yat-sen University), Huseyin Inan (Microsoft Research), Harsha Nori (Microsoft Research), Haotian Jiang (Microsoft Research), Huishuai Zhang (Microsoft Research), Yin Tat Lee (Microsoft Research), Bo Li (University of Illinois Urbana-Champaign, University of Chicago), and Sergey Yekhanin (Microsoft Research)\nEfficient and Near-Optimal Noise Generation for Streaming Differential Privacy by Krishnamurthy (Dj) Dvijotham (Google DeepMind), H. Brendan McMahan (Google Research), Krishna Pillutla (ITT Madras), Thomas Steinke (Google DeepMind), and Abhradeep Thakurta (Google DeepMind)\nElephants Do Not Forget: Differential Privacy with State Continuity for Privacy Budget by Jiankai Jin (The University of Melbourne), Chitchanok Chuengsatiansup (The University of Melbourne), Toby Murray (The University of Melbourne), Benjamin I. P. Rubinstein (The University of Melbourne), Yuval Yarom (Ruhr University Bochum), and Olga Ohrimenko (The University of Melbourne)\nImproved Differentially Private Continual Observation Using Group Algebra by Monika Henzinger (Institute of Science and Technology (ISTA) Austria) and Jalaj Upadhyay (Rutgers University)\nInstance-Optimal Private Density Estimation in the Wasserstein Distance by Vitaly Feldman, Audra McMillan, Satchit Sivakumar (Boston University), and Kunal Talwar\nLeveraging Model Guidance to Extract Training Data from Personalized Diffusion Models by Xiaoyu Wu (Carnegie Mellon University), Jiaru Zhang (Purdue University), and Steven Wu (Carnegie Mellon University)\nLocal Pan-privacy for Federated Analytics by Vitaly Feldman, Audra McMillan, Guy N. Rothblum, and Kunal Talwar\nNearly Tight Black-Box Auditing of Differentially Private Machine Learning by Meenatchi Sundaram Muthu Selva Annamalai (University College London) and Emiliano De Cristofaro (University of California, Riverside)\nOn the Price of Differential Privacy for Hierarchical Clustering by Chengyuan Deng (Rutgers University), Jie Gao (Rutgers University), Jalaj Upadhyay (Rutgers University), Chen Wang (Texas A&M University), and Samson Zhou (Texas A&M University)\nOperationalizing Contextual Integrity in Privacy-Conscious Assistants by Sahra Ghalebikesabi (Google DeepMind), Eugene Bagdasaryan (Google Research), Ren Yi (Google Research), Itay Yona (Google DeepMind), Ilia Shumailov (Google DeepMind), Aneesh Pappu (Google DeepMind), Chongyang Shi (Google DeepMind), Laura Weidinger (Google DeepMind), Robert Stanforth (Google DeepMind), Leonard Berrada (Google DeepMind), Pushmeet Kohli (Google DeepMind), Po-Sen Huang (Google DeepMind), and Borja Balle (Google DeepMind)\nPREAMBLE: Private and Efficient Aggregation via Block Sparse Vectors by Hilal Asi, Vitaly Feldman, Hannah Keller (Aarhus Univiersity; work done while at Apple), Guy N. Rothblum, Kunal Talwar\nPrivacy amplification by random allocation by Vitaly Feldman (Apple) and Moshe Shenfeld (The Hebrew University of Jerusalem)\nPrivacy of Noisy Stochastic Gradient Descent: More Iterations without More Privacy Loss by Jason Altschuler (MIT) and Kunal Talwar\nPrivately Estimating a Single Parameter by John Duchi (Stanford University), Hilal Ali, and Kunal Talwar\nScalable Private Search with Wally by Hilal Asi, Fabian Boemer, Nicholas Genise, Muhammad Haris Mughees, Tabitha Ogilvie, Rehan Rishi, Guy N. Rothblum, Kunal Talwar, Karl Tarbe, Ruiyu Zhu, and Marco Zuliani\nShifted Composition I: Harnack and Reverse Transport Inequalities by Jason Altschuler (University of Pennsylvania) and Sinho Chewi (IAS)\nShifted Interpolation for Differential Privacy by Jinho Bok (University of Pennsylvania), Weijie Su (University of Pennsylvania), and Jason Altschuler (University of Pennsylvania)\nTractable Agreement Protocols by Natalie Collina (University of Pennsylvania), Surbhi Goel (University of Pennsylvania), Varun Gupta (University of Pennsylvania), and Aaron Roth (University of Pennsylvania)\nTukey Depth Mechanisms for Practical Private Mean Estimation by Gavin Brown (University of Washington) and Lydia Zakynthinou (University of California, Berkeley)\nUser Inference Attacks on Large Language Models by Nikhil Kandpal (University of Toronto & Vector Institute), Krishna Pillutla (Google), Alina Oprea (Google, Northeastern University), Peter Kairouz (Google), Christopher A. Choquette-Choo (Google), and Zheng Xu (Google)\nUniversally Instance-Optimal Mechanisms for Private Statistical Estimation by Hilal Asi, John C. Duchi (Stanford University), Saminul Haque (Stanford University), Zewei Li (Northwestern University), and Feng Ruan (Northwestern University)\n\"What do you want from theory alone?\" Experimenting with Tight Auditing of Differentially Private Synthetic Data Generation by Meenatchi Sundaram Muthu Selva Annamalai (University College London), Georgi Ganev (University College London, Hazy), and Emiliano De Cristofaro (University of California, Riverside)\nAcknowledgments\nMany people contributed to this workshop including Hilal Asi, Anthony Chivetta, Vitaly Feldman, Haris Mughees, Martin Pelikan, Rehan Rishi, Guy Rothblum, and Kunal Talwar.\nRelated readings and updates.\nApple Workshop on Human-Centered Machine Learning 2024\nJuly 24, 2025research area Accessibility, research area Fairness, research area Human-Computer Interaction\nA human-centered approach to machine learning (HCML) involves designing ML machine learning & AI technology that prioritizes the needs and values of the people using it. This leads to AI that complements and enhances human capabilities, rather than replacing them. Research in the area of HCML includes the development of transparent and interpretable machine learning systems to help people feel safer using AI, as well as strategies for predicting...\nA Multi-Task Neural Architecture for On-Device Scene Analysis\nJune 7, 2022research area Computer Vision, research area Methods and Algorithms\nScene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one\u2019s life, outputs (or \"signals\") produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016."
    },
    {
      "url": "https://appleinsider.com/inside/apple-intelligence",
      "text": "Apple Intelligence\nAll about Apple Intelligence\nTable of Contents\nMachine learning led to more powerful technologies people ultimately dubbed \"artificial intelligence.\" While it is mostly a misnomer based on science fiction, Apple eventually began to embrace the term until it moved past it with Apple Intelligence.\nApple CEO Tim Cook will tell you that Apple Intelligence isn't named because of the \"AI\" that came before it. Instead, he'll insist that it was given the name that best described the technology.\nRegardless, Apple Intelligence is Apple's take on generative technologies that have taken the world by storm. OpenAI, Google, and others have flooded the market with their imperfect implementations in the hope of wrestling away some early market share.\nDespite pundit declarations that Apple was behind and would never catch up, the company has had a hand in machine intelligence for over a decade. Now, finally, Apple has chosen to appease these complainers by introducing a more nuanced version of the technology.\nA limited set of features like Writing Tools and the new Siri animation became available as of iOS 18.1, released in October 2024.\niOS 18.2 released on December 11, 2024. It includes Image Playground, ChatGPT integration, Visual Intelligence for iPhone 16, and Genmoji.\nApple released iOS 18.3 in January 2025 with few updates focused on AI. The company announced that the app intent system that would create a more contextually aware Apple Intelligence and Siri is delayed into the coming year.\nWhat's next for Apple Intelligence\nApple Intelligence is an on-device model with custom prompts depending on the mode it is being used in, like Writing Tools or Image Playground. Siri will get smarter thanks to Apple Intelligence, integrations with ChatGPT, and third-party app intents, but it still doesn't have an LLM backend.\nSiri may have new features that let it understand user prompts better and will soon gain a better understanding of what's on the display, but these are AI parts in a machine learning product, not a full overhaul. Such an overhaul of Siri's backend may come, but not until sometime in 2026.\nA rumor suggests Apple will rebuild Siri with an LLM backend that will fundamentally change the assistant into a full-on chatbot similar to ChatGPT. That wasn't announced during WWDC 2025, but Apple may be holding back on pre-announcing features until they are closer to being ready.\nMeanwhile, users are still waiting on the more contextual Siri and Apple Intelligence promised with iOS 18. That feature set was delayed and may not appear until early 2026 in a version of iOS 26.\nAnother rumor says Apple may abandon using Apple Intelligence as the backend for Siri and opt for something built by OpenAI or Anthropic. However, it seems more likely that this rumor suggests Apple is working to bring new models to Private Cloud Compute rather than replacing their own.\nIn essence, it would mean users could call out to private and secure versions of ChatGPT, Claude, Gemini, and Apple Intelligence via Private Cloud Compute. Previous rumors called this a sort of AI App Store where users could choose the models and access any or all of them from iPhone, iPad, and Mac.\niOS 26 and AI updates\nApple Intelligence was the focus of WWDC 2024 and mentioned constantly throughout the keynote, so it is understandable to believe Apple forgot about it during WWDC 2025. There was a brief mention at the top of the 2025 keynote and new features sprinkled throughout, but it's still a big year for Apple's AI efforts.\nDevelopers get access to Apple Intelligence via Apple's new Foundation Models framework. That means developers can perform tasks using Apple's on-device, private, and secure models instead of having to pay for an external model that sucks up user data.\nShortcuts gets new Apple Intelligence functions, enabling users to add steps for including Writing Tools or calling out to the Private Cloud Compute model directly. Responses and results can be fed back into the Shortcut for complex actions.\nVisual Intelligence is now available via the screenshot action. Users can screenshot objects, event posters, or text and search Google or ask ChatGPT to take action on the item.\nImage Playground and Genmoji can now be generated using ChatGPT as a prompt. The ChatGPT tool inside of the app has presets and lets users type out descriptions for manipulating an image.\nApple Watch got some AI action thanks to Workout Buddy in watchOS 26. A virtual assistant will now provide encouragement while working out and generate voice output based on available data.\nLive Translation\nApple devices will now provide translation in various systems across the ecosystem. In addition to the translation features already available for webpages in Safari and the Translate app, Apple has added automatic and Live Translation to other apps.\nOne of the features includes Live Translaton of lyrics in Apple Music. Some songs will show the English equivalent of a lyrics while others will provide a pronunciation guide \u2014 there's a UI option to toggle it on and off.\nLive Translation is also available in Messages, Phone, and FaceTime. Calls can be translated back and forth to both users on the call to help overcome language barriers when needed.\nConcerns regarding Apple Intelligence\nSo-called artificial intelligence hit the market running and has been iterated at a full sprint since. The explosive growth of the AI industry relied on having access to mountains of data gathered from the internet and its users.\nSome companies, like Google, had a strong foundation to train their models thanks to the literal mountains of data collected from users. However, there wasn't any way to predict exactly how models would react to learning from the trove of human-produced data.\nThe result, so far, has been chatbots and search engines that hallucinate because of poorly sourced data from non-factual sources. Replies to searches and chats can sometimes result in funny suggestions like glue on pizza, but other times result in something potentially dangerous.\nOther competitors, like OpenAI's ChatGPT, release updates that seem to lean into the meme culture of the internet. The image generation tool became viral as users created Studio Ghibli-inspired art based on photos they fed the tool.\nApple Intelligence is a group of foundational models built to run either at a smaller scale for iPhone, iPad, and Mac or at twice the size or bigger on a server. It is a testament to Apple's vertical integration of hardware and software.\nSince the models are local to the user's device, they can access sensitive data like contacts, photos, and location without the user needing to worry about compromising that data. Everything gathered by the Appel AI never leaves the device and isn't used to train the model.\nIn the event a request can't be run on the device, it can be sent to a server owned by Apple, running Apple Silicon with a Secure Enclave. All data sent off the device is encrypted and treated with the same protections it would have if it stayed local. This server-side technology is called Apple Cloud Compute.\nEven with privacy and security at its foundation, Apple Intelligence still had to be trained on something. Apple purchased licensed content for some of the training, but other data was sourced from Applebot, a web scraper that has existed since 2015 for surfacing Spotlight search data.\nThere is an ethical conundrum surrounding Apple's model training practices, but the company has assured that it has taken steps to ensure all public data is freely available on the web, lacks copyrighted or personal content, and is filtered for low-quality content. It's a small reassurance, given other companies had no regard for such precautions.\nThere has also been some concern about Apple Intelligence and the environment. On-device models don't have an impact on Apple's green energy initiatives, and server-side computation is performed on Apple Silicon, which is powered by green energy sources.\nApple Intelligence launch features\nApple Intelligence is made up of several models that have specific training to accomplish different tasks with minimal risk of error or hallucinations. There were several planned features at launch, with more arriving over 2024 and 2025 with iOS 18, iPadOS 18, and macOS Sequoia.\nWriting Tools works anywhere text entry is supported across the device's operating system. It analyzes text to proofread for errors, change the tone of the text to be more professional, or summarize the text.\nPriority Notifications analyze incoming notifications to ensure only the most important are at the top, and they are summarized too. It ensures messages, deliveries, and other important information is available at a glance.\nData summarization is everywhere in the supported operating systems. Summarize web pages, emails, text threads, and more.\nApple has come under fire due to notification summaries sometimes combining different news headlines into one that's wrong, confusing, or suggests someone is dead that isn't. As a result, notifications that are summarized will get an updated UI element to ensure users know it is an AI summary and not a headline.\nImage Playground is a app from Apple that generates images like emoji or cartoon-styled art. There is a standalone app, but the function appears in different locations, such as in iMessage. It was included in iOS 18.2 with mixed reviews as the results are not flattering or useful in their launch state.\nApple has more AI features separate from its proprietary systems thanks to a partnership with OpenAI. They are off by default, but if enabled, users can send some requests to ChatGPT. Every request must be screened and approved by the user, and all data from the interaction is discarded.\nUsers can to carry out more natural conversations with Siri, even if they make a mistake and correct themselves mid sentence. And as long as the user asks questions within the same relative timeframe, Siri will remember context from previous queries.\nMore advancements are coming to Siri later thanks to a planned update involving app intents.\nApp intents & contextual AI\nSiri is set to get a huge boost from Apple Intelligence by having access to data presented by the various on-device models. The smart assistant is more knowledgeable about the user, their plans, who they're talking to, and what data is within apps.\nThe voice interactions with Siri will primarily serve as a way to activate Apple Intelligence features from anywhere. However, Siri will have additional abilities thanks to app intents providing context for what is visible on the display.\nApps with Apple Intelligence Siri support:\n- Books\n- Calendar\n- Camera\n- Contacts\n- Files\n- Freeform\n- Keynote\n- Magnifier\n- News\n- Notes\n- Photos\n- Reminders\n- Safari\n- Stocks\n- Settings\n- Voice Memos\nThe upgrade to Siri could take some time. Rumors suggested Apple would not release the new Siri until iOS 18.4 sometime in spring 2025, but that has been delayed \u2014 likely an iOS 26 release.\nIt's Glowtime\nApple's iPhone 16 event occured on September 9, 2024 and had a clear theme around Apple Intelligence. The \"Glowtime\" name is a reference to the new Siri glow, which was a core part of the event.\nThe entire iPhone 16 lineup can take advantage of Apple Intelligence features. The A18 and A18 Pro have a more powerful neural engine, and the new devices are what Apple calls the first iPhones designed from the ground up for AI.\nThe Visual Intelligence features attached to the Camera Control button was also revealed. It is an exclusive AI feature for the iPhone 16 lineup.\niPhone 16 supercycle\nAnalysts expect Apple to have a record release cycle with iPhone 16 thanks to Apple Intelligence. Anyone that doesn't own an iPhone 15 Pro or iPhone 15 Pro Max, which is most people, will have to upgrade for access to Apple's AI.\nThat means 2024 was expected to be a significant year for iPhone sales similar to the 5G boom and the bigger iPhone craze launched with iPhone 6 Plus. Consumers know about AI like ChatGPT and want access to it, and Apple's promise of private, on-device models could have been enticing.\nHowever, evidence showed that while demand was strong for the iPhone 16 lineup, a supercycle didn't happen. It could be attributed to the slow rollout of AI features or lack of overtly exciting party tricks at launch.\nApple has since toned down its approach to AI and advertising the features. WWDC 2025 had Apple Intelligence sprinkled throughout, but it definitely wasn't front and center like it was in 2024.\nThe iPhone 17 lineup will likely have more AI features associated with them, but this time it seems they will only be ones available at launch. Apple was clearly burned from announcing features that weren't ready for prime time and likely won't make that mistake again anytime soon.\nVisual Intelligence\nPress and hold on the Camera Control button on any iPhone 16 and the Visual Intelligence experience will launch. It acts as a Google Lens-like feature that lets the iPhone \"see\" the world and interact with different elements.\nPoint the camera at a concert poster to add the concert to the user's calendar. Or, launch directly into the visual lookup feature and check dog breeds or flower names just by taking a photo.\nTwo buttons at the bottom will take whatever is in the viewfinder and pass it to Google for search or ChatGPT for parsing. As is standard with Apple Intelligence and AI on the iPhone, all interactions with AI is user controlled.\nVisual Intelligence was later spread to the iPhone 15 Pro and iPhone 16e via the Action button. Apple upgraded the feature to include it in the screenshot tool with iOS 26.\nApple Intelligence requirements\nApple claims that Apple Intelligence couldn't exist until it could run locally on a device, and that wasn't possible until the A17 Pro. So, the available devices for the technology are very limited.\nThe base requirements appear to be tied to the size of the Neural Engine and the amount of RAM. The iPhone 15 Pro and iPhone 15 Pro Max have 8GB of RAM and run the A17 Pro with a sizeable Neural Engine.\nThe launch of the iPhone 16 lineup added four more devices that support Apple Intelligence, which could spur demand.\nMacs and iPads can run Apple Intelligence as long as they have an M-series processor. The desktop-class processor has always had a large Neural Engine and enough RAM at 8GB minimum for the chipset.\nApple Intelligence release date\nApple Intelligence launched to the public with iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1 on October 28, 2024. The initial features included Writing Tools, Photos Clean Up, and system-wide summaries.\niOS 18.2 arrived in December 2024 with Image Playground, ChatGPT integration, and Visual Intelligence. More features are delayed until iOS 26.\nEverything below this point is based on pre-WWDC 2024 rumors and leaks. It has been preserved for reference.\nPre-announcement AI rumors\nApple is known to adopt its own marketing terms to describe existing phenomenons, like calling VR or MR by the in-house term \"spatial computing.\" It seemed like Apple was going to lean into Artificial Intelligence, or AI, as a term, but it is expected to add a twist by calling it \"Apple Intelligence.\"\nWhatever Apple calls it, the result is the same. There will be system-wide tools that run using generative technologies that are much more advanced than the machine learning algorithms they'll replace or augment.\nApple may reveal plans for a server-side LLM during WWDC, but rumors suggest it isn't ready for prime time. Instead, the focus will be on local models made by Apple that can call out to third-party server-side LLMs like ChatGPT.\nLocal Apple Intelligence\nOpenAI's ChatGPT and Google's Gemini Large Language Models (LLMs) have one thing in common \u2014 they are gigantic models trained on every ounce of available data on the internet that live in servers.\nThe \"Large\" in LLM is literal, as these models can't live on something like an iPhone or Mac. Instead, users call the models and their various versions through the cloud via APIs. That requires data to leave the user's device and head to a server they don't control.\nServer-side models are a must for such enormous data sets, but they come at the cost of privacy and security. It may be harmless to ask an LLM to generate a photo or write an essay about historical events, but asking it to perform a budget may cross a line.\nApple hopes to address these types of concerns by providing more limited local models across the iPhone, iPad, Mac, and Apple Vision Pro. The pre-trained models will serve specific purposes and will contain very limited but specialized data sets.\nThe on-device models will learn based on user data. The data will never leave the device without explicit permission.\nUpgrading Siri\nA part of local Apple Intelligence will be providing significant upgrades to Siri. Apple's assistant is expected to become much more conversational and understand contexts better.\nA specific model, likely based on the \"Apple Ask\" pre-trained model that's been tested internally, will be the driver behind Siri. Its limited data set is said to be a protection against hallucinations, which plague LLMs like Google Gemini and ChatGPT.\nUsers can ask Siri to perform many tasks across Apple-made apps, from simple to complex. It isn't yet known if an API will allow developers to target this new Siri, but it is likely.\nApps with Apple Intelligence Siri support:\n- Books\n- Calendar\n- Camera\n- Contacts\n- Files\n- Freeform\n- Keynote\n- Magnifier\n- News\n- Notes\n- Photos\n- Reminders\n- Safari\n- Stocks\n- Settings\n- Voice Memos\nMany Siri functions in these apps involve general navigation and search. Like opening a specific book or creating a new slide.\nIt seems many of the functions found in the new Siri commands are derived from previously available accessibility options that could control the device via voice. This melding of features will provide more control for all users, not just ones relying on accessibility options.\nApple Intelligence at WWDC\nAll will be revealed during WWDC 2024. Apple is expected to reveal Apple Intelligence during its keynote, where iOS 18, iPadOS 18, macOS 15, tvOS 18, watchOS 11, and visionOS 2 will also be announced."
    },
    {
      "url": "https://appleinsider.com/articles/25/07/25/apple-researchers-take-aim-at-ai-hallucinations-and-true-conversations",
      "text": "Apple Intelligence researchers have released a whole series of new academic papers concerned with furthering AI's ability to be personalized and understanding how errors occur.\nThere is still this belief that Apple is behind the industry, but its researchers continue to publish papers that go far beyond Apple products and into the issues that affect all AI tools. The company's research work extends back many years, but its latest papers have concentrated on AI flaws, and how to prevent unwanted AI actions.\nNow its researchers have released eight new papers that chiefly extend this angle, and a whole series of videos from their presentations from Apple's 2024 workshops on Human-Centered Machine Learning 2024.\nBenchmarking AI and finding errors\nOne of the new Apple papers proposes what its researchers call the Massive Multitask Agent Understanding (MMAU) benchmark. It's a system of evaluating different Large Language Models (LLMs) across \"five essential capabilities,\" which are:\n- Understanding\n- Reasoning\n- Planning\n- Problem-solving\n- Self-correction\nApple says that its MMAU benchmark consists of \"20 meticulously designed tasks encompassing over 3K distinct prompts.\" It's claimed to be a comprehensive way of evaluating LLMs.\n\"Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance,\" continues Apple.\nThe purpose is to make improvements by understanding where errors originate, which Apple says is currently an issue because existing \"evaluation methods blur the distinctions between different types of failures.\" Its MMAU is also intended to be simpler to use than current alternatives.\nThis full paper can be read via Cornell University's research paper archive.\nPersonalizing AI and learning from conversations\nApple suggests that AI LLMs are constrained by how they cannot be sufficiently personalized, such as to the extent that they remember previous conversations. The company says that up to now, attempts to personalize responses have concentrated on \"incorporating small factoids\" about the user's preferences.\nInstead, Apple proposes a system it calls the Pipeline for Learning User Conversations in Large Language Models, or PLUM. This \"extracts question-answer pairs from conversations,\" building up a method of \"injecting knowledge of prior user conversations into the LLM.\"\nRead the full paper here.\nExternal validation of LLMs and AI\nLLMs can famously offer significantly different responses if a prompt is repeated with a different order of words, or just a longer or shorter version of the same. Apple describes this by saying that \"AI annotators have been observed to be susceptible to a number of biases.\"\nHowever, Apple also argues that, presented with a response, humans have been persuaded \"by responses' assertiveness.\" It's the way that AI will proclaim its results as absolute and intractable fact, until you ask it again and it admits, no, none of it is true.\nSo in a paper called \"Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?\", Apple wants to make better responses. It proposes doing so using \"external validation tools based on web search and code execution.\"\nIt notes, though, that in its research, this type of validation was only \"often, but not always,\" able to produce better results.\nRead the full paper here.\nApple continues to present papers at AI events\nAlongside research papers, Apple has also now published a series of eight videos from its 2024 # Human-Centered Machine Learning workshop. They range in length from 10 minutes to 38 minutes, and cover topics such as AI interfaces, and UI Understanding\nThe videos are all from sessions held in 2024, but Apple researchers are continuing to speak at new AI events. From July 27, 2025, to August 1, Apple will present new research at the annual Association for Computational Linguistics (ACL) in Vienna.\nIt's presenting or sponsoring 18 workshops, many of which are based around its latest papers described here. Details of the Apple schedule at ACL are on Apple's Machine Learning site."
    },
    {
      "url": "https://appleinsider.com/inside/siri",
      "text": "Siri\nAll about Siri\nTable of Contents\n- Siri\n- 1. Siri with Apple Intelligence\n- 2. Siri as an LLM chatbot\n- Private Cloud Compute\n- 3. Siri Features\n- How Siri Works\n- List of Siri commands\n- Personalization\n- Privacy and Security\n- On-device Siri on Apple TV\n- Shortcuts\n- Translate\n- 4. Devices\n- iPhone, iPad, and iPod touch\n- Apple Watch\n- AirPods and Beats Pro\n- Headphones, Headsets, and CarPlay\n- Apple TV\n- HomePod and HomePod mini\n- Mac\n- 5. Social awareness\n- 6. The Future of Siri\nApple's smart assistant launched over a decade ago and has evolved tremendously during that time. Siri can sound natural, respond to queries conversationally, remember previous conversations, and control aspects of your life \u2014 all while remaining private and secure.\nThe neural networks and algorithms used by Siri work both on-device and in the cloud to provide information to the user. However, the smart assistant is still highly reliant on cloud computing and can't do much beyond control system actions when used offline.\nSiri with Apple Intelligence\nSiri is set to benefit heavily from Apple Intelligence, which is Apple's stab at integrating artificial intelligence into its line of products. As Apple prides itself on security, much of Apple Intelligence is done on device.\nAdditionally, Apple describes Apple Intelligence as \"personal Intelligence.\" Its primary focus is to improve the way users interface with the apps and services they already use, rather than focusing on novelty use cases.\nApple Intelligence is limited to M-series Macs and iPads, as well as iPhones running the A17 Pro chip and newer.\nArguably, Siri may be the most substantial beneficiary of the Apple Intelligence repertoire. There are millions of queries done per day by Apple customers now \u2014 with varying success.\nSiri will eventually be able to tap into your personal information, all locally on the device. This means that it can be far more aware of you and your needs versus third-party AI tools.\nIt's will be able to answer questions about your texts, emails, photos, videos, schedules, and more once the contextual update powered by app intents is released. Importantly, this is something no other AI assistant is capable of doing with the same degree of privacy.\nA new invocation animation was also added, now illuminating the edges of the screen rather than the small glowing orb used previously. Siri can understand queries better too, but more AI features aren't expected until later in 2025 or early 2026.\nApple has already asked developers to begin work on the new app intent system that will give Siri on-screen contextual awareness. ChatGPT integration is available with iOS 18.2.\nThe app intent system for contextual Apple Intelligence and Siri was delayed in early 2025. Apple says the feature is expected within the following year, which means it'll likely be an iOS 19 feature.\nSiri as an LLM chatbot\nDespite the emergence of Apple Intelligence and its integration with Siri, Apple's smart assistant is still not running an AI backend. That could change with iOS 19 as soon as 2026.\nApple will reveal iOS 19 in June 2025, but the feature set that makes Siri run on a Large Language Model (LLM) may not arrive until late in that operating system release cycle. It would be the last step to turning Siri into something closer to ChatGPT.\nUsers can talk to Siri or type to it today, but it is still parsing data in a similar way to how it handled it previously. It calls to different systems, like Reminders, Phone, or Google Search rather than having those tools built into the Siri backend.\nThe introduction of Apple Intelligence and ChatGPT furthered that interaction. Users make a call to Siri, and if it's related to an AI tool or ChatGPT, it passes that data to the relevant system.\nThe LLM Siri should be able to handle everything locally and privately using data it knows about the user. It would be a big shift for the 13 year old smart assistant.\nPrivate Cloud Compute\nWhile much of Apple Intelligence is done on-device, there are times when it must draw from outside computing power. Private Cloud Compute allows Apple Intelligence to work in the cloud while preserving security and privacy. Models are run on servers running Apple Silicon, using the security aspects of Swift.\nProcesses on-device work out if the request is sent to cloud servers or if it can be handled locally.\nApple insists that the servers are secure, that they don't store user data, and that they use cryptographic elements to maintain security. This includes the devices never talking to a server unless that server has been publicly logged for inspection by independent experts.\nSiri Features\nWhat follows is a description of Siri without Apple Intelligence. Most Apple users will still experience this version of Siri on their HomePods, older iPhones, and Apple TV.\nSiri is an umbrella name for the smart voice assistant built into nearly every device in the Apple ecosystem. It also includes all the technologies surrounding machine learning and on-device intelligence used for smart suggestions.\nUnlike other smart assistants, Siri doesn't have a central hub or speaker to call home. The HomePod and HomePod mini are the closest thing to a \"Siri device\" that Apple sells, though it isn't advertised that way.\nApple acquired the original voice assistant from SRI, a research company that had an app of the same name in the App Store. Soon after, Apple integrated Siri into iOS to release alongside the iPhone 4S.\nAt its start, Siri was barely a task manager capable of responding to simple commands. Apple has slowly added features and improved upon Siri's intelligence while trying to keep up with the competition.\nHow Siri Works\nWith a tap, press, or wake command, Apple's smart assistant is easily activated across all Apple devices. The universal wake word \"Hey Siri\" allows you to make commands or queries from nearly any Apple device.\nIn iOS 17 and the HomePod Software Version 17, \"Hey\" has been removed, and now users can just say \"Siri\" before a command.\nWhen speaking, your command is processed locally to determine if it can be done on the device. If not, it is sent to an Apple server with a random identifier. All voice data is encrypted end-to-end, so all data is synced without fear of snooping from Apple or anyone else that may try to look.\nSiri intelligence uses machine learning for various tasks. It can find and suggest calendar appointments in email or messages, suggest web pages in Safari from links in iMessage, and offer smart text suggestions based on recent websites or news articles the user interacted with. It will also recommend tasks you perform frequently or during specific times of the day.\nThird-party developers can tap into Apple APIs and create their own integrated functions. Devices like the HomePod were heavily scrutinized by regulators who thought Apple gave too much preference to its own services. Now, companies can utilize APIs to add services like Spotify to the HomePod.\nList of Siri commands\nApple has baked in a lot of standard commands to its voice assistant. Plus, some Easter eggs pop up from time to time, so try everything.\n- Make phone calls: \"Call Mom.\"\n- Hang up a call: \"Hang up.\"\n- Send messages: \"Send a message to Steve\" or \"Ask Dad, where are the keys?\"\n- Read back previous messages: \"Read back previous messages.\"\n- Get directions: \"Get directions home\"\n- Play music: \"Play music I like\" or \"Play 'Georgia' by Vance Joy.\"\n- Stop music: \"Stop playing\" or \"Play something else.\"\n- Ping nearby devices: \"Where is my iPhone.\"\n- Set a timer: \"Set a timer for 10 minutes\" and set multiple timers by naming them, \"Set a noodle timer for 8 minutes.\"\n- Check on a timer: \"How much time is left on the timer?\"\n- Stop an alarm: \"Stop\" or \"Stop the alarm\" or add a person's name if their device is alarming and belongs to your Apple Home.\n- Control HomeKit devices: \"Turn on the lights\" when commanding a HomePod in the room you'd like to turn the lights on in. Or, provide a zone, \"Turn on the lights downstairs.\"\n- Set a Reminder: \"Remind me of this\" for something you're looking at in an app like Safari. Or \"Remind me to take out the trash when I get home.\"\n- Take a photo with the Camera app: \"take a photo\" or \"take a selfie.\"\n- Play media: \"Play the next episode of 'Ted Lasso'\" or \"Start playing 'Prehistoric Planet' in the Living Room.\"\n- Ask for information: general queries can be thrown at Siri, and results will vary. \"When did 'Cinderella come out?\" or \"When is the next showing of 'Super Mario Brothers The Movie?'\" or \"What day does Easter fall on?\" or \"What's the Redsocks score?\"\n- Create Calendar events: \"Create a Calendar event on April 14 for a doctors appointment at 3 p.m.\"\n- Calculations and random chance: \"What is 4,400 divided by 3?\" or \"Flip a coin\" or \"roll a dice\" or \"roll two dice.\"\n- Activate Shazam: \"What song is this?\"\nSiri has access to nearly every app you have installed on your iPhone. See how each app interacts with Siri and Spotlight by opening the \"Siri & Search\" menu in Settings.\nPersonalization\nAs you use Siri on various Apple products, the smart assistant will use what it has learned to grow more personalized. In essence, each user's Siri is unique to them.\nThere are also multiple voices to choose from to customize Siri further. There are five American voices to choose from, and they are identified by numbers, not genders, to provide better inclusion. Apple also offers voices in Australian, British, Indian, Irish, and South African.\nOther than the suggestions that occur across your device, you can also create custom actions called Shortcuts. As of iOS 13, Shortcuts are built into the system, allowing even casual users to create quick interactions with their phones. Siri Intelligence and the voice assistant both play a part in this.\nFurther customization can take place within Apple apps or inside the voice assistant itself. You can tell Siri which of your contacts are family members, how to spell and pronounce certain names or words, and set a nickname that it will use for you in conversation.\nTry asking, \"Siri, what's my update?\" This provides a personalized readout of the weather, upcoming calendar events, and even plays a short news podcast that can be configured between NPR, Fox, and Apple News.\nPrivacy and Security\nAs with everything Apple does, the company built Siri and its systems with privacy and security in mind. The Secure Element found in Apple Silicon stores encrypted personal data on the device. It only sends complex actions or questions to servers for parsing using a random identifier.\nFrom settings, you can delete and reset this identifier and all personalization and data at any time.\nAs of iOS 14, all text-to-speech recognition takes place on-device using the Neural engine. This means that features like dictation will take place locally without being sent off to a server for processing.\nThere was some concern over how voice data was being used by Apple, so the company introduced the ability for users to turn off certain diagnostic tools. Previously, recordings were sent to Apple for accuracy analysis, but now this is an opt-in-only feature, and transcriptions of interactions are used instead.\nApple stepped up Siri privacy further with Apple Watch Series 9 and Apple Watch Ultra 2 with the S9 SiP. If a command can be processed without pinging the user's iPhone or the internet, it will.\nA class action lawsuit accused Apple of using recorded conversations with Siri to build an advertising profile of users. Of course, this isn't how the technology works and despite an initial attempt for a judge to throw it out, Apple ultimately settled to end the case in January 2025.\nOn-device Siri on Apple TV\nInstead of relying on servers to process queries, Apple announced at WWDC 2024 that it will be bringing more Siri processing to the Apple TV itself.\nTo users, this could result in quicker responses for basic queries. It also helps improve privacy and security, simply by keeping some queries locally processed instead of traversing the Internet to a server and back.\nShortcuts\nShortcuts is the next stage of Workflow, a third-party iOS app that used available APIs to allow users to create on-device automation. Apple purchased Workflow to lay the groundwork for Shortcuts.\nAt first, Apple distinguished manually controlled Shortcuts from voice-activated \"Siri Shortcuts.\" A third category of Shortcuts was also used for Shortcuts that had been suggested by third-party apps.\nLater, the different types of Shortcuts were combined into one format to reduce confusion. Now, all Shortcuts can be used via Siri by saying the Shortcut name as the command.\nApple updated several aspects of Shortcuts with iOS 14. Automations can run without user interaction, Shortcuts placed on the Home Screen no longer launch the Shortcuts app, and the Shortcuts app has folders for sorting Shortcuts.\nThe Dynamic Island on iPhone 14 Pro brings Shortcut alerts up and out of the way. So, now users can see Shortcut automations as they occur without disruption.\nDevelopers also have an improved system for suggesting Shortcuts right from within the Shortcuts app.\nTranslate\nThe Translate app in iOS 14 lets Siri translate conversations in real-time. The app intelligently understands what language the speaker is using and translates it on the fly.\nWhen held in landscape mode, Translate displays the conversation in both languages on either side of the screen, so each person can see translations easily.\nThe Translate app works entirely offline and features two-way translation for the following languages:\n- English\n- Mandarin Chinese\n- French\n- German\n- Spanish\n- Italian\n- Japanese\n- Korean\n- Arabic\n- Portuguese\n- Russian\nDevices with Siri\nSiri launched as an exclusive iPhone feature, starting with the iPhone 4s. The feature has since spread to every part of the Apple ecosystem. The following examples illustrate where Siri is and how users can implement it.\niPhone, iPad, and iPod touch\n\"Hey Siri,\" or just \"Siri\" with iOS 17, will wake any modern Apple device, triggering listening for a query. You can also summon Siri by holding down the home button on Touch ID devices or the side button on Face ID devices.\nThe iPhone can learn to listen only for your voice, reducing accidental activations from commercials or other users.\nApple Watch\nIf a user's Apple Watch is awake with the screen fully on, speaking the wake command will activate Siri on the watch.\nApple's wearable also includes a feature called \"Raise to Speak.\" It allows you to raise your watch to your mouth and speak a command without saying the wake phrase. The feature first appeared on the Apple Watch Series 3.\nWhen held, the Digital Crown will summon the assistant as well. This is a foolproof way of getting Siri's attention without the need of using a wake word or hand gesture. Users can disable this method if inadvertent activations are frequent.\nThe Apple Watch also has a dedicated Siri watch face, although Apple hasn't updated it much since its release. It still exists in watchOS 10, but most of its functionality has moved to widget stacks.\nApple Watches with the S9 SiP or newer have access to offline Siri commands.\nAirPods and Beats Pro\nSecond-generation AirPods, AirPods Pro, AirPods Max, Powerbeats Pro, and Solo Pro all offer \"Hey Siri\" functionality. First-generation Airpods can summon Siri with a double-tap gesture.\nThe new \"Siri\" command without the \"Hey\" works only with AirPods Pro 2.\nDevices with the H1 processor and newer will read your incoming messages using \"announce messages with Siri.\" Users can reply to these messages without saying the wake word.\nWhen Siri is used by headphones of any kind, it is controlled by the connected iPhone or iPad rather than relying on an integrated headphone-based Siri.\nHeadphones, Headsets, and CarPlay\nSome devices with built-in mics connected over Bluetooth, Lightning cable, or audio jack will work with Siri. On compatible devices, you can press and hold on the play/pause button or call button to summon Apple's assistant.\nCarPlay functionality depends on the car's hardware. Usually, it will allow \"Hey Siri\" or a long press on the touchscreen home button or steering wheel call button to summon the assistant.\nUsing the assistant in the car allows for safe hands-free controls like making a call or getting directions. Interactions via CarPlay limit the interactions to voice-only so the driver isn't distracted.\nApple TV\nSiri on the Apple TV has limited functionality. To use the assistant on Apple TV, hold down the Siri button on the Siri Remote. You can request movie or music playback, search for movies using universal search, ask some trivia questions, or control your home. When viewing media, you can ask \"who is in this movie\" or \"what did they say\" to get specific results designed for the Apple TV.\nHomePod and HomePod mini\nSiri is the primary user interface with HomePod. You can summon the assistant with \"Hey Siri\" or by long-pressing the glass surface on the HomePod's top.\nThe \"Siri\" command without the \"Hey\" works on the second-generation HomePod and HomePod mini.\nApple refers to Siri on the HomePod as a personal Music expert, offering other functions unique to the HomePod. These include describing who an artist is rather than showing a page result when asked about an artist.\nMac\nOn Mac, you can activate Siri via a dedicated Siri button. You can find this in the Touch Bar on any MacBook with Touch Bar or the menu bar. You can also trigger the assistant with a keyboard shortcut.\n\"Hey Siri\" works on Macs with built-in mics and T2 chips made after 2018. This version is also limited, only recently gaining the ability to control HomeKit. Dropping the \"Hey\" only works on Macs with Apple Silicon running macOS Sonoma.\nApple, Siri, and Social Awareness\nSiri resides on every iOS device and nearly every Mac, making the assistant a primary source of information for many users. Apple claims millions of users activate it every day, and the company wants to keep Siri relevant and learning.\nApple tends to release statements during significant national events and social movements, and Siri technology sometimes participates as well. Large events like elections or acts of terrorism spurred Apple into adding special Siri queries. In these situations, the company wants to keep the public informed beyond merely presenting a Wikipedia page.\nIn 2020, Apple made Siri smarter about the events surrounding the election, providing users with helpful information about each state's primaries and caucuses. The assistant would often read an answer aloud and show a link to special Apple News coverage.\nAs 2020's unprecedented events proceeded, Apple adapted its digital assistant accordingly. The spread of coronavirus sparked fear and doubt, and Apple again wanted its smart assistant to be a useful tool. Apple gave Siri specific answers to questions about COVID-19 and offered links to the CDC and Maps directions to testing centers.\nWhen the George Floyd protests broke out, demand for fact-checked information came with it. As misinformation spread, Apple gave Siri an update to push people toward facts and appropriate context. If users asked about \"All Lives Matter,\" Siri would respond by showing users a link to the Black Lives Matter page.\nApple updated Siri on a beta version of iOS 14.5 with new English-speaking voices, but more importantly, the update removed gender identifiers from the voices. Users will also be given a choice at setup rather than defaulting to a specific voice sound. These announcements were made on the International Transgender Day of Visibility.\niOS 15.4 adds another non-gendered voice to the English-speaking Siri.\nThe Future of Siri\nApple discontinued the original HomePod in 2021 without a replacement device at the ready. Some feared that Apple would step away from the smart assistant race and abandon the smart speaker market altogether.\nHowever, Apple introduced a new large HomePod in January 2023. Even as other companies like Amazon are toning down their commitment to smart assistants, Apple is doubling down with new hardware.\nBesides the general improvements to voice recognition and processing, Apple hasn't introduced many new features to its smart assistant since iOS 14. Offline commands work better with iOS 15, and iOS 16 improved Shortcuts actions.\nRumors point to Large Language Models like ChatGPT and Google Gemini as a solution for making Siri smarter. While Apple won't be introducing a chatbot, it could use the technology to enhance natural language processing."
    },
    {
      "url": "https://appleinsider.com/articles/25/07/29/apples-delayed-siri-update-spawns-another-securities-lawsuit",
      "text": "A second law firm is seeking Apple investors to lead an attempted class action lawsuit over allegations of securities fraud linked to the delayed Siri upgrade.\nThe rollout of Apple Intelligence has been problematic in many ways, but the worst problem has been Siri. After being promised a more personalized version of the digital assistant, Apple has admitted that the new version has been significantly delayed.\nWith the Siri upgrade now happening beyond iOS 18, lawyers are trying to capitalize on this broken marketing promise.\nThe Rosen Law Firm has put a call out to buyers of Apple securities who may have bought the company's shares on the strength of the Apple Intelligence launch.\nA press release picked up by Morning Star seeks out investors who bought Apple shares between June 10, 2024 and June 9, 2025. It explains there is a \"lead plaintiff deadline\" of August 19, 2025.\nWhile there is a deadline connected to the suit, Rosen Law admits that it is trying to make it a certified class action lawsuit, but it isn't at the time of publication.\nThe lawsuit asserts that Apple had made \"false and misleading statements\" or failed to disclose that it had underestimated how long it would take for the AI-based Siri to go live. Apple also apparently wronged investors because the new Siri was unlikely to be available in time to be used on the iPhone 16.\nIn doing so, sales of the iPhone 16 were allegedly harmed, therefore Apple overstated its financial prospects. The suit adds that the public statements from Apple about the all-new Siri were \"materially false and misleading,\" which meant that investors suffered damages.\nClass-action pile-on\nA perceived marketing mistake affecting shareholders is certainly something that can attract lawsuits, especially at Apple's size. Rosen is not the only law firm to smell blood in the water.\nIn May, lawyers from Bronstein, Gewirtz, & Grossman LLC announced it was investigating whether to try its own class-action suit filing over the Siri fiasco. For its own announcement, the law firm also insisted that the Apple stock price fell after Apple confirmed the Siri changes were delayed.\nAnother lawsuit filed in March went after Apple over its advertising. Specifically how Apple Intelligence was included in adverts demonstrating delayed features."
    },
    {
      "url": "https://arxiv.org/abs/2405.05175",
      "text": "Computer Science > Cryptography and Security\n[Submitted on 8 May 2024 (v1), last revised 18 Sep 2024 (this version, v2)]\nTitle:AirGapAgent: Protecting Privacy-Conscious Conversational Agents\nView PDF HTML (experimental)Abstract:The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns. While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors. We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.\nGrounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task. Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality. For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective.\nSubmission history\nFrom: Eugene Bagdasaryan [view email][v1] Wed, 8 May 2024 16:12:45 UTC (1,972 KB)\n[v2] Wed, 18 Sep 2024 20:53:06 UTC (2,086 KB)\nCurrent browse context:\ncs.CR\nReferences & Citations\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."
    }
  ],
  "argos_summary": "Apple recently hosted a workshop on Privacy-Preserving Machine Learning (PPML) to discuss methods for safeguarding user data in AI applications. The event featured presentations from various researchers, including those from Google and Microsoft, focusing on topics like differential privacy and secure AI models. Apple aims to demonstrate its commitment to user privacy by sharing insights from the workshop and emphasizing its ethical approach to AI training, particularly in light of ongoing scrutiny in the industry.",
  "argos_id": "GAE03K0QG"
}