{
  "url": "https://www.zdnet.com/article/i-tried-every-new-ai-feature-on-the-google-pixel-10-series-my-thoughts-as-an-ai-expert/",
  "authorsByline": "Sabrina Ortiz",
  "articleId": "b6a16e4fb20d468eba7cddc32634de37",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/21b0794fc2765b2a856aac93f848f0241528e2e3/2025/08/15/a9eb5124-2f3c-4c05-abcb-17a49035104f/dsc06977.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-23T07:30:21+00:00",
  "addDate": "2025-08-23T07:33:21.882122+00:00",
  "refreshDate": "2025-08-23T07:33:21.882123+00:00",
  "score": 1.0,
  "title": "I tried every new AI feature on the Google Pixel 10 series - my thoughts as an AI expert",
  "description": "Google introduced a handful of flashy new AI features for the new Pixel 10 series, like verbal photo editing. Here's how useful they actually are.",
  "content": "\u2022 Google's new Pixel 10 lineup incorporates a handful of new AI-powered features.\n\u2022 They include photo editing with speech, language translation that uses your voice sample, and prediction of users' needs.\n\u2022 The Pixel 10 Series also includes a one-year subscription to the Google AI Pro plan.\n\nGet more ZDNET: Add us as a preferred Google source on Chrome and Chromium browsers.\n\nThe generative AI explosion means that nearly every phone launch in the past year has been accompanied by its own suite of AI features. Google's launch of its Pixel 10 Series is no different, with some new features Google hopes might draw in new users from other devices.\n\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\n\nAt its Made by Google event, Google unveiled the new Pixel 10 Pro and 10 Pro XL, Pixel 10 Pro Fold, and Pixel 10, as well as the Pixel Watch 4 and Pixel Buds 2a. Beyond fun new phone colors, better camera systems, and other hardware upgrades, AI is at the core, powered by the new Google Tensor G5 processor.\n\nThe chipset, co-designed with Google DeepMind, can run Gemini Nano on the device, the engine powering all these new AI experiences. Here's a full round-up of the new AI features and my personal hands-on experiences with each.\n\nWhile AI tools like chatbots can be helpful, they require you to context-switch to other tabs, ask your question, go back to whichever window you were originally working out of, and paste it in. In order to upgrade that experience and level up the assistance, AI needs to predict your needs -- which is exactly what Magic Cue aims to do.\n\nAlso: I tried the standard Google Pixel 10 and didn't miss the Pro models one bit\n\nThe Magic Cue feature suggests relevant information and actions based on what you're doing. For example, in the demo featured in the image above, the user was calling an airline, so Magic Cue automatically surfaced the flight details to prevent scrambling to find them while on the call.\n\nIn another demo, the user got a text asking where the reservation was. Instead of the demoer having to find the details, Magic Cue surfaced the reservation information based on what was in the user's inbox. Then, all the user had to do was tap and send it. Since the feature leverages the Google Tensor G5 chip, information is processed locally on the device.\n\nSticking with the trend of making information as accessible as possible, the new Daily Hub feature found in your Discover feed puts all of the information you need in one place. It also includes an integration with Magic Cue, which gathers insights from your apps.\n\nAs seen in the image, it can remind you of actions from your Google Keep and Gmail, including dinner plans, reservations, and flight details. In practice, it is very similar to Samsung's At a Glance feature, just native to the Google experience powering Pixel.\n\nOne use case where generative AI really excels is language translation. Because LLMs have a deep grasp of language and how people speak -- including conversational and non-linear speech -- speech translation has gotten a lot more accurate. As a result, many smartphone manufacturers, like Apple, are incorporating AI translation in their own products, and Google unveiled its own.\n\nWith Voice Translation, you can hear a translation in real time while on a phone call. The most noteworthy part is that it copies the sound of the speaker's voice, making the new audio sound as free-flowing and natural as possible. There is also a really helpful transcript you can follow to help keep track of the dialogue even further.\n\nAlso: I'm a longtime iPhone user, but the Google Pixel 10 has me reconsidering my loyalty\n\nWhen I demoed the feature, I was pleasantly surprised at how quick it was, and most importantly, at how well it copied my voice to make me sound like I was actually speaking German. At rollout, Voice Translate works when translating to or from English with Spanish, German, Japanese, French, Hindi, Italian, Portuguese, Swedish, Russian, and Indonesian.\n\nA good chunk of the new AI features are focused on elevating your photo-taking and editing experience. The new Camera Coach feature aims to take the guesswork out of taking a photo, using Gemini's multimodal capabilities to help you snap the perfect shot by suggesting angles, lighting, and camera modes.\n\nBeyond step-by-step instructions, Gemini will even generate a sample image of what you should aim for in the final product. For example, in a demo, the user was trying to take a photo of a plant leaf but had the angle off-center. Gemini generated the image of the leaf positioned correctly, so then all the demoer had to do was match the visual by adjusting his position.\n\nAlso: Google just copied the worst feature of modern iPhones (but not all hope is lost)\n\nOne of the features I was most impressed by was Camera Coach's ability to recognize what was being shown in the display, down to the specific location. For example, in the image above, the Camera Coach asked the demoer what they wanted to be shown in the photo by identifying that it was a waterfront view, and that it was Chelsea Piers in New York, alongside the Hudson River.\n\nProducing a good photo is equal parts capturing the right shot and post-production editing to optimize it. However, photo editing can often be tricky (especially if you're inexperienced), as there are so many little tools often used in combination. This new feature allows you to describe the change you want using natural language and have the AI make the changes instantaneously.\n\nAlso: I tried Pixel Watch 4 - and these are my 7 favorite upgrades in Google's new watch\n\nFor example, the image at the top of the article was very nicely framed, but it had a distracting glare. Instead of having to find the right tools, you can just say that you'd like to remove the glare, and in a couple of seconds, it's done. Other, more general applications include asking it to \"make it better\" resulted in actions like straightening it out and improving the lighting.\n\n6. Improved the Add Me feature and Auto Best Take\n\nWhile the Add Me and Auto Best Take features are not new, Google upgraded them to make them even more helpful. The Add Me feature, which lets people add the photographer to the photo, now works with photos with an even bigger group of people.\n\nMeanwhile, Auto Best Take automatically finds the best picture in which everyone looks great, where previously you would have had to manually select which ones you wanted to combine.\n\nGoogle's AI Pro Plan costs $20 per month and packages all of Google's best AI offerings. These include the best models in Gemini and standalone tools, including expanded access to Gemini 2.5 Pro, NotebookLM, Deep Research, Veo 3, and Jules.",
  "medium": "Article",
  "links": [
    "https://www.zdnet.com/article/everything-announced-at-made-by-google-2025-pixel-10-pro-fold-watch-4-and-more/",
    "https://www.zdnet.com/article/the-google-pixel-9-pros-add-me-feature-is-an-ar-camera-tool-youll-actually-use/",
    "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
    "https://www.google.com/preferences/source?q=zdnet.com",
    "https://www.zdnet.com/article/everything-announced-at-made-by-google-2025-pixel-10-pro-fold-watch-4-and-more/#link={%22role%22:%22standard%22,%22href%22:%22https://www.zdnet.com/article/everything-announced-at-made-by-google-2025-pixel-10-pro-fold-watch-4-and-more/%22,%22target%22:%22_blank%22,%22absolute%22:%22%22,%22linkText%22:%22%3Cstrong%3EEverything%20announced%20at%20Made%20by%20Google%202025:%20Pixel%2010%20Pro,%20Fold,%20Watch%204,%20and%20more%3C/strong%3E%22}",
    "https://www.zdnet.com/article/camera-coach-on-pixel-10-helps-you-take-better-photos-as-you-shoot-them/",
    "https://www.zdnet.com/article/stop-using-ai-for-these-9-work-tasks-heres-why/",
    "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
    "https://www.zdnet.com/article/what-is-generative-ai-and-why-is-it-so-popular-heres-everything-you-need-to-know/",
    "https://www.zdnet.com/article/i-tried-pixel-watch-4-and-these-are-my-7-favorite-upgrades-in-googles-new-watch/",
    "https://www.zdnet.com/article/im-a-longtime-iphone-user-but-the-google-pixel-10-has-me-reconsidering-my-loyalty/",
    "https://one.google.com/about/google-ai-plans/",
    "https://www.zdnet.com/article/google-just-copied-the-worst-feature-of-modern-iphones-but-not-all-hope-is-lost/",
    "https://www.zdnet.com/article/i-tried-the-standard-google-pixel-10-and-didnt-miss-the-pro-models-one-bit/",
    "https://www.zdnet.com/article/this-is-the-fastest-local-ai-ive-tried-and-its-not-even-close-how-to-get-it/",
    "https://www.zdnet.com/article/googles-new-ai-tool-makes-photo-editing-as-easy-as-asking-and-pixel-10-gets-it-first/"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI features",
      "weight": 0.08735349
    },
    {
      "name": "Google",
      "weight": 0.0823293
    },
    {
      "name": "Google DeepMind",
      "weight": 0.080776274
    },
    {
      "name": "new users",
      "weight": 0.07914018
    },
    {
      "name": "feature",
      "weight": 0.06985659
    },
    {
      "name": "fun new phone colors",
      "weight": 0.06505521
    },
    {
      "name": "photo editing",
      "weight": 0.06398883
    },
    {
      "name": "Pixel Watch",
      "weight": 0.063775316
    },
    {
      "name": "generative AI",
      "weight": 0.06262446
    },
    {
      "name": "new AI-powered features",
      "weight": 0.06100792
    }
  ],
  "topics": [
    {
      "name": "Electronics"
    },
    {
      "name": "Google Pixel"
    },
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.9326171875
    },
    {
      "name": "/Computers & Electronics/Software/Operating Systems",
      "score": 0.8896484375
    },
    {
      "name": "/Internet & Telecom/Mobile & Wireless/Mobile Phones",
      "score": 0.77294921875
    },
    {
      "name": "/Shopping/Consumer Resources/Product Reviews & Price Comparisons",
      "score": 0.43017578125
    }
  ],
  "sentiment": {
    "positive": 0.22254524,
    "negative": 0.20306307,
    "neutral": 0.57439166
  },
  "summary": "Google's new Pixel 10 series features a number of new AI-powered features, including photo editing with speech, language translation, and prediction of users' needs. The Pixel 10 Series also includes a one-year subscription to the Google AI Pro plan. The new features are powered by the new Google Tensor G5 processor, co-designed with Google DeepMind. The Magic Cue feature suggests relevant information and actions based on user activity. The Daily Hub feature in your Discover feed also includes an integration with Magic Cue, which gathers insights from apps. Voice Translation can hear a translation in real time while on a phone call, copying the sound of the speaker's voice, and improving speech translation accuracy.",
  "shortSummary": "Google's new Pixel 10 series features AI-powered photo editing, language translation, and predictive functionality, aiming to attract new users and enhance functionality across devices.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "7b7557a2af4e4b2db5e88e115e8d07e7",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
      "text": "Is ChatGPT Plus still worth $20 when the free version offers so much - including GPT-5?\nWhen ChatGPT first launched over two years ago, the AI chatbot was met with such high demand that OpenAI introduced a premium plan called ChatGPT Plus. This plan guaranteed access to the chatbot even during blackout periods. ChatGPT Plus perks also included access to OpenAI's most advanced models, making the $20 plan almost a no-brainer for superusers. However, as OpenAI's offerings have evolved over the past couple of years, so have its plans.\nAlso: How ChatGPT could replace the internet as we know it\nIf you consider yourself an AI power user or are looking to get the most out of your AI usage, you're likely wondering which ChatGPT tier you should try. In this guide, we'll help you decide whether a free plan, ChatGPT Plus, or a $200-per-month ChatGPT Pro subscription is the best fit for you.\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nYou should use ChatGPT Plus if...\nChatGPT Plus costs $20 per month. You're probably asking yourself: Why pay when you can use it for free? There are five main advantages, but the TL;DR is that the free tier has heavy usage limits, and the Plus tier gets you the latest features and models first.\nIf you only use ChatGPT now and then for fun, it's not worth subscribing. But if you use it for work, writing, creating images, using it as a search engine, or you find yourself opening the app all day, every day, then subscribing is likely a good idea.\n1. You want access to legacy models\nWhen OpenAI launched GPT-5, the company had it replace all of its legacy models in ChatGPT. The model is meant to combine the best of OpenAI's offerings with a smart model for most queries and a deeper reasoning model for harder problems. However, many paying subscribers were upset because they preferred the prior models for their workflows that were already in place.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nSam Altman even acknowledged in an X post that \"suddenly deprecating old models that users depended on in their workflows was a mistake.\" As a result, OpenAI brought back the option for ChatGPT Plus users to access legacy models, including: GPT-4o, o3, o4-mini, GPT-4.1, and GPT-4.1-mini, in addition to GPT-5 (Auto, Fast, Thinking mini, and Thinking).\n2. You want to access GPT-5's different modes\nWhile GPT-5 combines the best of a deep reasoning model (GPT-5 thinking) and a standard smart model for a balance of speed and quality, users with a Plus subscription can also manually toggle between GPT-5 Thinking, GPT-5 Thinking Mini, and GPT-5 Fast.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nThe benefit: Users can choose to use a reasoning model when they think it would be the best fit instead of having the real router feature automatically understand which model to use based on the conversation, the complexity of the prompt, and more, which is still available with the GPT-5 Auto option. If you often work on harder problems that require the model to think more to output the best possible answer, then having the option to toggle on GPT-5 Thinking could be a compelling reason to subscribe.\n3. You want to access Sora\nOpenAI's Sora video-generating model allows users to create stunning, realistic videos from text prompts or their own assets with up to 1080p resolution. Videos can be up to 20 seconds long, with a limit of 10 seconds for ChatGPT Plus users, and can be widescreen, vertical, or square aspect ratios. Whether you have a real workflow reason for AI-generated content or just want to tinker, it is a fun model to use.\nTo create images, ChatGPT also uses the GPT-4o image generator, OpenAI's most capable image generator to date.\nWith it, you can simply ask \"create an image of\u2026,\" describe a scene, and receive an AI-generated image that can include realistic human generations and even text. You can also upload an existing image and describe edits for GPT-4o to apply -- like adding text, refining details, or changing backgrounds. Just be aware that complex requests may take a couple of minutes.\nAlso: ChatGPT's new image generator shattered my expectations - and now it's free to try\nUsage is capped for free users, but Plus and Pro subscribers can enjoy much higher daily quotas.\nOpenAI said free-tier users can use GPT-4o only a limited number of times within a five-hour window. It will notify you once you've reached the limit and invite you to upgrade to ChatGPT Plus. If you're on the Free tier and rely heavily on image creation, you might see a message like: \"You've reached your image creation limit. Upgrade to ChatGPT Plus or try again tomorrow...\"\nSo, if you often use AI for visual assets, such as videos or images, ChatGPT Plus is an easy choice.\n4. You want to use the Codex AI coding agent\nOpenAI recently integrated its powerful Codex AI coding agent into ChatGPT Plus, so now anyone with the $20-a-month Plus can get AI-powered coding suggestions without shelling out $200. You can point it at your GitHub repo, and it'll whip up code changes, run checks to make sure nothing's broken, and even handle installing dependencies online if you let it. Just keep in mind it doesn't \"remember\" anything between sessions, so you have to give it clear instructions every time.\nAlso: You can use OpenAI's super powerful AI coding agent Codex for just $20 now\nAlso, since the coding agent is new to ChatGPT, you might encounter some speed bumps if many people are using it at once.\n5. You want expanded access to ChatGPT's best features\nYou're probably noticing a theme here: The majority of ChatGPT's most advanced features are eventually made available to free users, but with heavy usage limitations. Therefore, the main benefit of upgrading is getting to use features like Deep Research, Advanced Voice Mode, and GPT-4o image generation a lot more frequently than you could on the free tier. Subscribing to ChatGPT Plus also gives you priority access to OpenAI's latest models and newest tools well before they reach the free tier.\nWith a Plus account, you can access extended limits on:\n- messaging\n- file uploads\n- image creation\n- data analysis\n- Deep Research\n- Agent Mode\n- GPT-4 image generation\n- memory and context\nIf you don't want to miss out on the latest ChatGPT features and models, you should subscribe to the Plus tier.\nYou should use ChatGPT Pro if...\nOK, so now that we've discussed why Plus is worth it, let's look at the more expensive Pro plan that costs $200 a month. Most people would never pay a car payment's worth of money to use AI, but there are special instances in which it could be worth it.\n1. You want what ChatGPT Plus has on a greater scale\nAll ChatGPT Plus perks and features are included in ChatGPT Pro, but with far fewer usage constraints. Pro also adds several exclusive capabilities. Here's a rundown of the biggest benefits, should you be seriously considering the upgrade.\n- Unlimited access to all reasoning models: This includes GPT-5 with pro reasoning, GPT-4o, GPT-4.1 (and its mini variant), o3, o3-pro, o4-mini, and GPT-4.5 (research preview).\n- Unlimited and faster image generation using GPT-4o.\n- Maximum Deep Research, memory, and context, Agent Mode\n- Priority access to Sora video generation: You get up to 1080p video generation, 20-second videos, five concurrent generations, and watermark-free downloads.\n- Higher access to the Codex agent, projects, tasks, and custom GPTs.\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\nAs you can see, Pro offers more features and higher limits than the Plus tier. Pro users reportedly rarely encounter any constraints and can truly leverage the full power of ChatGPT and its newest tools.\n2. You want access to OpenAI's most powerful models\nAs mentioned in the No. 1 selling point of ChatGPT Plus, there are many perks to being able to access the older models, including sticking to what you were already using in your workflow and what you are accustomed to. In addition to all of the legacy models accessible in the Plus tier, with the Pro subscription, you can also access GPT-5 Pro, the most capable version of the model aimed toward complex tasks, and the Thinking 4.5 (research previews), which is only available to Pro users because \"it costs a lot of GPUs,\" according to Altman.\nYou should use free ChatGPT if...\nFinally, let's look at the free version of ChatGPT. I discussed most of its features above, so in this section, I'll break down what advantages it can offer over the paid plans. It essentially comes down to how often you use ChatGPT and whether having access to the latest and greatest features matters to you. If you don't use it much and don't care, the free version is 100% the way to go.\n1. You don't want to pay a monthly fee\nThe free subscription now offers many competitive AI features, reducing the need for a Plus subscription, especially for tools that were once paywalled. Free ChatGPT users can access:\n- GPT-5\n- ChatGPT Search (web browsing for timely information and sources)\n- Image Generation via GPT-4o (daily limit applies)\n- Deep Research (up to five \"lightweight\" tasks per month)\n- ChatGPT Voice (free monthly preview)\n- File and Photo Uploads for in-chat discussions (limited usage)\n- Memory Feature for referencing recent conversations (lightweight version)\nIf you only need occasional AI assistance -- and do not mind daily usage caps -- the free tier should serve you well. It's surprisingly robust and now includes many advanced features that were once behind a paywall. For instance, when OpenAI launched GPT-5, it also became available for free users.\n2. You're a casual ChatGPT user\nIf you rarely hit the daily usage limits for text, voice, or image generation, upgrading to Plus or Pro might not be necessary.\nAlso: ChatGPT can record, transcribe, and analyze your meetings now\nThe free version offers enough headroom for light interactions -- asking a few questions per day, generating a handful of images, briefly exploring advanced data analysis, and more. However, if you often see messages about hitting your limits (for text, voice, images, etc.), it might be time to consider paying for a subscription. For superusers who need top-tier capacity and extended access to advanced features, there's ChatGPT Pro, but Plus is a more affordable middle ground.\nUltimately, if your needs are minimal and limitations don't bother you, sticking with the free tier is perfectly fine.\nFAQs\nHow much do ChatGPT Free, Plus, and Pro cost?\nAs of August 2025, here are the ChatGPT Free, Plus, and Pro pricing tiers and how they differ, as displayed by OpenAI on its website:\nDoes the Pro tier include everything you get with Plus?\nYes, Pro contains everything in Plus. That means unlimited or higher limits on certain features and exclusive access to GPT-5 Pro and Research preview of new features.\nDo ChatGPT Pro users experience downtime or blackout periods?\nPro subscribers have the highest priority for uptime, making downtime extremely rare. However, no tier can guarantee 100% uptime if OpenAI undergoes major outages or scheduled maintenance.\nIf you upgrade to Plus, can you later switch to Pro?\nYou can upgrade from Plus to Pro at any time through your account settings. Your billing date may adjust based on when you switch.\nHow to get early access to new ChatGPT features on the free tier\nUnfortunately, free users typically must wait until OpenAI rolls them out publicly. Plus and Pro subscribers receive early or exclusive testing opportunities before features reach the free plan."
    },
    {
      "url": "https://www.zdnet.com/article/google-just-copied-the-worst-feature-of-modern-iphones-but-not-all-hope-is-lost/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nGoogle made a bold move with the Pixel 10, even if it's not obvious yet. Here's why\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- The Google Pixel 10 series in the US is losing the physical SIM card slot and going eSIM only.\n- The Google Pixel 10 Pro Fold is the only exception, with a later launch date in October.\n- To accommodate the networking change, Google is offering enhanced SIM-swap support.\nGoogle just announced its latest smartphone lineup, the Pixel 10 series, and yes, on the surface, they look a whole lot like last year's Pixel 9. However, there is one very significant change under the hood: the move toward using eSIMs only, ditching the physical SIM card entirely.\nWith last year's Pixel 9 series, users had the option of having a traditional SIM card or an eSIM. With the Pixel 10, foregoing the SIM tray altogether is a rather unexpected move that might not be popular with everyone. Note that this change applies to phones in the US only.\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\nSo what's the difference between the two? Traditional SIM cards -- familiar to most smartphone users -- are physical, removable chips associated with a single carrier and number. An eSIM (or embedded SIM) is a built-in component that's part of the phone's hardware.\nThe benefits of an eSIM include the ability to have different profiles attached to different numbers or plans -- even different carriers -- on the same device, allowing for different lines for personal or business, for example.\nThe Pixel 10 series phones will support two eSIM \"slots,\" allowing for two different numbers. This is all managed by the phone's software or by scanning a QR code, instead of extracting a chip from the device.\nAlso: Finally, Pixel 10 ends Android's MagSafe envy with its new PixelSnap feature\nAll the new Pixel 10 models announced will move toward eSIM only, except, notably, the Pixel 10 Pro Fold, which will continue to support both forms.\nIt's mostly newer phones that have gone the eSIM route, and something Apple has done since the iPhone 14 was released back in 2022. Removing the physical tray that houses a physical SIM card also frees up valuable real estate in the phone for other components, part of the continual march toward smaller and smaller devices.\nA few other Androids support eSIM, including Samsung's Galaxy S22 series, the Galaxy Z Fold 4, and Z Flip 4, among others. But as a whole, eSIM support across Android devices is an inconsistent patchwork based on different models and carriers.\nAlso: 5 new AI features on Pixel 10 that feel like magic\nFor example, older Pixel phones do support eSIM, but they're largely limited to Google Fi -- the company's own phone carrier service. The Pixel 10, by comparison, is compatible with other carriers like Verizon.\nGoogle is now squarely at the forefront of eSIM support on Android, as its adoption on the Pixel 10 series goes to show. The bottom line is there's still a lot of compatibility issues to iron out in the Android ecosystem, leaving a lot of users tethered to traditional SIM cards."
    },
    {
      "url": "https://www.zdnet.com/article/everything-announced-at-made-by-google-2025-pixel-10-pro-fold-watch-4-and-more/#link={%22role%22:%22standard%22,%22href%22:%22https://www.zdnet.com/article/everything-announced-at-made-by-google-2025-pixel-10-pro-fold-watch-4-and-more/%22,%22target%22:%22_blank%22,%22absolute%22:%22%22,%22linkText%22:%22%3Cstrong%3EEverything%20announced%20at%20Made%20by%20Google%202025:%20Pixel%2010%20Pro,%20Fold,%20Watch%204,%20and%20more%3C/strong%3E%22}",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nEvery Pixel device announced at Made by Google this week: 10 Pro Fold, Watch, Buds, more\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nIt's that time of the year again. No, not Apple's product launch -- that's next month. I'm talking about Made by Google 2025. The tech giant held its annual event on Wednesday, where it showed off its upcoming releases; namely the Pixel 10 series, Pixel Watch 4, and the Pixel Buds 2a.\nAlso: I went hands-on with every Google Pixel 10 model - and was surprised by the one I loved most\nWe managed to get a sneak peek at the devices ahead of the big day, and we're here to share everything we know. Admittedly, none of the hardware announcements are super surprising. We've had a pretty good idea that the aforementioned devices were going to be headliners, especially since Google unveiled the Pixel 10 series early in July.\nIt isn't all hardware reveals, however, as the event packs a few notable surprises -- including new smartphone features like Magic Cue and Pixelsnap. Google provided a more entertaining look during its presentation, but here's a quick breakdown of all the products and software updates being shown off at Made by Google.\n1. Google Pixel 10 series: Pro, XL, and Fold\nGoogle's Pixel 10 series is the star of the show. The line consists of four models: the base Pixel 10, Pixel 10 Pro, Pixel 10 Pro XL, and Pixel 10 Pro Fold. Unlike the previous generation, this batch isn't an overhaul of the Pixel line. It primarily builds on what the Pixel 9 line brought to the table.\nPixel 10: Starting with the base model, the Pixel 10 sports a 6.3-inch Actua display, 12GB of RAM, and a new 5x telephoto camera alongside the 50MP wide and 48MP ultrawide lenses. This new option delivers 10x optical image quality and Super Res Zoom up to 20x, letting you capture high-quality close-up shots from further away. Pricing starts at $799, and the model will be available in Obsidian (black), Indigo (blue), Frost (white), and Lemongrass (green).\nPixel 10 Pro: The Pro model is similar in size, sporting a 6.3-inch Super Actua display that outputs at a new peak brightness of 3,300 nits. Notable upgrades include 16GB of RAM, a purported 30-hour battery life, and a better 5x telephoto lens, allowing the Pixel 10 Pro to support 100x Pro Res Zoom for great-looking closeups. Prices start at $999, and it'll be available in Obsidian (black), Moonstone (gray), Porcelain (white), and Jade (green).\nAlso: I'm a longtime iPhone user, but Google just sold me on the Pixel 10 with these features\nPixel 10 Pro XL: The Pixel 10 Pro XL has many of the same features and specifications as the Pixel 10 Pro, with one major difference: It has a 6.8-inch Super Actua display. Prices start at $1,199, and it'll be available in the same colors as the 10 Pro.\nPixel 10 Pro Fold: The Pixel 10 Pro Fold is larger than the previous generation, boasting a 6.4-inch Actua outer display and an 8-inch Super Actua inner display. Google made interesting design changes, namely what it describes as a \"new gearless, high-strength hinge\" that allows the 10 Pro Fold to survive over 10 years of constant opening and closing.\nPerhaps more importantly, the Pixel 10 Pro Fold is the first foldable phone to feature an IP68 rating, providing complete protection against dust and allowing it to survive brief submersion underwater. Its closest competitor, the Samsung Galaxy Z Fold 7, only supports an IP48 rating. Pricing for the Fold starts at $1,799, and it'll be available in two colors: Moonstone and Jade.\nAll four models run on Google's new Tensor G5 chipset, which the company says is 60% more powerful than the previous generation Tensor G4. Each smartphone boasts 30-hour battery life, key support from Gemini AI, and receives seven years of software and security updates. Preorders for the Pixel 10 series have begun, August 20, with open sales availability starting next week on August 28.\nYou will have to wait a while to order the Pixel 10 Pro Fold, as it won't be available until October 9.\n2. Magic Cue, Pixelsnap, and more AI tools\nGoogle is once again utilizing artificial intelligence (AI) to elevate the user experience. This time, the company is focused on improving AI assistance on its smartphones to make certain features easier to use. Additionally, Google is making an interesting design change that could revamp the way people interact with their Pixel phones.\nAlso: How to clear your Android phone cache (and give it a serious speed boost)\nMagic Cue: Magic Cue is another personal assistant AI, but one that's more proactive. Instead of waiting for your input, the feature anticipates your needs and suggests actions or provides information \"based on the context of your phone.\" For example, let's say you're calling your airline about an upcoming flight. Magic Cue will instantly bring up your flight details during the call.\nCamera updates: All Pixel 10 models will receive two new camera features. First is Camera Coach, which will provide suggestions on how to set up shots or recommend certain camera modes to use. Auto Best Take allows the phone to combine similar photographs into one perfect image. Then there's the improved Add Me. While functioning much like before, the feature now lets you add the photographer to bigger group photos.\nPixelsnap: Pixelsnap is a new magnetic technology that will be included on all Pixel 10 phones. It allows users to snap Qi2-compatible wireless chargers, stands, and grips to the back of the device. If that sounds familiar, iPhones have supported similar technology for years.\nAlso: PixelSnap is the MagSafe for Android phones we've been waiting for - here's our first look\nTo coincide with the implementation of Pixelsnap, Google will be rolling out supporting accessories. The Pixelsnap Charger ($40) provides Qi2 wireless charging up to 25W. The Pixelsnap Charger with Stand ($70) does the same thing but sports a stable dock to support smartphones. Then there's the Pixelsnap Ring Stand ($30) for hands-free viewing. Google will also release special Pixelsnap compatible phone cases at $50 each.\n3. Google Pixel Buds 2a\nGoogle is revealing the Pixel Buds 2a, a new budget-friendly version of the Pixel Buds 2 Pro. Housed inside the familiar egg-shaped case, the company says this pair is the first A-Series earbuds to feature Active Noise Cancelation with Silent Seal 1.5 to block out outside noise. To hear the surrounding environment, users can activate Transparency Mode to allow sound to pass through.\nGoogle's Pixel Buds 2a shares multiple features with the Buds 2 Pro, including 11mm dynamic drivers, support for spatial audio, and a 5-band equalizer for adjusting the bass, treble, and other parts of the audio. The buds run on the Tensor A1 chipset and boast a 20-hour battery life with the charging case.\nAlso: I compared the new $130 Pixel Buds to Apple, Sony and Bose - here's how Google wins\nThe Pixel Buds 2a will be available on store shelves on October 9 for $130, and in two colors: Hazel (black) and Iris (light blue). Preorders open today, August 20. There is no word on when the Pixel Buds 3 Pro will roll out. However, if Google follows the same pattern with the Buds Pro and Buds 2 Pro, expect new earbuds next year.\n4. Google Pixel Watch 4\nRounding out the list is the Pixel Watch 4. Google's future wearable introduces multiple design updates and extra safety features.\nLike the Pixel Watch 3, this model sports an Actua 360 domed display made out of tough Corning Gorilla Glass. However, this generation's touchscreen is 10% larger than before and 50% brighter, boasting 3,000 nits. Its battery sees an improvement too, now lasting a purported 40 hours on a single charge for the 45mm model and 30 hours for the 41mm watch.\nAlso: I tried Pixel Watch 4 - and these are my 7 favorite upgrades in Google's new watch\nBoth sizes are gaining 25% faster charging speeds, along with a redesigned charging system that now flanks the side edge of the watch instead of the middle surface. How this plays out in the real world remains to be tested.\nThe most significant design change, however, is that the Pixel Watch 4 can actually be repaired. Google says the wearable is its \"first watch designed with serviceability in mind\" as users are able to replace the display and battery.\nMany safety features found on the Pixel Watch 4 are carryovers from the Pixel Watch 3. You have Loss of Pulse, which allows the Watch 4 to call for an ambulance if it detects that your heart has stopped beating. Fall and Crash Detection contacts emergency services if it detects that you've had a bad fall or were in a car accident.\nThere is a new feature in all this. Pixel Watch 4 LTE has an SOS satellite communication function that enables users to contact emergency services if they're ever in a location without any cellular or Wi-Fi connections.\nAlso: The best smartwatches 2025: I wore these for weeks and found the perfect one for your wrist\nThe Pixel Watch 4 will be available in two different sizes: 41mm and 45mm, with colors correlating to a certain size. The 45 Pixel Watch 4 will have three colors for its wrist bands: black, white, and gray. The 41mm Pixel Watch 4 keeps the white and black wrist bands, but also throws in Iris blue and Lemongrass green bands.\nThe 41mm Pixel Watch 4 will retail for $350, while the 45mm model will be a bit more expensive at $400. Preorders go live today, August 20. The wearable is set to hit store shelves on October 9."
    },
    {
      "url": "https://www.zdnet.com/article/im-a-longtime-iphone-user-but-the-google-pixel-10-has-me-reconsidering-my-loyalty/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nI'm a longtime iPhone user, but the Google Pixel 10 has me reconsidering my loyalty\nZDNET's key takeaways\n- Google's PixelSnap is the biggest upgrade for me on the company's new phones.\n- The $799 Pixel 10 now comes with a telephoto camera, a sizeable upgrade for the base model.\n- Gemini AI is much smarter and more capable than Siri.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nAs a phone reviewer, I can't stick to a single phone for long, but I've been on the iPhone for a while now. That's because of the extra convenience I get with my Mac Mini and a bunch of MagSafe-compatible chargers, power banks, and other travel accessories.\nI always have second thoughts about shifting to Android because it would hinder my workflow and require new backpack accessories. But the Google Pixel 10 is the first smartphone in months that I'm considering dropping iOS for.\nAlso: Camera Coach on Pixel 10 helps you take better photos as you shoot them\nIf you've been on the fence about switching from an iPhone, the new Pixel 10 phones are the best alternatives you can buy right now. The Pixel 10 seems like the perfect Android phone to switch to. Here are three features that have sold this iPhone user on the Pixel 10.\n1. PixelSnap is a long-awaited addition\nWhen I first shifted from my iPhone 16 Pro to the iPhone 16e, I thought I'd miss ProMotion the most, but it was MagSafe that had made its mark over the years. I realized how dependent I was on MagSafe when I couldn't use any of those accessories with Apple's budget phone.\nI'm more confident about switching to the new Google Pixel 10 because it will not make my MagSafe accessories ineffective.\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\nThe Google Pixel 10 series includes a new feature called PixelSnap, a Qi2-based MagSafe alternative for Android. This means, theoretically, I can use my new Baseus PicoGo 5,000mAh power bank, Shiftcam SnapStand Max, and more without any issues.\nThese are some of my travel essentials, and I won't need to purchase separate on-the-go accessories just because I got a new phone. The addition of PixelSnap is my favorite upgrade on the new Pixels.\n2. Smarter voice assistant\nApple Intelligence hasn't been up to the mark, at least for me. While Visual Intelligence is useful, I prefer Samsung's Galaxy AI suite of features on my Galaxy Z Fold 7. Moreover, I've been using Gemini Live more than expected and way more than I have ever used Siri on my iPhone.\nWhile Siri can't get reminders right or call my contacts in one go, Gemini Live has helped me organize my room and solve daily life problems. It is now getting support for more Google apps like Keep and Maps. So, I can add my thoughts to Google Keep without needing to touch my phone. Since I use Google apps in my workflow, Gemini makes things easier and more intuitive.\nAlso: PixelSnap is the MagSafe for Android phones we've been waiting for - here's our first look\nWhile I can access Gemini on my iPhone without any issues, Google's new Pixel-exclusive AI features make a strong case for switching. I'm looking forward to experiencing the new Magic Cue -- which is supposed to deliver more personalized and contextual AI -- and Camera Coach, which is touted to help compose shots better.\n3. Telephoto camera on the base model\nThe base iPhone 16 doesn't have a telephoto camera, whereas the iPhone 16 Pro sports a comparatively small 5x periscope sensor. I've used the latter, and it struggles in low light. To compare, the Google Pixel 10 Pro features a larger sensor with 5x optical zoom, and the base Pixel 10 also gets an additional zoom lens with the same optical zoom.\nI'm not expecting exceptional zoom quality from the Pixel 10's telephoto camera because it is a smaller sensor. However, it should be better than the iPhone 16's 5x digital zoom. I like shooting architecture photos, and having 5x optical zoom on a $799 phone is a welcome addition.\nAlso: Why I recommend this $400 Google Pixel over competing models from Samsung and OnePlus\nApart from these three features, I'm also looking forward to going back to the Pixel for all the Google extras. From fun features like Add Me and Best Take to useful Circle to Search and Magic Eraser, all of them add to the Google Pixel experience. While I'm not a fan of comparatively thicker bezels, the Pixel 10's brighter and stronger display is another nice-to-have addition that should make the experience better."
    },
    {
      "url": "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
      "text": "How to use GPT-5 in VS Code with GitHub Copilot\nZDNET's key takeaways\n- GitHub Copilot Pro now supports GPT-5 in VS Code.\n- A 30-day trial lets you test premium models for free.\n- Add your OpenAI key to bypass Copilot restriction.\nGPT-5 is now available for use with Microsoft's GitHub Copilot in VS Code. In this article, I'll walk you through the steps of setting up the linkages between VS Code, Copilot, and GPT-5. This process will also work for most other supported large language models you want to use.\nAlso: Microsoft rolls out GPT-5 across its Copilot suite - here's where you'll find it\nStep 1: Enable GitHub Copilot Pro\nYou'll need the Pro version of GitHub Copilot to use GPT-5 at this time. The new model might be available for Copilot's free tier someday, but not yet. There is, however, a 30-day free trial. I'll show you how to set that up here:\nFirst, open VS Code. Click the little Copilot icon (1). That action will open the Copilot pane. Next, click whatever model is listed at (2). Mine is GPT-4.1. Finally, click Add Premium Models (3).\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nThis will take you to a web page where you'll be given the opportunity to try Copilot Pro for 30 days at no cost. Click the big green button.\nUnfortunately, you'll have to add your credit card info, although it won't be charged for 30 days. To prevent abuse of the 30-day limit, Microsoft requires you to give your personal information and your credit card number:\nOnce you've done that, click the Activate Now button:\nStep 2: Enable GPT-5\nYou'll need to restart VS Code for the Pro mode to be made available. Once you do that, click the current model (in the screenshot, it's GPT-4.1), and then scroll and choose GPT-5:\nYou will then need to issue a prompt. In my Deep Research dump of my repository, I was told about some references to a product I sold a few years ago that were still in the product's code. I told GPT-5 to remove all such references.\nThat process resulted in the Enable button showing up. Basically, I think you can use any prompt with GPT-5 to trigger the Enable button. Then click it:\nStep 3: Bring your own key\nBy using the Pro account, you are given a certain number of times you can use the various models. It's unclear how that usage limit is calculated, so I have reached out to Microsoft for clarification. I'll update this article when I get more info.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nIf you want to bypass the possible restrictions and rate limitations, you can use your own API key as provided by your LLM service. You can learn more from Microsoft's language models page.\nWith ChatGPT, you can get an OpenAI Platform Key by pointing your browser here. If you don't already have an OpenAI account, you may need to give them some credit card information as well. Then click Create Key and follow the directions:\nOnce you have your key, go back to Manage Models (by clicking the current model you're using and choosing Manage Models). Select OpenAI:\nType or paste in your key. Press Enter to confirm:\nCongratulations, you're now running GPT-5 in Copilot.\nHave you used Copilot?\nHave you tried using GPT-4.1 or GPT-5 in VS Code yet? What do you think of the Copilot Pro experience so far? Does it feel like a worthwhile upgrade? Have you explored using your API key instead of relying on Microsoft's allocation?\nAlso: The best AI for coding in 2025 (and what not to use)\nWhat kinds of tasks or prompts have you found AI particularly helpful for in your coding work? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/i-tried-the-standard-google-pixel-10-and-didnt-miss-the-pro-models-one-bit/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nWhy I'm recommending this Google Pixel 10 model to most people (and don't regret it)\nZDNET's key takeaways\n- The Google Pixel 10 packs the Tensor G5 chipset, a new telephoto lens, and other long-awaited hardware upgrades.\n- On the software side, the Google Pixel 10's AI suite can predict your needs, edit photos for you, and more.\n- While packing all of these upgrades, the Pixel 10 still retails for $799.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nAs summer draws to a close and Techtember nears, Google is kicking off the transition with its Made by Google event this week. One of the major releases at the event is Google's latest entry-level flagship device, the Pixel 10 -- and out of the entire Pixel 10 Series, it's the device to keep an eye on.\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\nWhile the Pixel 10 is the company's baseline model, after spending hands-on time with all the new devices ahead of launch, I can confidently say it's the best fit for most users. Why? The Pixel 10 packs highly anticipated upgrades, such as a better camera, battery, and chipset (which supports a plethora of new and exciting AI features), all at the same $799 price point as its predecessor.\nHere are my first impressions and a round-up of what makes this phone a solid option.\n1. Design\nThe traditional compact and sleek feel of the Pixel baseline models continues with the Pixel 10, which sports similar dimensions and weight (a mere 0.2 ounces heavier) as its predecessor and feels just as comfortable in the palm of your hand.\nWith rounded corners, aluminum sides, a smooth Corning Gorilla Glass Victus 2 back (also featured on the front), and a camera bar, the device maintains the same aesthetic as the Pixel 9 while also sporting fun new phone colors.\nAlso: I went hands-on with every Google Pixel 10 model - and was surprised by the one I loved most\nWhile the new lineup includes the Obsidian color from prior years' releases, the phone also comes in new colors: Indigo, a bright blue color similar to Apple's Ultramarine color for the iPhone 16; Indigo, a white shade with a subtle blue undertone; and Lemongrass, a vibrant yellow-lime shade.\nThe glassy finish provides a nice sheen on the exterior, but if you mind smudges, you may want to watch out. Like last year, the phones still feature an IP68 rating for durability against water and dust.\nThe displays got a facelift for a better viewing experience in indoor and outdoor settings, across both productivity and entertainment activities. The 6.3-inch Actua display now supports up to 3000 nits of peak brightness (up from last year's 2700 nits), and keeps the buttery smooth 120Hz refresh rate, 2424x1080 resolution, and OLED panel found on the Pixel 9.\n2. Most capable camera system yet on a base Pixel\nOne of the biggest upgrades of the Pixel 10 is its camera system, which is the best on any base Pixel. The phone features a new 5x telephoto lens, which enables a wider optical zoom range, with a Super Res zoom up to 20x. In practice, this capability means that the phone is better when cropping in close on a subject, even from a distance, helping you capture crisper shots while maximizing the zoom. This feature set also closes the gap between the base and Pro phones even further.\nAlso: Google just settled the foldable phone debate with one number -- and I hope Samsung is watching\nThe triple camera array replaces the dual camera system, featuring a 48MP main camera, a 13MP 120-degree ultrawide, and a new 10.8MP telephoto lens. If you were hoping for an upgrade from the lower-quality front camera, you'll have to wait until the next model, as the Pixel 10 keeps last year's 10.5MP selfie camera. It's not bad, but also not the best.\nWhile the addition of a third lens meant a downgrade in the sensors' specs compared to those of the previous year's model, the camera's Imaging Signal Processor (ISP), enabled by the new Tensor G5 chipset, is set to improve the photo quality. Google promises this setup delivers Pixel's \"best image and video quality yet.\"\nAlso: Changing these 10 settings on my Pixel phone greatly improved the user experience\nWhen taking photos in the demo room, I was satisfied with the crispness and quality of the shots from both the front and back cameras. To truly judge how well the Pixel 10 does in different light conditions, I will have to wait for my in-depth testing. In addition to the actual camera upgrades, the photo-taking software received the bulk of AI upgrades, which will likely boost your photo-taking and editing experience, resulting in overall better picture quality.\n3. Tensor G5\nThe Pixel 10 features an expansive suite of AI features, all enabled by the new Tensor G5 chipset. The processor was designed for Gemini and is Google's most AI-optimized chipset yet, with a TPU that is up to 60% more powerful and a CPU that is 34% faster, according to Google.\nWhile we'll have to test the phone in the real world to gauge performance, the chipset supports computational enhancements on the Pixel 10, such as the ISP upgrades discussed above, a longer battery life (30 hours), and improved experiences when using AI features.\nAlso: 7 Google Pixel phone settings you should change immediately (and how to do it)\nOn-device processing is a win for users as it not only allows them to experience faster speeds when using AI features by avoiding cloud-based processing, but also provides privacy, as all of their information stays on the device. The Tensor G5 can run Google's Gemini Nano model on-device, a powerful model that supports the extensive suite of AI features that are sprinkled across nearly every phone function, from the camera to texts and phone calls.\n4. AI features\nAs with every device launch from the last year, Google's new hardware is accompanied by a ton of new AI features. While most tech companies are in a race where they release similar features to keep up with their rivals, I was impressed that Google managed to include some truly original features that should add value to some users' lives.\nA perfect example is the new Camera Coach feature, which, as its name implies, uses AI to help you get the perfect shot. The feature gives step-by-step instructions and even generates a sample image of what you should aim for in the final picture. Some of the feedback includes correcting your angle, lighting, and more.\nAlso: I upgraded to Android 16 - here's what I love and what's still missing\nOther new features include: Edit with Ask Photos, which allows you to use natural language prompts to edit something out of an image, such as removing reflection; Magic Cue, which can anticipate your needs and suggest relevant information and actions based on what you are doing on your phone; and a new Daily Hub feature, which puts all the information you need in one place, including AI-powered insights about your day. Even older features, such as Best Take and Add Me, have received a boost.\nGoogle might trickle these experiences down to older models via a software update. However, the company may choose to keep these features exclusive to the Tensor G5 chipset due to processing demands or as an incentive to upgrade.\n5. Meet Pixelsnap\nIf you have ever looked at an iPhone user with MagSafe envy, Google has finally brought the same experience to Pixel phones via its new Pixelsnap magnetic technology. Pixelsnap allows users to attach their phones to compatible magnetic accessories, including chargers, mounts, docks, and more, unlocking new experiences with their phones.\nWhen this functionality is paired with the Qi2 wireless charging support available on the Pixel 10, users can take advantage of the full glory of fast wireless charging.\nAlso: PixelSnap is the MagSafe for Android phones we've been waiting for - here's our first look\nGoogle also launched a Pixelsnap Charger with Stand and a Pixelsnap Ring Stand, which allow users to prop their phones up for a hands-free experience. This approach is ideal for viewing movies, scrolling on social media, video calls, and more.\nEven if you don't want to purchase the accessories, they work with existing MagSafe accessories, so there are thousands of products already on the market that now work with Pixel phones."
    },
    {
      "url": "https://www.zdnet.com/article/stop-using-ai-for-these-9-work-tasks-heres-why/",
      "text": "Stop using AI for these 9 work tasks - here's why\nZDNET's key takeaways\n- Sometimes an AI can cause you or your company irreparable harm.\n- Sharing confidential data with an AI could have legal consequences.\n- Don't let an AI talk to customers without supervision.\nA few weeks ago, I shared with you \"9 programming tasks you shouldn't hand off to AI - and why.\" It's full of well-reasoned suggestions and recommendations for how to avoid having an AI produce code that could ruin your whole day.\nThen, my editor and I got talking, and we realized the whole idea of \"when not to use an AI\" could apply to work in general. In this article, I present to you nine things you shouldn't use AI for while at work. This is far from a comprehensive list, but it should make you think.\nAlso: This one feature could make GPT-5 a true game changer (if OpenAI gets it right)\n\"Always keep in mind that AI isn't going to read you your Miranda Rights, wrap your personal information in legal protections like HIPAA, or hesitate to disclose your secrets,\" said LinkedIn Learning AI instructor Pam Baker, the bestselling author of ChatGPT For Dummies and Generative AI For Dummies.\n\"That goes double for work AI, which is monitored closely by your employer. Whatever you do or tell AI can and likely will be used against you at some point.\"\nTo keep things interesting, read on to the end. There, I share some fun and terrifying stories about how using AI at work can go terribly, horribly, and amusingly wrong.\nWithout further ado, here are nine things you shouldn't do with AI at work.\n1. Handling confidential or sensitive data\nThis is an easy one. Every time you give the AI some information, ask yourself how you would feel if it were posted to the company's public blog or wound up on the front page of your industry's trade journal.\nAlso: The best AI for coding in 2025 (and what not to use)\nThis concern also includes information that might be subject to disclosure regulations, such as HIPAA for health information or GDPR for personal data for folks operating in the EU.\nRegardless of what the AI companies tell you, it's best to simply assume that everything you feed into an AI is now grist for the model-training mill. Anything you feed in could later wind up in a response to somebody's prompt, somewhere else.\n2. Reviewing or writing contracts\nContracts are designed to be detailed and specific agreements on how two parties will interact. They are considered governing documents, which means that writing a bad contract is like writing bad code. Baaad things will happen.\nDo not ask AIs for help with contracts. They will make errors and omissions. They will make stuff up. Worse, they will do so while sounding authoritative, so you're more likely to use their advice.\nAlso: You can use Google's Math Olympiad-winning Deep Think AI model now - for a price\nAlso, the terms of a contract are often governed by the contract. In other words, many contracts say that what's actually in the contract is confidential, and that if you share the particulars of your agreement with any outside party, there will be dire consequences. Sharing with an AI, as discussed above, is like publishing on the front page of a blog.\nLet me be blunt. If you let an AI work on a contract and it makes a mistake, you (not it) will be paying the price for a long, long time.\n3. Using an AI for legal advice\nYou know the trope where what you share with your lawyer is protected information and can't be used against you? Yeah, your friendly neighborhood AI is not your lawyer.\nAs reported in Futurism, OpenAI CEO (and ChatGPT's principal cheerleader) Sam Altman told podcaster Theo Von that there is no legal confidentiality when using ChatGPT for your legal concerns.\nAlso: Even OpenAI CEO Sam Altman thinks you shouldn't trust AI for therapy\nEarlier, I discussed how AI companies might use your data for training and embed that data in prompt responses. However, Altman took this assertion up a notch. He suggested OpenAI is obligated to share your conversations with ChatGPT if they are subpoenaed by a court.\nJessee Bundy, a Knoxville-based attorney, amplified Altman's statement in a tweet: \"There's no legal privilege when you use ChatGPT. So if you're pasting in contracts, asking legal questions, or asking it for strategy, you're not getting legal advice. You're generating discoverable evidence. No attorney/client privilege. No confidentiality. No ethical duty. No one to protect you.\"\nShe summed up her observations with a particularly damning statement: \"It might feel private, safe, and convenient. But lawyers are bound to protect you. ChatGPT isn't, and can be used against you.\"\n4. Using an AI for health or financial advice\nWhile we're on the topic of guidance, let's hit two other categories where highly trained, licensed, and regulated professionals are available to provide advice: healthcare and finance.\nLook, it's probably fine to ask ChatGPT to explain a medical or financial concept to you as if you were a five-year-old. But when it comes time to ask for real advice that you plan on considering as you make major decisions, just don't.\nLet's step away from the liability risk issues and focus on common sense. First, if you're using something like ChatGPT for real advice, you have to know what to ask. If you're not trained in these professions, you might not know.\nAlso: What Zuckerberg's 'personal superintelligence' sales pitch leaves out\nSecond, ChatGPT and other chatbots can be spectacularly, overwhelmingly, and almost unbelievably wrong. They misconstrue questions, fabricate answers, conflate concepts, and generally provide questionable advice.\nAsk yourself, are you willing to bet your life or your financial future on something that a people-pleasing robot made up because it thought that's what you wanted to hear?\n5. Presenting AI-generated work as your own\nWhen you ask a chatbot to write something for you, do you claim it as your own? Some folks have told me that because they wrote the prompts, the resulting output is a result of their creativity.\nAlso: I found 5 AI content detectors that can correctly identify AI text 100% of the time\nYeah? Not so much. Webster's defines \"plagiarize\" as \"to steal and pass off (the ideas or words of another) as one's own,\" and to \"use (another's production) without crediting the source.\" The dictionary also defines plagiarize as \"to commit literary theft: present as new and original an idea or product derived from an existing source.\"\nDoes that not sound like what a chatbot does? It sure does \"present as new and original an idea\u2026derived from an existing source.\" Chatbots are trained on existing sources. They then parrot back those sources after adding a bit of spin.\nLet's be clear. Using an AI and saying its output is yours could cost you your job.\n6. Talking to customers without monitoring the chatter\nThe other day, I had a technical question about my Synology server. I filed a support ticket after hours. A bit later, I got an email response from a self-identified support AI. The cool thing was that the answer was complete and just what I needed, so I didn't have to escalate my ticket to a human helper.\nAlso: Is AI overhyped or underhyped? 6 tips to separate fact from fiction\nBut not all AI interactions with customers go that well. Even a year and a half later, I'm still chuckling about the Chevy dealer chatbot that offered a $55,000 Chevy Tahoe truck to a customer for a buck.\nIt's perfectly fine to provide a trained chatbot as one support option to customers. But don't assume it's always going to be right. Ensure customers have the option to talk with a human. And monitor the AI-enabled process. Otherwise, you could be giving away $1 trucks, too.\n7. Making final hiring and firing solutions\nAccording to a survey by resume-making app Resume Builder, a majority of managers are using AI \"to determine raises (78%), promotions (77%), layoffs (66%), and even terminations (64%).\"\n\"Why are you firing me?\"\n\"It's not my fault. The AI made me do it.\"\nYeah, that. Worse, apparently at least 20% of managers, most of whom haven't been trained in the rights and wrongs of AI usage, are using AIs to make final employment decisions without even bothering to oversee the AI.\nAlso: Open-source skills can save your career when AI comes knocking\nBut here's the rub. Jobs are often governed by labor laws. Despite the current anti-DEI push coming from Washington, bias can still lead to discrimination lawsuits. Even if you haven't technically done anything wrong, defending against a lawsuit can be expensive.\nIf you cause your company to be on the receiving end of a lawsuit because you couldn't be bothered to be human enough to double-check why your AI was canning Janice in accounting, you'll be the next one being handed a pink slip. Don't do it. Just say no.\n8. Responding to journalists or media inquiries\nI'm going to tell you a little secret. Journalists and writers do not exist solely to promote your company. We'd like to help, certainly. It feels good knowing we're helping folks grow their businesses. But, and you'll need to sit down for this news, there are other companies.\nWe are also busy. I get thousands of emails every day. Hundreds of them are about the newest and by far most innovative AI company ever. Many of those pitches are AI-generated because the PR folks couldn't be bothered to take the time to focus their pitch. Some of them are so bad that I can't even tell what the PRs are trying to hawk.\nBut then, there's the other side. Sometimes, I'll reach out to a company, willing to use my most valuable resource -- time -- on their behalf. When I get back a response that's AI-driven, I'll either move on to the next company (or mock them on social media).\nAlso: 5 entry-level tech jobs AI is already augmenting, according to Amazon\nSome of those AI-driven answers are really, really inappropriate. However, because the AI is representing the company instead of, you know, maybe a thinking human, an opportunity is lost.\nKeep in mind that I don't like publishing things that will cost someone their job. But other writers are not necessarily similarly inclined. A properly run business will not only use a human to respond to the press, but will also limit the humans allowed to represent the company to those properly experienced in what to say.\nOr go ahead and cut corners. I always need fun fodder for my Facebook feed.\n9. Using AI for coding without a backup\nEarlier, I wrote \"9 programming tasks you shouldn't hand off to AI,\" which detailed programming tasks you should avoid passing along to an AI. I've long been nervous about ceding too much responsibility to an AI, and quite concerned about managing codebase maintenance.\nBut I didn't really understand how far stupid could go when it came to delegating coding responsibility to the AI. I mean, yes, I know AIs can be stupid. And I sure know humans can be stupid. But when AIs and humans work in tandem to advance the cause of their stupidity together, the results can be truly awe-inspiring.\nIn \"Bad vibes: How an AI agent coded its way to disaster,\" my ZDNET colleague Steven Vaughan-Nichols wrote about a developer who happily vibe-coded himself to an almost-complete piece of software. First, the AI hard-coded lies about how unit tests performed. Then the AI deleted his entire codebase.\nIt's not necessarily wrong to use AI to help you code. But if you're using a tool that can't be backed up, or you don't bother to back up your code first, you're simply doing your best to earn a digital Darwin award.\nBonus: Other examples of what not to do\nHere's a lightning round of boneheaded moves using AI. They're just too good (and by good, I mean bad) not to recount:\n- Letting a chatbot manage job applicant data: Remember how we told you not to use an AI for hiring and firing? McDonald's uses a chatbot to screen applicants. Apparently, the chatbot exposed millions of applicants' personal information to a hacker who used the password 123456.\n- Replacing support staff with an AI, and gloating: A CEO of e-commerce platform Dukaan terminated 90% of his support staff and replaced them with an AI. Then he bragged about it. On Twitter/X. The public response was less than positive. Way less.\n- Produce a reading list consisting of all fake titles: The Chicago Sun-Times, normally a very well-respected paper, published a summer reading list generated by an AI. The gotcha? None of the books were real.\n- Suggesting terminated employees turn to a chatbot for comfort: An Xbox producer (yes, that's Microsoft) suggested that ChatGPT or Copilot could \"help reduce the emotional and cognitive load that comes with job loss\" after Microsoft terminated 9,000 employees. Achievement unlocked.\nWhat about you? Have you seen an AI go off the rails at work? Have you ever been tempted to delegate a task to a chatbot that, in hindsight, probably needed a human touch? Do you trust AI to handle sensitive data, communicate with customers, or make decisions that affect people's lives? Where do you draw the line in your work? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/this-is-the-fastest-local-ai-ive-tried-and-its-not-even-close-how-to-get-it/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThis is the fastest local AI I've tried, and it's not even close - how to get it\nZDNET's key takeaways\n- The gpt-oss:20b model is very fast.\n- You'll get blazing-fast answers to your queries with gpt-oss:20b.\n- With the latest version of Ollama installed, you can use this model.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nLet's talk about local AI and speed. There are a lot of factors that go into getting the most speed out of your AI, such as:\n- Whether you have a dedicated GPU.\n- The context length you use (the smaller, the faster).\n- The complexity of your query.\n- The LLM you use.\nI've tried quite a few different local LLMs, using Ollama on both Linux and MacOS, and I've recently run into one that blew all the others away -- with regard to speed. That model is gpt-oss:20b. I've found that on both Linux and MacOS, that model is lights-out faster than the others I've used. This model generates 30 tokens per second.\nAlso: My go-to LLM tool just dropped a super simple Mac and PC app for local AI - why you should try it\nWhat is a token? Think of them as pieces of words used for the processing of natural language. For example, with English text, 1 token is approximately 4 characters or 0.75 words, which means gpt-oss:20b can process 120 characters per second.\nThat's not bad.\nConsider a localized version of llama3.2, which can achieve around 14 tokens per second. See the difference?\nOK, now that I've (hopefully) convinced you that gpt-oss:20b is the way to go, how do you use it as a local LLM?\nHow to update Ollama\nWhat you'll need: To make this work, you'll need either a running version of Ollama (it doesn't matter what desktop OS you're using) or you'll need to install it fresh.\n1. Update Ollama on Linux\nIf you're using Linux, you can update Ollama with the same command used to install it, which is:\ncurl -fsSL https://ollama.com/install.sh | sh\n2. Update Ollama on MacOS or Windows\nTo update Ollama on either MacOS or Windows, you would simply download the binary installer, launch it, and follow the steps as described in the wizard. If you get an error that it cannot be installed because Ollama is still running, you'll need to stop Ollama before running the installer. To stop Ollama, you can either find it in your OS's process monitor or run the command:\nosascript -e 'tell app \"Ollama\" to quit'\nOn Windows, that command would be:\ntaskkill /im ollama.exe /f\nYou might run into a problem. If, after upgrading, you get an error (when pulling gpt-oss) that you need to run the latest version of Ollama, you'll have to install the latest iteration from the Ollama GitHub page. How you do that will depend on which OS you use.\nAlso: How I feed my files to a local AI for better, more relevant responses\nIt is necessary to be running at least Ollama version 0.11.4 to use the gpt-oss models.\nHow to pull the gpt-oss LLM\nThe next step is to pull the LLM from the command line. Remember, the model we're looking for is gpt-oss:20b, which is roughly 13GB in size. There's also the larger model, gpt-oss:120b, but that one requires over 60 GB of RAM to function properly. If your machine has less than 60 GB of RAM, stick with 20b.\nAlso: How to run DeepSeek AI locally to protect your privacy - 2 easy ways\nTo pull the LLM, run the following command (regardless of OS):\nollama pull gpt-oss:20b\nDepending on your network speed, this will take a few minutes to complete.\nHow to use gpt-oss\nOK, now that you've updated Ollama and pulled the LLM, you can use it. If you interact with Ollama from the command line, run the model with:\nollama run gpt-oss:20b\nOnce you're at the Ollama console, you can start querying the newly added LLM.\nIf you use the Ollama GUI app (on MacOS or Windows), you should be able to select gpt-oss:20b from the model drop-down in the app.\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nAnd that's all there is to making use of the fastest local LLM I've tested to date."
    },
    {
      "url": "https://www.zdnet.com/article/googles-new-ai-tool-makes-photo-editing-as-easy-as-asking-and-pixel-10-gets-it-first/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nDid Google just give us the ultimate AI photo-editing tool? I tested it on the Pixel, and hard agree\nZDNET's key takeaways\n- Google unveiled its Edit with Ask Photos feature at Made by Google.\n- The new feature lets you make edits to your image using natural language.\n- Android and iOS users will have the feature rolled out to them over time.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nThere was a time when removing objects or distractions from an image required learning how to use advanced tools like Adobe Photoshop and finagling with a lasso tool for much too long. Generative AI has changed that, making complex edits possible with simple taps. Google's new AI-powered feature takes that to the next level.\nThe company held its Made by Google hardware event on Wednesday, and, unsurprisingly, AI releases were a big part of the new Pixel Series 10 launch. The AI features were sprinkled across the phone's different apps, but a bulk of the most exciting AI features focused on helping users get better-quality photos.\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\nThe new features included a new Camera Coach, improved the Add Me feature and Auto Best Take features, and, perhaps the most useful, the new Edit with Ask Photos feature, which makes editing a photo as easy as typing in a prompt.\nAnnoying reflection in a photo? How about a photo bomber in the background? You can't get the composition right? Now all you have to do is ask your Photos app to edit it for you.\nHow 'Edit with Ask Photos' works\nGoogle Photos now has a conversational photo editor, which lets you enter a request for an edit you'd like done to your photo and then have AI do it for you. You don't need to know which slider or tool to use, either. All you have to do is describe the end result you'd like to see. It's really that easy.\nThe edits can be as simple or as complex as you'd like. For example, if you just want help with a simple task, such as making the photo brighter or straighter, you can ask just that; but it can also perform much more complicated tasks, such as having an old photo entirely restored, you can also ask.\nIf you don't know what you want to ask but know you are not happy with what you are looking at, you can also use broad prompts such as \"Make it better,\" according to Google. You can also follow up with as many conversational queries as you need to get the result you are visualizing.\nAlso: Wish you could take better pics? Camera Coach on the Pixel 10 can help - here's how\nIn my demo, using a conversational request that read, \"Remove the reflection,\" the demoer was able to, in seconds, remove a glare in the photo that prevented the subject in the photo from being clearly seen. This is a particularly impressive task because performing this task using legacy tools would have required a higher level of proficiency in photo editing tools and more allocated time to get it done.\nBeyond practical edits, you can also use it for fun edits, such as adding new elements to a photo. In one of my demos, we added glasses to a llama, as seen in the image above. While not super practical for everyday use, it could be fun for silly social media posts or texts with friends.\nAlso: I went hands-on with every Google Pixel 10 model - and was surprised by the one I loved most\nRegardless of what you use the feature for, all images edited using the AI feature will have that delineated in the C2PA Content Credentials, which function as a nutrition label showcasing how an image was made. In this case, the image's Content Credentials would say \"Edited with AI tools.\" The industry objective with these types of measures is to promote transparency and help users discern when an image is real or fabricated.\nAvailability\nThe feature will first be available in the Pixel 10 series, which is available for preorder right now. In the coming weeks, it will gradually roll out to all other Google Photos users on Android and iOS devices."
    },
    {
      "url": "https://one.google.com/about/google-ai-plans/",
      "text": "Are you a student? Get Google AI Pro free for 1 year\nPrep for your exams, perfect your writing, and tackle your homework with the best of Google AI, including limited access to video generation with Veo 3 Fast. Check your eligibility\nGoogle AI Pro1\n-\nGemini app: Get more access to our most capable model 2.5 Pro and Deep Research on 2.5 Pro, plus unlock video generation with limited access to Veo 3 Fast3\n-\nWhisk:5 Higher limits for image-to-video creation with Veo 2\n-\n1,000 monthly AI credits: Across Flow and Whisk\n-\nNotebookLM: Research and writing assistant with 5x more Audio Overviews, notebooks, and more\n-\nGoogle Search:6 Get access to Gemini 2.5 Pro model and Deep Search in AI Mode, plus expanded access to AI-powered calling for local business pricing (US only)\n-\nJules:7 Higher limits to our asynchronous coding agent for software developers\n-\nGemini in Gmail, Docs, and more: Access Gemini directly in Google apps\n-\nGemini in Chrome: Your personal assistant to browse the web (US only)\n-\nStorage: 2 TB of total storage for Photos, Drive, and Gmail\nGoogle AI Ultra2\n-\nEverything in Pro\n-\nGemini app: Highest limits and exclusive access to 2.5 Deep Think (our most advanced reasoning model) and Veo 33 (our latest video generation model)\n-\nWhisk:5 Highest limits for image-to-video creation with Veo 2\n-\n25,000 monthly AI credits: Across Flow and Whisk\n-\nNotebookLM: Highest limits and best model capabilities (coming soon)\n-\nGoogle Search:6 Highest limits to Gemini 2.5 Pro model and Deep Search in AI Mode, plus AI-powered calling for local business pricing (US only)\n-\nJules:7 Highest limits to our asynchronous coding agent for software developers\n-\nGemini in Gmail, Docs, and more: Highest limits to Gemini directly in Google apps\n-\nProject Mariner (early access):8 Streamline tasks with an agentic research prototype (US only)\n-\nYouTube Premium individual plan:9 YouTube ad-free, offline, and in the background\n-\nStorage: 30 TB of total storage for Photos, Drive, and Gmail\nWant to experience Google AI Ultra with your work or school account? Learn more\nCreate cinematic moments with state\u2011of\u2011the\u2011art video generation\nGet access to Google\u2019s most capable AI models\nTackle complex projects with Gemini Pro and Ultra\nNext-gen AI\nOur latest models are far more capable at logical reasoning, analysis, coding, and creative collaboration so you can get more done, faster. Plus, be among the first to try our newest experimental models as they become available.\nGet the most from Gemini features\nExpanded access\nUnlock expanded access to Gemini features, including Deep Research. It automatically browses and analyzes hundreds of websites in real-time to give you comprehensive research reports on nearly any topic \u2014 in a matter of minutes.\nAnalyze up to 1,500 pages of text\nProcess vast amounts of info\nDive into dense research, textbooks, industry reports, and more with a 1-million-token context window, allowing you to efficiently explore and analyze information to solve more complex problems than ever before.\nBoost productivity with Gemini in your Google apps\nGo from idea to draft in seconds in Gmail and Google Docs\nHelp me write\nComplete a simple prompt to get help drafting invites, resumes, and more with AI-powered writing and proofreading tools.\nCreate images in Google Slides and videos in Google Vids10\nHelp me visualize\nCreate custom images in Slides and short video clips in Vids using a simple prompt. Add music, voiceovers, and more to bring your ideas to life and make your presentations more engaging.\nEnhance your video quality in Google Meet\nHelp me connect\nMagically enhance the quality of your video by reducing noise, increasing sharpness, and fixing the lighting.\nStudy smarter, not harder\nTake your learning to the next level\nUnderstand and master topics faster\nStudy and research smarter with 5X more audio overviews, notebooks and sources per notebook.\nSave hours on complex research\nCondense hours of searching\nYour research assistant powered by 2.5 Pro helps you turn hours of work into minutes. Dive into detailed reports on any topic and ask follow-up questions to sharpen your insights.\nStay on top of every project\nStay organized and move projects forward\nSimplify your daily tasks and get help writing, organizing, and visualizing directly in your favorite Google apps.\nAutomate tasks from your browser with Project Mariner\nWith Project Mariner,8 you can use AI agents to streamline tasks like trip planning, ordering items, and making reservations.\nGet even more with Google One\nFind the right Google One plan for you\n|\nGoogle AI Pro\n|\nNew\nGoogle AI Ultra\n|\n|\n|---|---|---|\n|\nStorage\n|\n||\n|\nStorage for Gmail, Google Drive, Google Photos\n|\n2 TB\n|\n30 TB\n|\n|\nFamily sharing with up to 5 people\n|\n||\n|\nGoogle AI\n|\n||\n|\nGemini\n|\n||\n|\nAccess to our most capable models\n|\nHigher\n|\nHighest\n|\n|\nCustom AI experts\n|\n||\n|\nGemini Live\n|\n||\n|\nDeep research\n|\nHigher\n|\nHighest\n|\n|\nFile upload\n|\nHigher\n|\nHighest\n|\n|\nVideo generation model access\n|\nVeo 3 Fast3\n|\nVeo 33\n|\n|\nPriority access to new features\n|\n||\n|\nExpanded token context window\n|\n1 million\n|\n1 million\n|\n|\nGemini in Gmail, Docs, and more\n|\n||\n|\nHelp me write\n|\n||\n|\nHelp me visualize\n|\n||\n|\nHelp me connect\n|\n||\n|\nHelp me generate video\n|\n||\n|\nNotebookLM\n|\n||\n|\nNotebook customization\n|\n||\n|\nCollaboration and sharing\n|\n||\n|\nHigher usage limits\n|\n||\n|\nFlow4\n|\n||\n|\nVideo generation model access\n|\nVeo 3 Fast\n|\nVeo 3\n|\n|\nWhisk5\n|\n||\n|\nWhisk Animate with Veo 2\n|\nHigher\n|\nHighest\n|\n|\nMonthly pooled AI credits11\n|\n1,000 credits11\n|\n25,000 credits11\n|\n|\nGoogle Search6\n|\n||\n|\nGemini 2.5 Pro model and Deep Search in AI Mode\n|\n||\n|\nAI-powered calling for local business pricing\n|\nExpanded\n|\nHighest\n|\n|\nJules7\n|\n||\n|\nTask limits\n|\nExpanded\n|\nHighest\n|\n|\nConcurrent task limits\n|\nExpanded\n|\nHighest\n|\n|\nAccess to latest models\n|\nExpanded\n|\nHighest\n|\n|\nProject Mariner8\n|\n||\n|\nAccess to an agentic research prototype\n|\n||\n|\nMore benefits\n|\n||\n|\nPremium video calling features in Google Meet\n|\n||\n|\nEnhanced appointment scheduling in Google Calendar\n|\n||\n|\nYouTube Premium\n|\nAdd-on\n|\n|\nAre you a student? Get Google AI Pro free for 1 year\nPrep for your exams, perfect your writing, and tackle your homework with the best of Google AI, including limited access to video generation with Veo 3 Fast. Check your eligibility\nFrequently asked questions\nThere are two Google AI plans that users can subscribe to: Google AI Pro and Google AI Ultra.\nGoogle AI Pro gives you access to:\n-\nGemini app with 2.5 Pro and limited access to Veo 3 Fast3\n-\nWhisk5 with Veo 2\n-\nNotebookLM\n-\nGoogle Search:6 Get access to Gemini 2.5 Pro model and Deep Search in AI Mode, plus AI-powered calling for local business pricing (US only)\n-\nJules:7 Higher task and concurrency limits, and access to latest models\n-\nGemini in Gmail, Docs, Vids, and more\n-\nGemini in Chrome (US only)\n-\n2 TB of storage for Google Photos, Drive, and Gmail\n-\nOther premium member benefits\nGoogle AI Ultra gives you the highest level of access to everything in Google AI Pro, as well as:\n-\nGemini app with 2.5 Deep Think and Veo 33\n-\nWhisk5 with highest limits\n-\nProject Mariner8 (US only)\n-\nYouTube Premium9 individual plan\n-\nGoogle Search:6 Highest limits to Gemini 2.5 Pro model and Deep Search in AI Mode, plus AI-powered calling for local business pricing (US only)\n-\nJules:7 Highest task and concurrency limits, and access to latest models\n-\n30 TB of storage for Google Photos, Drive, and Gmail\n-\nOther premium member benefits\nGoogle AI Ultra is available in more than 140 countries \u2014 see full list of countries.\nFamily plan members on a Google AI Pro plan can enjoy AI benefits and features at no extra cost. Learn more.\nOnly personal Google Accounts can sign up for Google AI plans. Workspace customers can sign up for a Gemini add-on to their existing subscription. Learn more about which offering works best for you and your business.\nGoogle AI Pro is available in over 150 countries. Google AI Ultra is available in more than 140 countries \u2014 see full list of countries.\nGemini app and Gemini in Gmail, Docs, and more in Google AI plans are only available for ages 18+. Gemini in Gmail, Docs, and more is available in select languages \u2014 see full list of languages.\nWhen you start a trial or are subscribed to a Google AI plan, you can use the following AI features:\n-\nGemini app: Access it at gemini.google.com or through the mobile app.\n-\nGenerate AI videos in Flow, Gemini, and Whisk.\n-\nGemini in Gmail, Docs, Slides, Sheets, and Meet: Capabilities can be used directly within these Google products.\n-\nNotebookLM: Access it at notebooklm.google.\nOur Google AI plans give you access to video generation in Gemini, Flow, and Whisk to help you bring your ideas in motion.\nThe Google AI Premium plan has a new name: Google AI Pro. It includes the same benefits like the Gemini app, NotebookLM, 2 TB of storage, and more."
    },
    {
      "url": "https://www.zdnet.com/article/camera-coach-on-pixel-10-helps-you-take-better-photos-as-you-shoot-them/",
      "text": "Camera Coach on Pixel 10 helps you take better photos as you shoot them\nZDNET's key takeaways\n- Pixel 10 series adds new AI tools, including Camera Coach.\n- It generates sample images to guide perfect composition.\n- Camera Coach suggests tips to improve your photo shots.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nWhether you are trying to take content creation seriously, want photos to frame on the wall, or, like me, like to overshare on the internet, you likely want your photos to look as good as possible. However, this can sometimes be tricky, as many factors, such as angles, lighting, and zoom, impact the quality of your photo. Now, AI can help.\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\nOn Wednesday, Google held its Made by Google event, where it unveiled its latest Pixel devices, including the Pixel 10 Series, Pixel Watch 4, Pixel Buds 2a, and a lot of new AI features. The best part: some of the new features have yet to be done by competitors, providing users with a helpful, original feature like the Camera Coach.\nThe new feature can level up your photo game (or that of those taking photos of you) with some helpful guidance that might just make you a better photographer in the long run. Keep reading below for a dive into the feature and my experience with it.\nCamera Coach\nThe Camera Coach feature assists you with snapping the perfect shot by suggesting ways to correct your angle, lighting, modes, and more, depending on your goal for taking the picture. For example, if you point your camera at a subject, it will ask what type of photo you want to take (portrait, photo, etc.). Then it will give a series of steps you can click through, with suggestions like \"Use Portrait mode\" and \"Frame for [insert subject name].\"\nAlso: I went hands-on with every Google Pixel 10 model - and was surprised by the one I loved most\nBeyond providing step-by-step instructions, Camera Coach will also generate a reference image of how you should frame your shot, which is especially helpful for visual learners. Beyond capturing better photos in the moment, the goal is for Camera Coach to teach users how to produce better images in the long run by learning foundational principles of photography.\nMy experience\nI had the opportunity to demo the feature before the Made by Google event and was pleasantly surprised by the experience. In my first demo, the user tried to take a photo outside the window of a landscape shot of a pier.\nAlso: PixelSnap is the MagSafe for Android phones we've been waiting for - here's our first look\nCamera Coach first analyzed the scene and surfaced options for what the user might want to focus on in the shot, highlighting different elements such as \"Activity in the Water\" or \"The Structure of the Gulf Netting.\" I was particularly impressed with its ability to recognize exactly what it was looking at, presenting the option, \"Focus on the Chelsea Piers Building,\" with no additional context beyond what it was seeing.\nOnce the demoer selected that he wanted to feature the buildings and sky, the steps were generated in seconds. The first directive was to turn the phone into landscape, then tilt the phone toward the sky, move the phone to the left to avoid the black window bar, and so on until the frame was perfectly composed for the photo.\nAlso: I'm a diehard Pixel fan, but I'm not upgrading to the Pixel 10. Here's why\nIn another demo, we pointed the camera at a plant. Again, it provided different shot options, including some more original ones you may not have thought of, such as \"Textured Leaf Close-Up.\" Once that option was selected, it generated similar steps. The biggest standout was that it generated an image of the leaf perfectly composed so we could mimic it to take the picture.\nHow to access\nCamera Coach is available on the Pixel 10 Series, which launched today, and is available for preorder starting Aug. 20. The feature may trickle down to older models in the future, but Google has yet to confirm one way or the other.\nAlso: This hidden Pixel camera setting gave my photos the pop they've been missing\nWhether a moderately tech-savvy person like myself would find a use for this feature on a daily basis is debatable, but I could see the need for users like my mom, who can't ever seem to get the framing just right when taking a photo, which in turn should get me better photos of myself -- and that I can be hyped about.\nYou can keep up with my latest stories and tech adventures on social media. Follow me on Twitter/X at @sabrinaa_ortiz and on Instagram at @sabrinaa.ortiz."
    },
    {
      "url": "https://www.zdnet.com/article/everything-announced-at-made-by-google-2025-pixel-10-pro-fold-watch-4-and-more/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nEvery Pixel device announced at Made by Google this week: 10 Pro Fold, Watch, Buds, more\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nIt's that time of the year again. No, not Apple's product launch -- that's next month. I'm talking about Made by Google 2025. The tech giant held its annual event on Wednesday, where it showed off its upcoming releases; namely the Pixel 10 series, Pixel Watch 4, and the Pixel Buds 2a.\nAlso: I went hands-on with every Google Pixel 10 model - and was surprised by the one I loved most\nWe managed to get a sneak peek at the devices ahead of the big day, and we're here to share everything we know. Admittedly, none of the hardware announcements are super surprising. We've had a pretty good idea that the aforementioned devices were going to be headliners, especially since Google unveiled the Pixel 10 series early in July.\nIt isn't all hardware reveals, however, as the event packs a few notable surprises -- including new smartphone features like Magic Cue and Pixelsnap. Google provided a more entertaining look during its presentation, but here's a quick breakdown of all the products and software updates being shown off at Made by Google.\n1. Google Pixel 10 series: Pro, XL, and Fold\nGoogle's Pixel 10 series is the star of the show. The line consists of four models: the base Pixel 10, Pixel 10 Pro, Pixel 10 Pro XL, and Pixel 10 Pro Fold. Unlike the previous generation, this batch isn't an overhaul of the Pixel line. It primarily builds on what the Pixel 9 line brought to the table.\nPixel 10: Starting with the base model, the Pixel 10 sports a 6.3-inch Actua display, 12GB of RAM, and a new 5x telephoto camera alongside the 50MP wide and 48MP ultrawide lenses. This new option delivers 10x optical image quality and Super Res Zoom up to 20x, letting you capture high-quality close-up shots from further away. Pricing starts at $799, and the model will be available in Obsidian (black), Indigo (blue), Frost (white), and Lemongrass (green).\nPixel 10 Pro: The Pro model is similar in size, sporting a 6.3-inch Super Actua display that outputs at a new peak brightness of 3,300 nits. Notable upgrades include 16GB of RAM, a purported 30-hour battery life, and a better 5x telephoto lens, allowing the Pixel 10 Pro to support 100x Pro Res Zoom for great-looking closeups. Prices start at $999, and it'll be available in Obsidian (black), Moonstone (gray), Porcelain (white), and Jade (green).\nAlso: I'm a longtime iPhone user, but Google just sold me on the Pixel 10 with these features\nPixel 10 Pro XL: The Pixel 10 Pro XL has many of the same features and specifications as the Pixel 10 Pro, with one major difference: It has a 6.8-inch Super Actua display. Prices start at $1,199, and it'll be available in the same colors as the 10 Pro.\nPixel 10 Pro Fold: The Pixel 10 Pro Fold is larger than the previous generation, boasting a 6.4-inch Actua outer display and an 8-inch Super Actua inner display. Google made interesting design changes, namely what it describes as a \"new gearless, high-strength hinge\" that allows the 10 Pro Fold to survive over 10 years of constant opening and closing.\nPerhaps more importantly, the Pixel 10 Pro Fold is the first foldable phone to feature an IP68 rating, providing complete protection against dust and allowing it to survive brief submersion underwater. Its closest competitor, the Samsung Galaxy Z Fold 7, only supports an IP48 rating. Pricing for the Fold starts at $1,799, and it'll be available in two colors: Moonstone and Jade.\nAll four models run on Google's new Tensor G5 chipset, which the company says is 60% more powerful than the previous generation Tensor G4. Each smartphone boasts 30-hour battery life, key support from Gemini AI, and receives seven years of software and security updates. Preorders for the Pixel 10 series have begun, August 20, with open sales availability starting next week on August 28.\nYou will have to wait a while to order the Pixel 10 Pro Fold, as it won't be available until October 9.\n2. Magic Cue, Pixelsnap, and more AI tools\nGoogle is once again utilizing artificial intelligence (AI) to elevate the user experience. This time, the company is focused on improving AI assistance on its smartphones to make certain features easier to use. Additionally, Google is making an interesting design change that could revamp the way people interact with their Pixel phones.\nAlso: How to clear your Android phone cache (and give it a serious speed boost)\nMagic Cue: Magic Cue is another personal assistant AI, but one that's more proactive. Instead of waiting for your input, the feature anticipates your needs and suggests actions or provides information \"based on the context of your phone.\" For example, let's say you're calling your airline about an upcoming flight. Magic Cue will instantly bring up your flight details during the call.\nCamera updates: All Pixel 10 models will receive two new camera features. First is Camera Coach, which will provide suggestions on how to set up shots or recommend certain camera modes to use. Auto Best Take allows the phone to combine similar photographs into one perfect image. Then there's the improved Add Me. While functioning much like before, the feature now lets you add the photographer to bigger group photos.\nPixelsnap: Pixelsnap is a new magnetic technology that will be included on all Pixel 10 phones. It allows users to snap Qi2-compatible wireless chargers, stands, and grips to the back of the device. If that sounds familiar, iPhones have supported similar technology for years.\nAlso: PixelSnap is the MagSafe for Android phones we've been waiting for - here's our first look\nTo coincide with the implementation of Pixelsnap, Google will be rolling out supporting accessories. The Pixelsnap Charger ($40) provides Qi2 wireless charging up to 25W. The Pixelsnap Charger with Stand ($70) does the same thing but sports a stable dock to support smartphones. Then there's the Pixelsnap Ring Stand ($30) for hands-free viewing. Google will also release special Pixelsnap compatible phone cases at $50 each.\n3. Google Pixel Buds 2a\nGoogle is revealing the Pixel Buds 2a, a new budget-friendly version of the Pixel Buds 2 Pro. Housed inside the familiar egg-shaped case, the company says this pair is the first A-Series earbuds to feature Active Noise Cancelation with Silent Seal 1.5 to block out outside noise. To hear the surrounding environment, users can activate Transparency Mode to allow sound to pass through.\nGoogle's Pixel Buds 2a shares multiple features with the Buds 2 Pro, including 11mm dynamic drivers, support for spatial audio, and a 5-band equalizer for adjusting the bass, treble, and other parts of the audio. The buds run on the Tensor A1 chipset and boast a 20-hour battery life with the charging case.\nAlso: I compared the new $130 Pixel Buds to Apple, Sony and Bose - here's how Google wins\nThe Pixel Buds 2a will be available on store shelves on October 9 for $130, and in two colors: Hazel (black) and Iris (light blue). Preorders open today, August 20. There is no word on when the Pixel Buds 3 Pro will roll out. However, if Google follows the same pattern with the Buds Pro and Buds 2 Pro, expect new earbuds next year.\n4. Google Pixel Watch 4\nRounding out the list is the Pixel Watch 4. Google's future wearable introduces multiple design updates and extra safety features.\nLike the Pixel Watch 3, this model sports an Actua 360 domed display made out of tough Corning Gorilla Glass. However, this generation's touchscreen is 10% larger than before and 50% brighter, boasting 3,000 nits. Its battery sees an improvement too, now lasting a purported 40 hours on a single charge for the 45mm model and 30 hours for the 41mm watch.\nAlso: I tried Pixel Watch 4 - and these are my 7 favorite upgrades in Google's new watch\nBoth sizes are gaining 25% faster charging speeds, along with a redesigned charging system that now flanks the side edge of the watch instead of the middle surface. How this plays out in the real world remains to be tested.\nThe most significant design change, however, is that the Pixel Watch 4 can actually be repaired. Google says the wearable is its \"first watch designed with serviceability in mind\" as users are able to replace the display and battery.\nMany safety features found on the Pixel Watch 4 are carryovers from the Pixel Watch 3. You have Loss of Pulse, which allows the Watch 4 to call for an ambulance if it detects that your heart has stopped beating. Fall and Crash Detection contacts emergency services if it detects that you've had a bad fall or were in a car accident.\nThere is a new feature in all this. Pixel Watch 4 LTE has an SOS satellite communication function that enables users to contact emergency services if they're ever in a location without any cellular or Wi-Fi connections.\nAlso: The best smartwatches 2025: I wore these for weeks and found the perfect one for your wrist\nThe Pixel Watch 4 will be available in two different sizes: 41mm and 45mm, with colors correlating to a certain size. The 45 Pixel Watch 4 will have three colors for its wrist bands: black, white, and gray. The 41mm Pixel Watch 4 keeps the white and black wrist bands, but also throws in Iris blue and Lemongrass green bands.\nThe 41mm Pixel Watch 4 will retail for $350, while the 45mm model will be a bit more expensive at $400. Preorders go live today, August 20. The wearable is set to hit store shelves on October 9."
    },
    {
      "url": "https://www.zdnet.com/article/the-google-pixel-9-pros-add-me-feature-is-an-ar-camera-tool-youll-actually-use/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThe Google Pixel 9 Pro's 'Add Me' feature is an AR camera tool you'll actually use\nI recently had the chance to try Google's new Pixel 9 flagship smartphone at this year's Made by Google event, and one of its most interesting new features, \"Add Me.\" It attempts to solve one of the oldest problems in digital photography: how do you get everyone in the group shot?\nAlso: Everything announced at Made by Google 2024: Pixel 9 Pro, Fold, Gemini, Watch 3, and more\nIf you're with a group of family or friends and want to take a photo, it means someone will inevitably be left out. You can either 1) ask a stranger to take the photo, which can be awkward and usually results in a bad shot, or 2) settle for a selfie, which is only as good as the person with the longest arms can stretch.\nThe \"Add Me\" feature addresses this problem by harnessing, you guessed it, AI to blend two photos together seamlessly so no one gets left out. Here's how it works:\nFirst, take the group shot like normal, but leave a physical space in the group for the person taking the photo (pretend they're actually standing there!). After you get the shot, have everyone clear the frame, and take a second photo of the person that missed the group shot, standing in the designated location.\nYou'll now have two photos: one with the group, and one with just the one person. The Add Me feature then merges the two images together into one photo in which everyone is together. The idea behind the feature is that the AI does it automatically without the need for any photo editing or cropping, resulting in a photo of the whole group that you can share on the spot.\nAlso: Why the cheapest Pixel 9 is my sleeper pick for best Google phone this year\nSounds easy, right?! Well, sort of, but there are some things to keep in mind here. While I demoed the feature, I learned right away that you really want to take the two photos as close to the exact same place as possible. That means holding the phone in the exact same spot for both pics. To aid in the process, there are indicators on the screen to direct the positioning of the shot, so you don't have to guess.\nThat said, there is still a bit of a learning curve. When we tried it out, the first person was holding the phone horizontally with the camera on the right side, and the next person held it with the camera on the left side. This caused problems, resulting in the phone's internal sensors being way off with the shot's positioning.\nAdditionally, you'll get the best results with a static background. Since you're merging two photos, you want the background to be the same in both shots, if possible. For that reason, I wouldn't recommend using this feature with a rapidly changing background like a busy street or a crowd of people, since the AI might confuse who's supposed to be in the photo and who's part of the background.\nI kept thinking about how precise you have to be to get the best results from this feature, and if it's something my mom could do, since family group shots are one of the most common use cases of this feature. I'm not so sure that she could nail the precision required (at least not on the first try).\nOnce we get a chance to do full reviews of the new Google Pixel 9 and Pixel 9 Pro, you can bet I'm going to test this AI feature out in a variety of different scenarios. I want to see how well it can handle challenging backgrounds and the limit to which the AI functionality can account for imprecise photo taking.\nAlso: The Pixel phone camera was already the best, but these new features make it even better\nThis feature will also likely need plenty of user input to improve over time, something I can see Google working on in future versions of the feature. For now, however, it's an ambitious and super useful feature that attempts to solve a photography problem we've all faced at one point or another."
    },
    {
      "url": "https://www.zdnet.com/article/what-is-generative-ai-and-why-is-it-so-popular-heres-everything-you-need-to-know/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nWhat is generative AI and why is it so popular? Here's everything you need to know\nWhat is generative AI?\nGenerative artificial intelligence (AI) refers to models or algorithms that create brand-new output, such as text, photos, videos, code, data, or 3D renderings, from the huge amount of data they are trained on. The models 'generate' new content by referring to the data they have been trained on, making new predictions.\nAlso: The best free AI courses (and whether AI 'micro-degrees' and certificates are worth it)\nThe purpose of generative AI is to create content, as opposed to other forms of AI, which suit different purposes, such as analyzing data, making ad recommendations, parsing through applications, helping to control a self-driving car, etc.\nWhat is an example of generative AI?\nAs mentioned above, generative AI is simply a subsection of AI that uses its training data to 'generate' or produce a new output. AI chatbots or AI image generators are quintessential examples of generative AI models. These tools use vast amounts of materials they were trained on to create new text or images.\nWhy is generative AI a hot topic right now?\nThe term generative AI is causing a buzz because of the increasing popularity of generative AI models, such as OpenAI's conversational chatbot ChatGPT and its AI image generator DALL-E 3.\nThese and similar tools use generative AI to produce new content, including computer code, essays, emails, social media captions, images, poems, Excel formulas, and more, within seconds, which has the potential to boost peoples' workflows significantly.\nAlso: The end-to-end AI chain emerges - it's like talking to your company's top engineer\nChatGPT became extremely popular quickly, accumulating over one million users a week after launching. Many other companies saw that success and rushed to compete in the generative AI marketplace, including Google, Microsoft's Bing, and Anthropic. These companies quickly developed their own generative AI models.\nThe buzz around generative AI will keep growing as more companies enter the market and find new use cases to help the technology integrate into everyday processes. For example, there has been a recent surge of new generative AI models for video and audio.\nWhat does machine learning have to do with generative AI?\nMachine learning refers to the subsection of AI that teaches a system to make a prediction based on data it's trained on. An example of this prediction is when DALL-E 3 creates an image based on the prompt you enter by discerning what the prompt means.\nAlso: How AI can rescue IT pros from job burnout and alert fatigue\nGenerative AI is, therefore, a machine-learning framework, but all machine-learning frameworks are not generative AI.\nWhat is the difference between generative AI and LLM?\nWhen discussing generative AI models, you often hear the term large language model (LLM) because it is the technology that powers AI chatbots.\nAs ZDNET's Maria Diaz explains: \"One of the most renowned types of AI right now is large language models (LLM). These models use unsupervised machine learning and are trained on massive amounts of text to learn how human language works. These texts include articles, books, websites, and more.\"\nAlso: What does GPT stand for? Understanding GPT 3.5, GPT 4, and more\nThese LLMs have advanced natural language processing abilities and are often used for AI chatbots. These chatbots need to understand conversational prompts from users, but they also need to output prompts conversationally.\nSome of the most popular LLMs are OpenAI's GPT-3.5, which powers the free version of ChatGPT, and GPT-4, which powers ChatGPT Plus and Microsoft's Copilot.\nWhat are text-based generative AI models trained on?\nText-based models, such as ChatGPT, are trained on massive amounts of data in a process known as self-supervised learning. Here, the model learns from the information it's fed to make predictions and generate answers in future scenarios\nAlso: What is Copilot (formerly Bing Chat)? Here's everything you need to know\nOne concern with generative AI models, especially those that generate text, is that many are trained on data from the entirety of the internet. This data includes copyrighted material and information that might not have been shared with the owner's consent.\nWhat is generative AI art?\nGenerative AI art, including images, is created by AI models trained on billions of images. The model uses this data to learn styles of pictures and then uses this insight to generate new art when prompted by an individual through text.\nAlso: How to use ChatGPT to make charts and tables\nA popular example of an AI art generator is DALL-E. However, plenty of other AI generators are on the market and are just as good, if not more capable. These tools can also be used for different requirements.\nImage Creator from Microsoft Designer is Microsoft's take on the technology, which leverages OpenAI's most advanced text-to-image model, DALL-E 3, and is currently viewed by ZDNET as the best AI image generator.\nSome models, such as DALL-E, are trained with images found across the internet, even if the creator's permission wasn't granted. Others, such as Adobe's Firefly, take a more ethical approach, reportedly using only Adobe Stock Images or public domain content where the copyright has expired.\nWhat are the problems with art generated by text-to-image models?\nMany generative AI art models are trained on billions of images from the internet. This content often includes artwork and images produced by artists and creatives. These images are then reimagined and repurposed by AI to generate your image. The catch is that the artists of the original work did not consent to their artwork being used to train AI models and inspire others.\nAlso: Google releases two new free resources to help you optimize your AI prompts\nAlthough it's not the same image, the new image has elements of an artist's original work, which is not credited to them. A specific style unique to the artist can be replicated by AI and used to generate a new image, without the original artist knowing or approving. The debate about whether AI-generated art is 'new' or even 'art' will continue for many years.\nWhat are some shortcomings of generative AI?\nGenerative AI models take a vast amount of content from across the internet and then use the information they are trained on to make predictions and create an output for the prompts you input. These predictions are based on the data the models are fed, but there are no guarantees the prediction will be correct, even if the responses sound plausible.\nThe responses might also incorporate biases inherent in the content the model has ingested from the internet, but there is often no way of knowing whether that's true. These shortcomings have caused major concerns regarding the spread of misinformation due to generative AI.\nAlso: 4 things Claude AI can do that ChatGPT can't\nGenerative AI models don't necessarily know whether their output is accurate. Users are unlikely to know where information has come from. They are also unlikely to understand how the algorithms process data to generate content.\nThere are examples of chatbots providing incorrect information or simply making things up to fill the gaps. While the results from generative AI can be intriguing and entertaining, it would be unwise, certainly in the short term, to rely on the information or content they create.\nSome generative AI models, such as Copilot, are attempting to bridge that source gap by providing footnotes with sources that enable users to understand where their response comes from and verify its accuracy."
    },
    {
      "url": "https://www.zdnet.com/article/i-tried-pixel-watch-4-and-these-are-my-7-favorite-upgrades-in-googles-new-watch/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nI tried the Google Pixel Watch 4 - and these key features made it feel indispensable\nZDNET key takeaways\n- Pixel Watch 4 launches Oct. 9 at $350, the same price as last year.\n- New features: faster charging, satellite link, and health AI.\n- First Pixel Watch with a replaceable battery for sustainability.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nThe time has finally come for Google Pixel users. A new lineup of Google phones, earbuds, and, my personal favorite, smartwatches, is coming to Android users, Google announced at its Made by Google event earlier this week.\nThe Pixel Watch 4 is Google's latest smartwatch, and the wearable comes packed with new, intriguing features that raise the bar across the smartwatch board. I got to try several of them out ahead of the smartwatch's official launch, and they impressed me and got me excited to wear the watch myself.\nAlso: Everything announced at Made by Google 2025: Pixel 10 Pro, Fold, Watch 4, and more\nWhile the features of the fourth-generation smartwatch are new, the price of the watch will stay the same, at $350 for the 41mm and $400 for the 45mm. The 41mm watch cases come in matte black with an obsidian active band, polished silver with a porcelain active band, champagne gold with a lemongrass active band, and polished silver with an iris active band.\nOn the 45mm, the watch cases include matte black with an obsidian active band, polished silver with a porcelain active band, and satin moonstone with a moonstone active band.\nPreorders for Pixel Watch 4 begin on Aug. 20, and the smartwatches hit shelves on Oct. 9.\n1. New charging dock\nI had to fumble with the charger on the Pixel Watch 3 several times to land the smartwatch's charging mechanisms in the right place. It's annoying and tedious. The Pixel Watch 4's charging port erases this problem with its new orientation. Instead of flat charging that required a user to perfectly align the magnets to initiate a charge on Watch 3, the Pixel Watch 4 falls immediately into place with the side-charging port.\nPlus, Google says the Watch 4 is its longest and fastest-charging watch to date, with a 25% faster charging speed.\n2. Satellite communications\nSeveral smartwatches have satellite communications. What separates them from Pixel Watch 4 is Google's first-of-its-kind standalone smartwatch with satellite communications. Most satellite-enabled smartwatches require a phone to connect to. The Pixel Watch 4 doesn't.\nAlso: The best satellite phones of 2025: Expert tested and reviewed\nIt's powered by Snapdragon's processor, which connects to geostationary satellites to guide you and connect you when you're off grid. This feature is smart, unique, and useful to outdoor adventurers or those with poor connectivity. It is available on the Pixel Watch 4 LTE.\n3. Improved health features\nOutside of being a smartphone's wrist-side companion, a smartwatch is built to health-track with sensors and functionalities you can't find on a Pixel phone. Google is making investments in both sleep and activity tracking with the Pixel Watch 4.\nThe watch gets Google's most accurate sleep tracking and sleep stage detection to date. Additionally, the sensors can more acutely detect variations in skin temperature.\nNow, when users are exercising and forget to record their workout, Google automatically detects and classifies activities while also recording heart rate information. The AI-powered feature learns from your workout regimen to accurately classify these activities over time.\nAlso: Is all this data about our health good for our health?\nThe Watch 4 also uses Gemini as a personal health coach that's been developed with Fitbit to create customized sleep and activity regimens tailored to the user. Whether a Watch 4 user is running their first marathon or aiming for a new PR in the gym, Gemini offers up training plans and advice along the way.\nGoogle also unveiled on Wednesday that it's revamping the Fitbit Premium experience to make the app feel and act more like a personalized health coach.\nI didn't get to try this revamped Fitbit, as the revamp arrives on watches in October, but Google says it will adapt to and create custom routines, generate workouts based on the data it's collected on your activity level and sleep quality, and offer check-ins and adjustments mid-workout.\n4. Watch faces adapt to customizable themes\nAs somebody who loves form equally, if not more, than function, I can't deny myself an aesthetic touch. Google's wearable comes in the form of watch faces that adapt to customizable themes, so that your display is cohesive, personalized, and, in my opinion, cool to look at.\n5. Brighter display\n3,000 nits seems to be the magic number for watch brightness this tech season. Google upgraded its smartwatch, matching the brightness of Samsung's recently released Galaxy Watch 8 and Apple's Ultra Watch 2. The display was bright and vivid.\nAlso: Google is officially replacing Assistant with Gemini - and there's only one way to keep it\nAdditionally, Google developed a domed Actua-360 display that's made to be more glanceable and complement the new edge-hugging buttons that take up more of the Pixel Watch screen.\n6. Replaceable battery\nFor the first time, Google is making the Pixel Watch 4's battery and display replaceable. This is a major stride for both customers and sustainability and seems to be one way the tech giant is addressing the issue of e-waste. Once your Pixel Watch 4's battery dies or its display cracks, you can replace it with a new one and avoid buying a new watch altogether.\n7. Goodbye, Hey Google\nIf you wanted to activate Google's assistant on previous watches, you'd have to say Hey Google. Now, all you have to do is raise the smartwatch and start speaking for quicker query activation.\nThis update makes triggering Gemini quicker, convenient, and conversational."
    }
  ],
  "argos_summary": "Google\u2019s Made by Google event unveiled the Pixel\u202f10 series, which builds on the Pixel\u202f9 with a new Tensor\u202fG5 chip and a host of AI\u2011powered features such as Magic\u202fCue, Camera\u202fCoach, Edit\u202fwith\u202fAsk\u202fPhotos, Voice\u202fTranslation, and the new Pixelsnap magnetic accessory. The phones also drop the physical SIM tray in the U.S., offer a 30\u2011hour battery, and include a one\u2011year Google AI\u202fPro subscription. The lineup spans a base Pixel\u202f10, Pro, Pro\u202fXL, and a foldable Pro\u202fFold, all priced from $799 to $1,799, and is set to ship in late August with the foldable arriving in October.",
  "argos_id": "CRKME52E0"
}