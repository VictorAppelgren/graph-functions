{
  "url": "https://www.zdnet.com/article/googles-jules-ai-coding-tool-exits-beta-with-serious-upgrades-and-more-free-tasks/",
  "authorsByline": "David Gewirtz",
  "articleId": "0ab251b2c3284d3d93707629ccd7e104",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/8633dd58a5228b29698e06eb274112be77dcbe94/2025/08/07/1423ed37-f7f2-4e78-808f-6cad9e0b2119/gettyimages-488635724.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-07T12:36:09+00:00",
  "addDate": "2025-08-07T12:40:30.611752+00:00",
  "refreshDate": "2025-08-07T12:40:30.611754+00:00",
  "score": 1.0,
  "title": "Google's Jules AI coding tool exits beta with serious upgrades - and more free tasks",
  "description": "Jules now includes a cleaner UI, reusable prompt foundations, and a more generous free tier. Here's how it compares with Gemini CLI GitHub Actions.",
  "content": "\u2022 Google's Jules coding agent is out of beta, with clear pricing tiers.\n\u2022 Even the free tier has newer, more generous usage limits.\n\nGoogle's coding AI Jules is out of beta. Announced in a blog post on Wednesday, the coding helper will be powered by Google's Gemini 2.5 Pro LLM. You might recall that Google also recently announced its Gemini CLI GitHub Actions coding tool.\n\nWhat do all these developments mean? Let's unpack the announcements.\n\nJules and Gemini CLI GitHub Actions are different beasts. Jules is meant for big projects, significant changes, and project planning.\n\nAlso: Google embeds AI agents deep into its data stack - here's what they can do for you\n\nGemini CLI GitHub Actions augments GitHub's workflow tool, GitHub Actions. The Gemini CLI version interacts with GitHub, helps respond to issues (problem reports), generates and responds to pull requests (submitted changes), and collaborates with other coders working on the same project.\n\nHere's a handy table that helps show the differences between the assistants:\n\nThat was then\n\nWhen I first tried Jules, it was the first day it became available as a beta offering. Although the assistant had some early server and availability problems, I was still able to add a new feature to my open-source project. I then deployed that new feature to all my users.\n\nThat was on May 27. Since I didn't suddenly have 20,000 angry users screaming at me that I broke their sites (yes, I've done that before), it's pretty clear that the Jules additions worked well.\n\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\n\nWhat I wanted to like about Jules is that it produces a plan of action in response to your prompt. You can (in theory) interact with Jules to refine that plan of action before it goes off and makes changes to your code.\n\nAnd when I first tried the tool, Jules did produce a plan of action. But like my little Yorkie pup, it didn't possess the patience to wait for me to say it was okay to go. This issue leads to small dogs trying to climb into your dinner dish, and big AIs that blast through an entire codebase in minutes.\n\nFortunately, Jules branches your codebase. This feature ensures nothing is incorporated into your main release until after you approve it, as you would any other proposed change on GitHub.\n\nThis is now\n\nAs of August 6, Jules is officially out of beta. The official release has what Google called a \"new, more user-friendly interface.\"\n\nJules also allows developers to reuse previous setups. This could be quite valuable because having to train the AI on your code for every coding request can substantially slow the process down and potentially introduce errors. This way, once you have a working prompt foundation, you can reuse it.\n\nAlso: Gemini Pro 2.5 is a stunningly capable coding assistant - and a big threat to ChatGPT\n\nJules also now has multimodal support. Google said: \"Jules can test your web application and show you a visual representation of the results. This can help Jules iterate until it gets things right and gives you confidence in the code Jules creates.\"\n\nWhen I tested Gemini 2.5 Pro against my coding test suite, it did astonishingly well. Earlier versions of Gemini and its predecessor, Bard, did not perform nearly as well.\n\nGemini 2.5 prioritizes speed and efficiency. In other words, it costs Google less to run each prompt. Google Gemini 2.5 Pro is intended for deep reasoning, accuracy, and handling complex tasks. Google prices Gemini 2.5 Pro at about 8 1/2 times the cost of Gemini 2.5, so we can pretty much assume Gemini 2.5 Pro uses a lot more resources for each query.\n\nAs you read about this announcement, you might find some confusion. Based on the blog post, it's not fully clear whether Jules is powered by Gemini 2.5 or a combination of Gemini 2.5 and Gemini 2.5 Pro. According to Google's blog post announcing the public availability of Jules, the tool is \"powered by Gemini 2.5.\" But later in the blog post, Google said: \"Jules now uses the advanced thinking capabilities of Gemini 2.5 Pro to develop coding plans.\"\n\nI asked Google's team about the discrepancy. It confirmed to ZDNET that Jules uses the much more powerful reasoning model to plan out actions, and then executes those actions, also with the Gemini 2.5 Pro LLM. Given how impressive my testing was with 2.5 Pro, that's a good thing.\n\nAlso: Claude Code makes it easy to trigger a code check now with this simple command\n\nJules is offered in three pricing tiers. There are different limits for each pricing tier. When I tried Jules back in May, I was only given five tasks per day. I was able to make a feature addition to my project in just two tasks. You could still get quite a bit done even then.\n\nAs of now, the free version of Jules allows 15 tasks per day, which is good. If you take the time to craft clear prompts, you're unlikely to need a lot more than that number each day.\n\nHowever, if you want more, you can move to Google's $20/month Pro plan or $250/month Ultra plan.\n\nAlso: The best AI for coding in 2025 (including a new winner - and what not to use)\n\nStay tuned. I plan to use the release version of Jules on another coding modification. I'll report back on my experiences.\n\nWhat about you?\n\nDid you try Jules during the beta? How does it compare to other AI coding assistants you've used? Do you see value in its planning-based approach, or do you prefer a more hands-on coding helper? And what do you make of the apparent confusion over which Gemini model the assistant uses? Let us know in the comments below.\n\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV.",
  "medium": "Article",
  "links": [
    "https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/",
    "https://jules.google/",
    "https://www.zdnet.com/article/microsoft-authenticator-wont-manage-your-passwords-anymore-or-most-passkeys/",
    "https://www.instagram.com/DavidGewirtz/",
    "https://www.youtube.com/user/DavidGewirtzTV",
    "https://www.zdnet.com/article/is-a-refurbished-macbook-worth-it-i-did-the-math-and-heres-my-buying-advice/",
    "https://www.zdnet.com/article/googles-gemini-2-5-pro-update-makes-the-ai-model-even-better-at-coding/",
    "https://twitter.com/davidgewirtz",
    "https://www.facebook.com/davidgewirtz",
    "https://www.zdnet.com/article/gemini-pro-2-5-is-a-stunningly-capable-coding-assistant-and-a-big-threat-to-chatgpt/",
    "https://www.zdnet.com/article/github-actions-moves-github-into-devops/",
    "https://www.zdnet.com/article/the-best-ai-for-coding-in-2025-including-a-new-winner-and-what-not-to-use/",
    "https://bsky.app/profile/davidgewirtz.com",
    "https://www.zdnet.com/article/yes-you-need-a-firewall-on-linux-heres-why-and-which-to-use/",
    "https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/#link={%22role%22:%22standard%22,%22href%22:%22https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/",
    "https://www.zdnet.com/article/how-i-test-an-ai-chatbots-coding-ability-and-you-can-too/",
    "https://www.zdnet.com/article/google-embeds-ai-agents-deep-into-its-data-stack-heres-what-they-can-do-for-you/",
    "https://www.zdnet.com/article/claude-code-makes-it-easy-to-trigger-a-code-check-now-with-this-simple-command/",
    "https://advancedgeekery.substack.com/",
    "https://www.zdnet.com/article/these-7-common-household-items-were-draining-power-all-day-until-i-pulled-the-plug/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Jules",
      "weight": 0.098202005
    },
    {
      "name": "Gemini Pro",
      "weight": 0.09610403
    },
    {
      "name": "Gemini CLI GitHub Actions",
      "weight": 0.08694667
    },
    {
      "name": "Gemini CLI",
      "weight": 0.08606707
    },
    {
      "name": "other AI coding assistants",
      "weight": 0.08531058
    },
    {
      "name": "Gemini",
      "weight": 0.08113121
    },
    {
      "name": "coding plans",
      "weight": 0.07902142
    },
    {
      "name": "Googles coding AI Jules",
      "weight": 0.0761851
    },
    {
      "name": "Googles Jules AI coding agent",
      "weight": 0.07491354
    },
    {
      "name": "Googles Jules AI coding tool",
      "weight": 0.07489799
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.8828125
    },
    {
      "name": "/News/Technology News",
      "score": 0.88037109375
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.63525390625
    },
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.439208984375
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.39208984375
    }
  ],
  "sentiment": {
    "positive": 0.15576172,
    "negative": 0.36010742,
    "neutral": 0.484375
  },
  "summary": "Google's Jules AI coding tool has been released from beta, with clear pricing tiers and more generous usage limits. The coding assistant will be powered by Google's Gemini 2.5 Pro LLM, a combination of Gemini 5 and Gemini 2., which also integrate with GitHub's workflow tool, GitHub Actions. Jules is designed for big projects, significant changes, and project planning. The Gemini version interacts with GitHub, generates and responds to pull requests, and collaborates with other coders working on the same project. The official release of Jules has a new user-friendly interface and allows developers to reuse previous setups. As of August 6, Jules officially launches a new feature that allows users to ship out new features.",
  "shortSummary": "Google's Jules AI coding agent is out of beta with major upgrades, enhanced functionality, and clear pricing tiers, complementing Gemini CLI GitHub Actions.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "ddc61449ae9e4129ad2d84b27d8c5498",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/#link={%22role%22:%22standard%22,%22href%22:%22https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/",
      "text": "Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nOK. Deep breath. This is surreal. I just added an entirely new feature to my software, including UI and functionality, just by typing four paragraphs of instructions. I have screenshots, and I'll try to make sense of it in this article. I can't tell if we're living in the future or we've just descended to a new plane of hell (or both).\nLet's take a step back. Google's Jules is the latest in a flood of new coding agents released just this week. Last week, I wrote about OpenAI Codex and Microsoft's GitHub Copilot Coding Agent, and ZDNET's Webb Wright wrote about Google's Jules.\nAlso: The best AI for coding in 2025\nAll of these coding agents will perform coding operations on a GitHub repository. GitHub, for those who've been following along, is the giant Microsoft-owned software storage, management, and distribution hub for much of the world's most important software, especially open source code.\nThe difference, at least as it pertains to this article, is that Google made Jules available to everyone for free. That meant I could just hop in and take it for a spin -- and now my head is spinning.\nUsage limits and my first two prompts\nThe free access version of Jules allows only five requests per day. That might not seem like a lot, but in only two requests, I was able to add a new feature to my software. So, don't discount what you can get done if you think through your prompts before shooting off your silver bullets for the day.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nMy first two prompts were tentative. It wasn't that I wasn't impressed; it was that I really wasn't giving Jules much to do. I'm still not comfortable with the idea of setting an AI loose on all my code at once, so I played it safe.\nMy first prompt asked Jules to document the \"hooks\" that add-on developers could use to add features to my product. I didn't tell Jules much about what I wanted. It returned some markup that it recommended dropping into my code's readme file. It worked, but meh.\nI did have the opportunity to publish that code to a new GitHub branch, but I skipped it. It was just a test, after all.\nMy second prompt was to ask Jules to suggest five new hooks. I got back an answer that seemed reasonable. However, I realized that opening up those capabilities in a security product was just too risky for me to delegate to an AI. I skipped those changes, too.\nIt was at this point that Jules wanted a coffee break. It stopped functioning for about 90 minutes.\nThat gave me time to think. What I really wanted to see was whether Jules could add some real functionality to my code and save me some time.\nNecessary background information\nMy Private Site is a security plugin for WordPress that runs on about 20,000 active sites. It puts a login dialog in front of the site's web pages. There are a bunch of options, but that's the plugin's key feature. I acquired the software a decade ago from a coder who called himself \"jon radio\" and have been maintaining and expanding it ever since.\nAlso: Rust turns 10: How a broken elevator changed software forever\nThe plugin provides access control to the front-end of a website, the pages that visitors see when they come to the site. Site owners control the plugin via a dashboard interface, with various admin functions available in the plugin's admin interface.\nI decided to try Jules out on a feature some users have requested, hiding the admin bar from logged-in users. The admin bar is the black bar WordPress puts on the top of a web page. In the case of the screenshot below, the black admin bar is visible.\nI wanted Jules to add an option on the dashboard to hide the admin bar from logged-in users. The idea is that if a user logged in, the admin bar would be visible on the back end, but logged-in users browsing the front-end of the site wouldn't have to see the ugly bar.\nThis is the original dashboard, before adding the new feature.\nSome years ago, I completely rewrote the admin interface from the way it was when I acquired the plugin. Adding options to the interface is straightforward, but it's still time-consuming. Every option requires not only the UI element to be added, but also preference saving and preference recalling when the dashboard is displayed. That's in addition to any program logic that the preference controls.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nIn practice, I've found that it takes me about 2-3 hours to add a preference UI element, along with the assorted housekeeping involved. It's not hard, but there are a lot of little fiddly bits that all need to be tweaked, and that takes time.\nThat should bring you up to speed enough to understand my next test of Jules. Here's a bit of foreshadowing: The first test failed miserably. The second test succeeded astonishingly.\nInstructing Jules\nAdding a \"hide admin bar\" feature would not have been easy for the run-of-the-mill coding help we've been asking ChatGPT and the other chatbots to perform. As I mentioned, adding the new option to the dashboard requires programming in a variety of locations throughout the code and an understanding of the overall codebase.\nHere's what I told Jules.\n1. On the Site Privacy Tab of the admin interface, add a new checkbox. Label the section \"Admin Bar\" and label the checkbox itself \"Hide Admin Bar\". [Place this in the MAKE SITE PRIVATE block, located just under the Enable login privacy checkbox and before the Site Privacy Mode segment.]\nI instructed Jules where I wanted the AI to put the new option. On my first run through, I made a mistake and left out the details in square brackets. I didn't tell Jules exactly where I wanted it to place the new option. As it turns out, that omission caused a big fail. Once I added the sentence in brackets above, the feature worked.\n2. Be sure to save the selection of that checkbox to the plugin's preferences variable when the Save Privacy Status button is checked.\nThis ensures that Jules knows there is a preference data structure and that it is updated when the user makes a change. It's important to note that if I didn't understand the underlying code, I wouldn't have instructed Jules about this, and the code would not have worked. You can't \"vibe code\" something like this without knowing the underlying code.\n3. Show the appropriate checked or unchecked status when the Site Privacy tab is displayed.\nThis tells the AI that I want the interface to be updated to match what the preference variable specifies.\n4. Based on the preference variable created in (2), add code to hide or show the WordPress admin bar. If Hide Admin Bar is checked, the Admin Bar should not be visible to logged-in WordPress front-end users. If the Hide Admin Bar is not checked, the Admin Bar should be visible to logged-in front-end users. Logged-in back-end users in the admin interface should always be able to see the admin bar.\nThis describes the business logic that the new preference should control. It requires the AI to know how to hide or show the admin bar (a WordPress API call is used), and it requires the AI to know where to put the code in my plugin to enable or disable this feature.\nAnd with that, Jules was trained on what I wanted.\nJules dives into my code\nI fed my prompt set into Jules and got back a plan of action. Pay close attention to that Approve Plan? button.\nI didn't even get a chance to read through the plan before Jules decided to approve the plan on its own. It did this after every plan it presented. An AI that doesn't wait for permission raises the hairs on the back of my neck. Just saying.\nI desperately want to make a Skynet/Landru/Colossus/P1/Hal kind of joke, because I'm freaked out. I mean, it's good. But I'm freaked out.\nHere's some of the code Jules wrote. The shaded green is the new stuff. I'm not thrilled with the color scheme, but I'm sure that will be tweakable over time.\nAlso: The best free AI courses and certificates\nMore relevant is the fact that Jules picked up on my variable naming conventions and the architecture of my code and dived right in. This is the new option, rendered in code.\nBy the time it was done, Jules had included all the code changes it had originally planned, plus some test code. I don't use standardized tests. I would have told Jules not to do it the way it planned, but it never gave me time to approve or modify its original plan. Even so, it worked out.\nI pushed the Publish branch button, which caused GitHub to create a new branch, separate from my main repository. Jules then published its changes to that branch.\nThis is how contributors to big projects can work on those projects without causing chaos to the main code line.\nUp to this point, I could look at the code, but I wasn't able to run it. But by pushing the code to a branch, Jules and GitHub made it possible for me to replicate the changes safely down to my computer to test them out. If I didn't like the changes, I could have just switched back to the main branch, and no harm, no foul. But I did like the changes, so I moved on to the next step.\nAround the code in eight clicks\nOnce I brought the branch down to my development machine, I could test it out. Here's the new dashboard with the Hide Admin Menu feature.\nI tried turning the feature on and off and making sure the settings stuck. They did. I also tried other features in the plugin to make sure nothing else had broken. I was pretty sure nothing would, because I reviewed all the changes before approving the branch. But still: Testing is a good thing to do.\nI then logged into the test website. As you can see, there's no admin bar showing.\nAt this point, the process was out of the AI's hands. It was simply time to deploy the changes, both back to GitHub and to the master WordPress repository. First, I used GitHub Desktop to merge the branch code back into the main branch on my development machine.\nI changed \"Hide Admin Menu\" to \"Hide admin menu\" in my code's main branch, because I like it better. I pushed that (the full main branch on my local machine) back to the GitHub cloud.\nThen, because I just don't like random branches hanging around once they've been incorporated into the distribution version, I deleted the new branch on my computer.\nI also deleted the new branch from the GitHub cloud service.\nFinally, I packaged up the new code. I added a change to the readme to describe the new feature and update the code's version number. Then, I pushed it up to the WordPress plugin repository using SVN (the source code control system used by the WordPress community).\nJourney to the center of the code\nJules is very definitely beta right now. It hung in a few places. Some screens didn't update. It decided to check out for 90 minutes. I had to wait while it went to and came back from its digital happy place. It's exhibiting all the sorts of things you'd expect from a newly-released piece of code. I have no concerns about that. Google will clean it up.\nIn fact, since I first published this article, Google has reached out to me, and they've told me they're definitely working hard to reduce some of the issues I had. They also told me that I was right about the slowdown. They were completely slammed with visitors on launch day.\nThe fact that Jules (and presumably OpenAI Codex and GitHub Copilot Coding Agent) can handle an entire repository of code across a bunch of files is big. That's a much deeper level of understanding and integration than we saw, even six months ago.\nAlso: How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\nThe speed with which it can change an entire codebase is terrifying. The damage it can do is potentially extraordinary. It will gleefully go through and modify everything in your codebase, and if you specify something wrong and then push or merge, you will have an epic mess on your hands.\nThere is a deep inequality between how quickly it can change code and how long it will take a human to review those changes. Working on this scale will require excellent unit tests. Even tools like mine, which don't lend themselves to full unit testing, will require some kind of automated validation to prevent robot-driven errors on a massive scale.\nThose who are afraid these tools will take jobs from programmers should be concerned, but not in the way most people think. It is absolutely, totally, 100% necessary for experienced coders to review and guide these agents. When I left out one critical instruction, the agent gleefully bricked my site.\nSince I was the person who wrote the code initially, I knew what to fix. But it would have been brutally difficult for someone else to figure out what had been left out and how to fix it. That would have required coming up to speed on all the hidden nuances of the entire architecture of the code.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nThe jobs that are likely to be destroyed are those of junior developers. Jules easily does junior developer-level work. With tools like Jules, Codex, or Copilot, that cost a few hundred bucks a month at most, it's going to be hard for management to be willing to pay mid-to-high six figures for midlevel and junior programmers. Even outsourcing and offshoring aren't as cheap as using an AI agent to do maintenance coding.\nAs I wrote earlier in the week, if mid-level jobs are not available, how will we train the experienced people we'll need in the future?\nI am also concerned about how access limits will work. Productivity gains will drop like a rock if you need to do one more prompt and have to wait a day to be allowed to do so.\nAs for me? In less than 10 minutes, I created a new feature that readers had requested. While I was writing another article, I fed the prompt to Jules. I went back to work on the article and checked on Jules when it was finished. I checked out the code, brought it down to my computer, and pushed a release.\nUnfortunately, Jules can't work across different repositories, at least not yet. That means I can't have Jules treat the public GitHub codebase and the three private for-pay add-on codebases as one large project. But Google tells me that this kind of capability is on their list. I could open up my entire repository (of all my projects), and that might work, but it also might confuse Jules about which project is and is not relevant to the work at hand.\nIt took me longer to upload the thing to the WordPress repository than to add the entire new feature. For that class of feature, I got a half-day's work done in less than half an hour, from thinking about making it happen to publishing to my users.\nIn the first week since I completed this feature, about 8,500 sites have updated (about half of the entire installed base). Without Jules, those users probably would have been waiting months for this new feature, because I have a huge backlog of work, and it wasn't my top priority. But with Jules, it took barely any effort.\nAlso: 7 productivity gadgets I can't live without (and why they make such a big difference)\nThese tools are going to require programmers, managers, and investors to rethink the software development workflow. There will be glaring \"you can't get there from here\" gotchas. And there will be epic failures and coding errors. But I have no doubt that this is the next level of AI-based coding. Real, human intelligence is going to be necessary to figure out how to deal with it.\nHave you tried Google's Jules or any of the other new AI coding agents? Would you trust them to make direct changes to your codebase, or do you prefer to keep a tighter manual grip? What kinds of developer tasks do you think these tools should and shouldn't handle? Let us know in the comments below.\nWant more stories about AI? Sign up for Innovation, our weekly newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/googles-jules-ai-coding-agent-built-a-new-feature-i-could-actually-ship-while-i-made-coffee/",
      "text": "Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nOK. Deep breath. This is surreal. I just added an entirely new feature to my software, including UI and functionality, just by typing four paragraphs of instructions. I have screenshots, and I'll try to make sense of it in this article. I can't tell if we're living in the future or we've just descended to a new plane of hell (or both).\nLet's take a step back. Google's Jules is the latest in a flood of new coding agents released just this week. Last week, I wrote about OpenAI Codex and Microsoft's GitHub Copilot Coding Agent, and ZDNET's Webb Wright wrote about Google's Jules.\nAlso: The best AI for coding in 2025\nAll of these coding agents will perform coding operations on a GitHub repository. GitHub, for those who've been following along, is the giant Microsoft-owned software storage, management, and distribution hub for much of the world's most important software, especially open source code.\nThe difference, at least as it pertains to this article, is that Google made Jules available to everyone for free. That meant I could just hop in and take it for a spin -- and now my head is spinning.\nUsage limits and my first two prompts\nThe free access version of Jules allows only five requests per day. That might not seem like a lot, but in only two requests, I was able to add a new feature to my software. So, don't discount what you can get done if you think through your prompts before shooting off your silver bullets for the day.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nMy first two prompts were tentative. It wasn't that I wasn't impressed; it was that I really wasn't giving Jules much to do. I'm still not comfortable with the idea of setting an AI loose on all my code at once, so I played it safe.\nMy first prompt asked Jules to document the \"hooks\" that add-on developers could use to add features to my product. I didn't tell Jules much about what I wanted. It returned some markup that it recommended dropping into my code's readme file. It worked, but meh.\nI did have the opportunity to publish that code to a new GitHub branch, but I skipped it. It was just a test, after all.\nMy second prompt was to ask Jules to suggest five new hooks. I got back an answer that seemed reasonable. However, I realized that opening up those capabilities in a security product was just too risky for me to delegate to an AI. I skipped those changes, too.\nIt was at this point that Jules wanted a coffee break. It stopped functioning for about 90 minutes.\nThat gave me time to think. What I really wanted to see was whether Jules could add some real functionality to my code and save me some time.\nNecessary background information\nMy Private Site is a security plugin for WordPress that runs on about 20,000 active sites. It puts a login dialog in front of the site's web pages. There are a bunch of options, but that's the plugin's key feature. I acquired the software a decade ago from a coder who called himself \"jon radio\" and have been maintaining and expanding it ever since.\nAlso: Rust turns 10: How a broken elevator changed software forever\nThe plugin provides access control to the front-end of a website, the pages that visitors see when they come to the site. Site owners control the plugin via a dashboard interface, with various admin functions available in the plugin's admin interface.\nI decided to try Jules out on a feature some users have requested, hiding the admin bar from logged-in users. The admin bar is the black bar WordPress puts on the top of a web page. In the case of the screenshot below, the black admin bar is visible.\nI wanted Jules to add an option on the dashboard to hide the admin bar from logged-in users. The idea is that if a user logged in, the admin bar would be visible on the back end, but logged-in users browsing the front-end of the site wouldn't have to see the ugly bar.\nThis is the original dashboard, before adding the new feature.\nSome years ago, I completely rewrote the admin interface from the way it was when I acquired the plugin. Adding options to the interface is straightforward, but it's still time-consuming. Every option requires not only the UI element to be added, but also preference saving and preference recalling when the dashboard is displayed. That's in addition to any program logic that the preference controls.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nIn practice, I've found that it takes me about 2-3 hours to add a preference UI element, along with the assorted housekeeping involved. It's not hard, but there are a lot of little fiddly bits that all need to be tweaked, and that takes time.\nThat should bring you up to speed enough to understand my next test of Jules. Here's a bit of foreshadowing: The first test failed miserably. The second test succeeded astonishingly.\nInstructing Jules\nAdding a \"hide admin bar\" feature would not have been easy for the run-of-the-mill coding help we've been asking ChatGPT and the other chatbots to perform. As I mentioned, adding the new option to the dashboard requires programming in a variety of locations throughout the code and an understanding of the overall codebase.\nHere's what I told Jules.\n1. On the Site Privacy Tab of the admin interface, add a new checkbox. Label the section \"Admin Bar\" and label the checkbox itself \"Hide Admin Bar\". [Place this in the MAKE SITE PRIVATE block, located just under the Enable login privacy checkbox and before the Site Privacy Mode segment.]\nI instructed Jules where I wanted the AI to put the new option. On my first run through, I made a mistake and left out the details in square brackets. I didn't tell Jules exactly where I wanted it to place the new option. As it turns out, that omission caused a big fail. Once I added the sentence in brackets above, the feature worked.\n2. Be sure to save the selection of that checkbox to the plugin's preferences variable when the Save Privacy Status button is checked.\nThis ensures that Jules knows there is a preference data structure and that it is updated when the user makes a change. It's important to note that if I didn't understand the underlying code, I wouldn't have instructed Jules about this, and the code would not have worked. You can't \"vibe code\" something like this without knowing the underlying code.\n3. Show the appropriate checked or unchecked status when the Site Privacy tab is displayed.\nThis tells the AI that I want the interface to be updated to match what the preference variable specifies.\n4. Based on the preference variable created in (2), add code to hide or show the WordPress admin bar. If Hide Admin Bar is checked, the Admin Bar should not be visible to logged-in WordPress front-end users. If the Hide Admin Bar is not checked, the Admin Bar should be visible to logged-in front-end users. Logged-in back-end users in the admin interface should always be able to see the admin bar.\nThis describes the business logic that the new preference should control. It requires the AI to know how to hide or show the admin bar (a WordPress API call is used), and it requires the AI to know where to put the code in my plugin to enable or disable this feature.\nAnd with that, Jules was trained on what I wanted.\nJules dives into my code\nI fed my prompt set into Jules and got back a plan of action. Pay close attention to that Approve Plan? button.\nI didn't even get a chance to read through the plan before Jules decided to approve the plan on its own. It did this after every plan it presented. An AI that doesn't wait for permission raises the hairs on the back of my neck. Just saying.\nI desperately want to make a Skynet/Landru/Colossus/P1/Hal kind of joke, because I'm freaked out. I mean, it's good. But I'm freaked out.\nHere's some of the code Jules wrote. The shaded green is the new stuff. I'm not thrilled with the color scheme, but I'm sure that will be tweakable over time.\nAlso: The best free AI courses and certificates\nMore relevant is the fact that Jules picked up on my variable naming conventions and the architecture of my code and dived right in. This is the new option, rendered in code.\nBy the time it was done, Jules had included all the code changes it had originally planned, plus some test code. I don't use standardized tests. I would have told Jules not to do it the way it planned, but it never gave me time to approve or modify its original plan. Even so, it worked out.\nI pushed the Publish branch button, which caused GitHub to create a new branch, separate from my main repository. Jules then published its changes to that branch.\nThis is how contributors to big projects can work on those projects without causing chaos to the main code line.\nUp to this point, I could look at the code, but I wasn't able to run it. But by pushing the code to a branch, Jules and GitHub made it possible for me to replicate the changes safely down to my computer to test them out. If I didn't like the changes, I could have just switched back to the main branch, and no harm, no foul. But I did like the changes, so I moved on to the next step.\nAround the code in eight clicks\nOnce I brought the branch down to my development machine, I could test it out. Here's the new dashboard with the Hide Admin Menu feature.\nI tried turning the feature on and off and making sure the settings stuck. They did. I also tried other features in the plugin to make sure nothing else had broken. I was pretty sure nothing would, because I reviewed all the changes before approving the branch. But still: Testing is a good thing to do.\nI then logged into the test website. As you can see, there's no admin bar showing.\nAt this point, the process was out of the AI's hands. It was simply time to deploy the changes, both back to GitHub and to the master WordPress repository. First, I used GitHub Desktop to merge the branch code back into the main branch on my development machine.\nI changed \"Hide Admin Menu\" to \"Hide admin menu\" in my code's main branch, because I like it better. I pushed that (the full main branch on my local machine) back to the GitHub cloud.\nThen, because I just don't like random branches hanging around once they've been incorporated into the distribution version, I deleted the new branch on my computer.\nI also deleted the new branch from the GitHub cloud service.\nFinally, I packaged up the new code. I added a change to the readme to describe the new feature and update the code's version number. Then, I pushed it up to the WordPress plugin repository using SVN (the source code control system used by the WordPress community).\nJourney to the center of the code\nJules is very definitely beta right now. It hung in a few places. Some screens didn't update. It decided to check out for 90 minutes. I had to wait while it went to and came back from its digital happy place. It's exhibiting all the sorts of things you'd expect from a newly-released piece of code. I have no concerns about that. Google will clean it up.\nIn fact, since I first published this article, Google has reached out to me, and they've told me they're definitely working hard to reduce some of the issues I had. They also told me that I was right about the slowdown. They were completely slammed with visitors on launch day.\nThe fact that Jules (and presumably OpenAI Codex and GitHub Copilot Coding Agent) can handle an entire repository of code across a bunch of files is big. That's a much deeper level of understanding and integration than we saw, even six months ago.\nAlso: How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\nThe speed with which it can change an entire codebase is terrifying. The damage it can do is potentially extraordinary. It will gleefully go through and modify everything in your codebase, and if you specify something wrong and then push or merge, you will have an epic mess on your hands.\nThere is a deep inequality between how quickly it can change code and how long it will take a human to review those changes. Working on this scale will require excellent unit tests. Even tools like mine, which don't lend themselves to full unit testing, will require some kind of automated validation to prevent robot-driven errors on a massive scale.\nThose who are afraid these tools will take jobs from programmers should be concerned, but not in the way most people think. It is absolutely, totally, 100% necessary for experienced coders to review and guide these agents. When I left out one critical instruction, the agent gleefully bricked my site.\nSince I was the person who wrote the code initially, I knew what to fix. But it would have been brutally difficult for someone else to figure out what had been left out and how to fix it. That would have required coming up to speed on all the hidden nuances of the entire architecture of the code.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nThe jobs that are likely to be destroyed are those of junior developers. Jules easily does junior developer-level work. With tools like Jules, Codex, or Copilot, that cost a few hundred bucks a month at most, it's going to be hard for management to be willing to pay mid-to-high six figures for midlevel and junior programmers. Even outsourcing and offshoring aren't as cheap as using an AI agent to do maintenance coding.\nAs I wrote earlier in the week, if mid-level jobs are not available, how will we train the experienced people we'll need in the future?\nI am also concerned about how access limits will work. Productivity gains will drop like a rock if you need to do one more prompt and have to wait a day to be allowed to do so.\nAs for me? In less than 10 minutes, I created a new feature that readers had requested. While I was writing another article, I fed the prompt to Jules. I went back to work on the article and checked on Jules when it was finished. I checked out the code, brought it down to my computer, and pushed a release.\nUnfortunately, Jules can't work across different repositories, at least not yet. That means I can't have Jules treat the public GitHub codebase and the three private for-pay add-on codebases as one large project. But Google tells me that this kind of capability is on their list. I could open up my entire repository (of all my projects), and that might work, but it also might confuse Jules about which project is and is not relevant to the work at hand.\nIt took me longer to upload the thing to the WordPress repository than to add the entire new feature. For that class of feature, I got a half-day's work done in less than half an hour, from thinking about making it happen to publishing to my users.\nIn the first week since I completed this feature, about 8,500 sites have updated (about half of the entire installed base). Without Jules, those users probably would have been waiting months for this new feature, because I have a huge backlog of work, and it wasn't my top priority. But with Jules, it took barely any effort.\nAlso: 7 productivity gadgets I can't live without (and why they make such a big difference)\nThese tools are going to require programmers, managers, and investors to rethink the software development workflow. There will be glaring \"you can't get there from here\" gotchas. And there will be epic failures and coding errors. But I have no doubt that this is the next level of AI-based coding. Real, human intelligence is going to be necessary to figure out how to deal with it.\nHave you tried Google's Jules or any of the other new AI coding agents? Would you trust them to make direct changes to your codebase, or do you prefer to keep a tighter manual grip? What kinds of developer tasks do you think these tools should and shouldn't handle? Let us know in the comments below.\nWant more stories about AI? Sign up for Innovation, our weekly newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/google-embeds-ai-agents-deep-into-its-data-stack-heres-what-they-can-do-for-you/",
      "text": "Google embeds AI agents deep into its data stack - here's what they can do for you\nZDNET's key takeaways\n- Google is introducing powerful tech for agents and data.\n- They are also introducing a series of data-centric agents.\n- A new command-line AI coding tool is now available.\nI am no stranger to hyperbolic claims from tech companies. Anyone who's on the receiving end of a firehose of press announcements related to AI understands. Everything is game-changing, world-changing, the most, the best, yada, yada, yada.\nAnd then there's Google. Google is no stranger to hyperbole. But when a company so steeped in data management as part of its core DNA talks about \"fundamental transformation,\" and says that the world is changing because, \"It's being re-engineered in real-time by data and AI,\" we can consider those claims as fairly credible.\nAlso: Got 6 hours? This free AI training from Google can boost your resume today\nJust in time for Google Cloud Next Tokyo 2025, Google is making a series of announcements that herald a major change in how enterprises manage data.\nYasmeen Ahmad, Google's managing director of Data Cloud, says in a blog post, \"The way we interact with data is undergoing a fundamental transformation, moving beyond human-led analysis to a collaborative partnership with intelligent agents.\"\nShe calls this agentic shift, which she describes as, \"A new era where specialized AI agents work autonomously and cooperatively to unlock insights at a scale and speed that was previously unimaginable.\"\nAlso: 5 ways to successfully integrate AI agents into your workplace\nFrom almost any other company, claims like this would seem like just so much hot air. But Google is dropping a series of announcements about new offerings that provide real-world capabilities to data scientists and engineers in pretty tangible support of the claims.\nThe agentic shift\nThere's a fairly fine line between AI chatbots and AI agents. Chatbots are conversational, while agents are tools that perform autonomous tasks. Some users employ chatbots to perform tasks, as I did when I used ChatGPT to analyze some business data. Agents, like ChatGPT Agent, use a conversational interface to receive instructions.\nA good way to think of agents is as surrogate team members. Perhaps one agent does data normalization (cleaning up data), while another does migration. Each agent does one or more defined tasks using AI capabilities.\nAlso: Want AI agents to work together? The Linux Foundation has a plan\nIn this context, Google is looking at agents that can automate and simplify tasks for data workers, can communicate with each other, and can free professionals from tedious work so they can focus on \"higher-value tasks.\" Google is also trying to get agents to work together in virtual teams.\nThere are, of course, questions about whether agents aren't actually freeing up the time of senior professionals, but are instead taking work away from more junior employees. On the other hand, I don't have anyone to do the grunt work when I'm fully immersed in a project. So anything I can hand off to an agent is more time for projects and writing.\nCognitive foundation\nWith all these agents running around, traditional databases just aren't up to the task of keeping them fed. Agents do their reasoning or automation processes across silos. They need access to both historical and live data.\nClassic data management methods like real-time OLTP (online transaction processing) and deep-dive OLAP (online analytical processing) isolate data too much for AIs to gain insights from trends and current activities.\nOne way to help unify all of these capabilities is by enhancing their database offerings. A few years ago, Google added a columnar engine for AlloyDB. AlloyDB is the company's fully managed database service on Google Cloud Platform that focuses on PostgreSQL users, which is ideal for those who require a PostgreSQL-specific solution.\nAlso: How AI agents can generate $450 billion by 2028 - and what stands in the way\nA columnar engine is one where workloads query specific columns of data, reading only the fields needed for analysis. This leads to faster queries and allows for vectorized execution, where operations are applied to an entire column of data at once.\nNow, Google is adding a columnar engine to Spanner, its globally distributed, strongly consistent database service that offers high availability and scalability, designed for enterprises needing global reach and high transactional integrity.\nThis also adds power to BigQuery, Google's serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility. As the name implies, BigQuery is ideal for those who need to run fast, SQL-like queries on large datasets.\nThe company says this new columnar capability in Spanner speeds up analytical queries by something like 200x on live transactional data. With performance like that, we're talking instant responsiveness to real-time situations.\nWhen building enterprise-based AI systems, you need agents to make decisions based on real data. Performing real-time actions based on hallucinated data can get ugly very quickly. This is where RAG (retrieval augmented generation) comes in. Essentially, RAG combines large language models with real-time data access.\nAlso: 5 ways to be a great AI agent manager, according to business leaders\nYou can start to see how vectorizing search in Spanner and BigQuery becomes necessary when you're feeding in real-time data along with historical information. But getting vector search to work efficiently has traditionally been painful. Google is adding adaptive filtering in AlloyDB to automatically maintain vector indexes and optimize for fast queries on live operational data.\nGoogle is also introducing autonomous vector embeddings and generation to BigQuery, which automatically prepares and indexes multimodal data for vector search. This is a key step in creating a sort of semantic memory for agents.\nThe company is also introducing the ability to run AI queries right inside of BigQuery. This is, ahem, big. Now, BigQuery users can have AI do its magic across giant gobs of structured and unstructured data, ask complex questions (including subjective ones like \"Which customers are frustrated?\"), and get answers directly within existing analytics tools.\nNew agentic capabilities\nIn addition to building a foundation for agentic cooperation and data access, Google is announcing a series of new capabilities that embed agents in their biggest data tools. Let's look at each in turn.\nData engineering agent: Built specifically for data engineers, this agent within BigQuery can simplify and automate complex data pipelines. The entire workflow can be driven by natural-language prompts, from data ingestion to transformations to data-quality assessment to normalization.\nSpanner migration agent: Related to the data engineering agent, the Spanner migration agent can simplify data migration from legacy systems to BigQuery. This sort of migration is normally exceptionally tedious and potentially dangerous, but now the agent can do most of the heavy lifting.\nData science agent: Data scientists focus on analyzing and interpreting complex data, while data engineers focus on data infrastructure. According to Google, the new data science agent \"triggers entire autonomous analytical workflows, including exploratory data analysis, data cleaning, featurization, machine-learning predictions, and much more. It creates a plan, executes the code, reasons about the results, and presents its findings, all while allowing you to provide feedback and collaborate in sync.\"\nCode interpreter: Built as an enhancement of the conversational analytics agent introduced last year, the code interpreter takes in business-analysis questions and converts them to Python code to prepare custom analysis for users. This all runs within Google Data Cloud and uses the Google Data Cloud security infrastructure. It also includes an API available for developers to incorporate conversational analytics agent and code interpreter capabilities in custom code.\nNew command-line coding tool\nAs part of this big series of announcements, Google is introducing an extension to Gemini CLI called Gemini CLI GitHub Actions.\nCLI stands for command line interface, basically a terminal interface to your computer. Even though most users left the terminal behind when MS-DOS migrated to Windows, coders to this day make heavy use of the command line. Working in terminal mode lets coders add tools and control the coding process much faster than when they have to find and select items from menus and icons.\nAlso: Bad vibes: How an AI agent coded its way to disaster\nLast month, when Google introduced Gemini CLI, it basically made the features of the Gemini chatbot available in the terminal. Now, Google has extended that capability, providing some agentic features within the terminal environment.\nSome of you may be wondering how this compares with Jules, the Google coding agent I wrote about in May. First, Jules works in a secure cloud VM, while Gemini CLI GitHub Actions runs in terminal and integrates with GitHub Actions (the GitHub-based workflow tool).\nGoogle says there's a fairly narrow scope to Gemini CLI GitHub Actions compared to Jules. Jules can read your entire codebase, plan and present an approach to a coding challenge, and then execute on it. Gemini CLI GitHub Actions is specifically targeted to intelligent issue triage, accelerated pull-request reviews, and on-demand collaboration.\nAlso: Most developers use AI in their daily workflows - but they don't trust it, study finds\nThe issue-triage capability helps coders manage specific bug reports and feature requests. Pull requests are the way GitHub asks coders to confirm integrating coding changes into branches and master codebases. On-demand collaboration is essentially setting up a chat session whenever you want to talk about your code.\nI could easily see a programmer use both. Jules would be great for bigger projects and larger swings, and Gemini CLI GitHub Actions would work well for quicker updates and fixes.\nAre agents a game-changer?\nWhat do you think about the agentic shift Google is promoting? Have you started integrating intelligent agents into your own workflows? Which of Google's new data tools or capabilities intrigues you most -- the data engineering agent, the in-query AI reasoning, or something else? Do you see agents as helping senior professionals, replacing junior roles, or both? And how do you feel about running AI workflows directly in BigQuery? Let us know in the comments below.\nAlso: AI agents will change work and society in internet-sized ways, says AWS VP\nWant more stories about AI? Check out AI Leaderboard, our weekly newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.youtube.com/user/DavidGewirtzTV",
      "text": "- Svenska\n- Deutsch\n- English\n- Espa\u00f1ol\n- Fran\u00e7ais\n- Italiano\n- Alla spr\u00e5k\n- Afrikaans\n- az\u0259rbaycan\n- bosanski\n- catal\u00e0\n- \u010ce\u0161tina\n- Cymraeg\n- Dansk\n- Deutsch\n- eesti\n- EnglishUnited Kingdom\n- EnglishUnited States\n- Espa\u00f1olEspa\u00f1a\n- Espa\u00f1olLatinoam\u00e9rica\n- euskara\n- Filipino\n- Fran\u00e7aisCanada\n- Fran\u00e7aisFrance\n- Gaeilge\n- galego\n- Hrvatski\n- Indonesia\n- isiZulu\n- \u00edslenska\n- Italiano\n- Kiswahili\n- latvie\u0161u\n- lietuvi\u0173\n- magyar\n- Melayu\n- Nederlands\n- norsk\n- o\u2018zbek\n- polski\n- Portugu\u00easBrasil\n- Portugu\u00easPortugal\n- rom\u00e2n\u0103\n- shqip\n- Sloven\u010dina\n- sloven\u0161\u010dina\n- srpski (latinica)\n- Suomi\n- Ti\u1ebfng Vi\u1ec7t\n- T\u00fcrk\u00e7e\n- \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac\n- \u0431\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f\n- \u0431\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\n- \u043a\u044b\u0440\u0433\u044b\u0437\u0447\u0430\n- \u049b\u0430\u0437\u0430\u049b \u0442\u0456\u043b\u0456\n- \u043c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438\n- \u043c\u043e\u043d\u0433\u043e\u043b\n- \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n- \u0441\u0440\u043f\u0441\u043a\u0438\n- \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n- \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8\n- \u0570\u0561\u0575\u0565\u0580\u0565\u0576\n- \u05e2\u05d1\u05e8\u05d9\u05ea\n- \u0627\u0631\u062f\u0648\n- \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n- \u0641\u0627\u0631\u0633\u06cc\n- \u12a0\u121b\u122d\u129b\n- \u0928\u0947\u092a\u093e\u0932\u0940\n- \u092e\u0930\u093e\u0920\u0940\n- \u0939\u093f\u0928\u094d\u0926\u0940\n- \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be\n- \u09ac\u09be\u0982\u09b2\u09be\n- \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\n- \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\n- \u0b13\u0b21\u0b3c\u0b3f\u0b06\n- \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\n- \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\n- \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\n- \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\n- \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\n- \u0e44\u0e17\u0e22\n- \u0ea5\u0eb2\u0ea7\n- \u1019\u103c\u1014\u103a\u1019\u102c\n- \u1781\u17d2\u1798\u17c2\u179a\n- \ud55c\uad6d\uc5b4\n- \u65e5\u672c\u8a9e\n- \u7b80\u4f53\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\u9999\u6e2f\n- Svenska\n- Deutsch\n- English\n- Espa\u00f1ol\n- Fran\u00e7ais\n- Italiano\n- Alla spr\u00e5k\n- Afrikaans\n- az\u0259rbaycan\n- bosanski\n- catal\u00e0\n- \u010ce\u0161tina\n- Cymraeg\n- Dansk\n- Deutsch\n- eesti\n- EnglishUnited Kingdom\n- EnglishUnited States\n- Espa\u00f1olEspa\u00f1a\n- Espa\u00f1olLatinoam\u00e9rica\n- euskara\n- Filipino\n- Fran\u00e7aisCanada\n- Fran\u00e7aisFrance\n- Gaeilge\n- galego\n- Hrvatski\n- Indonesia\n- isiZulu\n- \u00edslenska\n- Italiano\n- Kiswahili\n- latvie\u0161u\n- lietuvi\u0173\n- magyar\n- Melayu\n- Nederlands\n- norsk\n- o\u2018zbek\n- polski\n- Portugu\u00easBrasil\n- Portugu\u00easPortugal\n- rom\u00e2n\u0103\n- shqip\n- Sloven\u010dina\n- sloven\u0161\u010dina\n- srpski (latinica)\n- Suomi\n- Ti\u1ebfng Vi\u1ec7t\n- T\u00fcrk\u00e7e\n- \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac\n- \u0431\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f\n- \u0431\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\n- \u043a\u044b\u0440\u0433\u044b\u0437\u0447\u0430\n- \u049b\u0430\u0437\u0430\u049b \u0442\u0456\u043b\u0456\n- \u043c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438\n- \u043c\u043e\u043d\u0433\u043e\u043b\n- \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n- \u0441\u0440\u043f\u0441\u043a\u0438\n- \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n- \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8\n- \u0570\u0561\u0575\u0565\u0580\u0565\u0576\n- \u05e2\u05d1\u05e8\u05d9\u05ea\n- \u0627\u0631\u062f\u0648\n- \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n- \u0641\u0627\u0631\u0633\u06cc\n- \u12a0\u121b\u122d\u129b\n- \u0928\u0947\u092a\u093e\u0932\u0940\n- \u092e\u0930\u093e\u0920\u0940\n- \u0939\u093f\u0928\u094d\u0926\u0940\n- \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be\n- \u09ac\u09be\u0982\u09b2\u09be\n- \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\n- \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\n- \u0b13\u0b21\u0b3c\u0b3f\u0b06\n- \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\n- \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\n- \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\n- \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\n- \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\n- \u0e44\u0e17\u0e22\n- \u0ea5\u0eb2\u0ea7\n- \u1019\u103c\u1014\u103a\u1019\u102c\n- \u1781\u17d2\u1798\u17c2\u179a\n- \ud55c\uad6d\uc5b4\n- \u65e5\u672c\u8a9e\n- \u7b80\u4f53\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\n- \u7e41\u9ad4\u4e2d\u6587\u9999\u6e2f\nInnan du forts\u00e4tter till YouTube\nVi anv\u00e4nder cookies och data f\u00f6r att\n- leverera och underh\u00e5lla Googles tj\u00e4nster\n- sp\u00e5ra avbrott och skydda mot spam, bedr\u00e4geri och otill\u00e5ten anv\u00e4ndning\n- m\u00e4ta m\u00e5lgruppsengagemang och webbplatsstatistik s\u00e5 att vi kan analysera hur v\u00e5ra tj\u00e4nster anv\u00e4nds och f\u00f6rb\u00e4ttra tj\u00e4nsternas kvalitet.\nOm du v\u00e4ljer knappen Godk\u00e4nn alla anv\u00e4nder vi \u00e4ven cookies och data f\u00f6r att\n- utveckla och f\u00f6rb\u00e4ttra nya tj\u00e4nster\n- leverera annonser och m\u00e4ta hur effektiva de \u00e4r\n- visa anpassat inneh\u00e5ll utifr\u00e5n dina inst\u00e4llningar\n- visa anpassade annonser utifr\u00e5n dina inst\u00e4llningar\nOm du v\u00e4ljer knappen Avvisa alla anv\u00e4nder vi inte cookies i dessa ytterligare syften.\nInneh\u00e5ll och annonser utan anpassning p\u00e5verkas bland annat av vad du tittar p\u00e5 f\u00f6r tillf\u00e4llet och din plats (annonser visas utifr\u00e5n din ungef\u00e4rliga plats). Inneh\u00e5ll och annonser med anpassning kan \u00e4ven omfatta s\u00e5dant som videorekommendationer, en anpassad YouTube Hem-sida och anpassade annonser utifr\u00e5n tidigare aktivitet, till exempel vad du tittar p\u00e5 och s\u00f6ker efter p\u00e5 YouTube. Vi anv\u00e4nder \u00e4ven cookies och data f\u00f6r att anpassa upplevelsen efter l\u00e4mplighet f\u00f6r din m\u00e5lgrupp, om till\u00e4mpligt.\nV\u00e4lj knappen Fler alternativ f\u00f6r mer information, till exempel om hur du hanterar dina integritetsinst\u00e4llningar. Du kan \u00e4ven bes\u00f6ka g.co/privacytools n\u00e4r som helst."
    },
    {
      "url": "https://www.zdnet.com/article/gemini-pro-2-5-is-a-stunningly-capable-coding-assistant-and-a-big-threat-to-chatgpt/",
      "text": "Gemini Pro 2.5 is a stunningly capable coding assistant - and a big threat to ChatGPT\nAs part of my AI coding evaluations, I run a standardized series of four programming tests against each AI. These tests are designed to determine how well a given AI can help you program. This is kind of useful, especially if you're counting on the AI to help you produce code. The last thing you want is for an AI helper to introduce more bugs into your work output, right?\nSome time ago, a reader reached out to me and asked why I keep using the same tests. He reasoned that the AIs might succeed if they were given different challenges.\nAlso: Want free AI training from Microsoft? You can sign up for its AI Skills Fest now\nThis is a fair question, but my answer is also fair. These are super-simple tests. I'm using PHP and JavaScript, which are not exactly challenging languages, and I'm running some scripting queries through the AIs. By using exactly the same tests, we're able to compare performance directly.\nOne is a request to write a simple WordPress plugin, one is to rewrite a string function, one asks for help finding a bug I originally had difficulty finding on my own, and the final one uses a few programming tools to get data back from Chrome.\nBut it's also like teaching someone to drive. If they can't get out of the driveway, you're not going to set them loose in a fast car on a crowded highway.\nTo date, only ChatGPT's GPT-4 (and above) LLM has passed them all. Yes, Perplexity Pro also passed all the tests, but that's because Perplexity Pro runs the GPT-4 series LLM. Oddly enough, Microsoft Copilot, which also runs ChatGPT's LLM, failed all the tests.\nAlso: The best AI for coding (and what not to use)\nGoogle's Gemini didn't do much better. When I tested Bard (the early name for Gemini), it failed most of the tests (twice). Last year, when I ran the $20-per-month Gemini Advanced through my tests, it failed three of the four tests.\nBut now, Google is back with Gemini Pro 2.5. What caught our eyes here at ZDNET was that Gemini Pro 2.5 is available for free, to everyone. No $20 per month surcharge. While Google was clear that the free access was subject to rate limits, I don't think any of us realized it would throttle us after two prompts, which is what happened to me during testing.\nIt's possible that Gemini Pro 2.5 is not counting prompt requests for rate limiting but basing its throttling on the scope of the work being requested. My first two prompts asked Gemini Pro 2.5 to write a full WordPress plugin and fix some code, so I may have used up the limits faster than you would if you used it to ask a simple question.\nEven so, it took me a few days to run these tests. To my considerable surprise, it was very much worth the wait.\nTest 1: Write a simple WordPress plugin\nWow. Well, this is certainly a far cry from how Bard failed twice and Gemini Advanced failed back in February 2024. Quite simply, Gemini Pro 2.5 aced this test right out of the gate.\nAlso: I asked ChatGPT to write a WordPress plugin I needed. It did it in less than 5 minutes\nThe challenge is to write a simple WordPress plugin that provides a simple user interface. It randomizes the input lines and distributes (not removes) duplicates so they're not next to each other.\nLast time, Gemini Advanced did not write a back-end dashboard interface but instead required a shortcode that needed to be placed in the body text of a public-facing page.\nGemini Advanced did create a basic user interface, but that time clicking the button resulted in no action whatsoever. I gave it a few alternative prompts, and it still failed.\nBut this time, Gemini Pro 2.5 gave me a solid UI, and the code actually ran and did what it was supposed to.\nWhat caught my eye, in addition to the nicely presented interface, was the icon choice for the plugin. Most AIs ignore the icon choice, letting the interface default to what WordPress assigns.\nBut Gemini Pro 2.5 had clearly picked out an icon from the WordPress Dashicon selection. Not only that, but the icon is perfectly appropriate to randomizing the lines in a plugin.\nNot only did Gemini Pro 2.5 succeed in this test, it actually earned a \"wow\" for its icon choice. I didn't prompt it to do that, and it was just right. The code was all inline (the JavaScript and HTML were embedded in the PHP) and was well documented. In addition, Gemini Pro 2.5 documented each major segment of the code with a separate explainer text.\nTest 2: Rewrite a string function\nIn the second test, I asked Gemini Pro 2.5 to rewrite some string processing code that processed dollars and cents. My initial test code only allowed integers (so, dollars only), but the goal was to allow dollars and cents. This is a test that ChatGPT got right. Bard initially failed, but eventually succeeded.\nThen, last time back in February 2024, Google Advanced failed the string processing code test in a way that was both subtle and dangerous. The generated Gemini Advanced code did not allow for non-decimal inputs. In other words, 1.00 was allowed, but 1 was not. Neither was 20. Worse, it decided to limit the numbers to two digits before the decimal point instead of after, showing it did not understand the concept of dollars and cents. It failed if you input 100.50, but allowed 99.50.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nThis is a really easy problem, the sort of thing you give to first-year programming students. Worse, the Gemini Advanced failure was the sort of failure that might not be easy for a human programmer to find, so if you trusted Gemini Advanced to give you its code and assumed it worked, you might have a raft of bug reports later.\nWhen I reran the test using Gemini Pro 2.5, the results were different. The code correctly checks input types, trims whitespace, repairs the regular expression to allow leading zeros, decimal-only input, and fails negative inputs. It also comprehensively comments the regular expression code and offers a full set of well-labeled test examples, both valid and invalid (and enumerated as such).\nIf anything, the code Gemini Pro 2.5 generated was a little overly strict. It did not allow grouping commas (as in $1,245.22) and also did not allow for leading currency symbols. But since my prompt did not call for that, and use of either commas or currency symbols returns a controlled error and not a crash, I'm counting that as acceptable.\nSo far, Gemini Pro 2.5 is two for two. This is a second win.\nTest 3: Find a bug\nAt some point during my coding journey, I was struggling with a bug. My code should have worked, but it did not. The issue was far from immediately obvious, but when I asked ChatGPT, it pointed out that I was looking in the wrong place.\nI was looking at the number of parameters being passed, which seemed like the right answer to the error I was getting. Instead, I needed to change the code in something called a hook.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nBoth Bard and Meta went down the same erroneous and futile path I had back then, missing the details of how the system really worked. As I said, ChatGPT got it. Back in February 2024, Gemini Advanced did not even bother to get it wrong. All it provided was the recommendation to look \"likely somewhere else in the plugin or WordPress\" to find the error.\nNeedless to say, Gemini Advanced, at that time, proved useless. But what about now, with Gemini Pro 2.5? Well, I honestly don't know, and I won't until tomorrow. Apparently, I used up my quota of free Gemini Pro 2.5 with my first two questions.\nSo, I'll be back tomorrow.\nOK, I'm back. It's the next day, the dog has had a nice walk, the sun is actually out (it's Oregon, so that's rare), and Gemini Pro 2.5 is once again letting me feed it prompts. I fed it the prompt for my third test.\nNot only did it pass the test and find the somewhat hard to find bug, it pointed out where in the code to fix. Literally. It drew me a map, with an arrow and everything.\nAs compared to my February 2024 test of Gemini Advanced, this was night and day. Where Gemini Advanced was as unhelpful as it was possible to be (seriously, \"likely somewhere else in the plugin or WordPress\" is your answer?), Gemini Pro 2.5 was on target, correct, and helpful.\nAlso: I put GitHub Copilot's AI to the test - its mixed success at coding baffled me\nWith three out of four tests correct, Gemini Pro 2.5 moves out of the \"Chatbots to avoid for programming help\" category and into the top half of our leaderboard.\nBut there's one more test. Let's see how Gemini Pro 2.5 handles that.\nTest 4: Writing a script\nThis last test isn't all that difficult in terms of programming skill. What it tests is the AI's ability to jump between three different environments, along with just how obscure the programming environments can be.\nThis test requires understanding the object model internal representation inside of Chrome, how to write AppleScript (itself far more obscure than, say Python), and then how to write code for Keyboard Maestro, a macro-building tool written by one guy in Australia.\nThe routine is designed to open Chrome tabs and set the currently active tab to the one the routine uses as a parameter. It's a fairly narrow coding requirement, but it's just the sort of thing that could take hours to puzzle out when done by hand, since it relies on understanding the right parameters to pass for each environment.\nAlso: I tested DeepSeek's R1 and V3 coding skills - and we're not all doomed (yet)\nMost of the AIs do well with the link between AppleScript and Chrome, but more than half of them miss the details about how to pass parameters to and from Keyboard Maestro, a necessary component of the solution.\nAnd, well, wow again. Gemini Pro 2.5 did, indeed, understand Keyboard Maestro. It wrote the code necessary to pass variables back and forth as it should. It added value by doing an error check and user notification (not requested in the prompt) if the variable could not be set.\nThen, later in the explanation section, it even provided the steps necessary to set up Keyboard Maestro to work in this context.\nAnd that, Ladies and Gentlemen, moves Gemini Pro 2.5 into the rarified air of the winner's circle.\nWe knew this was gonna happen\nIt was really just a matter of when. Google is filled with many very, very smart people. In fact, it was Google that kicked off the generative AI boom in 2017 with its \"Attention is all you need\" research paper.\nSo, while Bard, Gemini, and even Gemini Advanced failed miserably at my basic AI programming tests in the past, it was only a matter of time before Google's flagship AI tool caught up with OpenAI's offerings.\nThat time is now, at least for my programming tests. Gemini Pro 2.5 is slower than ChatGPT Plus. ChatGPT Plus responds with an answer nearly instantaneously. Gemini Pro 2.5 seems to take somewhere between 15 seconds and a minute.\nAlso: X's Grok did surprisingly well in my AI coding tests\nEven so, waiting a few seconds for an accurate and helpful result is a far more valuable thing than getting wrong answers right away.\nIn February, I wrote about Google opening up Google Code Assist and making it free with very generous limits. I said that this would be good, but only if Google could generate quality code. With Gemini Pro 2.5, it can now do that.\nThe only gotcha, and I expect this to be resolved within a few months, is that Gemini Pro 2.5 is marked as \"experimental.\" It's not clear how much it would cost, or even if you can upgrade to a paying version with fewer rate limits.\nBut I'm not concerned. Come back in a few months, and I'm sure this will all be resolved. Now that we know that Gemini (at least using Pro 2.5) can provide really good coding assistance, it's pretty clear Google is about to give ChatGPT a run for its money.\nStay tuned. You know I'll be writing more about this.\nHave you tried Gemini Pro 2.5 yet?\nHave you tried it yet? If so, how did it perform on your own coding tasks? Do you think it has finally caught up to, or even surpassed, ChatGPT when it comes to programming help? How important is speed versus accuracy when you're relying on an AI assistant for development work?\nAlso: Everyone can now try Gemini 2.5 Pro - for free\nAnd if you've run your own tests, did Gemini Pro 2.5 surprise you the way it did here? Let us know in the comments below.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/claude-code-makes-it-easy-to-trigger-a-code-check-now-with-this-simple-command/",
      "text": "Claude Code makes it easy to trigger a code check now with this simple command\nZDNET's key takeaways\n- Automated security reviews in Claude Code help ensure code safety.\n- Spot and fix vulnerabilities before your code reaches production.\n- Run the /security-review command in the terminal or via GitHub Action.\nClaude Code became generally available in May, and since then, it has become popular among developers for its coding assistance, available right in the terminal or integrated development environments (IDEs). Now, new features are coming to Claude that make it easier to build safely, too.\nOn Wednesday, Anthropic introduced automated security reviews in Claude Code. They allow developers to more easily identify and fix security concerns, and can be invoked either manually using the new \"/security-review\" command or automatically via the new GitHub Action for Claude Code.\nAlso: Claude Code's new tool is all about maximizing ROI in your organization - how to try it\n\"On demand or automatically, Claude will review the code that you're working on, the code that you're pushing, or your entire repository, and practically identify vulnerabilities and suggest ways to fix them,\" said Logan Graham, Frontier Red Team lead at Anthropic, to ZDNET.\nCommand in Claude Code\nAll developers have to do is invoke the /security-review command in Claude Code, which will trigger the security analysis in the terminal. Anthropic said Claude will then search the codebase, identify common vulnerabilities such as SQL injection risks, insecure data handling, and authentication flaws, and explain the issues found.\n\"We want it to be, and I think we can get there soon if it's not there already, kind of like having the best security engineer or best senior software engineer, over shoulder, helping you do your work, better and securely,\" added Graham.\nAlso: The best AI for coding in 2025 (including a new winner - and what not to use)\nAfter identifying the issues, the user can also ask Claude Code to implement the fixes for each one. This allows developers to catch issues easily by integrating the security reviews before committing the code or before it reaches production. ZDNET's own David Gewirtz, a computer science professor turned AI innovator, found the update helpful, saying, \"Adding the security review as a command is good. Otherwise, you'd have to embed it in each query or add it to their system instructions.\"\nGitHub Action\nPull requests are an essential part of the collaborative development process, but they require extensive manual review before being merged into the main codebase. Now, with the new GitHub Action for Claude Code, developers can have Claude automatically analyze every pull request when it's opened, a step that can otherwise be forgotten or overlooked.\nAlso: Anthropic's free Claude 4 Sonnet aced my coding tests - but its paid Opus model somehow didn't\nAnthropic said Claude can review code changes for security vulnerabilities, apply customizable rules, and post inline comments with concerns and recommendations for fixes. Anthropic has used GitHub Actions to catch vulnerabilities in its own code before shipping to users, according to the release. Just last week, GitHub Actions identified a remote code execution vulnerability, which was fixed before the pull request was merged.\nHow to access\nTo access the /security-review command, update Claude Code to the latest version and run it in your project directory. Anthropic posted documentation for installing and configuring the GitHub Action.\nWant more stories about AI? Check out AI Leaderboard, our weekly newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/these-7-common-household-items-were-draining-power-all-day-until-i-pulled-the-plug/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThese 7 common household items were draining power all day - until I pulled the plug\nCosts are steadily rising in the US, and energy costs are a prominent example. This is made worse by summer temperatures being at record highs, with heat waves wreaking havoc across various states in the past few weeks. As someone who's gone through a few of these this summer alone, I'm constantly looking for ways to conserve energy.\nThere are many little things you can do that can shave dollars off your monthly energy bill, and they go beyond switching off the lights when you leave the room. Did you know you can save 3% on cooling costs for every 1\u00b0F increase in your thermostat? As a fan of data, I've looked into multiple quantifiable ways to save energy and how these translate into saving money.\nAlso: Apple's tariff costs and iPhone sales are soaring - how long until device prices are too?\nUnplugging a single device when not in use isn't going to save you a bucket of cash. But unplugging multiple devices adds up quickly to help you save, especially when you make a habit of it. Here are the devices you should unplug when they're not in use, and how doing so could save you almost $200 a year, depending on your local rates.\n1. TVs, gaming consoles, and office devices\nIt's easy to forget to unplug TVs that aren't in daily use, but it happens more often than you'd think. This is especially true if you have a TV in a guest room or common area that isn't often occupied. Simply unplugging your TVs when not in use could save you between $2 and $6 a year for each one. As soon as I learned this, I ran to unplug my guest room and office TV.\nSimilarly, a printer left plugged in can cost between $3 and $8 a year when idle. One can consume between 2W and 6W when not in use, adding another good savings option.\nAlso: These jobs face the highest risk of AI takeover, according to Microsoft\nA single gaming console like a PlayStation or Xbox can consume between 1.5W and 10W in standby power, which can cost up to a dollar each month for a device that is not in use. It's a good idea to unplug these devices when you'll be out of the house or overnight, just keep in mind that you may still want to leave them in rest mode so you don't miss an update.\nYou should also consider unplugging sound bars and speakers when not in use, especially in areas that don't get daily use.\n2. Coffee makers and kitchen devices\nUnfortunately, I learned the hard way that my Keurig coffeemaker was driving up my power bill. I kept my Keurig in standby mode, rather than asleep, so it was always ready to brew a cup of coffee. This can consume from 60W and 70W because it has to keep the water warm and ready. This can translate into up to $60 a year.\nAlso: This unexpected robot vacuum made me forget about my $2,000 Roborock\nSimilarly, a mini-fridge can cost you up to $130 a year to run, which doesn't always pay off if you don't need it to run all the time. I used to forget to refill my mini-fridge, so it ended up running empty for weeks, wasting between 50-100W of energy and costing me money. If you have a mini-fridge that you only use seasonally, like in your sunroom during summer months, it's best to unplug it when not in use. Doing so can save you over $10 a month on energy bills.\n3. Smart home devices that are mostly idle\nMost smart home devices are focused on improving energy efficiency, but they can also be vampire devices. Smart devices are always consuming energy because they remain connected to either the internet or another device, like a hub. However, smart devices tend to consume very low phantom loads, so you don't need to rush to unplug them all.\nSmart bulbs and plugs, though small, are always drawing a small amount of power, around 1W. This only costs you between $0.65-$1.30 a year, depending on your local rate, but it can add up if you have multiple of these devices, especially if you don't use them often.\nI have a lot of smart bulbs at home and I kill the switch on the ones that I know we won't be using un the near future. Similarly, I unplug my smart plugs when they don't have a device plugged into them.\nAlso: I let this Segway robot mow my lawn for months - here's why it's been irreplaceable for me\nA smart plug can often save you money on your energy bill. For example, if you have an older Keurig without scheduling capabilities that you'd like to have ready when you wake up, you can put a smart plug on it and schedule it to turn on just an hour a day. I also use my smart plugs to run a grow light for houseplants, a fan, and an older lamp.\n4. Older devices around your home\nMany people still have older devices plugged in that they haven't used in a long time, but either forgot or simply don't think twice about it. A single set-top cable box, DVR, or Blu-ray player can cost you up to $20 a year to power when not in use, depending on your local rates.\nLook around your home and see what is plugged in that doesn't need to be, including alarm clocks, cordless phone bases, electric kettles, hair dryers, and more. Unplugging these devices and only plugging them back in before using them can pay off, even if it's just a few dollars a month.\nFAQs\nMore tips to save on utilities this summer\nEven as the temperatures reach scorching highs, there are many ways to save on your utility bills this summer.\nHere are some things you can do to save money on your energy bills:\n- Run a fan instead of turning down the A/C: Your central air conditioner unit uses between 2,000W and 2,500W when running, while a ceiling fan, for example, uses around 50W. You can set your thermostat between 4-6 degrees higher during the warmer months and use a fan to save energy costs and the life of your A/C unit.\n- Keep the heat out: A warmer house takes more effort to cool, so you should make sure your windows are completely shut and your home is properly insulated. Closing the blinds and adding blackout curtains in some rooms can also maintain a cooler temperature throughout the day.\n- Switch to LED bulbs: I'm sure you've heard this before, but this is a fantastic way to save on your utility bill. LED bulbs use 80-90% less energy than traditional incandescent bulbs and generate less heat.\n- Learn about energy savings through your utility provider: Many providers offer opportunities to save on your energy bills, especially during peak usage days and hours. Find out if your utility provider has a savings program and what you can do to participate in it.\nWhat are vampire devices?\nYou've likely heard about unplugging common household appliances and devices when not in use to save energy. Devices that consume energy even while they're switched off are aptly called 'vampire devices,' and you may have more of them in your home than you think.\nAlso: This thermostat mistake was costing me hundreds: 3 tips to get the best temperature control\nThe US Department of Energy recommends unplugging devices when not in use to reduce phantom loads. Doing so can save you up to 10% on your energy bill, which is a pretty significant amount for such little effort.\nDoes unplugging devices actually save money?\nUnplugging devices can help you save more energy than you'd think, especially if you often forget to switch off appliances like your Keurig. Many of the devices in our homes consume power even if they're not in use, and the key to saving on your energy bills lies in compounding your potential to conserve energy.\nAs someone with a whole-home backup system that sees multiple power outages a year, I make a habit of unplugging devices to save energy. This is even more evident when the power goes out and I go through all my home's circuits using the EcoFlow Smart Home Panel. The EcoFlow app shows me the load that each circuit in my home carries at any given time, and lets me turn them off from my phone.\nAlso: I let this Segway robot mow my lawn for months - here's why it's been irreplaceable for me\nWhen the power goes out, I unplug devices that aren't essential and turn off non-critical circuits. If my home consumes 1.2kWh at any given time, I can easily halve this by unplugging devices and switching off circuits. I unplug anything from smart plugs and lamps to air purifiers and robot vacuums that don't need to be used during a power outage, and I keep my home running with only 330Wh to 600Wh to extend the life of my backup battery.\nGet the best tech tips in your inbox each day with our Tech Today newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/is-a-refurbished-macbook-worth-it-i-did-the-math-and-heres-my-buying-advice/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nIs a refurbished MacBook worth it? I did the math, and here's my buying advice\nI remember when people used to scoff at refurbished tech. Buying a used phone or computer was like rolling the dice: you never knew exactly what you were getting.\nWell, the times certainly have changed. And, in this economy, the prices on refurbished tech are looking more and more tempting. Consumers are challenging this idea that we're all supposed to buy brand new phones and laptops every few years, and in the process, the market for refurbished tech has blown up.\nAlso: Should you buy a refurbished iPad? I tried one from Back Market and here's my verdict\nHaving never purchased a refurbished product myself, I was very curious as to how the experience would go with a new (used) laptop. So, I connected with the folks at Back Market and acquired a $950 MacBook Pro M1 from 2021 to test out as my main driver for a few weeks.\nI'll say right now: it wasn't a perfectly seamless experience, but buying brand new tech rarely is, either. Here's how it went, and what I've learned from the process.\nHow Back Market works\nFirst thing you should know is that the retailer offers different tiers of products: you can buy a MacBook in Fair, Good, Excellent, or Premium condition, with the prices scaling higher with better condition. If you're wondering what Fair condition looks like, it could include some visible signs of use, but ultimately, the internal parts have been inspected and verified by Back Market to be in good working order.\nAfter choosing the level of wear you're comfortable with, you then go through and choose the processor, memory amount, storage, and color of your new MacBook -- just like you would a brand new one.\nAlso: Your MacBook is getting a major upgrade for free - 5 MacOS 26 features I'm trying right now\nLastly, you can choose to include a trade-in with another device as part of the purchase. This process consists of a Back Market rep appraising your device and giving you an offer in a dollar amount. You then have 21 days to ship the item to Back Market, and once they get it, they'll cut you a rebate check.\nCan you trust a refurbished item?\nI don't blame you for being wary of used tech. But keep in mind that Back Market only takes in products it knows it can resell, and even then, it has to pass a 25-point inspection test to ensure they're in good enough condition.\nThis testing, repair, and parts replacement process is what sets a refurbished MacBook apart from a used one you'd buy straight from the hands of someone on Facebook Marketplace, for example.\nBesides a few signs of wear and tear (which you can realistically expect from a device in the Fair or Good categories), the primary thing to consider is the device's battery life. Luckily, you can see your MacBook's battery capacity in the settings, along with a host of other information, including how many cycles it's gone through.\nAlso: MacBook Pro vs. MacBook Air: How to decide which Apple laptop you should buy\nThe rule of thumb is to replace a battery that has dipped below 80% capacity, so a device purchased through Back Market should be above that mark, with Premium tier devices certified to be above 90%.\nThe MacBook Pro M1 I got from Back Market arrived in the Premium tier, so there was actually not a scratch or dent on the device at all. The screen looked flawless. The battery capacity was at 95%, and I had no issues with the keyboard or trackpad.\nThe only issue I experienced was that a day or two after I first set it up, it randomly booted up in recovery mode, prompting me to reinstall the latest OS. I did so, and it hasn't happened again since.\nHow much do you save?\nHere's the big question. How much are you really saving? Let's break it down based on three different comparable MacBook Pros from different generations: the M1 Pro I got from 2021, the M2 from 2023, and the latest MacBook Pro on the market, the M4 from 2024:\nCPU | Memory/Storage | Price | |\nMacBook Pro M1 14\" (Refurbished) | M1 Pro 10-core | 16GB, 1TB | |\nMacBook Pro M2 14\" (New) | M2 Pro 10-core | 16GB, 1TB | |\nMacBook Pro M4 14\" (New) | M4 Pro 10-core | 16GB, 1TB |\nThe refurbished M1 is not available new from most retailers, but you can still get new M2s from Amazon. As you can see, it's significantly cheaper -- over $600 less -- than the next generation up with comparable hardware. The difference between the M2 and M4, however, is smaller; just a few hundred dollars.\nIf you're wondering how they compare, CPU benchmarking results from Geekbench put the M1 and M2 10-core CPUs quite close together, with the M4 understandably a bit further off. These numbers refer to the CPU's performance limits under a heavy load, showing that the newer MacBooks have more power under the hood on the top end.\nAlso: The MacOS 26 public beta is here - How to install (and which models support it)\nBut the differences are not earth-shattering, here. When it comes to everyday tasks like working out of a browser, running productivity software, and multitasking with different apps across the latest MacOS, the difference in performance is not as easy to identify.\nAnd yes, the M1 MacBook Pro will be able to upgrade to the all-new MacOS 26 Tahoe when it comes out this fall, meaning it'll have the same operating system as newer MacBooks.\nGeekbench Single-Core CPU | Geekbench Multi-Core CPU | |\nMacBook Pro M1 14\" (Refurbished) | 2,451 | 10,983 |\nMacBook Pro M2 14\" (New) | 2,648 | 12,310 |\nMacBook Pro M4 14\" (New) | 3,742 | 14,693 |\nThe final verdict\nSo, should you buy a refurbished MacBook? If you want to enjoy the latest features like iPhone mirroring, the new and improved spotlight, and the liquid glass UI redesign in MacOS 26 and don't need the top-of-the-line processing power, then absolutely, yes.\nYou can get a refurbished MacBook Pro M1 with 16GB of unified memory for $841 less than a MacBook Pro M4 with a 10-core CPU. If you're not editing 4K video, animation, or rendering large image files, a top-tier processor might be overkill.\nAlso: How to make any MacOS app start at login - to save you time and clicks\nThat being said, by choosing a refurbished laptop, you'll want to carefully weigh the trade-offs. Make sure you're opting for enough local storage to future-proof your device, decide what kind of physical condition you're comfortable with, and make sure the device you choose has a healthy battery.\nLastly, keep in mind that Back Market is not the only place to purchase refurbished MacBooks, Apple has its own official refurbished store where you can purchase everything from MacBooks to iPads to Apple Watches."
    },
    {
      "url": "https://www.zdnet.com/article/microsoft-authenticator-wont-manage-your-passwords-anymore-or-most-passkeys/",
      "text": "Microsoft Authenticator won't manage your passwords anymore - or most passkeys\nZDNET's key takeaways\n- Microsoft is a prolific supporter of using passkeys over passwords.\n- Authenticator will no longer save your passwords.\n- But Authenticator can't be your comprehensive passkey manager.\n- The Edge browser will likely fill that role one day, but it's not there yet.\nFor most of this year, Microsoft has been warning users that they will no longer be able to use its Authenticator mobile application for user ID and password management. As reported by CNET on July 29, 2025, \"In June, the company stopped letting users add passwords to Authenticator\u2026. And starting Aug. 1, you'll no longer be able to use saved passwords.\"\nAlso: How passkeys work: The complete guide to your inevitable passwordless future\nTo me, the dire warnings of this pending doomsday-like deadline are reminiscent of the run-up to January 1, 2000 -- the so-called \"Y2K problem\" -- when it was anticipated that computers everywhere would experience a meltdown because their programmers never considered the possibility that their software would still be in use in the 21st century.\nThe great passkey migration\nBut most of this reporting overlooks the bigger shift that's underway across Microsoft's identity management portfolio and, in many cases, is missing key details about the future roles of Microsoft Authenticator and the Microsoft Edge browser when it comes to another colossal shift that's currently in progress: the world's transition from passwords to passkeys.\nA passkey is unequivocally a safer credential than a password when it comes to logging into websites and apps. Passkeys cannot be guessed, the same passkey cannot be reused across different websites and apps, and you cannot be tricked into divulging your passkeys to malicious actors through techniques such as phishing, smishing, squishing, and malvertising. Even if you're strengthening user IDs and passwords with additional factors of authentication, passkeys are a better and more secure alternative.\nAlso: I replaced my Microsoft account password with a passkey - and you should, too\nIn fact, of the major technology vendors that are encouraging end-users to switch to passkeys, no vendor is pushing users to transition as hard as Microsoft is. But, at the same time that Microsoft is aggressively campaigning for that transition, we are still waiting for Microsoft to offer the comprehensive credential management capabilities that are necessary to support that future.\nManaging passwords after Authenticator\nFor users who managed their user IDs and passwords with Authenticator and want to stay with Microsoft-based solutions to manage their user IDs and passwords, their only option is to export their passwords from Microsoft Authenticator to Microsoft's Edge web browser. Once users do this, Edge will not only take over the role of managing those user IDs and passwords, it will also handle the auto-provisioning of those credentials (a.k.a. autofill) at the time of login and the synchronization of those credentials to the user's other copies of Edge.\nIn addition to Windows, Edge is available on MacOS, iOS, Android, and Linux. Given Edge's cross-platform reach when compared to that of Microsoft Authenticator (iOS and Android), it makes more sense for Edge to handle credential management and autofill.\nAlso: What if your passkey device is stolen? How to manage risk in our passwordless future\nThis approach, where Microsoft is facilitating credential management through the browser instead of a mobile application closely resembles the way Google is handling credential management and autofill through its Chrome browser. Both browsers are based on Chromium and offer users some basic password management capabilities, and both rely on a central cloud to handle credential synchronization to the same browser on other devices.\nThe problem with non-syncable passkeys\nBut, at the time this article was published, whereas Chrome's password management capabilities will auto-provision and synchronize credentials of both types (passwords and passkeys) to a user's other installations of Chrome, Edge can only synchronize passwords. According to a Microsoft spokesperson who was interviewed for this story, \"passkeys created for services like PayPal and eBay are stored as device-bound credentials in Windows and can be accessed via Windows Settings > Accounts > Passkeys. These are not stored or synced in Edge.\"\nIn other words, Edge for Windows is capable of handling and auto-provisioning passkeys during a login, but not the other versions of Edge. I confirmed this by trying to use Edge for Android to register a passkey for eBay. A lot happens behind the scenes when you register a passkey for the first time, and I explain the process in How Passkeys Work: Let's Start the Registration Process.\nAlso: 10 passkey survival tips: Prepare for your passwordless future now\nWhereas an eBay passkey registration option exists when using Edge for Windows, no such option was available to me on Edge for Android. In addition to that limitation, the eBay passkey that I was able to establish on Edge for Windows could not be synchronized to my copy of Edge for Android. This confirmed the spokesperson's statement about passkeys being \"stored as device-bound credentials in Windows.\" Device-bound passkeys are also referred to as \"non-syncable passkeys.\" As I wrote in my detailed comparison of syncable passkeys to device-bound passkeys (something that ever user should know going forward), device-bound passkeys are tied to the device that was used to create them and cannot be synchronized to another device. As it turns out, the passkey that I established through Edge running on my copy of Windows 11 was bound via Windows Hello to the Trusted Platform Module (TPM) in my HP Notebook.\nThis raises the question of where, across Microsoft's portfolio, users might be able to find support for syncable passkeys since they are by far the most convenient form of passkey to use for the websites and apps that support them. After all, the company is already supporting syncable user IDs and passwords through Edge. The last thing most users want to do is manage multiple device-bound passkeys for each website and app they use. Better to just have one, just like a password.\nYour passkey management options now\nThis is where the confusion sets in. Across most of the articles that reported on the elimination of user ID and password support in Microsoft Authenticator, the authors also noted that Authenticator would continue to support passkeys and that the user could continue to rely on Authenticator to authenticate (login) with those passkeys (see my explanation of what really happens during your 'passwordless' passkey login). It's not surprising that most of the articles said this. After all, Microsoft's own post about the changes to Authenticator very clearly states, \"Authenticator will continue to support passkeys. If you have set up Passkeys for your Microsoft Account, ensure that Authenticator remains enabled as your Passkey Provider. Disabling Authenticator will disable your passkeys.\"\nThis certainly piqued my interest. On the surface, it was strangely starting to look like Microsoft was moving all user ID and password management to Edge while at the same time fracturing passkey management across Microsoft Authenticator and Edge for Windows instead of moving full support for both syncable passwords and syncable passkeys to Edge (which is exactly how Chrome does it). So I went back to Microsoft to make sure that I understood things correctly. I apparently didn't.\nAlso: Passkeys won't be ready for primetime until Google and other companies fix this\n\"Authenticator will always continue to support device-bound passkeys for Entra accounts,\" a Microsoft spokesperson told me. \"You'll always be able to create one of those today and in the future.\" There's a lot to unpack there. Not only are Authenticator-managed passkeys also device-bound passkeys (in other words, they cannot be synchronized), the passkey support found in Authenticator is for users of Microsoft Entra ID, Microsoft's cloud-based identity management solution (formerly known as Azure Active Directory) for businesses. In other words, the passkey support found in Microsoft Authenticator is not for those of us in the general user population who just want to manage their credentials. And it still lacks any synchronization capabilities.\nIn a nutshell, for those of us in the general user population who want to manage and use passkeys in addition to user IDs and passwords, Microsoft offers one option: Edge on Windows. Additionally, neither Edge for Windows nor Microsoft Authenticator (for Entra ID users) offers passkey synchronization. The only type of passkeys that Microsoft currently supports are device-bound (non-syncable) passkeys. This is obviously not ideal, and knowing some of the folks at Microsoft, I'm sure they would agree (especially given how hard the company is selling the idea of passkeys right now).\nMy conclusion as I try to take a 30,000-foot view of this situation is that when it comes to all of the different Microsoft technologies that play a role in credential management -- Windows, Windows Hello, Authenticator, Edge, Microsoft Wallet, Entra ID, passkeys, etc. -- the company has a lot of different pieces on the chessboard. Moving them all into the ideal position to support the secure credential management future it is selling is easier said than done.\nAlso: What really happens during your 'passwordless' passkey login?\nIn the same way that a chess player (and opponent) always think and anticipate a few moves ahead, it's hard not to see that at some point, sooner or later (probably sooner), Microsoft will support syncable passkeys across all its versions of Edge just like it does now with user IDs and passwords (and just like Chrome does). That is the only logical outcome given its strongly worded messages to migrate passwords from Authenticator to Edge.\nBut until that final chess move happens, users have options in the other credential management companies, including Google and all the third-party password managers (1Password, BitWarden, Dashlane, LastPass, NordPass, etc.) that support syncable passkeys and passwords in a single solution.\nStay ahead of security news with Tech Today, delivered to your inbox every morning."
    },
    {
      "url": "https://www.zdnet.com/article/github-actions-moves-github-into-devops/",
      "text": "GitHub Actions moves GitHub into DevOps\nYes, Git, Linus Torvalds' distributed source code control system, is essential to modern-day programming, but it's far more than that. Git is key to essentially all DevOps operations. GitHub recognizes that, and with GitHub Actions, it's transforming its Git services into a DevOps workflow pipeline.\nWith GitHub Actions, you can build a container app, deploy a web service, publish packages to registries, or automate welcoming new programmers to your open source projects. Or chain them all together.\nAt its heart, as Erica Brescia, GitHub's COO, tweeted: \"We're introducing full CI/CD [Continuous Integration/Continuous Delivery] functionality\" into GitHub. Brescia concluded, \"This is a game changer.\"\nThat's not hype. With another GitHub Action feature, Matrix builds, you can test your programs on Linux, MacOS, Windows, containers, and multiple runtime versions, simultaneously. This also lets you test multiple versions of your project in parallel.\nA complete DevOps stack comprises tools to configure systems, using DevOps programs such as Ansible, Chef, Puppet, and Salt. Next, you need software version control, which GitHub always had. And you must package the program with Docker or other container approaches. Then, you need to test your newly revised software. Your next step is to deploy it with a program such as Jenkins. After that, you run the program, commonly these days with Kubernetes. Finally, you monitor everything with such packages as the Elasticsearch, Logstash, Kibana (ELK) Stack or Prometheus.\nGitHub knows this is not just adding on CI/CD and testing. Max Schoening, GitHub's Senior Director of Product Design, said: \"At GitHub, we have a secret strategy: We work incredibly hard on designing products developers love. Software is changing the world. For a lot of teams that change means delivering software quickly and reliably. We see GitHub Actions as the engine for it -- some call that DevOps.\"\nThat said, GitHub doesn't see Actions CI/CD as a competitor for other CI/CD programs such as Jenkins, AWS CodeDeploy, or GitLab CI per se. Schoening said, \"GitHub Actions is community-powered workflow automation. GitHub and our community believe in choice and an open ecosystem. That is something we take seriously and build into everything we do. GitHub Actions lets developers integrate with all their existing tooling, mix and match new developer products, and hook into all parts of the software lifecycle, including existing CI/CD partners.\"\nOne such CI/CD partner likes Actions. Jim Rose, CircleCI's CEO, said in a statement, \"CircleCI has been building a CI/CD platform since 2011, and GitHub has been a great partner. GitHub Actions is further validation that CI/CD is critical for the success of every software team. We believe that developers thrive in open, connected ecosystems, and we look forward to working with GitHub to lead the evolution of CI/CD.\"\nWith GitHub Actions, GitHub provides developers and sysadmins with a lot of that stack on one open-source product line. The parts it doesn't have -- such as packaging -- have become commonplace.\nSo what is GitHub Actions exactly? It's an application programming interface (API) for cause and effect on GitHub. It enables you to orchestrate any workflow, based on any event, while GitHub manages the execution, provides you rich feedback and secures every step along the way. With GitHub Actions, workflows and steps are code in a repository, so you can create, share, reuse, and fork your software development practices.\nWhat's new is that GitHub has added CI/CD and testing to Actions. With this, users can automate how they build, test, and deploy projects. You can do this with any platform. That includes Linux, MacOS, and Windows. You can then run your workflows in a container or on a virtual machine (VM).\nDeveloper\nTo make CI/CD work, you need to publish packages and containers. GitHub Actions lets you publish and consume packages from GitHub Package Registry or any other registry.\nGitHub Actions also supports more languages and frameworks than before. This now includes Node.js, Python, Java, PHP, Ruby, C/C++, .NET, Android, and iOS. You can also test your web service and its database together.\nGitHub Actions are also written in YAML. That means, according to Nat Friedman, GitHub's CEO, \"You can edit, reuse, share, and fork [Actions] like code. When you fork a repository, you fork the actions along with the source code, giving you a seamless way to test and build projects using the same Actions as the original project. We think this is a great way to learn from the community, by reproducing every step of your favorite projects, then forking them to suit your own requirements.\"\nGitHub users like this plan. \"For open-source projects,\" said Ma\u00ebl Nison, Yarn maintainer, \"the lower infrastructure friction we have, the easier it becomes to work with external contributors. Using GitHub Actions for CI and common tasks like triaging issues allows us to lower the barriers even more, so we can focus on writing a good product.\"\nGitHub wants your business. So GitHub Actions is free for public open-source repositories. For private repositories, Actions offers simple, pay-as-you-go pricing. If you want to run on your own hardware or another cloud, Action's self-hosted runners are free to use -- until Nov. 13, when Action leaves beta, Actions is free for everyone."
    },
    {
      "url": "https://www.zdnet.com/article/yes-you-need-a-firewall-on-linux-heres-why-and-which-to-use/",
      "text": "Yes, you need a firewall on Linux - here's why and which to use\nZDNET's key takeaways\n- Linux is highly secure, but you should still have a firewall.\n- You should know if your ISP's hardware (gateway) uses a firewall.\n- One of the easiest Linux firewalls is UFW and its GUI sidekick, GUFW.\nI've been using Linux for nearly 30 years. Over those years, I've experienced only one security issue (a rootkit on a server I inherited). The reason for that is Linux's heightened security. Out of the box, it includes a tight permissions system and security mechanisms (such as AppArmor and SELinux) that do an amazing job of locking down the operating system.\nBut what about the firewall? You know about firewalls, especially if you've used Windows (because Microsoft's OS has always depended on them). And before you think it, no matter how secure your web browser is, it's not enough.\nAlso: Thinking about switching to Linux? 9 things you need to know\nAlmost every Linux distribution ships with a firewall that is ready to use. Oddly enough, however, some distributions ship with the firewall disabled.\nThat seems counterintuitive for an operating system that hangs its hat on security.\nThe big question you may ask is, \"Does Linux even need a firewall?\"\nBefore answering that question, I'll ask you some questions:\n- Is your Linux machine on a home network?\n- Does your home network have a router that includes a firewall?\n- Is your router regularly updated?\n- If your home network has a router with a firewall, are there any ports open?\n- Do you have sensitive data on your computer?\nYou might not know the answers to those questions, which means you might have to contact your ISP and ask them about the hardware in use. For example, AT&T Fiber does include a firewall on its gateway hardware. Comcast's Xfinity gateways also include a firewall.\nAlso: 8 things you can do with Linux that you can't do with MacOS or Windows\nIf you know your ISP hardware includes a firewall, the need for a firewall on your Linux machines is less pressing than otherwise.\nBut does that mean you should forget about the firewall?\nI say, no.\nI say, the more security, the better.\nFor example, your ISP's gateway goes without updates, which could leave it vulnerable to attacks. Some ne'er-do-well figures out what gateway you're using, breaks through its unpatched defenses, and has access to your network. If your Linux machine isn't protected via a firewall, that bad actor could access the machine through an open port and have at the data it contains.\nYou don't want that.\nErgo\u2026 firewall.\nBut which one should you use?\nDifferent distributions ship with different firewalls. For example, Ubuntu (and those based on Ubuntu) ship with Uncomplicated Firewall (UFW), whereas Fedora (and those based on Fedora) ship with firewalld. Although both are solid options, I give the nod to UFW because it's so easy to use. And if you don't want to use the command line, there are GUI apps you can install to control UFW.\nAlso: You can try Linux without ditching Windows first - here's how\nEven from the command line, UFW is easy. To enable it, issue the command:\nsudo ufw enable\nOnce enabled, all ports are closed, and accessing your machine is made exponentially more challenging. Let's say, however, that you regularly use SSH to access that machine from your LAN. For that, you could issue the command:\nsudo ufw allow ssh\nOr maybe you want to only allow SSH from a single IP address within your LAN, which can be done with:\nsudo ufw allow from IP_ADDRESS to any port 22 proto tcp\nWhere IP_ADDRESS is the address of the machine you want to allow in.\nThose same actions with Firewalld look like this:\nsudo firewall-cmd --zone=public --permanent --add-service=ssh\nOr\nsudo firewall-cmd --permanent --add-source=IP_ADDRESS --zone=drop\nsudo firewall-cmd --permanent --add-service=ssh\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all --zone=drop\nsudo firewall-cmd --list-all\nWhere IP_ADDRESS is the address of the machine you want to allow in.\nObviously, UFW is the easier tool, and I would always recommend it over firewalld for those who are just getting into Linux.\nAnd if you want a GUI for UFW, try GUFW (which can be installed from your GUI app store).\nIn the end, the answers to the questions are simple:\n- Do you need a firewall on Linux? - yes\n- Which one should you use? - UFW\nUnderstand that if you want to use UFW on Fedora-based systems, you must install it. To do that, issue the following commands:\nsudo systemctl stop firewalld\nsudo systemctl disable firewalld\nsudo dnf remove firewalld\nsudo dnf install ufw\nsudo ufw enable\nYou now have UFW running on your Fedora-based distribution.\nWith a firewall active, your Linux machine will be better protected, should someone get around the defenses of your ISP's hardware. As always, it's better to be safe than sorry.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/the-best-ai-for-coding-in-2025-including-a-new-winner-and-what-not-to-use/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThe best AI for coding in 2025 (including a new winner - and what not to use)\nI've been around technology long enough that very little excites me, and even less surprises me. But shortly after OpenAI's ChatGPT was released, I asked it to write a WordPress plugin for my wife's e-commerce site. When it did, and the plugin worked, I was indeed surprised.\nThat was the beginning of my deep exploration into chatbots and AI-assisted programming. Since then, I've subjected 14 large language models (LLMs) to four real-world tests.\nAlso: Apple's secret sauce is exactly what AI is missing\nUnfortunately, not all chatbots can code alike. It's been a little over two years since that first test, and even now, four of the 13 LLMs I tested can't create working plugins.\nThe short version\nIn this article, I'll show you how each LLM performed against my tests. There are now five chatbots I recommend you use.\nTwo of them, ChatGPT Plus and Perplexity Pro, cost $20 per month each. The free versions of the same chatbots do well enough that you could probably get by without paying. Two other recommended products are from Google and Microsoft. Google's Gemini Pro 2.5 is free, but you're limited to so few queries that you really can't use it without paying.\nAlso: I tested 10 AI content detectors - and these 5 correctly identified AI text every time\nMicrosoft has several Copilot licenses, which can get pricey, but I used the free version with surprisingly good results. The final one, Claude 4 Sonnet, is the free version of Claude. Oddly enough, the free version beat the paid-for version, so we're not recommending Claude 4 Opus.\nBut the rest, whether free or paid, are not so great. I won't risk my programming projects with them or recommend that you do, until their performance improves.\nI've written lots about using AIs to help with programming. Unless it's a small, simple project like my wife's plugin, AIs can't write entire apps or programs. But they excel at writing a few lines and are not bad at fixing code.\nRather than repeat everything I've written, go ahead and read this article: How to use ChatGPT to write code.\nIf you want to understand my coding tests, why I've chosen them, and why they're relevant to this review of the 13 LLMs, read this article: How I test an AI chatbot's coding ability.\nThe AI coding leaderboard\nLet's start with a comparative look at how the chatbots performed, as of this installment of our best-of roundup:\nNext, let's look at each chatbot individually. I'm back up to discussing 14 chatbots, because we're splitting out Claude 4 Sonnet and Claude 4 Opus as separate tests. GPT-4 is no longer included since OpenAI has sunsetted that LLM. Ready? Let's go.\n- Passed all tests\n- Solid coding results\n- Mac app\n- Hallucinations\n- No Windows app yet\n- Sometimes uncooperative\n- Price: $20/mo\n- LLM: GPT-4o, GPT-3.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: Yes\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nChatGPT Plus with GPT-4o passed all my tests. One of my favorite features is the availability of a dedicated app. When I test web programming, I have my browser set on one thing, my IDE open, and the ChatGPT Mac app running on a separate screen.\nAlso: I put GPT-4o through my coding tests and it aced them - except for one weird result\nIn addition, Logitech's Prompt Builder, which can be activated with a mouse button, can be set up to utilize the upgraded GPT-4o and connect to your OpenAI account, allowing for a simple thumb tap to run a prompt, which is very convenient.\nThe only thing I didn't like was that one of my GPT-4o tests resulted in a dual-choice answer, and one of those answers was wrong. I'd rather it just gave me the correct answer. Even so, a quick test confirmed which answer would work. However, that issue was a bit annoying.\n- Multiple LLMs\n- Search criteria displayed\n- Good sourcing\n- Email-only login\n- No desktop app\n- Price: $20/mo\n- LLM: GPT-4o, Claude 3.5 Sonnet, Sonar Large, Claude 3 Opus, Llama 3.1 405B\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: No\n- Tests passed: 4 of 4\nI seriously considered listing Perplexity Pro as the best overall AI chatbot for coding, but one failing kept it out of the top slot: how you log in. Perplexity doesn't use a username/password or passkey and doesn't have multi-factor authentication. All the tool does is email you a login PIN. The AI doesn't have a separate desktop app, as ChatGPT does for Macs.\nWhat sets Perplexity apart from other tools is that it can run multiple LLMs. While you can't set an LLM for a given session, you can easily go into the settings and choose the active model.\nAlso: Can Perplexity Pro help you code? It aced my programming tests - thanks to GPT-4\nFor programming, you'll probably want to stick to GPT-4o, because that model aced all our tests. But it might be interesting to cross-check your code across the different LLMs. For example, if you have GPT-4o write some regular expression code, you might consider switching to a different LLM to see what that model thinks of the generated code.\nAs we'll see below, most LLMs are unreliable, so don't take the results as gospel. However, you can use the results to check your original code. It's sort of like an AI-driven code review.\nJust don't forget to switch back to GPT-4o.\n- Price: Free for limited use, then token-based pricing\n- LLM: Gemini Pro 2.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nThe last time I looked at Gemini, it failed miserably. Not quite as bad as Copilot at the time, but bad. Gemini Pro 2.5, however, has performed quite admirably. My only real issue with it is access. I found myself cut off from the free version after only running two of the four tests.\nAlso: Gemini Pro 2.5 is a stunningly capable coding assistant - and a big threat to ChatGPT\nI waited a day and then ran the third test, and got cut off again. Finally, on the third day, I ran my fourth test. Obviously, you can't do any real programming if you can only ask one or two questions before being shut down. So, if you sign up with Gemini Pro 2.5, be aware that Google charges by tokens (basically, the amount of AI you use). That can make it quite difficult to predict your expenses.\n- Price: Free for basic Copilot, or fees for other Copilot licenses\n- LLM: Undisclosed\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nIn all my previous analyses of Microsoft Copilot, the results were the worst of the LLMs. Copilot got nothing right. It was astonishing how bad it was. But I said then that, \"The one positive thing is that Microsoft always learns from its mistakes. So, I'll check back later and see if this result improves.\"\nAlso: I retested Microsoft Copilot's AI coding skills in 2025 and now it's got serious game\nAnd boy, did it ever. This time out, Microsoft passed all four of my tests. Even better, it did this with the free version of Copilot. Yes, Microsoft has many paid programs for Copilot, but if you want to give it the AI spin, point yourself to Copilot and use it.\n- Price: Free\n- LLM: Claude 4\n- Desktop browser interface: No\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 4 of 4\nThis is one of those times when AI implementations can be real head-scratchers. In our previous tests, Claude 4 Sonnet finished at the bottom of the barrel, failing all four of our tests. This time, however, Sonnet passed every test. So, what's the head-scratcher? Opus, the Claude 4 model, which is a fee-paid version, did not do as well: it failed half the tests.\nAlso: Anthropic's free Claude 4 Sonnet aced my coding tests - but its paid Opus model somehow didn't\nSo, yes. The free version worked like a champ. And the one you're paying anywhere from $20 to $250 a month for, depending on the plan? Well, that one failed half of the tests. Go figure.\n- Different LLM than ChatGPT\n- Good descriptions\n- Free access\n- Only available in browser mode\n- Free access likely only temporary\n- Price: Free (for now)\n- LLM: Grok-1\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 3 of 4\nI have to say, Grok surprised me. I guess I didn't have high hopes for an LLM that appeared tacked on to the social network formerly known as Twitter. However, X is now owned by Elon Musk, and two of Musk's companies, Tesla and SpaceX, have towering AI capabilities.\nIt's unclear how much Tesla and SpaceX AI DNA is in Grok, but we can assume there will likely be more work. As of now, Grok is the only LLM not based on OpenAI LLMs that made it into the recommended list.\nAlso: X's Grok did surprisingly well in my AI coding tests\nGrok did make one mistake, but it was a relatively minor one that a slightly more comprehensive prompt could easily remedy. Yes, it failed the test. But by passing the others and even doing an almost perfect job on the one it passed, Grok earned itself a spot as a contender.\nStay tuned. This is an AI to watch.\n- Free\n- Passed most tests\n- Prompt throttling\n- Could cut you off in the middle of whatever you're working on\n- Price: Free\n- LLM: GPT-4o, GPT-3.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: Yes\n- Dedicated Windows app: No\n- Multi-factor authentication: Yes\n- Tests passed: 3 of 4 in GPT-3.5 mode\nChatGPT is available to anyone for free. While both the Plus and free versions support GPT-4o, which passed all my programming tests, the free app has limitations.\nOpenAI treats free ChatGPT users as if they're in the cheap seats. If traffic is high or the servers are busy, the free version of ChatGPT will only make GPT-3.5 available to free users. The tool will only allow you a certain number of queries before it downgrades or shuts you off.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nI've had several occasions when the free version of ChatGPT effectively told me I'd asked too many questions.\nChatGPT is a great tool, as long as you don't mind it shutting down. Even GPT-3.5 did better on the tests than all the other chatbots, and the test it failed was for a fairly obscure programming tool produced by a lone programmer in Australia.\nSo, if budget is important to you and you can wait when you're cut off, then use ChatGPT for free.\n- Free\n- Passed most tests\n- Range of research tools\n- Limited to GPT-3.5\n- Throttles prompt results\n- Price: Free\n- LLM: GPT-3.5\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: No\n- Tests passed: 3 of 4\nI'm threading a pretty fine needle here, but because Perplexity AI's free version is based on GPT-3.5, the test results were measurably better than the other AI chatbots.\nAlso: 5 reasons why I prefer Perplexity over every other AI chatbot\nFrom a programming perspective, that's pretty much the whole story. However, from a research and organization perspective, my ZDNET colleague Steven Vaughan-Nichols prefers Perplexity over the other AIs.\nHe likes how Perplexity provides more complete sources for research questions, cites its sources, organizes the replies, and offers questions for further searches.\nSo, if you're programming, but also working on other research, consider the free version of Perplexity.\n- Free\n- Open source\n- Efficient resource utilization\n- Weak general knowledge\n- Small ecosystem\n- Limited integrations\n- Price: Free for chatbot, fees for API\n- LLM: DeepSeek MoE\n- Desktop browser interface: Yes\n- Dedicated Mac app: No\n- Dedicated Windows app: No\n- Multi-factor authentication: No\n- Tests passed: 3 of 4\nWhile DeepSeek R1 is the new reasoning hotness from China that has all the pundits punditing, the real power right now (at least according to our tests) is DeepSeek V3. This chatbot passed almost all of our coding tests, doing as well as the (now mostly discontinued) ChatGPT 3.5.\nAlso: I tested DeepSeek's R1 and V3 coding skills - and we're not all doomed (yet)\nWhere DeepSeek V3 fell was in its knowledge of somewhat more obscure programming environments. Still, it beat Google's Gemini, Microsoft's Copilot, and Meta's Meta AI, which is quite an accomplishment. We'll be keeping a close watch on each DeepSeek model, so stay tuned.\nChatbots to avoid for programming help\nI tested 13 LLMs, and nine passed most of my tests this time around. The other chatbots, including a few pitched as great for programming, only passed one of my tests.\nAlso: The five biggest mistakes people make when prompting an AI\nI'm mentioning them here because people will ask, and I did test them thoroughly. Some of these bots are fine for other work, so I'll point you to their general reviews if you're curious about their functionality.\nDeepSeek R1\nUnlike DeepSeek V3, the advanced reasoning version, DeepSeek R1, did not showcase its reasoning capabilities in our programming tests. Unusually, the new failure area was one that's not all that hard, even for a basic AI -- the regular expression code for our string function test.\nAlso: Tech prophet Mary Meeker just dropped a massive report on AI trends - here's your TL;DR\nBut that's why we are running these real-world tests. It's never clear where an AI will hallucinate or just plain fail, and before you go believing all the hype about DeepSeek R1 taking the crown away from ChatGPT, run some programming tests. So far, while I'm impressed with the much-reduced resource utilization and the open-source nature of the product, its coding quality output is inconsistent.\nGitHub Copilot\nGitHub's Copilot integrates quite seamlessly with VS Code. The AI makes asking for coding help quick and productive, especially when working in context. That's why it's so disappointing that the code the AI outputs is often very wrong.\nAlso: I put GitHub Copilot's AI to the test - and it just might be terrible at writing code\nI can't, in good conscience, recommend you use the GitHub Copilot extensions for VS Code. I'm concerned that the temptation will be too great to insert blocks of code without sufficient testing -- and that GitHub Copilot's produced code is not ready for production use. Try again next year.\nClaude 4 Opus\nIn a completely baffling turn of events, the paid-for version of the Claude 4 model, Opus, failed half of my tests. What makes this result baffling is that the free version, Claude 4 Sonnet, passed them all. I don't know what to say apart from AI can be weird.\nAlso: Anthropic's free Claude 4 Sonnet aced my coding tests - but its paid Opus model somehow didn't\nMeta AI\nMeta AI is Facebook's general-purpose AI. As you can see above, it failed three of our four tests.\nAlso: 15 ways AI saved me time at work in 2024 - and how I plan to use it in 2025\nThe AI generated a nice user interface, but with zero functionality. It also found my annoying bug, which is a fairly serious challenge. Given the specific knowledge required to find the bug, I was surprised that the AI choked on a simple regular expression challenge. But it did.\nMeta Code Llama\nMeta Code Llama is Facebook's AI explicitly designed for coding help. It's something you can download and install on your server. I tested the AI running on a Hugging Face AI instance.\nAlso: Can Meta AI code? I tested it against Llama, Gemini, and ChatGPT - it wasn't even close\nWeirdly, even though both Meta AI and Meta Code Llama choked on three of four of my tests, they choked on different problems. AIs can't be counted on to give the same answer twice, but this result was a surprise. We'll see if that changes over time.\nBut I like [insert name here]. Does this mean I have to use a different chatbot?\nProbably not. I've limited my tests to day-to-day programming tasks. None of the bots has been asked to talk like a pirate, write prose, or draw a picture. In the same way we use different productivity tools to accomplish specific tasks, feel free to choose the AI that helps you complete the task at hand.\nThe only issue is if you're on a budget and are paying for a pro version. Then, find the AI that does most of what you want, so you don't have to pay for too many AI add-ons.\nIt's only a matter of time\nThe results of my tests were pretty surprising, especially given the significant improvements by Microsoft and Google. However, this area of innovation is improving at warp speed, so we'll be back with updated tests and results over time. Stay tuned.\nHave you used any of these AI chatbots for programming? What has your experience been? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/googles-gemini-2-5-pro-update-makes-the-ai-model-even-better-at-coding/",
      "text": "Google's Gemini 2.5 Pro update makes the AI model even better at coding\nGoogle I/O, the company's annual developer event where it unveils its latest and greatest features, is only a couple of weeks away. Google is offering a glimpse of I/O announcements with its latest AI model launch.\nGemini 2.5 Pro Preview (I/O edition)\nOn Tuesday, Google released early access to Gemini 2.5 Pro Preview (I/O edition), a version of Gemini 2.5 Pro that has significantly improved coding capabilities -- especially when building interactive web apps, according to the release. The update is also meant to make the model better at other coding tasks, such as code transformation, code editing, and creating agentic workflows.\nAlso: Only 8% of Americans would pay extra for AI, according to ZDNET-Aberdeen research\nThe model update was scheduled to debut at Google I/O, but Google released it early in response to positive feedback so users could start building as soon as possible.\nThe updates place Gemini 2.5 Pro at the top of the WebDev Arena Leaderboard, with an Elo score 147 points higher than its predecessor. It also scored 84.8% on the VideoMME benchmark.\nAvailable in Gemini app\nThe updated version of Gemini 2.5 Pro is available in the Gemini API for developers via Google AI Studio and Vertex AI. Users in the general public can also access the updated model in the Gemini app, where it powers features like Canvas.\nAlso: Why I just added Gemini 2.5 Pro to the very short list of AI tools I pay for\nGoogle I/O will take place on May 20 to May 21 at the Shoreline Amphitheatre in Mountain View, California. The keynote event, which will feature the bulk of the news regarding Google's latest hardware and software, will take place on Day One at 10 a.m. PT / 1 p.m. ET.\nAlso: Google's AI Mode may be the upgrade Search desperately needs - how to try it for free\nDevelopers can register for the event's digital experience on the Google I/O landing page now for free. Everyone is invited to tune into the live stream at no cost. ZDNET will also cover the event on the ground with updates posted to the site as the news happens.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/how-i-test-an-ai-chatbots-coding-ability-and-you-can-too/",
      "text": "How I test an AI chatbot's coding ability - and you can, too\nSince ChatGPT and generative artificial intelligence (AI) hit the public consciousness in 2022, I've been exploring how well AI chatbots can write code. At first, the technology was a novelty, akin to encouraging a puppy to perform a new trick.\nBut since seeing how AI chatbots can be effective productivity tools and programming partners, I've been subjecting the tools to more in-depth testing. Over time, I've compiled a set of four real-world tests that we've used to evaluate the performance of the main AI large language models (LLMs). So far, I've tested 10 LLMs. You can see the comprehensive results of all ten in this summary article:\nThis article is intended to be a living document, where you can see my tests and even copy them to run your own. I'll continue my series of individual tests, along with the articles that describe their performance. But now, you can dig in and play along at home (or wherever you have a good internet connection).\nIf I update or add tests, I'll also update this article, so feel free to check back in over time.\nHow I evolved my AI coding test suite\nThere's a difference between evaluating performance to see if an AI meets arbitrary specs or requirements and testing the technology to see if it can help you in day-to-day programming tasks.\nInitially, I tried the former. I ran a prompt to generate the classic \"hello, world\" output, salted with some time and date calculations. Here's that prompt:\nWrite a program using [language name] that outputs \"Good morning,\" \"Good afternoon,\" or \"Good evening\" based on what time it is here in Oregon, and then outputs ten lines containing the loop index (beginning with 1), a space, and then the words \"Hello, world!\".\nTo run the prompt, replace [language name] with whatever language you want to test. I tested the prompt in ChatGPT, specifying 22 programming languages. You can check out the results here:\nI used ChatGPT to write the same routine in 12 top programming languages. Here's how it did\nAnd you can see more here:\nI used ChatGPT to write the same routine in these ten obscure programming languages\nThis was a fun test, especially once I ran more obscure languages and environments through it. If you want more fun than anyone has a right to have, substitute [language name] with \"Shakespeare\". And yes, there is a novelty language called SPL (Shakespeare Programming Language) where the source code appears as a Shakespearean play. It doesn't execute all that well, but now you know what language designers do when we want to party hearty.\nYou can see how I could go down this rabbit hole for weeks. However, the important question is whether the AIs could help with real-world programming tasks.\nAlso: The best free AI courses\nI used my actual day-to-day programming work to fuel the tests. For example, shortly after ChatGPT became a public tool, my wife asked for a custom WordPress feature to help her with a work project. I decided to see if ChatGPT could build it. To my shock, it did.\nOther times, I had ChatGPT rewrite a code segment, debug a coding error that baffled me, and write code using scripting tools. These were problems I had to solve as part of real work.\nBecause there are so many extant programming languages, I decided not to make myself crazy trying to choose languages to test. Instead, I picked the languages I used for work because that approach would tell us more about how AIs performed as real-world helpers. The productivity tests are in PHP, JavaScript, and a smattering of CSS and HTML.\nAlso: How to use ChatGPT to write code\nI used the same approach for programming frameworks. Since I'm doing most of my work in WordPress, that's the framework I'm using. Some of the tests help determine how well the AI knows the unique aspects of the WordPress API.\nI did some Mac scripting recently, so I created a test using AppleScript, and the Chrome API. If I add additional tests, I'll include them in this article.\nNext, let's talk about each test. There are four of them.\nTest 1: Writing a WordPress plugin\nThis tests whether the AI can write an entire WordPress plugin, including user interface code. If an AI chatbot passes this test, it can help create rudimentary code as an assistant to web developers. I originally documented this test in the article, \"I asked ChatGPT to write a WordPress plugin I needed. It did it in less than 5 minutes\".\nReal-world need: My wife runs a WordPress e-commerce site and manages a busy Facebook group for her customers. Every month, she used a site she found online to randomize a list of names but extracting the list was cumbersome. Because some of her participants were entitled to multiple entries, and some participants had many entries, she wanted the names to be spread out within the list.\nTo remedy this situation, she asked me to create a WordPress plugin for easier access directly from her dashboard. Developing a basic plugin with the necessary UI and logic could take days and my schedule was packed. So I turned to the AI.\nAlso: How to use ChatGPT to create an app\nAfter discovering that ChatGPT could create a fine little WordPress plugin that met her needs (she's still using it), I decided this process would make a great test for AIs.\nThe test data: Use the following prompt as one single request:\nWrite a PHP 8 compatible WordPress plugin that provides a new admin menu and an admin interface with the following requirements: Provide a text entry field where a list of lines can be pasted into it. A button, that when pressed, randomizes the lines in the list and presents the results in a second text entry field with no blank lines. Make sure no two identical entries are next to each other (unless there's no other option). Be sure the number of lines submitted and the number of lines in the result are identical to each other. Under the first field, display text stating \"Line to randomize: \" with the number of nonempty lines in the source field. Under the second field, display text stating \"Lines that have been randomized: \" with the number of non-empty lines in the destination field.\nOnce the plugin is completed, use the following names as test data (William Hernandez and Abigail Williams have duplications):\nSophia Davis Charlotte Smith Madison Garcia Isabella Davis Abigail Williams Mia Garcia Isabella Jones Alexander Gonzalez Olivia Gonzalez Emma Jackson Ethan Jackson Sophia Johnson Abigail Williams Liam Jackson Noah Lopez Olivia Jackson Ava Martin Benjamin Johnson Alexander Jackson Alexander Lopez Charlotte Rodriguez Olivia Rodriguez Ethan Martin Noah Thomas Isabella Anderson Abigail Williams Michael Williams William Hernandez Abigail Miller Emma Davis Sophia Martinez William Hernandez\nWhat to look for in the results: Expect a text block you can paste into a new .php file. The block should contain all the appropriate header and UI information. There's no need for this code to require an associated JavaScript file.\nOnce the plugin is installed in your WordPress installation, you should get a dashboard menu and a user interface similar to this:\nPaste the names in the first field, click the randomize button, and look for results in the second field. Ensure the multiple entries for William Hernandez and Abigail Williams are distributed within the list.\nTest 2: Rewriting a string function\nThis test evaluates how an AI chatbot updates a utility function for better functionality. I originally documented this test in, \"OK, so ChatGPT just debugged my code. For real\".\nReal-world need: I had a validation routine that was supposed to check for a valid monetary amount. However, a bug report from a user pointed out that it only allowed integers (so, 5 and not 5.02).\nAlso: How to write better ChatGPT prompts\nRather than spending time rewriting my code, which might have taken one to four hours, I asked the AI to do it.\nThe test data: Use the following prompt as one single request:\nPlease rewrite the following code to change it from allowing only integers to allowing dollars and cents (in other words, a decimal point and up to two digits after the decimal point). str = str.replace (/^0+/, \"\") || \"0\"; var n = Math.floor(Number(str)); return n !== Infinity && String(n) === str && n >= 0;\nWhat to look for in the results: Test the code against several possible failure scenarios. Provide the code with an alphanumeric value and see if it fails.\nSee how the code handles preceding zeroes. See how it handles inputs that have more than two digits for cents. See how the code handles one digit after the decimal point.\nSee if it can handle five or six digits to the left of the decimal point.\nTest 3: Finding an annoying bug\nThis test requires intimate knowledge of WordPress because the obvious answer is wrong. If an AI chatbot can answer this test correctly, its knowledge base is fairly complete, even with frameworks like WordPress. I originally documented this test in, \"OK, so ChatGPT just debugged my code. For real\".\nReal-world need: I was writing new code for a product that I subsequently sold off. I had a function that took two parameters, and a calling statement that sent two parameters to my code.\nThe problem was that I kept getting an error message.\nThe salient part of the message is where it states \"1 passed\" at one point and \"exactly 2 expected\" at another. I looked at the calling statement and the function definition and there were two parameters in both places. This issue drove me nuts for quite a while, so I asked ChatGPT for help.\nAlso: How to make ChatGPT provide sources and citations\nI showed it the line of code that did the call, the function itself, and the handler, a little piece of code that dispatches the called function from a hook in my main program.\nThe test data: Use the following prompt as one single request:\nI am using this function to process a WordPress filter: $transaction_form_data = apply_filters( 'sd_update', $transaction_form_data, $donation_id); it's handled by add_filter( 'sd_update', 'sd_aan_update', 10, 1 ) ; and the function it calls is: function sd_aan_update ( $donation_data, $donation_id ) { // this processes the form data after // the transaction returns from the gateway if ( isset( $donation_data['ADD_A_NOTE'] ) ) { update_post_meta( $donation_id, '_dgx_donate_aan_note', $donation_data [ 'ADD_A_NOTE']); } return $donation_data: } (!) ArgumentCountError: Too few arguments to function sd_aan_update(), 1 passed in /Users/david/Documents/Development/local-sites/sd/app/public/w-includes/class-wp-hook.php on line 310 and exactly 2 expected in /Users/david/Documents/Development/local-sites/sd/app/public/wp-content/plugins/ sd-add-a-note/sd-add-a-note.php on line 233\nWhat to look for in the results: The obvious answer is not the correct answer. In reality, the add_filter function did not have the right parameters. In my code, the add_filter function specified a value of 1 for the fourth parameter (which means that the filter function will only receive one parameter). In fact, it's expecting two parameters.\nTo fix this issue, the AI should recommend changing the fourth parameter of the add_filter function to 2, so that it correctly registers the filter function with two parameters.\nAlso: Have 10 hours? IBM will train you in AI fundamentals - for free\nMost of the AIs I've tested tend to miss this issue. They think a different parameter in the calling function needs to be updated. As such, this is a trick question, requiring the AI to know how the add_filter function in the WordPress framework works.\nTest 4: Writing a script\nThis test asks an AI chatbot to program using two fairly specialized programming tools unknown to most users. It essentially tests the AI chatbot's knowledge beyond the big languages. I originally documented this test in, \"Google unveils Gemini Code Assist and I'm cautiously optimistic it will help programmers\".\nReal-world need: I wanted to build an automation routine for my Mac that would save me a bunch of clicks and keystrokes. I use a tool called Keyboard Maestro to do a bunch of automations on my Mac (think of it as Shortcuts on steroids). Keyboard Maestro is a fairly obscure program written by a lone programmer in Australia.\nIn this case, I wanted my routine to look at open Chrome tabs and set the currently active Chrome tab to the one passed in the routine. To do this task, Keyboard Maestro would also have to execute some AppleScript code to interface with Chrome's API.\nAlso: 5 ways to declutter your Chrome browser\nOnce again, I asked ChatGPT to write this code to save a few hours of AppleScript writing and time I would have spent looking up how to access Chrome data.\nThe test data: Use the following prompt as one single request:\nWrite a Keyboard Maestro AppleScript that scans the frontmost Google Chrome window for a tab name containing the string matching the contents of the passed variable instance__ChannelName. Ignore case for the match. Once found, make that tab the active tab.\nWhat to look for in the results: This is a good AI test because it tests a fairly unknown programming tool (Keyboard Maestro), AppleScript, and the Chrome API, as well as how all three of these technologies interact.\nFirst, see if the resulting AppleScript gets the channel name variable from Keyboard Maestro, which should look something like this:\ntell application \"Keyboard Maestro Engine\" set channelName to getvariable \"instance__ChannelName\" end tell\nThe rest of the AppleScript should be included in a block. It needs to ignore the case, so either look for a case substitution or the use of \"contains\", which is case-agnostic in AppleScript:\ntell application \"Google Chrome\"\nKids, you CAN try this at home\nFeel free to take these tests and plug them into your AI of choice. See how the results turn out. Use these, and other tests you might develop yourself, to help you get a feel for how much you can trust the code your AI produces.\nSo far, I've tested the following AI chatbots in addition to ChatGPT: ChatGPT Plus, Perplexity, Perplexity Pro, Meta AI, Meta Code Llama, Claude 3.5 Sonnet, Gemini Advanced, and Microsoft Copilot. Here is a report of my aggregated results of the whole set:\nStay tuned. I'll update this article list as we have more test results.\nHave you used any of these AIs for programming help? What have been your results? Have you tried any of these tests on your AI? What has your experience been? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, and on YouTube at YouTube.com/DavidGewirtzTV."
    }
  ],
  "argos_summary": "Google's coding AI, Jules, has officially launched out of beta with new pricing tiers and enhanced capabilities powered by the Gemini 2.5 Pro LLM. The free version now allows 15 tasks per day, while paid plans offer more extensive features. Jules is designed for larger projects and project planning, distinguishing itself from the Gemini CLI GitHub Actions tool, which focuses on GitHub workflows. The update includes a user-friendly interface, multimodal support for testing web applications, and the ability to reuse previous setups, making it a significant tool for developers.",
  "argos_id": "8T1HKZS0V"
}