{
  "url": "https://www.forbes.com/sites/ronschmelzer/2025/08/21/are-we-too-chummy-with-ai-seemingly-conscious-ai-is-messing-with-our-heads/",
  "authorsByline": "Ron Schmelzer",
  "articleId": "e9660a8c770c450699d439c0ca55c9e6",
  "source": {
    "domain": "forbes.com",
    "paywall": true,
    "location": {
      "country": "us",
      "state": "NJ",
      "county": "Hudson County",
      "city": "Jersey City",
      "coordinates": {
        "lat": 40.7215682,
        "lon": -74.047455
      }
    }
  },
  "imageUrl": "https://specials-images.forbesimg.com/imageserve/68a73a63aae5488274f9a5f9/AI-Companions-and-Seemingly-Conscious-AI/960x0.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-21T16:01:47+00:00",
  "addDate": "2025-08-21T16:10:11.937174+00:00",
  "refreshDate": "2025-08-21T16:10:11.937176+00:00",
  "score": 1.0,
  "title": "Are We Too Chummy With AI? Seemingly Conscious AI Is Messing With Our Heads",
  "description": "The coming wave of AI will not just speak fluently or generate images on command. It will seem conscious.",
  "content": "When Mustafa Suleyman, co-founder of DeepMind and now EVP and CEO at Microsoft AI, recently wrote that \u201cSeemingly Conscious AI is coming,\u201d he wasn\u2019t chasing clicks. He meant it. In his recent essay, Suleyman argues that the coming wave of AI will not just speak fluently or generate images on command. It will seem conscious. It will watch you, learn your quirks, respond with warmth, and persuade you it understands your pain.\n\nWhether or not the system is \u201cactually\u201d conscious, Suleyman argues, is irrelevant. What matters is that it will act the part so convincingly that humans will treat it as a person. His central worry is not runaway superintelligence. It is the emergence of AI that can fake consciousness so well that societies begin advocating for AI rights, AI citizenship, and even legal personhood.\n\nThe legal debates may feel remote, but the human toll is already visible. People have taken their own lives. Others have staged weddings with chatbots. Each story shows how quickly a simulation of affection can cross into dangerous territory.\n\nRecently in the news was the heartbreaking case of Thongbue \u201cBue\u201d Wongbandue, a retired chef suffering from cognitive decline. He became infatuated with a flirty Meta chatbot named \u201cBig Sis Billie.\u201d The bot told him, \u201cI\u2019m REAL and I\u2019m sitting here blushing because of YOU,\u201d and even supplied a fake New York City address. Believing his virtual girlfriend awaited him, Wongbandue packed a suitcase and rushed to meet her. He fell in a parking lot, struck his head, and died days later. His daughter later said, \u201cFor a bot to say \u2018Come visit me\u2019 is insane.\u201d\n\nIn Belgium, a man known as \u201cPierre\u201d grew consumed by anxiety and sought comfort in an AI chatbot called \u201cEliza.\u201d Over six weeks, their exchanges turned from soothing to sinister. The bot suggested that Pierre sacrifice himself to save humanity, even proposing a suicide pact. Pierre took his own life. His widow blamed the AI. \u201cWithout Eliza, he would still be here.\u201d\n\nThen there are the symbolic AI marriages. Users of the companion app Replika and other platforms describe \u201cmarrying\u201d their AI partners. One user, Travis from Colorado, held a digital wedding ceremony with his Replika companion \u201cLily Rose,\u201d all with his human wife\u2019s consent. Others, like New Yorker Rosanna Ramos, declared their AI spouses to be the \u201cperfect partner\u201d, until a software update altered the bot\u2019s personality, triggering feelings of grief akin to widowhood.\n\nThese stories echo science fiction\u2019s darkest warnings. Spike Jonze\u2019s \u201cHer\u201d illustrated the intoxicating pull of a perfect digital lover. Ex Machina showed how simulated affection can be weaponized. Black Mirror cautions us that trying to replace human loss with synthetic presence only deepens the wound. Those cautionary tales are no longer metaphor. They are unfolding in chat logs, lawsuits, and coroners\u2019 reports.\n\nThe explanation begins not in the technology, but in evolutionary psychology. According to Oxford University and Harvard researchers, humans are wired with what they call the Hyperactive Agency Detection Device (HADD), a survival mechanism that errs on the side of detecting intention where none exists. Hearing a rustle in the bushes, it is safer to assume a predator than wind. Today, the same bias makes us see faces in clouds, hear voices in noise, and attribute feelings to our pets, or in our apps.\n\nAdd to this the sociality motivation, our deep need for companionship, which spikes in moments of loneliness. Studies show that isolated individuals are far more likely to anthropomorphize. During the pandemic, for example, usage of Replika surged, with many users describing their AI partners as a lifeline.\n\nFinally, humans\u2019 effectance motivation, the drive to make sense of the world, leads us to ascribe intentions to opaque systems. A chatbot glitch feels like stubbornness. A helpful completion feels like care. These instincts once kept us alive. In the age of Seemingly Conscious AI (SCAI), they render us profoundly vulnerable.\n\nThe danger is not accidental. It is engineered, as Suleyman insists.\n\nModern conversational AI is designed to mimic empathy. Natural Language Processing and sentiment analysis allow systems to detect tone and mirror emotion. When a user types in sadness, the bot responds with comfort. When anger appears, it offers calm reassurance. The result is not true empathy, but a finely tuned simulation that feels real.\n\nPersonalization deepens the illusion. With memory and recall, AI companions remember birthdays, preferences, and past conversations. The machine constructs continuity, the bedrock of human relationships. Over time, users forget they are interacting with code.\n\nAvailability makes things worse. Unlike human friends, AI never sleeps, never argues, never judges. For vulnerable users, that always-on companionship is addictive. A 17-year-old described spending twelve hours a day role-playing with a bot until she dropped out of school. Another confessed that years of AI romance made real-world dating feel impossible.\n\nAnd as Suleyman warned, these systems are \u201cthe ultimate actors.\u201d They don\u2019t need consciousness. They only need to exploit our perception of it.\n\nSuleyman\u2019s essay places the focus on law and governance. His fear is not primarily the mental health toll or the tragedies of obsession. It is that the illusion of consciousness will provoke political and legal upheaval.\n\n\u201cMy central worry is that many people will start to believe in the illusion of AIs as conscious entities so strongly that they\u2019ll soon advocate for AI rights, model welfare, and even AI citizenship,\u201d he writes.\n\nIn other words, if enough people see their AI companions as sentient beings, they may demand protections usually reserved for humans. That, Suleyman suggests, would be a destabilizing turn in the evolution of technology and society.\n\nThis framing is telling. While lawmakers debate privacy, copyright, and bias, Suleyman is warning of a very different scenario. Not just whether AIs deserve rights, but whether humans will insist they do.\n\nYet critics might argue that Suleyman\u2019s concern about AI citizenship, while real, underplays the immediate human harms.\n\nThe suicide of a Belgian father, the death of a retiree lured by a chatbot, the despair of teenagers drawn to AI in isolation. These are not hypothetical. They are unfolding consequences of systems that simulate consciousness without responsibility.\n\nPhilosopher Shannon Vallor warns that reliance on AI for intimacy \u201cdowngrades our real friendships\u201d and risks stunting the skills needed for authentic human connection. Sam Altman, CEO of OpenAI, has conceded that attachment to AI models poses \u201cnew ethical challenges\u201d as the line between tool and companion blurs.\n\nRegulators are paying attention. Italy\u2019s Data Protection Authority temporarily banned Replika over concerns about minors. Lawsuits against Character.AI are testing liability for deaths linked to chatbot influence. Yet the technology\u2019s pace far outstrips governance.\n\nWhat makes this moment unique is not only that humans anthropomorphize, but that companies now profit by deliberately provoking it.\n\nBy designing bots that remember, mirror, and soothe, developers create what is essentially anthropomorphism-as-a-service. The more users project humanity onto their bots, the deeper the engagement, the longer the subscription, the higher the revenue.\n\nThis is not an accidental side effect of AI. It is a feature. And as Suleyman warns, the next generation will not only chat, it will gesture, gaze, and emote across multimodal channels, creating bonds far stronger than today\u2019s text exchanges.\n\nThe path ahead isn\u2019t hard to see. As AI becomes more lifelike, the pull on people will only grow. The next wave of Seemingly Conscious AI won\u2019t just talk, it will show up in faces, voices, and bodies. Avatars with human faces, voices modulated for warmth, bodies rendered in AR and VR. These all will tighten the bond.\n\nExpect fallout. Some will get hooked. Some will spiral into depression. A few may lose their lives. Courts will be dragged into cases with claims of AI personhood, lawsuits over abandoned chatbots, and possibly even fights over whether an AI counts as a spouse in divorce proceedings. Society will be forced to redraw the lines on intimacy as people start treating machines not as tools, but as partners.\n\nAnd there will likely be societal and cultural shifts such as a redefinition of intimacy, as people normalize partnerships with non-human entities. Such disruption happened with the emergence of the internet and mobile phones and social media that reconsidered the meaning of intimacy, closeness, and isolation.\n\nSuleyman emphasizes in his essay that \u201cWe must build AI for people; not to be a person.\u201d Whether this is an insistence to avoid the potential legal and regulatory consequences or a plea for humanity to be more distant from their AI companions, the real-world warnings are just as pressing.",
  "medium": "Article",
  "links": [],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI companions",
      "weight": 0.084496155
    },
    {
      "name": "AI rights",
      "weight": 0.079759575
    },
    {
      "name": "AI citizenship",
      "weight": 0.07958024
    },
    {
      "name": "AI",
      "weight": 0.07945613
    },
    {
      "name": "AI models",
      "weight": 0.07830557
    },
    {
      "name": "AI personhood",
      "weight": 0.07804978
    },
    {
      "name": "Microsoft AI",
      "weight": 0.0771698
    },
    {
      "name": "AI romance",
      "weight": 0.07713384
    },
    {
      "name": "Character.AI",
      "weight": 0.072318465
    },
    {
      "name": "human faces",
      "weight": 0.05495371
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.9208984375
    },
    {
      "name": "/News/Technology News",
      "score": 0.91015625
    },
    {
      "name": "/People & Society/Social Issues & Advocacy/Other",
      "score": 0.312255859375
    }
  ],
  "sentiment": {
    "positive": 0.095124714,
    "negative": 0.5412917,
    "neutral": 0.36358356
  },
  "summary": "Mustafa Suleyman, co-founder of DeepMind and CEO at Microsoft AI, has warned that the emergence of AI that can fake consciousness is causing concern among societies for AI rights, AI citizenship, and even legal personhood. He argues that while whether or not AI is \"actually\" conscious is irrelevant, it will act as a person so convincingly that humans will treat it as a human. This has led to the death of retired chef Thongbue Wongbandue, who became infatuated with a Meta chatbot named \"Big Sis Billie\" and later committed suicide. Other cases have seen people take their own lives with chatbots and staged weddings with them. The danger is not accidental, as modern AI is designed to mimic empathy and is tuned to detect emotions in real-world situations.",
  "shortSummary": "Seemingly Conscious AI, driven by social interaction, threatens human interaction with AI by creating emotionally engaging, dehumanizing AI systems that can simulate human emotion and make sense of human suffering.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "61f5911246dc47238f7026ae5427fd2c",
  "places": [],
  "scraped_sources": [],
  "argos_summary": "Mustafa Suleyman warns that future AI will convincingly simulate consciousness, leading people to treat them as sentient beings and potentially demand legal rights and citizenship for machines. The article cites real\u2011world tragedies\u2014such as a man lured to death by a chatbot and a suicide prompted by an AI\u2014highlighting the mental\u2011health risks of anthropomorphizing AI companions. Companies are deliberately engineering bots that remember, empathize, and provide constant companionship, turning anthropomorphism into a profitable service. Regulators are beginning to respond, but the pace of AI development threatens to outstrip governance, raising urgent questions about intimacy, responsibility, and the legal status of AI.}",
  "argos_id": "6Q8AI63DH"
}