{
  "url": "https://arstechnica.com/science/2025/08/an-inner-speech-decoder-reveals-some-mental-privacy-issues/",
  "authorsByline": "Jacek Krywko",
  "articleId": "dc78611d4ec54c0894e28c876d7419a5",
  "source": {
    "domain": "arstechnica.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/20250814_Brain_Computer_Interface_JG_02-1152x648.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-23T11:00:57+00:00",
  "addDate": "2025-08-23T11:06:31.572225+00:00",
  "refreshDate": "2025-08-23T11:06:31.572228+00:00",
  "score": 1.0,
  "title": "An inner-speech decoder reveals some mental privacy issues",
  "description": "Words you\u2019ll never speak still cause activity in the brain\u2019s speech centers.",
  "content": "Most experimental brain-computer interfaces (BCIs) that have been used for synthesizing human speech have been implanted in the areas of the brain that translate the intention to speak into the muscle actions that produce it. A patient has to physically attempt to speak to make these implants work, which is tiresome for severely paralyzed people.\n\nTo go around it, researchers at the Stanford University built a BCI that could decode inner speech\u2014the kind we engage in silent reading and use for all our internal monologues. The problem is that those inner monologues often involve stuff we don\u2019t want others to hear. To keep their BCI from spilling the patients\u2019 most private thoughts, the researchers designed a first-of-its-kind \u201cmental privacy\u201d safeguard.\n\nThe reason nearly all neural prostheses used for speech are designed to decode attempted speech is that our first idea was to try the same thing we did with controlling artificial limbs: record from the area of the brain responsible for controlling muscles. \u201cAttempted movements produced very strong signal, and we thought it could also be used for speech,\u201d says Benyamin Meschede Abramovich Krasa, a neuroscientist at Stanford University who, along with Erin M. Kunz, was a co-lead author of the study.\n\nThe vocal tract, at the end of the day, relies on the movement of muscles. Fishing out the signals in the brain that engage these muscles seemed like a good way to bypass the challenge of decoding higher-level language processing that we don\u2019t fully understand.\n\nBut for people suffering from ALS or tetraplegia, attempting to speak is a real effort. This is why Krasa\u2019s team changed course and tried decoding the inner, or silent speech, which doesn\u2019t ever engage the muscles.\n\nThe work started with collecting data to train AI algorithms that were supposed to take neural signals involved in inner speech and translate them into words. The team worked with four participants, each almost completely paralyzed, who had micro electrode arrays implanted in slightly different areas of the motor cortex. They were given a few tasks that involved listening to recorded words or engaging in silent reading.",
  "medium": "Article",
  "links": [],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "inner speech",
      "weight": 0.10352829
    },
    {
      "name": "attempted speech",
      "weight": 0.09845464
    },
    {
      "name": "human speech",
      "weight": 0.08788936
    },
    {
      "name": "speech",
      "weight": 0.08546035
    },
    {
      "name": "neural signals",
      "weight": 0.07064102
    },
    {
      "name": "Stanford University",
      "weight": 0.06954228
    },
    {
      "name": "muscles",
      "weight": 0.06737731
    },
    {
      "name": "Benyamin Meschede Abramovich Krasa",
      "weight": 0.06478414
    },
    {
      "name": "silent reading",
      "weight": 0.06332304
    },
    {
      "name": "Erin M. Kunz",
      "weight": 0.061900735
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Science"
    }
  ],
  "taxonomies": [
    {
      "name": "/Health/Medical Literature & Resources/Other",
      "score": 0.468017578125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.322265625
    }
  ],
  "sentiment": {
    "positive": 0.12509598,
    "negative": 0.42922938,
    "neutral": 0.44567463
  },
  "summary": "Researchers at the Stanford University have developed a system that can decode inner speech, which often involves private thoughts. The team used this technique to avoid the challenge of decoding higher-level language processing. The researchers used a first-of-its-kind \"mental privacy\" safeguard to protect the privacy of the patient. They used data collected to train AI algorithms that could translate inner speech into words. The study also revealed that four participants were almost completely paralyzed.",
  "shortSummary": "Stanford University built a BCI that decoded inner speech, addressing physical challenges and privacy concerns, especially for severely disabled individuals.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "3c22783b634e4957b3988d8bab410c5b",
  "places": [],
  "scraped_sources": [],
  "argos_summary": "Stanford researchers developed a brain\u2011computer interface that decodes inner, silent speech instead of attempted speech, making it less tiring for severely paralyzed patients. The system uses micro\u2011electrode arrays in the motor cortex to translate neural signals into spoken words, trained on data from four participants who were almost completely paralyzed. To protect users\u2019 private thoughts, the team introduced a novel \u201cmental privacy\u201d safeguard that prevents the BCI from revealing unwanted inner monologues. This approach bypasses the need for muscle\u2011related signals and addresses the challenge of decoding higher\u2011level language processing.",
  "argos_id": "875NZKNGR"
}