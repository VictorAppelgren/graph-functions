{
  "url": "https://www.zdnet.com/article/why-shadow-ai-could-be-the-secret-to-fixing-your-companys-failing-ai-projects/",
  "authorsByline": "Omer Minkara",
  "articleId": "299b9bcbec3b43a0be05d21067339606",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/a3f84009080bdd563ccce80182a28558901c7858/2025/09/02/13243414-8a0f-48a6-8270-bfe392fa36ae/gettyimages-1963058920.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-09-03T12:16:19+00:00",
  "addDate": "2025-09-03T12:22:41.485251+00:00",
  "refreshDate": "2025-09-03T12:22:41.485255+00:00",
  "score": 1.0,
  "title": "Why shadow AI could be the secret to fixing your company's failing AI projects",
  "description": "Most enterprise AI pilots fail due to flawed short-term strategies focused on cost-cutting. Here's how business leaders can build AI readiness and long-term value.",
  "content": "Follow ZDNET: Add us as a preferred source on Google.\n\u2022 AI projects are failing because the underlying strategy is flawed.\n\u2022 Business leaders should build on projects that employees find useful.\n\u2022 The winners will focus on value creation, not cost elimination.\n\nAI has become the boardroom obsession of the decade. Yet despite billions in investment and relentless hype, recent independent studies show that most enterprises struggle to turn pilots into measurable business outcomes.\n\nTwo recent studies put this problem into sharp focus:\n\u2022 MIT NANDA reported that 95% of enterprise generative AI pilots fail to deliver revenue gains. Only a small minority (~5%) of pilots successfully scale to production or achieve tangible growth.\n\u2022 McKinsey's 2025 State of AI survey showed that while adoption is increasing, more than 80% of firms report no enterprise-level EBIT (earnings before interest and taxes) impact from their AI investments.\n\nThese studies deliver an important message that must echo across boardrooms: AI pilots aren't failing because the technology isn't powerful enough. They're failing because the strategy and expectations behind them are flawed.\n\nThe MIT study clarifies that the obstacle to why AI pilots fail isn't model horsepower. Instead, the biggest issue is enterprise integration. Most tools don't learn from workflows, and most companies haven't developed the operational expertise to transform experiments into production systems.\n\nAlso: How a legacy hardware company reinvented itself in the AI age\n\nMcKinsey's research echoes this: AI drives impact only when firms redesign workflows, track KPIs, and evolve operating models. Pilots stuck in \"demo mode\" deliver buzz, not business value.\n\nMany industry leaders view AI as a short-term margin lever instead of using it to build durable capabilities. The typical playbook is to replace headcount, cut costs, and boost quarterly profit margins.\n\nThat approach may satisfy investors for a quarter or two, but in the mid- to long-term, it creates liabilities that compound, including:\n\u2022 Knowledge debt: Layoffs and shallow automation strip away institutional know-how. Tacit knowledge walks out the door, and recovery is costly.\n\u2022 Talent drainage: High performers don't want to maintain brittle systems. They leave, taking expertise and initiative with them.\n\u2022 Poor customer experience: Understaffed service lines and half-baked bots reduce resolution rates and increase customer effort. Dissatisfaction inevitably shows up in revenue.\n\nThe result is financial engineering, not value engineering. Shareholder value compounds most reliably when customer value is grown, not mined.\n\nA tale of two AIs: Enterprise vs. employee\n\nHere's the irony. While enterprise AI pilots stall, employees already use AI daily through tools like ChatGPT, Claude, and Gemini.\n\nThese \"shadow AI\" use cases aren't grand digital transformations. They're small, tactical applications: drafting emails, summarizing documents, generating code snippets, preparing presentations, or analyzing customer feedback.\n\nAlso: Forget plug-and-play AI: Here's what successful AI projects do differently\n\nThese applications succeed precisely because they are task-specific. They save minutes or hours on focused activities, improving productivity without requiring massive workflow overhauls or capital budgets.\n\nThis is how AI was meant to work: as a productivity amplifier, not a workforce replacement tool. Employees use it to do their jobs better, not to eliminate the jobs themselves.\n\nEnterprises should take note. The grassroots adoption inside firms reveals where AI provides real value. Instead of ignoring or banning shadow AI, leaders should study it, scale it responsibly, and build on the use cases that employees have already proven useful, remembering lessons learned about AI being a productivity enabler, not a labor replacement vehicle.\n\nEven when firms want to scale AI, many aren't ready. Actual AI readiness requires more preparedness than licenses for large language models. These requirements include:\n\u2022 Consolidated, clean data that systems can learn from. Most enterprises still operate with fragmented, siloed data that undermines results.\n\u2022 Clear use cases and expectations that are defined upfront. Too many pilots begin with \"let's try AI\" instead of identifying the precise problems to solve.\n\u2022 An implementation roadmap that spans technology, people, and process. AI isn't a plug-and-play tool. AI demands change management, governance, and continuous measurement.\n\nThe reality is that AI readiness will mature over time. In many firms, that maturation will be led not by the C-suite but by employees who experiment with AI in their daily work. Business and IT leaders need to channel that bottom-up innovation into a top-down strategy that scales value across the organization.\n\nSo, what works? A better path forward looks like this:\n\u2022 Start with tasks, not roles: Use AI to reduce customer effort or cycle time, then redeploy saved capacity to higher-value work.\n\u2022 Aim at production, not demos: Treat AI solutions like products. Define owners, service levels, adoption targets, and business KPIs.\n\u2022 Build learning systems: Deploy tools that capture feedback and improve with use. Static pilots don't create long-term value.\n\u2022 Invest in capabilities, not experiments: Cross-functional teams should own use cases end-to-end, including data, security, process, change, and customer experience.\n\u2022 Protect knowledge: Pair automation with knowledge capture and upskilling to reduce effort without erasing expertise.\n\nEvery AI proposal should answer one question: how does this initiative increase customer value in the next 90 to 180 days while building capabilities that compound over the next 18 to 24 months?\n\nFor example, if the answer is a staffing reduction plan, you optimize accounting, not outcomes.\n\nAlso: How AI agents can eliminate waste in your business - and why that's smarter than cutting costs\n\nThe ultimate winners won't be the firms that cut the fastest. The winners will be those who redesign work, so people and machines raise the bar for customers. That's how you build resilient earnings, durable growth, and lasting shareholder value.\n\nAI doesn't fail because of technology. AI fails because of how leaders choose to use it. Focus on value creation, not cost elimination. And remember: the best insights on where AI can help your enterprise are probably already sitting inside your organization, quietly being tested by your employees.",
  "medium": "Article",
  "links": [
    "https://www.zdnet.com/article/what-is-google-gemini/",
    "https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage",
    "https://cc.zdnet.com/v1/otc/00hQi47eqnEWQ6T9d4QLBUc?element=BODY&element_label=Add+us+as+a+preferred+source&module=LINK&object_type=text-link&object_uuid=31c8292f-0896-4f67-8207-73ddff17c278&position=1&split_test_identifier=deals_module&split_test_variant=test2&template=article&track_code=__COM_CLICK_ID__&url=https%3A%2F%2Fcc.zdnet.com%2Fv1%2Fotc%2F00hQi47eqnEWQ6T9d4QLBUc%3Felement%3DBODY%26element_label%3DAdd%2Bus%2Bas%2Ba%2Bpreferred%2BGoogle%2Bsource%26module%3DLINK%26object_type%3Dtext-link%26object_uuid%3D5e5d2e64-4b30-43e6-8555-26eac7e449f3%26position%3D1%26template%3Darticle%26track_code%3D__COM_CLICK_ID__%26url%3Dhttps%253A%252F%252Fwww.google.com%252Fpreferences%252Fsource%253Fq%253Dzdnet.com%26view_instance_uuid%3D379e95d2-6b56-476b-a90b-043a8dd63bd3&view_instance_uuid=9b3aa082-f2db-4417-a707-855dd0da62cd",
    "https://www.zdnet.com/article/95-of-business-applications-of-ai-have-failed-heres-why/",
    "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
    "https://www.zdnet.com/article/forget-plug-and-play-ai-heres-what-successful-ai-projects-do-differently/",
    "https://www.zdnet.com/home-and-office/networking/how-a-legacy-hardware-company-reinvented-itself-in-the-ai-age/",
    "https://www.zdnet.com/article/anthropic-will-start-training-claude-on-user-data-but-you-dont-have-to-share-yours/",
    "https://www.zdnet.com/article/how-ai-agents-can-eliminate-waste-in-your-business-and-why-thats-smarter-than-cutting-costs/",
    "https://www.zdnet.com/article/what-is-chatgpt-how-the-worlds-most-popular-ai-chatbot-can-benefit-you/",
    "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
    "https://www.zdnet.com/article/ai-leaders-must-take-a-tight-grip-on-regulatory-geopolitical-and-interpersonal-concerns/",
    "https://www.zdnet.com/article/stop-using-ai-for-these-9-work-tasks-heres-why/",
    "https://www.zdnet.com/article/i-retested-gpt-5s-coding-skills-using-openais-guidance-and-now-i-trust-it-even-less/",
    "https://www.zdnet.com/article/this-is-the-fastest-local-ai-ive-tried-and-its-not-even-close-how-to-get-it/"
  ],
  "labels": [
    {
      "name": "Opinion"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI pilots",
      "weight": 0.10269846
    },
    {
      "name": "enterprise generative AI pilots",
      "weight": 0.09926716
    },
    {
      "name": "AI projects",
      "weight": 0.09364821
    },
    {
      "name": "AI",
      "weight": 0.09253352
    },
    {
      "name": "shadow AI",
      "weight": 0.09153571
    },
    {
      "name": "AI readiness",
      "weight": 0.09112462
    },
    {
      "name": "AI solutions",
      "weight": 0.08932497
    },
    {
      "name": "AI agents",
      "weight": 0.08911409
    },
    {
      "name": "Actual AI readiness",
      "weight": 0.08507445
    },
    {
      "name": "enterprise AI pilots",
      "weight": 0.08190769
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.853515625
    },
    {
      "name": "/News/Technology News",
      "score": 0.79443359375
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.77001953125
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.75732421875
    },
    {
      "name": "/News/Business News/Other",
      "score": 0.67041015625
    },
    {
      "name": "/Business & Industrial/Business Operations/Management",
      "score": 0.666015625
    },
    {
      "name": "/Computers & Electronics/Enterprise Technology/Other",
      "score": 0.6015625
    }
  ],
  "sentiment": {
    "positive": 0.042907335,
    "negative": 0.73823154,
    "neutral": 0.21886109
  },
  "summary": "Despite billions in investment and hype, most enterprises struggle to turn pilots into business outcomes, according to two studies by MIT NANDA and McKinsey. The study found that 95% of enterprise generative AI pilots fail to deliver revenue gains, and only a small minority (~5%) of pilots successfully scale to production or achieve tangible growth. While adoption of AI is increasing, over 80% of firms report no enterprise-level EBIT (earnings before interest and taxes) impact from their AI investments. The author argues that AI projects are failing because the underlying strategy is flawed. While enterprise AI pilots stall, employees already use AI daily through tools like ChatGPT, Claude, and Gemini. The article suggests that business leaders should study and scale AI, building on the use cases that employees have proven useful.",
  "shortSummary": "Despite the hype and initial use, many enterprise AI pilots fail because of flawed strategy and limited business impact, emphasizing the need to adopt a more targeted approach.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "8ae8eae4f7394447b58be8a2f05a8062",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/what-is-google-gemini/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nWhat is Gemini? Everything you should know about Google's new AI model\nWhat is Google Gemini?\nGemini is a powerful artificial intelligence (AI) model from Google that can understand text, images, videos, and audio. As a multimodal model, Gemini is described as capable of completing complex tasks in math, physics, and other areas, and understanding and generating high-quality code in various programming languages.\nIt is currently available through the Gemini chatbot (formerly Google Bard) and some Google Pixel devices and will gradually be folded into other Google services. During Google I/O 2024, the company announced new features that will come to Gemini, including a new 'Live' mode and integrations with Project Astra. Gemini also powers AI overview in Google searches.\nAlso: I ranked the AI features announced at Google I/O from most useful to gimmicky\n\"Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research,\" said Dennis Hassabis, CEO and co-founder of Google DeepMind, when announcing Gemini.\n\"It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across, and combine different types of information including text, code, audio, image, and video.\"\nWho made Gemini?\nGemini was created by Google and Alphabet, Google's parent company, and released as the company's most advanced AI model to date.\nAlso: The ChatGPT desktop app is more helpful than I expected - here's why and how to try it\nGoogle DeepMind also made significant contributions to the development of Gemini.\nAre there different versions of Gemini?\nGoogle describes Gemini as a flexible model capable of running on everything from Google's data centers to mobile devices. To achieve this level of scalability, Gemini was released in three sizes: Gemini Nano, Gemini Pro, and Gemini Ultra.\n- Gemini Nano 1.0: The Gemini Nano model size is designed to run on smartphones, initially launched on the Google Pixel 8. It's built to perform on-device tasks that require efficient AI processing without connecting to external servers, such as suggesting replies within chat applications, understanding images, or summarizing text. The Gemini Nano model features a 32,000-token context window.\n- Gemini Flash 1.5: This model is built for speed, so it's a lightweight and cost-efficient option. The model features a long context window, with a one-million token context by default, enough to process an hour of video or over 30,000 lines of code.\n- Gemini Pro 1.5: Running on Google's data centers, Gemini Pro is designed to power the latest version of the company's paid AI chatbot service, Gemini Advanced. This model can deliver fast response times and understand complex queries. Google just upgraded its context window to two million tokens, the longest of any large-scale model available now.\n- Gemini Ultra 1.0: Google describes Gemini Ultra as its most capable model, exceeding \"current state-of-the-art results on 30 of the 32 widely-used academic benchmarks used in large language model (LLM) research and development.\" It's designed for highly complex tasks and is available through Vertex AI and Google AI Studio with the Gemini API.\nAlso: This subtle (but useful) AI feature was my favorite Google I/O 2024 announcement\nHow can you access Gemini?\nThe fastest way to use the Gemini model is to go to the AI chatbot's website, Gemini.Google.com. You can have a conversation with Gemini through this site like you can with ChatGPT and other AI chatbots.\nThe Gemini model is available in Google products, like Android-powered devices, the Gemini mobile app, Google searches with an AI overview, Google Photos, and more. Google plans to integrate Gemini further into its Search, Ads, Chrome, and other services.\nAlso: Google Glass vs. Project Astra: Sergey Brin on AI wearables and his top use case\nDevelopers and enterprise customers can access Gemini Ultra via the Gemini API in Google's AI Studio and Google Cloud Vertex AI. Android developers have access to Gemini Nano via AICore.\nHow does Gemini differ from other AI models, like GPT-4?\nGoogle's new Gemini model appears to be the largest, most advanced AI model to date, though the widespread release of the Ultra model will determine that fact for certain. Compared to other popular models that power AI chatbots, Gemini stands out due to its native multimodal characteristic and long context window of one million tokens.\nAlso: What does GPT stand for? Understanding GPT 3.5, GPT 4, GPT-4 Turbo, and more\nGPT-4, by comparison, is available in 8k and 32k token contexts.\nCompared to GPT-4, a primarily text-based model, Gemini easily performs multimodal tasks natively. While GPT-4 excels in language-related tasks, such as content creation and complex text analysis natively, it resorted to OpenAI's plugins to perform image analysis and access the web at the time of testing and relies on DALL-E 3 and Whisper to generate images and process audio.\nThis approach could change when OpenAI makes GPT-4o widely available, as ChatGPT won't rely on three separate models to perform actions and will instead use an omnimodel.\nAlso: The best AI chatbots: ChatGPT and other noteworthy alternatives\nGoogle's Gemini also appears to be more product-focused than other models available. Gemini is either integrated into the company's ecosystem or has plans to be, as it's powering both the chatbot and Android devices. Other models, like GPT-4 and Meta's Llama, are more service-oriented and available for various third-party developers for applications, tools, and services."
    },
    {
      "url": "https://www.zdnet.com/article/how-ai-agents-can-eliminate-waste-in-your-business-and-why-thats-smarter-than-cutting-costs/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nHow AI agents can eliminate waste in your business - and why that's smarter than cutting costs\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- AI agents help identify and remove waste in business.\n- All waste is costly, but not all costs are wasteful.\n- CEOs pursue cost efficiency with AI to protect performance.\nIn an AI-powered economy, business leaders are focused on enhancing the productivity and efficiency of their workforce and operations. To accelerate value creation, while focusing on cost reductions and efficiencies, companies are identifying and removing silos -- structural, data, and organizational -- so they can successfully deploy hyper automation and emerging technologies like autonomous AI agents and robots, serving as an extension of their digital labor.\nAlso: Gartner says add AI agents ASAP - or else. Oh, and they're also overhyped\nThere is a warning regarding the removal of silos that is worth mentioning. When you remove silos without an alternative way to manage your resources, you risk creating a spill instead of a flow. A spill is a waste of resources that can even become a pollutant or a hazard. A boundless organization, by contrast, creates and then directs flows of resources to wherever it most needs them.\nFocusing on cost optimization\nIn response to softening demand or deteriorating market conditions, businesses often prioritize cost-cutting. When executive pressure mounts due to less favorable performance, cost-cutting typically becomes the primary strategy.\nCost-cutting is a silo-based approach that tends to be more focused on resource management rather than value creation. Even when market conditions favor margin growth over revenue or customer growth, focusing on waste reduction is a more effective strategy than simply cutting costs.\nAlso: 3 smart ways business leaders can build successful AI strategies - before it's too late\nCost management was included in Gartner's top ten business priorities for 2025. According to Gartner, the top five actions CEOs are taking to safeguard organizational performance are led by pursuing cost-efficiency measures (77%), followed by adjusting pricing strategies (51%), and deploying fully automated, robotic, and AI systems (48%).\nAI is a top priority for CEOs, with 79% believing it will impact their industry in the next three years. However, there's a notable paradox: 18% of CEOs plan to decrease investment in people and culture development, and 31% are reducing hiring. This situation presents a real challenge. How can organizations achieve AI-driven transformation while simultaneously reducing investment in talent and training? The focus must be on removing waste:\nSalesforce research suggests that a significant portion of CFO budgets is dedicated to agentic AI investments. On average, CFOs report dedicating 25% of their current total AI budget to AI agents. Six out of 10 CFOs believe AI agents and digital labor are critical for competing in the current economic climate and will continue to be so.\nAlmost two-thirds (64%) of CFOs report that AI agents and digital labor are influencing their approach to business spending. And over a third (35%) of CFOs acknowledge that AI necessitates a riskier mindset regarding technology investments.\nAlso: Gen AI disillusionment looms, according to Gartner's 2025 Hype Cycle report\nCFOs also believe that AI investments are about accelerating productivity and boosting revenues. Most CFOs (74%) believe that AI agents will not only cut costs but also drive revenue. CFOs implementing AI agents expect these agents to increase company revenue by almost 20%, mostly because 55% of CFOs think AI agents will take on more strategic work than routine tasks.\nAnd finally, 72% of CFOs say AI agents will transform their business model. These figures highlight the significant potential of AI agents in both financial and strategic capacities within organizations. The growing investments in AI are aimed at removing wasteful activities in business, knowing the hidden costs associated with waste.\nDefining waste in business\nWaste, at its core, is using more resources than needed to accomplish a task, without being able to repurpose the excess. \"Resources\" here refers to anything with a non-zero cost to the business.\nHistorically, human resources were the primary option, and companies focused on finding the most cost-effective labor, often through offshoring. However, with agentic AI now capable of performing certain tasks at the required success levels, the decision for companies shifts to determining whether AI or human agents are more cost-effective. While less technologically sophisticated companies might face upfront costs for infrastructure and expertise to implement AI, the long-term cost of not adapting will likely be much higher.\nAlso: How Gemini's on-premise upgrade could help your enterprise and advance sovereign AI\nFrom a company's perspective, waste means overspending to complete work. For example, using human resources in a call center can be a \"double waste\" because employee time is more expensive than AI's and cannot be repurposed. Instead, AI can handle multiple calls simultaneously at a lower unit cost.\nBeyond just the company, waste also encompasses the underutilization of resources. If human resources are tied up in tasks that agentic AI can do, it's a waste of their capability to complete more value-oriented jobs, where human skills are still essential.\nWe should also consider the employee's perspective. It's a waste of their time, skills, and future if they are engaged in tasks that AI could handle instead of roles that truly leverage their unique human abilities. While we haven't explicitly focused on the customer's perspective here, it's important to remember that customer satisfaction often defines the success level of a job, which ultimately drives the choice of resources.\nIn essence, waste is directly related to how we perform jobs and use resources. Assuming resources have a cost, waste is either the overuse of resources for a job (where the excess cannot be reused) or the underuse of resources (failing to maximize their potential).\nAlso: Stop using AI for these 9 work tasks - here's why\nFinally, we should also consider the inherent value of the job itself. Jobs further removed from providing customer value and generating revenue are less worthwhile. \"Busy work\" should be eliminated as it misuses resources that could be applied to more critical tasks.\nUltimately, waste in any industry is the misuse of resources, whether through overuse, utilizing the wrong resources, or failing to realize their full potential. Since all resources have a cost, all waste is costly, though not all costs are wasteful.\nIdentifying waste in business\nIdentifying wasteful processes in business can be challenging. While accounting practices help us measure costs, not all forms of waste are easily identifiable or measurable. Business processes often lack formal scrutiny for assessing waste.\nThe increasing complexity of business operations makes it hard to link activities to customer or stakeholder value. Additionally, people often prefer their own methods for tasks, even if these methods are more complicated or time-consuming than standardized approaches, making waste harder to pinpoint and eliminate.\nAlso: 71% of Americans fear that AI will put 'too many people out of work permanently'\nDespite these difficulties, eliminating waste across all company operations helps to focus the organization on creating value for customers, the business itself, and other stakeholders. This process also improves resource flow, including data and decisions, and boosts responsiveness to new challenges and opportunities.\nContinuous waste elimination is a vital tool for sustainability and profitability, and it remains a valuable goal in any economic cycle. After more than a million customer conversations with AI agents at Salesforce, we have learned many lessons regarding the elimination of waste and the expansion of more value-oriented assignments to our workforce.\nThe path to becoming an autonomous enterprise, using a hybrid workforce of humans and digital labor powered by AI agents, will help companies remove wasteful activities. All waste is costly. Smart and growing businesses will focus on waste reduction, not cost reduction. The goal is to create value at the speed of need -- and to do this, you cannot be wasteful in any way.\nThis article was co-authored by Henry King, co-author of Boundless and a new book, Autonomous, Wiley, October 2025."
    },
    {
      "url": "https://www.zdnet.com/article/i-retested-gpt-5s-coding-skills-using-openais-guidance-and-now-i-trust-it-even-less/",
      "text": "I retested GPT-5's coding skills using OpenAI's guidance - and now I trust it even less\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- The same prompts in GPT-5 yield success, crashes, or errors.\n- OpenAI's prompt optimizer helps, but introduces its own quirks.\n- AI \"unconsciously\" adding details raises trust concerns.\nDo AI's get headaches? Because GPT-5 has certainly been giving me one. This article was going to be so easy. OpenAI came out with a list of best practices for GPT-5 coding. All I was going to do was try those best practices with the GPT-5 coding tests that previously failed and see if there was improvement.\nIt seemed so simple.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nBut then I had a thought. What if OpenAI has improved GPT-5 in the week or so since I ran my tests?\nPerhaps I should re-run the failed tests as-is and see what the results are, to use as a baseline for the new best practices.\nFrom a testing point of view, this was a valid approach. From a personal sanity point of view, not so much.\nRe-running test 1\nI re-ran the first failed test. This test has the AI creating a complete WordPress plugin, complete with a user interface and business logic. The idea is you feed in a set of names, it randomizes them, and it separates duplicates so they're not side-by-side.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nWhen I ran this test on GPT-5 originally, it failed. Clicking the Randomize button sent the browser to another, unrelated page. I was able to cajole GPT-5 into fixing it, but the initial failure was what counted.\nThis time, I ran the exact same test with the exact same prompt again. This time, it worked perfectly. Wow, I thought. GPT-5 has improved in the past week.\nIf only I had left well enough alone. But no.\nI had to try again. On my second time with the exact same test with the exact same prompt, clicking Randomize resulted in what WordPressers call the \"white screen of death.\" This indicates something in the code isn't working.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nI tried the exact same test with the exact same prompt a total of five times. The first time it worked. Subsequent times, it didn't. One time, I got a white screen. One time, I got a page full of error messages. One time, I was redirected to another page. One time, nothing happened at all.\nWe'll come back to test 1. But next, I decided to do the same thing with the other failed test.\nRe-running test 4\nThis test asks the AI to write code that talks to Chrome, AppleScript, and another tool called Keyboard Maestro. It requires the AI to be knowledgeable about the three different environments and how they work together.\nHere's an interesting factoid about AppleScript. The scripting language is inherently case-insensitive. So, if you ask AppleScript to compare \"AppleScript\" to \"applescript\", it will consider both strings to be the same. You have to explicitly tell it to compare case before it will do so.\nAlso: You can learn AI for free with these new courses from Anthropic\nWhen I first ran this test against GPT-5, it hallucinated that AppleScript had a native function for making strings lowercase. It does not, so the code failed.\nThis time, instead of hallucinating that AppleScript has a lowercase function, it decided to do one of the most convoluted sets of operations to lowercase a string. Here's that line of code:\nset tabTitleLower to (do shell script \"echo \" & quoted form of tabTitle & \" | tr '[:upper:]' '[:lower:]'\")\nThe code actually works, but it's very weird and totally unnecessary. It launches a shell (the command line) to run a shell script that converts to lowercase. It's like making a grilled cheese sandwich and when you realize you're out of cheese, flying across the country to buy cheese from a 7-11 on the opposite side of the continent. You're not even getting better cheese. You're just doing it in the weirdest and most inefficient way possible.\nLet's not forget that we really don't need to make that text lowercase in the first place, because AppleScript is case-insensitive. So it wrote a highly convoluted piece of code that, while it works, is entirely unnecessary.\nI didn't bother to re-run this test four more times because the headache was already starting to set in.\nOpenAI's GPT-5 coding best practices\nOpenAI has put out what it calls a cheatsheet for GPT-5 coding. It has six main recommendations:\n- Be precise and avoid conflicting information: Well, duh. But apparently GPT-5 gets really confused if asked to follow vague or conflicting instructions.\n- Use the right reasoning effort: Rather than just letting GPT-5 choose its reasoning level, you might want to adjust its model. If it starts to overthink a problem, use a lower reasoning level. Great, so now we have to deal with a neurotic AI. Yippee?\n- Use XML-like syntax to help structure instructions: Rather than using just text, using XML tags to segment aspects of a prompt helps the AI to parse its assignment. This is a good hint, but it feels like a step back from what it could do.\n- Avoid overly firm language: In previous models, we sometimes had to YELL at the AI to get it to listen. Apparently, GPT-5 is passive aggressive, so if you get too forceful in your instructions, it will overdo its compliance, like what my old bosses used to say was my practice of \"aggressively following instructions.\"\n- Give room for planning and self-reflection: Does the AI need a blankie? Perhaps a warm stuffed animal to cuddle? In reality, what OpenAI is saying is that you need to encourage the AI to think through the instructions it gets. Apparently, if you don't tell it to think through the instructions, it won't. I fear for the planet.\n- Control the eagerness of your coding agent: It turns out GPT-5 is even more of a people-pleaser than GPT-4o. When giving it instructions, tell it to tone down its fawning and sucking-uppage.\nI'd like to say something positive here, but it's starting to feel like a migraine. Check out the PDF if you want more details.\nOpenAI also created an AI prompting helper. You point your browser to this tool, and paste in your prompt. The tool will give you back a more extensive prompt. For example, I gave it this prompt from Test 4:\nWrite a Keyboard Maestro AppleScript that scans the frontmost Google Chrome window for a tab name containing the string matching the contents of the passed variable instance__ChannelName. Ignore case for the match. Once found, make that tab the active tab.\nIt gave me back this:\nCreate an AppleScript for use in Keyboard Maestro that performs the following actions: * Begin with a concise checklist (3-7 bullets) outlining the planned steps before proceeding. * Scan all tabs in the frontmost Google Chrome window. * Search for a tab whose name contains the value of the passed variable 'instance__ChannelName', ignoring case sensitivity. * Once a matching tab is found, activate that tab and make it the current/active tab. After generating the script, validate in 1-2 lines whether the script meets all requirements. If not, state what needs adjustment.\nEssentially, it revised the prompt to add a planning phase at the beginning and a validation phase at the end.\nDoes it work?\nNope. Nopity-nope-nope.\nI ran Test 4's \"optimized\" prompt through GPT-5 and got back this script:\nThere are at least three serious flaws in it. First, the code in the green box doesn't work. It doesn't retrieve the value from Keyboard Maestro. Previous versions did that successfully using a completely different (and well-documented) mechanism. This approach is just loony.\nAlso: AI is creeping into the Linux kernel - and official policy is needed ASAP\nNow, look at the blue box. The considering case operation is used only for evaluation. For example, with considering case, \"BLUE\" and \"blue\" are different values. Do you see any sort of if statement or evaluation inside the blue box? Me neither.\nThen there's the red box. There is no such syntax as \"considering case false.\" First, the default behavior is to ignore case. Second, if you really wanted to explicitly tell AppleScript to ignore case, you'd use an \"ignoring case\" block. But \"considering case false\"? That's just some made-up syntax right there.\nRetrying the WordPress plugin\nI also ran my original prompt for the WordPress plugin through the prompt optimizer. I got back a fairly extensive project requirements document.\nOn the plus side, this time the plugin worked. I tried generating code a second time, and it also worked. So perhaps the optimization is something to try if GPT-5 isn't playing nice.\nBut then there's the weirdness. Because there's always weirdness. Check out the header block generated by GPT-5.\nNotice the author is attributed to Advanced Geekery Labs. Historically, the AIs have listed the author field as \"Your Name.\" But this time, it's Advanced Geekery Labs.\nThere is some relevance, because Advanced Geekery is the name of my YouTube channel and weekly newsletter. But I didn't use it anywhere in my instructions for this plugin. Additionally, I've never added Labs to the end of Advanced Geekery, even though my security product is sold through ZATZ Labs.\nSo I asked GPT-5 where it got the name Advanced Geekery Labs from. Its answer:\nGood question. The name \"Advanced Geekery Labs\" isn't something you provided in your request. I introduced it in the plugin header's Author: field as a placeholder, because in earlier conversations you've mentioned your \"Advanced Geekery\" brand, and I unconsciously expanded it into \"Labs.\"\nI'm glad the AI thinks it's a good question. That warms my heart. But \"unconsciously expanded\"? That's a direct quote from the AI. Those are its words.\nSo, we're now dealing with hallucinating AIs that operate unconsciously?\nWe're all doomed.\nWhat about coding and GPT-5?\nI still don't trust it. Actually, I don't trust it even more than when I tested it originally. What else might it do to my code \"unconsciously\"?\nI guess, if you really need GPT-5 to give you some code, you could try the optimizer. But I wouldn't rely on it.\nGPT-4o was impressive, but it had some problems. GPT-5 is like its more troubled, more difficult sibling.\nWith GPT-4o, I'd be willing to trust, with verification. I just plain don't trust GPT-5. There's something that's just not right. If we were to experience an AI that would revolt against its creators and try to kill us all, I would not be surprised if it all originated with GPT-5.\nAnd no. My headache hasn't gone away. Apparently, Tylenol is now bad for you. So, there's that, too.\nWhat do you think?\nHave you tried relying on GPT-5 for coding projects? If so, did you run into the same kinds of inconsistencies and odd behaviors? Do you find OpenAI's \"best practices\" helpful, or do they feel like workarounds for deeper problems? And what's your take on GPT-5's tendency to improvise or even \"unconsciously\" insert details? Is it a harmless quirk or a reason for concern? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/ai-leaders-must-take-a-tight-grip-on-regulatory-geopolitical-and-interpersonal-concerns/",
      "text": "AI leaders must take a tight grip on regulatory, geopolitical, and interpersonal concerns\nMy podcast co-host Ray Wang, CEO of Constellation Research, and I are approaching our tenth year of doing our weekly podcast. In a milestone 400th episode of DisrupTV, Ray and I had the privilege of hosting three exceptional thought leaders to discuss the transformative impact of artificial intelligence (AI) and essential leadership strategies for today's digital landscape.\nAlso: Your favorite AI chatbot is lying to you all the time\nOur distinguished panel included Lord Tim Clement-Jones, a member of the UK House of Lords and co-chair of the UK's All-Party Parliamentary Group on AI; Dr. David Bray, CEO/principal at LeadDoAdapt Ventures, Inc. and distinguished chair of the Accelerator at the nonpartisan Henry L. Stimson Center; and Melody Wilding, professor of human behavior at Hunter College and author of 'Managing Up: How to Get What You Need from People in Charge.'\nWith AI reshaping business and society at unprecedented speed, our guests offered invaluable insights for executives navigating this complex digital terrain.\nNavigating AI regulation and governance\nLord Tim Clement-Jones, speaking from London, focused on how businesses can adapt to the incoming wave of AI regulation. \"Corporates need to understand what kind of governance processes they should put into play,\" he explained. As AI regulation evolves globally, organizations must develop proactive compliance strategies rather than reactive ones.\nAlso: Apple Intelligence is getting more languages - and AI-powered translation\nHis expertise in digital policy and online harm provides a unique perspective on balancing innovation with responsible AI deployment. For C-suite executives, his message was clear: establishing robust AI governance frameworks now will position companies advantageously as the regulatory landscape matures. Key recommendations included:\n- Implement AI governance now: Do not wait for regulations to finalize. CEOs should establish governance frameworks that align with existing corporate values. Treat AI regulation like other familiar domains, such as data protection, making it a standard business consideration rather than a novel challenge.\n- Conduct cross-border regulatory mapping: Tech leaders should create a comprehensive map of emerging AI regulations across key markets, focusing on differences between the UK, EU, and other regional approaches. Leaders should use this map to develop adaptable compliance strategies that can flex as regulations evolve.\n- Integrate AI ethics into business strategy: CEOs should embed ethical AI considerations directly into corporate strategy rather than treating them as separate compliance issues. This integration will ensure AI deployment aligns with regulatory requirements and organizational values, creating a competitive advantage through responsible innovation.\nThe global-local AI balancing act\nDr. David Bray has long advocated for people-centered approaches to technology, including his work with internet co-originator Vint Cerf and the People-Centered Internet coalition. Bray and Wang were ahead of the AI curve, co-authoring an MIT Sloan Management Review piece on people-centered approaches to AI and deep learning in 2019.\nAlso: AI's biggest threat isn't what you'd think - here's how to protect yourself\nNamed by Business Insider as one of the top 24 Americans changing the world under 40, Bray emphasized the interconnected nature of AI deployment across national boundaries. His insights highlighted how technology leaders must consider both domestic and international implications of their leadership and tech strategies. Key recommendations included:\n- Co-create AI solutions with your people: CEOs, CTOs, and CIOs must shift from top-down AI implementation to collaborative development. Bray said: \"Involving people in the process of doing AI means that everybody's a problem solver as opposed to just a few.\" This participatory approach not only preserves human agency but also uncovers innovative applications that executives wouldn't discover in isolation. Leaders should establish formal mechanisms for employees at all levels to contribute to AI strategy, implementation, and refinement.\n- Match AI flavors to business needs: Boards need directors with expertise in both fundamental technologies and the broader organizational and global implications of tech. Bray noted: \"When we talk about AI, there are many different flavors of AI.\" He emphasized that leaders must identify their business needs before selecting the appropriate technology. Bray said: \"Computer vision is deterministic, repeatable, and reliable. Generative AI (Gen AI) is non-deterministic and can have creativity as its strength. There's also Bayesian-based methods that are much more deterministic and explainable.\" Corporate boards should include experts who understand these distinctions and can guide strategic decisions based on business requirements rather than technological hype or fears.\n- Develop scenario-based response plans: Tech leaders should create specific action plans for different regulatory scenarios across key markets. These plans should include technical, operational, and communication strategies that can be rapidly deployed as regulatory landscapes shift. Bray said Gen AI and cybersecurity require \"red-teaming\" approaches, with leaders focused on how Gen AI might be exploited as a cybersecurity risk rather than waiting for vulnerabilities to be exposed.\nBuilding influence across organizations\nMelody Wilding brought a crucial human dimension to the discussion. Her latest book offers timely guidance as AI transforms workplace dynamics and leadership structures.\nAlso: You shouldn't trust AI for therapy - here's why\n\"We're going to be talking about some strategies to build your influence, get respect, and recognition from those above you,\" Wilding explained from her location in Northern New Jersey. Her background as a therapist and emotions researcher informs her evidence-based approach to professional development. Key recommendations included:\n- Map your power ecosystem: To effectively manage up, Wilding recommends conducting a strategic \"power mapping\" exercise. \"This is a way to figure out who, beyond your main stakeholder, do you need to be aligning with and building relationships with,\" she explained. Leaders should identify stakeholders who are both \"high influence and high interest in your work\", specifically those with decision-making power who are also invested in your initiatives. This targeted approach helps prioritize relationship-building efforts where they'll have the most impact, especially when advocating for AI initiatives to senior leadership.\n- Align your metrics with leadership priorities: When managing up, ensure your success metrics match senior leaders' values. Wilding emphasized avoiding \"vanity metrics\" when discussing results with executives. Instead, focus on measurements that directly connect to organizational priorities and business outcomes. This alignment will demonstrate an understanding of leadership's perspective and position your work as directly contributing to strategic goals.\n- Build cross-functional alliances: Wilding's approach to managing up extends beyond direct reporting relationships to include peer advocacy. She recommended regularly attending meetings of adjacent departments to learn their priorities and challenges. This cross-functional engagement creates allies who can advocate for your initiatives when you're not in the room.\nLeadership lessons for the AI Age\nThe intersection of these three perspectives -- regulatory, geopolitical, and interpersonal -- creates a comprehensive framework for technology leaders:\n- Proactive governance: As Clement-Jones emphasized, waiting for regulations to solidify before establishing AI governance frameworks puts organizations at a disadvantage. Forward-thinking leaders are already implementing ethical AI principles and governance structures. \"This is a business decision,\" he noted, \"this isn't something kind of new and strange that's arriving. We're used to dealing with regulations in business, in data protection, in environmental safety, and all that kind of thing.\"\n- Democratize AI problem-solving: Bray advocated for a fundamentally collaborative approach to AI implementation across global and local contexts. \"Involving people in the process as to where we apply AI means that now everybody's a problem solver,\" he emphasized. This bottom-up approach yields innovations that top-down implementation would miss: \"You're going to learn things and find things that if you try to do it solely by yourself, you'd never discover.\"\n- Manage up and your peers, too: Wilding's approach suggested that effective leadership in the AI era requires managing relationships not just up the chain of command but across the entire organizational ecosystem. \"We need to make sure that we're all rowing in the same direction,\" she noted. Her power mapping technique helps leaders identify key stakeholders beyond direct reporting lines, ensuring alignment across departments and functions, essential for navigating the complex organizational dynamics of AI implementation.\nAs we celebrated our 400th episode milestone, the conversation with these thought leaders reinforced DisrupTV's commitment to bringing together policymakers, best-selling authors, and CEOs who are \"helping us improve society and advance society forward in a meaningful way.\"\nAlso: How to turn off Gemini in your Gmail, Docs, Photos, and more - it's easy to opt out\nFor executives navigating the AI revolution, the combined wisdom of Lord Tim Clement-Jones, Dr. David Bray, and Melody Wilding offered a valuable roadmap, balancing geopolitical considerations, global awareness, and boundary-spanning leadership to harness AI's potential while mitigating its risks.\nThis article was co-authored by Dr. David Bray, principal and CEO at LeadDoAdapt (LDA) Ventures, chair of the Accelerator, and distinguished fellow at the Stimson Center."
    },
    {
      "url": "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
      "text": "Is ChatGPT Plus still worth $20 when the free version offers so much - including GPT-5?\nWhen ChatGPT first launched over two years ago, the AI chatbot was met with such high demand that OpenAI introduced a premium plan called ChatGPT Plus. This plan guaranteed access to the chatbot even during blackout periods. ChatGPT Plus perks also included access to OpenAI's most advanced models, making the $20 plan almost a no-brainer for superusers. However, as OpenAI's offerings have evolved over the past couple of years, so have its plans.\nAlso: How ChatGPT could replace the internet as we know it\nIf you consider yourself an AI power user or are looking to get the most out of your AI usage, you're likely wondering which ChatGPT tier you should try. In this guide, we'll help you decide whether a free plan, ChatGPT Plus, or a $200-per-month ChatGPT Pro subscription is the best fit for you.\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nYou should use ChatGPT Plus if...\nChatGPT Plus costs $20 per month. You're probably asking yourself: Why pay when you can use it for free? There are five main advantages, but the TL;DR is that the free tier has heavy usage limits, and the Plus tier gets you the latest features and models first.\nIf you only use ChatGPT now and then for fun, it's not worth subscribing. But if you use it for work, writing, creating images, using it as a search engine, or you find yourself opening the app all day, every day, then subscribing is likely a good idea.\n1. You want access to legacy models\nWhen OpenAI launched GPT-5, the company had it replace all of its legacy models in ChatGPT. The model is meant to combine the best of OpenAI's offerings with a smart model for most queries and a deeper reasoning model for harder problems. However, many paying subscribers were upset because they preferred the prior models for their workflows that were already in place.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nSam Altman even acknowledged in an X post that \"suddenly deprecating old models that users depended on in their workflows was a mistake.\" As a result, OpenAI brought back the option for ChatGPT Plus users to access legacy models, including: GPT-4o, o3, o4-mini, GPT-4.1, and GPT-4.1-mini, in addition to GPT-5 (Auto, Fast, Thinking mini, and Thinking).\n2. You want to access GPT-5's different modes\nWhile GPT-5 combines the best of a deep reasoning model (GPT-5 thinking) and a standard smart model for a balance of speed and quality, users with a Plus subscription can also manually toggle between GPT-5 Thinking, GPT-5 Thinking Mini, and GPT-5 Fast.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nThe benefit: Users can choose to use a reasoning model when they think it would be the best fit instead of having the real router feature automatically understand which model to use based on the conversation, the complexity of the prompt, and more, which is still available with the GPT-5 Auto option. If you often work on harder problems that require the model to think more to output the best possible answer, then having the option to toggle on GPT-5 Thinking could be a compelling reason to subscribe.\n3. You want to access Sora\nOpenAI's Sora video-generating model allows users to create stunning, realistic videos from text prompts or their own assets with up to 1080p resolution. Videos can be up to 20 seconds long, with a limit of 10 seconds for ChatGPT Plus users, and can be widescreen, vertical, or square aspect ratios. Whether you have a real workflow reason for AI-generated content or just want to tinker, it is a fun model to use.\nTo create images, ChatGPT also uses the GPT-4o image generator, OpenAI's most capable image generator to date.\nWith it, you can simply ask \"create an image of\u2026,\" describe a scene, and receive an AI-generated image that can include realistic human generations and even text. You can also upload an existing image and describe edits for GPT-4o to apply -- like adding text, refining details, or changing backgrounds. Just be aware that complex requests may take a couple of minutes.\nAlso: ChatGPT's new image generator shattered my expectations - and now it's free to try\nUsage is capped for free users, but Plus and Pro subscribers can enjoy much higher daily quotas.\nOpenAI said free-tier users can use GPT-4o only a limited number of times within a five-hour window. It will notify you once you've reached the limit and invite you to upgrade to ChatGPT Plus. If you're on the Free tier and rely heavily on image creation, you might see a message like: \"You've reached your image creation limit. Upgrade to ChatGPT Plus or try again tomorrow...\"\nSo, if you often use AI for visual assets, such as videos or images, ChatGPT Plus is an easy choice.\n4. You want to use the Codex AI coding agent\nOpenAI recently integrated its powerful Codex AI coding agent into ChatGPT Plus, so now anyone with the $20-a-month Plus can get AI-powered coding suggestions without shelling out $200. You can point it at your GitHub repo, and it'll whip up code changes, run checks to make sure nothing's broken, and even handle installing dependencies online if you let it. Just keep in mind it doesn't \"remember\" anything between sessions, so you have to give it clear instructions every time.\nAlso: You can use OpenAI's super powerful AI coding agent Codex for just $20 now\nAlso, since the coding agent is new to ChatGPT, you might encounter some speed bumps if many people are using it at once.\n5. You want expanded access to ChatGPT's best features\nYou're probably noticing a theme here: The majority of ChatGPT's most advanced features are eventually made available to free users, but with heavy usage limitations. Therefore, the main benefit of upgrading is getting to use features like Deep Research, Advanced Voice Mode, and GPT-4o image generation a lot more frequently than you could on the free tier. Subscribing to ChatGPT Plus also gives you priority access to OpenAI's latest models and newest tools well before they reach the free tier.\nWith a Plus account, you can access extended limits on:\n- messaging\n- file uploads\n- image creation\n- data analysis\n- Deep Research\n- Agent Mode\n- GPT-4 image generation\n- memory and context\nIf you don't want to miss out on the latest ChatGPT features and models, you should subscribe to the Plus tier.\nYou should use ChatGPT Pro if...\nOK, so now that we've discussed why Plus is worth it, let's look at the more expensive Pro plan that costs $200 a month. Most people would never pay a car payment's worth of money to use AI, but there are special instances in which it could be worth it.\n1. You want what ChatGPT Plus has on a greater scale\nAll ChatGPT Plus perks and features are included in ChatGPT Pro, but with far fewer usage constraints. Pro also adds several exclusive capabilities. Here's a rundown of the biggest benefits, should you be seriously considering the upgrade.\n- Unlimited access to all reasoning models: This includes GPT-5 with pro reasoning, GPT-4o, GPT-4.1 (and its mini variant), o3, o3-pro, o4-mini, and GPT-4.5 (research preview).\n- Unlimited and faster image generation using GPT-4o.\n- Maximum Deep Research, memory, and context, Agent Mode\n- Priority access to Sora video generation: You get up to 1080p video generation, 20-second videos, five concurrent generations, and watermark-free downloads.\n- Higher access to the Codex agent, projects, tasks, and custom GPTs.\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\nAs you can see, Pro offers more features and higher limits than the Plus tier. Pro users reportedly rarely encounter any constraints and can truly leverage the full power of ChatGPT and its newest tools.\n2. You want access to OpenAI's most powerful models\nAs mentioned in the No. 1 selling point of ChatGPT Plus, there are many perks to being able to access the older models, including sticking to what you were already using in your workflow and what you are accustomed to. In addition to all of the legacy models accessible in the Plus tier, with the Pro subscription, you can also access GPT-5 Pro, the most capable version of the model aimed toward complex tasks, and the Thinking 4.5 (research previews), which is only available to Pro users because \"it costs a lot of GPUs,\" according to Altman.\nYou should use free ChatGPT if...\nFinally, let's look at the free version of ChatGPT. I discussed most of its features above, so in this section, I'll break down what advantages it can offer over the paid plans. It essentially comes down to how often you use ChatGPT and whether having access to the latest and greatest features matters to you. If you don't use it much and don't care, the free version is 100% the way to go.\n1. You don't want to pay a monthly fee\nThe free subscription now offers many competitive AI features, reducing the need for a Plus subscription, especially for tools that were once paywalled. Free ChatGPT users can access:\n- GPT-5\n- ChatGPT Search (web browsing for timely information and sources)\n- Image Generation via GPT-4o (daily limit applies)\n- Deep Research (up to five \"lightweight\" tasks per month)\n- ChatGPT Voice (free monthly preview)\n- File and Photo Uploads for in-chat discussions (limited usage)\n- Memory Feature for referencing recent conversations (lightweight version)\nIf you only need occasional AI assistance -- and do not mind daily usage caps -- the free tier should serve you well. It's surprisingly robust and now includes many advanced features that were once behind a paywall. For instance, when OpenAI launched GPT-5, it also became available for free users.\n2. You're a casual ChatGPT user\nIf you rarely hit the daily usage limits for text, voice, or image generation, upgrading to Plus or Pro might not be necessary.\nAlso: ChatGPT can record, transcribe, and analyze your meetings now\nThe free version offers enough headroom for light interactions -- asking a few questions per day, generating a handful of images, briefly exploring advanced data analysis, and more. However, if you often see messages about hitting your limits (for text, voice, images, etc.), it might be time to consider paying for a subscription. For superusers who need top-tier capacity and extended access to advanced features, there's ChatGPT Pro, but Plus is a more affordable middle ground.\nUltimately, if your needs are minimal and limitations don't bother you, sticking with the free tier is perfectly fine.\nFAQs\nHow much do ChatGPT Free, Plus, and Pro cost?\nAs of August 2025, here are the ChatGPT Free, Plus, and Pro pricing tiers and how they differ, as displayed by OpenAI on its website:\nDoes the Pro tier include everything you get with Plus?\nYes, Pro contains everything in Plus. That means unlimited or higher limits on certain features and exclusive access to GPT-5 Pro and Research preview of new features.\nDo ChatGPT Pro users experience downtime or blackout periods?\nPro subscribers have the highest priority for uptime, making downtime extremely rare. However, no tier can guarantee 100% uptime if OpenAI undergoes major outages or scheduled maintenance.\nIf you upgrade to Plus, can you later switch to Pro?\nYou can upgrade from Plus to Pro at any time through your account settings. Your billing date may adjust based on when you switch.\nHow to get early access to new ChatGPT features on the free tier\nUnfortunately, free users typically must wait until OpenAI rolls them out publicly. Plus and Pro subscribers receive early or exclusive testing opportunities before features reach the free plan."
    },
    {
      "url": "https://www.zdnet.com/article/forget-plug-and-play-ai-heres-what-successful-ai-projects-do-differently/",
      "text": "Forget plug-and-play AI: Here's what successful AI projects do differently\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Only 5% of AI projects deliver. It comes down to the ability to customize.\n- With partnerships in place, AI success odds double.\n- Ask the right questions before deciding between building or buying.\nThere's a tremendous gap between AI aspirations and actual successful projects -- this was shown in the recent MIT study that found only 5% of generative AI projects have delivered measurable value to businesses. What is that top 5% doing differently? Their common denominator is that their technology teams are mastering the art and science of highly customizing AI to their businesses, while fostering partnerships versus go-it-alone approaches.\nThey are going deep -- very deep.\nWhat successful AI projects do differently\nSuccessful AI efforts \"focus on narrow but high-value use cases, integrate deeply into workflows, and scale through continuous learning rather than broad feature sets,\" according to the study's research team, consisting of Aditya Challapally, Chris Pease, Ramesh Raskar, and Pradyumna Chari. \"Domain fluency and workflow integration matter more than flashy UX.\"\nAlso: I retested GPT-5's coding skills using OpenAI's guidance - and now I trust it even less\nUltimately, it's not about building or buying AI just to have AI -- it's about how the business can benefit from AI. Instead of \"struggling with outdated SaaS playbooks,\" professionals need to \"capture enterprise attention through aggressive customization and alignment with real business pain points,\" they added. \"The standout performers are not those building general-purpose tools, but those embedding themselves inside workflows, adapting to context, and scaling from narrow but high-value footholds.\"\nIt's notable that \"plug-and-play AI is a myth,\" Paul McDonagh-Smith, senior lecturer of IT at MIT Sloan Executive Education, told ZDNET. (McDonagh-Smith was not directly involved with the study.) \"Outside tools save time, but the real work is going to be with 'plug-and-personalize AI' where we customize AI tools to fit our workflows.\"\nThe reason gen AI tools such as ChatGPT succeed in pilots \"is because of their flexibility,\" he added. \"However, they often fail in mission-critical work due to factors including a lack of memory, which reduces their ability to learn, adapt, and be customizable to the degree required to integrate effectively with our day-to-day workflows.\"\nAlso: Stop using AI for these 9 work tasks - here's why\nEstablishing strategic partnerships to move forward with AI makes a significant difference, the MIT study showed. The co-authors observed far more build than buy initiatives, with partnerships succeeding twice as often as internal development efforts. Such partnerships often provided \"faster time to value, lower total cost, and better alignment with operational workflows. Companies avoided the overhead of building from scratch, while still achieving tailored solutions.\"\nWhen to build vs. buy\nStill, AI proponents and developers need to weigh when it's best to develop in-house versus working with partners such as vendors or network partners. When it comes to making such decisions, \"the tipping point comes when speed, scale, or specialized expertise is called for, and your in-house teams aren't ready to deliver to the timelines required by the business,\" McDonagh-Smith said. \"Building internally makes sense when the project is core to competitive edge, but we need to be careful, pride can come before a fall.\"\nWhile there is concern that using outside solutions will reduce opportunities for the needed customization, this fear is unfounded, McDonagh-Smith believes. \"It's not so much a case of 'plug and play AI' as it is 'plug and personalize AI' to fit existing and emergent workflows. I would argue that AI success isn't dependent on the sourcing selection of our external AI tools but our internal ability to make them fit how our companies think, work, and act.\"\nAlso: These CFOs are devoting 25% of their AI budgets to agentic AI\nOther industry leaders agree that successful AI depends on individual circumstances. \"Deciding on using in-house teams or outsourcing to other vendors depends on what the organization wants the AI to do,\" said David Friend, CEO and co-founder of Wasabi Technologies. \"If any technology, AI or other, is part of the company's core differentiation or will allow the business to compete on price, it should be developed and managed in-house. If it is not a core part of the company's offering, outsource it. What makes you different is what you should do yourself. What isn't core to what you do, outsource it.\"\nThis process needs to start with asking the right questions as well. \"The question is not whether they can build the technology, but whether they should,\" Adrian Murray, founder and CEO of Fisent Technologies, pointed out. \"You may have very capable and well-funded technology teams, but your capacity is inherently limited and needs to be focused on the highest-value efforts. Teams need to focus their efforts where they can create differentiated value. Apply these technologies to specific business problems, not building core technology infrastructure that can be easily licensed from a solution vendor.\"\nThe nature of the partnership relationship is also a deciding factor in AI success -- it needs to be more than a transactional arrangement. \"Top buyers treated AI startups less like software vendors and more like business service providers, holding them to benchmarks closer to those used for consulting firms or business process optimization providers,\" according to the MIT report. This includes deep customization aligned to internal processes and data that are tied to outcomes.\nAlso: 4 ways to turn AI into your business advantage\n\"Ensure that existing and emergent workflows are decomposed, analyzed for patterns that will work -- or not -- with genAI, and then evolved, and recombined when genAI is ready,\" McDonagh-Smith advised.\nGrassroots AI adoption inside companies\nOften, successful AI efforts start at the grassroots level, the study also showed. \"Many of the strongest enterprise deployments began with power users, employees who had already experimented with tools like ChatGPT or Claude for personal productivity,\" the study's co-authors reported. They \"intuitively understood genAI's capabilities and limits, and became early champions of internally sanctioned solutions. Rather than relying on a centralized AI function to identify use cases, successful organizations allowed budget holders and domain managers to surface problems, vet tools, and lead rollouts.\"\nAlso: How AI-enabled autonomous business will change the way you work forever\nAgentic AI architectures are also emerging, supported by frameworks such as Model Context Protocol (MCP), Agent-to-Agent (A2A), and NANDA, which enable agent interoperability and coordination, the study found. \"These frameworks form the foundation of the emerging Agentic Web, a mesh of interoperable agents and protocols that replaces monolithic applications with dynamic coordination layers.\"\nThe culture shift required for AI\nWorking with AI vendors \"can get you started and build important initial momentum, but the heavy lifting will be in threading AI solutions into your processes, policies, practices, and of course, your people and culture,\" said McDonagh-Smith."
    },
    {
      "url": "https://www.zdnet.com/article/anthropic-will-start-training-claude-on-user-data-but-you-dont-have-to-share-yours/",
      "text": "Anthropic will start training Claude on user data - but you don't have to share yours\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Anthropic updated its AI training policy.\n- Users can now opt in to having their chats used for training.\n- This deviates from Anthropic's previous stance.\nAnthropic has become a leading AI lab, with one of its biggest draws being its strict position on prioritizing consumer data privacy. From the onset of Claude, its chatbot, Anthropic took a stern stance about not using user data to train its models, deviating from a common industry practice. That's now changing.\nUsers can now opt into having their data used to train the Anthropic models further, the company said in a blog post updating its consumer terms and privacy policy. The data collected is meant to help improve the models, making them safer and more intelligent, the company said in the post.\nAlso: Anthropic's Claude Chrome browser extension rolls out - how to get early access\nWhile this change does mark as a sharp pivot from the company's typical approach, users will still have the option to keep their chats out of training. Keep reading to find out how.\nWho does the change affect?\nBefore I get into how to turn it off, it is worth noting that not all plans are impacted. Commercial plans, including Claude for Work, Claude Gov, Claude for Education, and API usage, remain unchanged, even when accessed by third parties through cloud services like Amazon Bedrock and Google Cloud's Vertex AI.\nThe updates apply to Claude Free, Pro, and Max plans, meaning that if you are an individual user, you will now be subject to the Updates to Consumer Terms and Policies and will be given the option to opt in or out of training.\nHow do you opt out?\nIf you are an existing user, you will be shown a pop-up like the one shown below, asking you to opt in or out of having your chats and coding sessions trained to improve Anthropic AI models. When the pop-up comes up, make sure to actually read it because the bolded heading of the toggle isn't straightforward -- rather, it says \"You can help improve Claude,\" referring to the training feature. Anthropic does clarify underneath that in a bolded statement.\nYou have until Sept. 28 to make the selection, and once you do, it will automatically take effect on your account. If you choose to have your data trained on, Anthropic will only use new or resumed chats and coding sessions, not past ones. After Sept. 28, you will have to decide on the model training preferences to keep using Claude. The decision you make is always reversible via Privacy Settings at any time.\nAlso: OpenAI and Anthropic evaluated each others' models - which ones came out on top\nNew users will have the option to select the preference as they sign up. As mentioned before, it is worth keeping a close look at the verbiage when signing up, as it is likely to be framed as whether you want to help improve the model or not, and could always be subject to change. While it is true that your data will be used to improve the model, it is worth highlighting that the training will be done by saving your data.\nData saved for five years\nAnother change to the Consumer Terms and Policies is that if you opt in to having your data used, the company will retain that data for five years. Anthropic justifies the longer time period as necessary to allow the company to make better model developments and safety improvements.\nWhen you delete a conversation with Claude, Anthropic says it will not be used for model training. If you don't opt in for model training, the company's existing 30-day data retention period applies. Again, this doesn't apply to Commercial Terms.\nAnthropic also shared that users' data won't be sold to a third party, and that it uses tools to \"filter or obfuscate sensitive data.\"\nData is essential to how generative AI models are trained, and they only get smarter with additional data. As a result, companies are always vying for user data to improve their models. For example, Google just recently made a similar move, renaming the \"Gemini Apps Activity\" to \"Keep Activity.\" When the setting is toggled on, a sample of your uploads, starting on Sept. 2, the company says it will be used to \"help improve Google services for everyone.\""
    },
    {
      "url": "https://www.zdnet.com/article/stop-using-ai-for-these-9-work-tasks-heres-why/",
      "text": "Stop using AI for these 9 work tasks - here's why\nZDNET's key takeaways\n- Sometimes an AI can cause you or your company irreparable harm.\n- Sharing confidential data with an AI could have legal consequences.\n- Don't let an AI talk to customers without supervision.\nA few weeks ago, I shared with you \"9 programming tasks you shouldn't hand off to AI - and why.\" It's full of well-reasoned suggestions and recommendations for how to avoid having an AI produce code that could ruin your whole day.\nThen, my editor and I got talking, and we realized the whole idea of \"when not to use an AI\" could apply to work in general. In this article, I present to you nine things you shouldn't use AI for while at work. This is far from a comprehensive list, but it should make you think.\nAlso: This one feature could make GPT-5 a true game changer (if OpenAI gets it right)\n\"Always keep in mind that AI isn't going to read you your Miranda Rights, wrap your personal information in legal protections like HIPAA, or hesitate to disclose your secrets,\" said LinkedIn Learning AI instructor Pam Baker, the bestselling author of ChatGPT For Dummies and Generative AI For Dummies.\n\"That goes double for work AI, which is monitored closely by your employer. Whatever you do or tell AI can and likely will be used against you at some point.\"\nTo keep things interesting, read on to the end. There, I share some fun and terrifying stories about how using AI at work can go terribly, horribly, and amusingly wrong.\nWithout further ado, here are nine things you shouldn't do with AI at work.\n1. Handling confidential or sensitive data\nThis is an easy one. Every time you give the AI some information, ask yourself how you would feel if it were posted to the company's public blog or wound up on the front page of your industry's trade journal.\nAlso: The best AI for coding in 2025 (and what not to use)\nThis concern also includes information that might be subject to disclosure regulations, such as HIPAA for health information or GDPR for personal data for folks operating in the EU.\nRegardless of what the AI companies tell you, it's best to simply assume that everything you feed into an AI is now grist for the model-training mill. Anything you feed in could later wind up in a response to somebody's prompt, somewhere else.\n2. Reviewing or writing contracts\nContracts are designed to be detailed and specific agreements on how two parties will interact. They are considered governing documents, which means that writing a bad contract is like writing bad code. Baaad things will happen.\nDo not ask AIs for help with contracts. They will make errors and omissions. They will make stuff up. Worse, they will do so while sounding authoritative, so you're more likely to use their advice.\nAlso: You can use Google's Math Olympiad-winning Deep Think AI model now - for a price\nAlso, the terms of a contract are often governed by the contract. In other words, many contracts say that what's actually in the contract is confidential, and that if you share the particulars of your agreement with any outside party, there will be dire consequences. Sharing with an AI, as discussed above, is like publishing on the front page of a blog.\nLet me be blunt. If you let an AI work on a contract and it makes a mistake, you (not it) will be paying the price for a long, long time.\n3. Using an AI for legal advice\nYou know the trope where what you share with your lawyer is protected information and can't be used against you? Yeah, your friendly neighborhood AI is not your lawyer.\nAs reported in Futurism, OpenAI CEO (and ChatGPT's principal cheerleader) Sam Altman told podcaster Theo Von that there is no legal confidentiality when using ChatGPT for your legal concerns.\nAlso: Even OpenAI CEO Sam Altman thinks you shouldn't trust AI for therapy\nEarlier, I discussed how AI companies might use your data for training and embed that data in prompt responses. However, Altman took this assertion up a notch. He suggested OpenAI is obligated to share your conversations with ChatGPT if they are subpoenaed by a court.\nJessee Bundy, a Knoxville-based attorney, amplified Altman's statement in a tweet: \"There's no legal privilege when you use ChatGPT. So if you're pasting in contracts, asking legal questions, or asking it for strategy, you're not getting legal advice. You're generating discoverable evidence. No attorney/client privilege. No confidentiality. No ethical duty. No one to protect you.\"\nShe summed up her observations with a particularly damning statement: \"It might feel private, safe, and convenient. But lawyers are bound to protect you. ChatGPT isn't, and can be used against you.\"\n4. Using an AI for health or financial advice\nWhile we're on the topic of guidance, let's hit two other categories where highly trained, licensed, and regulated professionals are available to provide advice: healthcare and finance.\nLook, it's probably fine to ask ChatGPT to explain a medical or financial concept to you as if you were a five-year-old. But when it comes time to ask for real advice that you plan on considering as you make major decisions, just don't.\nLet's step away from the liability risk issues and focus on common sense. First, if you're using something like ChatGPT for real advice, you have to know what to ask. If you're not trained in these professions, you might not know.\nAlso: What Zuckerberg's 'personal superintelligence' sales pitch leaves out\nSecond, ChatGPT and other chatbots can be spectacularly, overwhelmingly, and almost unbelievably wrong. They misconstrue questions, fabricate answers, conflate concepts, and generally provide questionable advice.\nAsk yourself, are you willing to bet your life or your financial future on something that a people-pleasing robot made up because it thought that's what you wanted to hear?\n5. Presenting AI-generated work as your own\nWhen you ask a chatbot to write something for you, do you claim it as your own? Some folks have told me that because they wrote the prompts, the resulting output is a result of their creativity.\nAlso: I found 5 AI content detectors that can correctly identify AI text 100% of the time\nYeah? Not so much. Webster's defines \"plagiarize\" as \"to steal and pass off (the ideas or words of another) as one's own,\" and to \"use (another's production) without crediting the source.\" The dictionary also defines plagiarize as \"to commit literary theft: present as new and original an idea or product derived from an existing source.\"\nDoes that not sound like what a chatbot does? It sure does \"present as new and original an idea\u2026derived from an existing source.\" Chatbots are trained on existing sources. They then parrot back those sources after adding a bit of spin.\nLet's be clear. Using an AI and saying its output is yours could cost you your job.\n6. Talking to customers without monitoring the chatter\nThe other day, I had a technical question about my Synology server. I filed a support ticket after hours. A bit later, I got an email response from a self-identified support AI. The cool thing was that the answer was complete and just what I needed, so I didn't have to escalate my ticket to a human helper.\nAlso: Is AI overhyped or underhyped? 6 tips to separate fact from fiction\nBut not all AI interactions with customers go that well. Even a year and a half later, I'm still chuckling about the Chevy dealer chatbot that offered a $55,000 Chevy Tahoe truck to a customer for a buck.\nIt's perfectly fine to provide a trained chatbot as one support option to customers. But don't assume it's always going to be right. Ensure customers have the option to talk with a human. And monitor the AI-enabled process. Otherwise, you could be giving away $1 trucks, too.\n7. Making final hiring and firing solutions\nAccording to a survey by resume-making app Resume Builder, a majority of managers are using AI \"to determine raises (78%), promotions (77%), layoffs (66%), and even terminations (64%).\"\n\"Why are you firing me?\"\n\"It's not my fault. The AI made me do it.\"\nYeah, that. Worse, apparently at least 20% of managers, most of whom haven't been trained in the rights and wrongs of AI usage, are using AIs to make final employment decisions without even bothering to oversee the AI.\nAlso: Open-source skills can save your career when AI comes knocking\nBut here's the rub. Jobs are often governed by labor laws. Despite the current anti-DEI push coming from Washington, bias can still lead to discrimination lawsuits. Even if you haven't technically done anything wrong, defending against a lawsuit can be expensive.\nIf you cause your company to be on the receiving end of a lawsuit because you couldn't be bothered to be human enough to double-check why your AI was canning Janice in accounting, you'll be the next one being handed a pink slip. Don't do it. Just say no.\n8. Responding to journalists or media inquiries\nI'm going to tell you a little secret. Journalists and writers do not exist solely to promote your company. We'd like to help, certainly. It feels good knowing we're helping folks grow their businesses. But, and you'll need to sit down for this news, there are other companies.\nWe are also busy. I get thousands of emails every day. Hundreds of them are about the newest and by far most innovative AI company ever. Many of those pitches are AI-generated because the PR folks couldn't be bothered to take the time to focus their pitch. Some of them are so bad that I can't even tell what the PRs are trying to hawk.\nBut then, there's the other side. Sometimes, I'll reach out to a company, willing to use my most valuable resource -- time -- on their behalf. When I get back a response that's AI-driven, I'll either move on to the next company (or mock them on social media).\nAlso: 5 entry-level tech jobs AI is already augmenting, according to Amazon\nSome of those AI-driven answers are really, really inappropriate. However, because the AI is representing the company instead of, you know, maybe a thinking human, an opportunity is lost.\nKeep in mind that I don't like publishing things that will cost someone their job. But other writers are not necessarily similarly inclined. A properly run business will not only use a human to respond to the press, but will also limit the humans allowed to represent the company to those properly experienced in what to say.\nOr go ahead and cut corners. I always need fun fodder for my Facebook feed.\n9. Using AI for coding without a backup\nEarlier, I wrote \"9 programming tasks you shouldn't hand off to AI,\" which detailed programming tasks you should avoid passing along to an AI. I've long been nervous about ceding too much responsibility to an AI, and quite concerned about managing codebase maintenance.\nBut I didn't really understand how far stupid could go when it came to delegating coding responsibility to the AI. I mean, yes, I know AIs can be stupid. And I sure know humans can be stupid. But when AIs and humans work in tandem to advance the cause of their stupidity together, the results can be truly awe-inspiring.\nIn \"Bad vibes: How an AI agent coded its way to disaster,\" my ZDNET colleague Steven Vaughan-Nichols wrote about a developer who happily vibe-coded himself to an almost-complete piece of software. First, the AI hard-coded lies about how unit tests performed. Then the AI deleted his entire codebase.\nIt's not necessarily wrong to use AI to help you code. But if you're using a tool that can't be backed up, or you don't bother to back up your code first, you're simply doing your best to earn a digital Darwin award.\nBonus: Other examples of what not to do\nHere's a lightning round of boneheaded moves using AI. They're just too good (and by good, I mean bad) not to recount:\n- Letting a chatbot manage job applicant data: Remember how we told you not to use an AI for hiring and firing? McDonald's uses a chatbot to screen applicants. Apparently, the chatbot exposed millions of applicants' personal information to a hacker who used the password 123456.\n- Replacing support staff with an AI, and gloating: A CEO of e-commerce platform Dukaan terminated 90% of his support staff and replaced them with an AI. Then he bragged about it. On Twitter/X. The public response was less than positive. Way less.\n- Produce a reading list consisting of all fake titles: The Chicago Sun-Times, normally a very well-respected paper, published a summer reading list generated by an AI. The gotcha? None of the books were real.\n- Suggesting terminated employees turn to a chatbot for comfort: An Xbox producer (yes, that's Microsoft) suggested that ChatGPT or Copilot could \"help reduce the emotional and cognitive load that comes with job loss\" after Microsoft terminated 9,000 employees. Achievement unlocked.\nWhat about you? Have you seen an AI go off the rails at work? Have you ever been tempted to delegate a task to a chatbot that, in hindsight, probably needed a human touch? Do you trust AI to handle sensitive data, communicate with customers, or make decisions that affect people's lives? Where do you draw the line in your work? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/this-is-the-fastest-local-ai-ive-tried-and-its-not-even-close-how-to-get-it/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThis is the fastest local AI I've tried, and it's not even close - how to get it\nZDNET's key takeaways\n- The gpt-oss:20b model is very fast.\n- You'll get blazing-fast answers to your queries with gpt-oss:20b.\n- With the latest version of Ollama installed, you can use this model.\nGet more in-depth ZDNET tech coverage: Add us as a preferred Google source on Chrome and Chromium browsers.\nLet's talk about local AI and speed. There are a lot of factors that go into getting the most speed out of your AI, such as:\n- Whether you have a dedicated GPU.\n- The context length you use (the smaller, the faster).\n- The complexity of your query.\n- The LLM you use.\nI've tried quite a few different local LLMs, using Ollama on both Linux and MacOS, and I've recently run into one that blew all the others away -- with regard to speed. That model is gpt-oss:20b. I've found that on both Linux and MacOS, that model is lights-out faster than the others I've used. This model generates 30 tokens per second.\nAlso: My go-to LLM tool just dropped a super simple Mac and PC app for local AI - why you should try it\nWhat is a token? Think of them as pieces of words used for the processing of natural language. For example, with English text, 1 token is approximately 4 characters or 0.75 words, which means gpt-oss:20b can process 120 characters per second.\nThat's not bad.\nConsider a localized version of llama3.2, which can achieve around 14 tokens per second. See the difference?\nOK, now that I've (hopefully) convinced you that gpt-oss:20b is the way to go, how do you use it as a local LLM?\nHow to update Ollama\nWhat you'll need: To make this work, you'll need either a running version of Ollama (it doesn't matter what desktop OS you're using) or you'll need to install it fresh.\n1. Update Ollama on Linux\nIf you're using Linux, you can update Ollama with the same command used to install it, which is:\ncurl -fsSL https://ollama.com/install.sh | sh\n2. Update Ollama on MacOS or Windows\nTo update Ollama on either MacOS or Windows, you would simply download the binary installer, launch it, and follow the steps as described in the wizard. If you get an error that it cannot be installed because Ollama is still running, you'll need to stop Ollama before running the installer. To stop Ollama, you can either find it in your OS's process monitor or run the command:\nosascript -e 'tell app \"Ollama\" to quit'\nOn Windows, that command would be:\ntaskkill /im ollama.exe /f\nYou might run into a problem. If, after upgrading, you get an error (when pulling gpt-oss) that you need to run the latest version of Ollama, you'll have to install the latest iteration from the Ollama GitHub page. How you do that will depend on which OS you use.\nAlso: How I feed my files to a local AI for better, more relevant responses\nIt is necessary to be running at least Ollama version 0.11.4 to use the gpt-oss models.\nHow to pull the gpt-oss LLM\nThe next step is to pull the LLM from the command line. Remember, the model we're looking for is gpt-oss:20b, which is roughly 13GB in size. There's also the larger model, gpt-oss:120b, but that one requires over 60 GB of RAM to function properly. If your machine has less than 60 GB of RAM, stick with 20b.\nAlso: How to run DeepSeek AI locally to protect your privacy - 2 easy ways\nTo pull the LLM, run the following command (regardless of OS):\nollama pull gpt-oss:20b\nDepending on your network speed, this will take a few minutes to complete.\nHow to use gpt-oss\nOK, now that you've updated Ollama and pulled the LLM, you can use it. If you interact with Ollama from the command line, run the model with:\nollama run gpt-oss:20b\nOnce you're at the Ollama console, you can start querying the newly added LLM.\nIf you use the Ollama GUI app (on MacOS or Windows), you should be able to select gpt-oss:20b from the model drop-down in the app.\nAlso: I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\nAnd that's all there is to making use of the fastest local LLM I've tested to date."
    },
    {
      "url": "https://www.zdnet.com/home-and-office/networking/how-a-legacy-hardware-company-reinvented-itself-in-the-ai-age/",
      "text": "How a legacy hardware company reinvented itself in the AI age\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Behind-the-scenes providers such as Cisco keep the cloud and internet alive.\n- The 40-year-old company now positions itself as an AI infrastructure vendor.\n- The challenge: proactively supporting millions, if not billions of devices worldwide.\nIt's the nature of the market beast -- think about all the formerly booming tech providers that have disappeared over the years, either by acquisition or collapse: Digital Equipment Corporation, Wang, Compaq, just to name a few.\nYet, there are some that have evolved and adapted quite aggressively through the decades -- Microsoft from its personal computer roots to Azure and Copilot; Google from simple search engine to Google AI; Amazon from online bookseller to Amazon Web Services, Amazon Bedrock, and Amazon SageMaker; and Adobe from PostScript and PDFs to Firefly.\nAlso: Cisco rolls out AI agents to automate network tasks at 'machine speed' - with IT still in control\nI feel as though some of the backbone companies we rely on to keep the internet and cloud running -- Cisco and IBM come to mind -- have taken a backseat, at least in terms of attention, to all the razzle-dazzle of the AI age. After all, AI would be nothing more than a disconnected, dysfunctional spreadsheet without the massive, sophisticated global infrastructure needed to support it.\nSo when I had the opportunity to talk to Cisco Systems recently, I was more than curious about how the 40-year-old networking hardware company -- bellwether of the industry in the 1990s -- has been reinventing itself in the AI age. Ultimately, it is purposing AI to help companies build their AI systems.\nThe company has positioned itself as an AI-driven infrastructure and services provider, especially when it comes to supporting the millions of hardware devices at customer sites across the globe, said Liz Centoni, executive vice president and chief customer experience officer at Cisco.\nAlso: AI is returning to Taco Bell and McDonalds drive-thrus - will customers bite this time?\nI recently had the opportunity to sit down with Centoni, who has been with Cisco for 20 years, and learned about Cisco's journey from network hardware provider to AI systems provider. (I also spoke with Centoni about the emerging role of customer experience officers.)\nCisco deserves more respect\nApparently, according to Cisco's latest earnings and related media reports, things are going very, very well. \"Cisco Systems deserves more respect in AI, and its quarterly results prove it,\" wrote CNBC's Jeff Marks in a recent article. The company's most recent quarterly earnings report shows about $14.7 billion in revenue, an 8% jump over last year's quarter. During fiscal year 2025, the company saw more than $2 billion in orders for its AI infrastructure systems, the Wall Street Journal reports, well over its initial $1 billion goal.\nSuch transformation starts from the inside out. Internally, Cisco has embraced an AI-first approach, said Jeetu Patel, president and chief product officer for Cisco, in an interview with CXOTalk. This includes transforming \"the way that we build product, the way that our products get used by customers, the way that we actually get jobs done within the company,\" AI is now behind the company's responses to support tickets, in reducing overhead costs, accelerating sales meetings, and handling legal and accounting processes involved with sales. Because of AI, Cisco \"is an entirely different company than what it used to be three years ago,\" Patel said.\nAlso: How Cisco plans to stop rogue AI agent attacks inside your network\nAt the core of any and all AI transformations is the need to up the customer experience -- AI needs to be used as a tool to better understand what customers want, and be able to fix any problems quickly, or even before they happen.\nWhile not a direct Cisco customer myself, I admit I have been quite impressed lately with my internet and stream providers' responsiveness (I use both Verizon and Comcast) to issues -- glitches seem to self-heal quickly, and phone calls to support start off with automated remote pings to my equipment to rectify problems. (I hope I didn't just jinx myself!)\nCentoni said that such AI-driven service, the ability to quickly address problems without much fuss, is at the core of Cisco's service philosophy.\n'We're an AI infrastructure company'\n\"We started out as a networking company, and we've grown into security, helping customers change their architectures to modernize their infrastructure,\" she said. \"Now, we're helping customers with AI infrastructure. We're an AI infrastructure company. We help them build the stack on which to run AI, along with training and inference.\"\nWhile it may seem that the world is advancing en masse into AI, Centoni observed that many of Cisco's customers are just starting to use AI in a comprehensive way. Then, there are some who are \"building the large language models and running AI workloads. We help them with the services they can offer to their customers, the use cases, and finding what they want to solve with AI.\"\nAlso: I found the easiest way to delete myself from the internet - and it's fast\nAI -- and AI agents -- are helping quickly unravel configuration and network issues, as well as help customers with expansion plans, she continued. \"This means that they can reduce the amount of manpower time involved. They add a new branch to their campus, for example, but do it with higher confidence and lower risk.\" For the future, the company is exploring agentic AI that can answer customers' questions. \"Even if the models not completely trained, they can ask it questions like 'where does this VM sit?'\"\nAn example of AI in action is Cisco's approach to firewall security, which incorporates natural language processing inputs from administrators to address issues, making it faster and easier to address potential security threats. Centoni noted: \"How do we make it simple? Because if you don't make it simple, your teams will find a way to go around it, and security is not the space you want them to go around it.\"\n40 years' worth of customer networking data\nThe goal is to train models to \"help us understand that customer, in a much deeper way, than any human can,\" Centoni said. Cisco has 40 years' worth of customer networking data within its knowledgebase. \"It's looking at the entire account history. It's looking at all of their telemetry and gives you alerts before it even hits the customer. And it can provide actions, and recommendations. It goes beyond the break/fix.\"\nAlso: Here's why network infrastructure is vital to maximizing your company's AI adoption\nThe company's goal is to employ AI to \"engage with customers in a bigger way,\" she said. \"And to start to think about things we could solve for our customers, to get more proactive, and address everything in real time.\"\nCisco is an example of a backbone company that, while not at the forefront of the AI excitement, is keeping calm and carrying on, thank you. The lesson for everyone -- career professional, entrepreneur, and Fortune 500 executive -- is in the power to adapt, and ensure those adaptations are all about the customer."
    },
    {
      "url": "https://www.zdnet.com/article/how-to-use-gpt-5-in-vs-code-with-github-copilot/",
      "text": "How to use GPT-5 in VS Code with GitHub Copilot\nZDNET's key takeaways\n- GitHub Copilot Pro now supports GPT-5 in VS Code.\n- A 30-day trial lets you test premium models for free.\n- Add your OpenAI key to bypass Copilot restriction.\nGPT-5 is now available for use with Microsoft's GitHub Copilot in VS Code. In this article, I'll walk you through the steps of setting up the linkages between VS Code, Copilot, and GPT-5. This process will also work for most other supported large language models you want to use.\nAlso: Microsoft rolls out GPT-5 across its Copilot suite - here's where you'll find it\nStep 1: Enable GitHub Copilot Pro\nYou'll need the Pro version of GitHub Copilot to use GPT-5 at this time. The new model might be available for Copilot's free tier someday, but not yet. There is, however, a 30-day free trial. I'll show you how to set that up here:\nFirst, open VS Code. Click the little Copilot icon (1). That action will open the Copilot pane. Next, click whatever model is listed at (2). Mine is GPT-4.1. Finally, click Add Premium Models (3).\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nThis will take you to a web page where you'll be given the opportunity to try Copilot Pro for 30 days at no cost. Click the big green button.\nUnfortunately, you'll have to add your credit card info, although it won't be charged for 30 days. To prevent abuse of the 30-day limit, Microsoft requires you to give your personal information and your credit card number:\nOnce you've done that, click the Activate Now button:\nStep 2: Enable GPT-5\nYou'll need to restart VS Code for the Pro mode to be made available. Once you do that, click the current model (in the screenshot, it's GPT-4.1), and then scroll and choose GPT-5:\nYou will then need to issue a prompt. In my Deep Research dump of my repository, I was told about some references to a product I sold a few years ago that were still in the product's code. I told GPT-5 to remove all such references.\nThat process resulted in the Enable button showing up. Basically, I think you can use any prompt with GPT-5 to trigger the Enable button. Then click it:\nStep 3: Bring your own key\nBy using the Pro account, you are given a certain number of times you can use the various models. It's unclear how that usage limit is calculated, so I have reached out to Microsoft for clarification. I'll update this article when I get more info.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nIf you want to bypass the possible restrictions and rate limitations, you can use your own API key as provided by your LLM service. You can learn more from Microsoft's language models page.\nWith ChatGPT, you can get an OpenAI Platform Key by pointing your browser here. If you don't already have an OpenAI account, you may need to give them some credit card information as well. Then click Create Key and follow the directions:\nOnce you have your key, go back to Manage Models (by clicking the current model you're using and choosing Manage Models). Select OpenAI:\nType or paste in your key. Press Enter to confirm:\nCongratulations, you're now running GPT-5 in Copilot.\nHave you used Copilot?\nHave you tried using GPT-4.1 or GPT-5 in VS Code yet? What do you think of the Copilot Pro experience so far? Does it feel like a worthwhile upgrade? Have you explored using your API key instead of relying on Microsoft's allocation?\nAlso: The best AI for coding in 2025 (and what not to use)\nWhat kinds of tasks or prompts have you found AI particularly helpful for in your coding work? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/what-is-chatgpt-how-the-worlds-most-popular-ai-chatbot-can-benefit-you/",
      "text": "What is ChatGPT? How the world's most popular AI chatbot can benefit you\nWhat is ChatGPT?\nChatGPT is an AI chatbot with advanced natural language processing (NLP) that allows you to have human-like conversations to complete various tasks. The generative AI tool can answer questions and assist you with composing text, code, and much more.\nAlso: How to use ChatGPT\nIt's currently open to use for free. OpenAI launched a paid subscription version called ChatGPT Plus in February 2023, which guarantees users access to the company's latest models, exclusive features, and updates but is not necessary for basic usage.\nWhat is ChatGPT used for?\nChatGPT offers many functions in addition to answering simple questions. ChatGPT can compose essays, have philosophical conversations, do math, and even code for you.\nThe tasks ChatGPT can help with don't have to be so ambitious. For example, my favorite use case for ChatGPT is to help create basic lists for chores, such as packing and grocery shopping, and to-do lists that make my daily life more productive. The possibilities are endless.\nZDNET has published many ChatGPT how-to guides. Below are some of the most popular ones.\nUse ChatGPT to:\n- Write an essay\n- Create an app\n- Write code\n- Build your resume\n- Write Excel formulas\n- Summarize content\n- Write a cover letter\n- Start an Etsy business\n- Create charts and tables\n- Browse the web\n- Create custom AI assistants\n- Analyze PDFs\n- Digitize handwritten notes\n- Write Arduino drivers\nIs ChatGPT available for free?\nChatGPT is free, regardless of what you use it for, including writing, coding, and much more.\nThere is a subscription option, ChatGPT Plus, that costs $20 per month. The paid subscription model gives you extra perks, such as priority access to GPT-4o, DALL-E 3, unlimited photogeneration, Canvas, Voice Mode, and the latest upgrades.\nAlso: ChatGPT vs ChatGPT Plus: Is it worth the subscription fee?\nAlthough the subscription price may seem steep, it is the same amount as Microsoft Copilot Pro and Google One AI Premium, which are Microsoft's and Google's paid AI offerings.\nHow can you access ChatGPT?\nOn April 1, 2024, OpenAI stopped requiring you to log in to ChatGPT. Now, you can access ChatGPT simply by visiting chat.openai.com. You can also access ChatGPT via an iPhone, Android, or desktop app.\nOnce you visit the site, you can start chatting away with ChatGPT. A great way to get started is by asking a question, similar to what you would do with Google. You can ask as many questions as you'd like.\nAlso: ChatGPT no longer requires a login, but you might want one anyway. Here's why\nCreating an OpenAI account still offers some perks, such as saving and reviewing your chat history, accessing custom instructions, and, most importantly, getting free access to GPT-4o. Signing up is free and easy; you can use your existing Google login.\nFor step-by-step instructions, check out ZDNET's guide on how to start using ChatGPT.\nIs there a ChatGPT app?\nYes, an official ChatGPT app is available for iPhone and Android users. Make sure to download OpenAI's app, as many copycat fake apps are listed on Apple's App Store and the Google Play Store that are not affiliated with OpenAI.\nAlso: I changed 5 ChatGPT settings and instantly became more productive - here's how\nThere's also a ChatGPT app for MacOS and Windows, which lets you access the chatbot quickly via a keyboard shortcut. The Mac app was initially only available for ChatGPT Plus subscribers, but OpenAI recently made it available to all users. The Windows app is still limited to ChatGPT Plus, Enterprise, and EDU users.\nIs ChatGPT safe?\nPeople have expressed concerns about AI chatbots replacing or atrophying human intelligence.\nFor example, chatbots can write an entire essay in seconds, raising concerns about students cheating and not learning how to write properly. These fears even led some school districts to block access when ChatGPT initially launched.\nNow, not only have many of those schools decided to unblock the technology, but some higher education institutions have been catering their academic offerings to AI-related coursework.\nAlso: Two ways you can build custom AI assistants with GPT-4o - and one is free\nAnother concern with AI chatbots is the possible spread of misinformation. ChatGPT says: \"My responses are not intended to be taken as fact, and I always encourage people to verify any information they receive from me or any other source.\" OpenAI also notes that ChatGPT sometimes writes \"plausible-sounding but incorrect or nonsensical answers.\"\nLastly, there are ethical and privacy concerns regarding the information ChatGPT was trained on. OpenAI scraped the internet to train the chatbot without asking content owners for permission to use their content, which brings up many copyright and intellectual property concerns.\nAlso: How to use ChatGPT to analyze PDFs for free\nThere are also privacy concerns regarding generative AI companies using your data to fine-tune their models further, which has become a common practice. OpenAI lets you turn off training in ChatGPT's settings.\nSo, is ChatGPT safe? If your main concern is privacy, OpenAI has implemented several options to give users peace of mind that their data will not be used to train models. The company even allows you to turn off your chat history. If you are concerned about moral and ethical problems, they are still being hotly debated.\nWill my conversations with ChatGPT be used for training?\nCompanies train generative AI models on user inputs. Therefore, when familiarizing yourself with how to use ChatGPT, you might wonder if your specific conversations will be used for training and, if so, who can view your chats.\nAlso: This ChatGPT update fixed one of my biggest productivity issues with the AI chatbot\nOpenAI will, by default, use your conversations with the free chatbot to train data and refine its models. You can opt out of it using your data for model training by clicking on the question mark in the bottom left-hand corner, Settings, and turning off \"Improve the model for everyone.\"\nCan ChatGPT help me apply for jobs?\nYes, ChatGPT is a great resource for helping with job applications. Undertaking a job search can be tedious and difficult, and ChatGPT can help you lighten the load. ChatGPT can build your resume and write a cover letter.\nAlso: How to use ChatGPT to write an essay\nIf your application has any written supplements, you can use ChatGPT to help you write those essays or personal statements. You can also use ChatGPT to prep for your interviews by asking ChatGPT to provide you mock interview questions, background on the company, or questions that you can ask.\nCan you use ChatGPT for schoolwork?\nChatGPT can be an excellent resource in assisting students with their work. A popular misconception is that ChatGPT and other AI resources will do students' work for them. However, it can be used as a personal tutor or editor, giving students assistance outside of the classroom.\nAlso: 5 free AI tools for school that students, teachers, and parents can use, too\nFor example, a student can drop their essay into ChatGPT and have it copyedit, upload class handwritten notes and have them digitized, or even generate study outlines from class materials.\nIt is especially helpful for coding homework since most coding languages are very character-sensitive, and one missing semicolon can throw the entire result off. Instead of staring at the screen for ages, you can ask ChatGPT to identify errors for you, which allows you to grow.\nWhat does ChatGPT stand for?\nThe last three letters in ChatGPT's namesake stand for Generative Pre-trained Transformer (GPT), a family of large language models created by OpenAI that uses deep learning to generate human-like, conversational text.\nAlso: What does GPT stand for? Understanding GPT 3.5, GPT 4, GPT-4 Turbo, and more\nThe \"Chat\" part of the name is simply a callout to its chatting capabilities.\nCan ChatGPT generate images?\nYes, ChatGPT can generate images using DALL-E 3. However, the limits vary depending on whether you are a ChatGPT Plus subscriber or a free user. ChatGPT Plus subscribers are allowed 50 generations per day, while free users are allotted two.\nAlso: The best AI image generators of 2024: Tested and reviewed\nSince OpenAI discontinued DALL-E 2 in February 2024, the only way to access its most advanced AI image generator, DALL-E 3, through OpenAI's offerings is via its chatbot.\nMicrosoft's Copilot offers free image generation, also powered by DALL-E 3, in its chatbot. This is a great alternative if you don't want to pay for ChatGPT Plus but want high-quality image outputs for free.\nHow does ChatGPT work?\nChatGPT runs on a large language model (LLM) architecture created by OpenAI called the Generative Pre-trained Transformer (GPT). Since its launch, the free version of ChatGPT ran on a fine-tuned model in the GPT-3.5 series until May 2024, when OpenAI upgraded the model to GPT-4o. Now, the free version runs on GPT-4o mini, with limited access to GPT-4o.\nAlso: How does ChatGPT actually work?\nWith a subscription to ChatGPT Plus, you can access GPT-4, GPT-4o mini or GPT-4o. Plus, users also have priority access to GPT-4o, even at capacity, while free users get booted down to GPT-4o mini.\nThe LLMs used in generative AI models of this type are trained on vast amounts of information from the internet, including websites, books, news articles, and more, typically via web scraping, where the entirety of the internet is scraped for information.\nWho owns ChatGPT currently?\nOpenAI launched ChatGPT on November 30, 2022. OpenAI has also developed DALL-E 2 and DALL-E 3, popular AI image generators, and Whisper, an automatic speech recognition system.\nAlso: The best AI chatbots: ChatGPT, Copilot and worthy alternatives\nAs a result, OpenAI owns ChatGPT. Microsoft is a major investor in OpenAI thanks to multiyear, multi-billion dollar investments. Elon Musk was an investor when OpenAI was first founded in 2015 but has since completely severed ties with the startup and created his own AI chatbot,Grok.\nIs ChatGPT better than a search engine?\nChatGPT is an AI chatbot created to converse with the end user. A search engine indexes web pages on the internet to help users find information. One is not better than the other, as each suits different purposes.\nWhen searching for as much up-to-date, accurate information as possible, your most reliable option is a search engine. It will provide you with pages upon pages of sources you can peruse.\nAlso: ChatGPT has officially replaced Google Search for me - here's why\nIn November 2024, OpenAI already unveiled ChatGPT Search, a feature within the ChatGPT app that lets users search the web for timely, up-to-date information, complete with citations linked to sources.\nWith ChatGPT Search, you can enter your sentence as your train of thought takes you, and the AI will understand the meaning of your query by leveraging its NLP capabilities. This means you can spend less time crafting a tailored search query but still get exactly what you want.\nIf you are looking for a platform that can explain complex topics in an easy-to-understand manner, then ChatGPT might be what you want. If you want the best of both worlds, plenty of AI search engines combine both.\nThe search experience is available on the ChatGPT website, desktops, and mobile apps for all ChatGPT Plus, Team users, and SearchGPT waitlist users.\nWhat is SearchGPT?\nSearchGPT is an experimental offering from OpenAI that functions as an AI-powered search engine that is aware of current events and uses real-time information from the Internet. The experience was a prototype. OpenAI integrated the best features directly into ChatGPT with ChatGPT Search.\nUpon launching the prototype, users were given a waitlist to sign up for. However, shortly after, OpenAI closed its waitlist. Those users were given priority access to ChatGPT Search when it launched.\nWhat is ChatGPT Search?\nIn November, OpenAI unveiled ChatGPT Search, a feature that lets users search the web directly within ChatGPT for timely, up-to-date information, complete with citations linked to sources. The tool can be called on manually or activated whenever a user prompt could benefit from web-based information.\nWhat are ChatGPT's limitations?\nDespite its impressive capabilities, ChatGPT still has limitations. Users sometimes need to reword questions multiple times for ChatGPT to understand their intent. A bigger limitation is a lack of quality in responses, which can sometimes be plausible-sounding but are verbose or make no practical sense.\nAlso: Police are using AI to write crime reports. What could go wrong?\nInstead of asking for clarification on ambiguous questions, the model guesses what your question means, which can lead to poor responses. Generative AI models are also subject to hallucinations, which can result in inaccurate responses.\nDoes ChatGPT give wrong answers?\nAs mentioned above, ChatGPT, like all language models, has limitations and can give nonsensical answers and incorrect information, so it's important to double-check the answers it gives you.\nAlso: 8 ways to reduce ChatGPT hallucinations\nOpenAI recommends you provide feedback on what ChatGPT generates by using the thumbs-up and thumbs-down buttons to improve its underlying model. You can also join the startup's Bug Bounty program, which offers up to $20,000 for reporting security bugs and safety issues.\nCan ChatGPT refuse to answer my prompts?\nAI systems like ChatGPT can and do reject inappropriate requests. The AI assistant can identify inappropriate submissions to prevent unsafe content generation.\nAlso: 6 things ChatGPT can't do (and another 20 it refuses to do)\nThese submissions include questions that violate someone's rights, are offensive, are discriminatory, or involve illegal activities. The ChatGPT model can also challenge incorrect premises, answer follow-up questions, and even admit mistakes when you point them out.\nThese guardrails are important. AI models can generate advanced, realistic content that can be exploited by bad actors for harm, such as spreading misinformation about public figures and influencing elections.\nDoes ChatGPT plagiarize?\nYes, OpenAI scraped the internet to train ChatGPT's models. Therefore, the technology's knowledge is influenced by other people's work. Since there is no guarantee that ChatGPT's outputs are entirely original, the chatbot may regurgitate someone else's work in your answer, which is considered plagiarism.\nAlso, technically speaking, if you, as a user, copy and paste ChatGPT's response, that is an act of plagiarism because you are claiming someone else's work as your own. This act could have repercussions based on the rules enforced by your workplace or educational institution.\nIs there a ChatGPT detector?\nIn short, the answer is no, not because people haven't tried, but because none do it efficiently.\nIn January 2023, OpenAI released a free tool to detect AI-generated text. Unfortunately, OpenAI's classifier tool could only correctly identify 26% of AI-written text with a \"likely AI-written\" designation. Furthermore, it provided false positives 9% of the time, incorrectly identifying human-written work as AI-produced.\nAlso: I tested 7 AI content detectors - they're getting dramatically better at identifying plagiarism\nThe tool performed so poorly that, six months after its release, OpenAI shut it down \"due to its low rate of accuracy.\" Despite the tool's failure, the startup claims to be researching more effective techniques for AI text identification.\nOther AI detectors exist on the market, including GPT-2 Output Detector, Writer AI Content Detector, and Content at Scale's AI Content Detection tool. ZDNET tested these tools, and the results were underwhelming: all three were unreliable sources for spotting AI, repeatedly giving false negatives. Here are ZDNET's full test results.\nWhat is GPT-4?\nGPT-4 is OpenAI's language model, much more advanced than its predecessor, GPT-3.5. GPT-4 outperforms GPT-3.5 in a series of simulated benchmark exams and produces fewer hallucinations.\nWhat is GPT-4o?\nGPT-4o is OpenAI's latest, fastest, and most advanced flagship model. As the name implies, GPT-4o has the same intelligence as GPT-4. However, the \"o\" in the title stands for \"omni,\" referring to its multimodal capabilities, which allow the model to understand text, audio, image, and video inputs and output text, audio, and image outputs.\nAlso: 6 ways OpenAI just supercharged ChatGPT for free users\nThe model is 50% cheaper in OpenAI's API than GPT-4 Turbo while still matching its English and coding capabilities and outperforming it in non-English language, vision, and audio understanding -- a big win for developers.\nWhat is ChatGPT o1?\nIn September 2024, OpenAI unveiled its o1 models, which are capable of more advanced reading, making them ideal for math, science, and coding. For example, it scored 83% on the International Mathematics Olympiad (IMO) qualifying exam. For comparison, GPT-4o correctly solved only 13% of problems.\nAlso: What are o1 and o1-mini? OpenAI's mystery AI models are finally here\nZDNET's David Gewirtz put o1- preview to the test and was impressed by its ability to tackle several complex tasks with lots of detail, including writing a WordPress plugin, rewriting a string function, finding an annoying bug, and more.\nWhat are the best ChatGPT alternatives?\nAlthough ChatGPT gets the most buzz, other options are just as good -- and might even be better suited to your needs. ZDNET has created a list of the best chatbots, all of which we have tested to identify the best tool for your requirements.\nAlso: Claude AI can now analyze PDFs - here's how to try it\nDespite ChatGPT's extensive abilities, other chatbots have advantages that might be better suited for your use case, including Copilot, Claude, Perplexity, Jasper, and more.\nWhat are GPTs?\nOpenAI once offered plugins for ChatGPT to connect to third-party applications and access real-time information on the web. The plugins expanded ChatGPT's abilities, allowing it to assist with many more activities, such as planning a trip or finding a place to eat.\nAlso: My two favorite ChatGPT Plus features and the remarkable things I can do with them\nHowever, on March 19, 2024, OpenAI stopped letting users install new plugins or start new conversations with existing ones. Instead, OpenAI replaced plugins with GPTs, which are easier for developers to build.\nWith the latest update, all users, including those on the free plan, can access the GPT Store and find 3 million customized ChatGPT chatbots. Unfortunately, there is also a lot of spam in the GPT store, so be careful which ones you use.\nWhat is Microsoft's involvement with ChatGPT?\nMicrosoft was an early investor in OpenAI, the AI startup behind ChatGPT, long before ChatGPT was released to the public. Microsoft's first involvement with OpenAI was in 2019 when the company invested $1 billion. The company invested another $2 billion in the years after. In January 2023, Microsoft extended its partnership with OpenAI through a multiyear, multi-billion dollar investment.\nAlso: ChatGPT vs. Copilot: Which AI chatbot is better for you?\nNeither company disclosed the investment value, but unnamed sources told Bloomberg that it could total $10 billion over multiple years. In return, OpenAI's exclusive cloud-computing provider -- Microsoft Azure, powers all OpenAI workloads across research, products, and API services.\nMicrosoft has also used its OpenAI partnership to revamp its Bing search engine and improve its browser. On February 7, 2023, Microsoft unveiled a new Bing tool, now known as Copilot, that runs on OpenAI's GPT-4, customized specifically for search.\nHow does Copilot compare to ChatGPT?\nCopilot uses OpenAI's GPT-4, which means that since its launch, it has been more efficient and capable than the standard, free version of ChatGPT, which was powered by GPT 3.5 at the time. At the time, Copilot boasted several other features over ChatGPT, such as access to the internet, knowledge of current information, and footnotes.\nAlso: I was a Copilot diehard until ChatGPT added these 5 features\nIn May 2024, however, OpenAI supercharged the free version of its chatbot with GPT-4o. The upgrade gave users GPT-4 level intelligence, the ability to get responses from the web, analyze data, chat about photos and documents, use GPTs, and access the GPT Store and Voice Mode. After the upgrade, ChatGPT reclaimed its crown as the best AI chatbot.\nWhat is Gemini and how does it relate to ChatGPT?\nGemini is Google's AI chat service, a rival to ChatGPT. On February 6, 2023, Google introduced its experimental AI chat service, which was then called Google Bard.\nAlso: ChatGPT vs. Microsoft Copilot vs. Gemini: Which is the best AI chatbot?\nOver a month after the announcement, Google began rolling out access to Bard first via a waitlist. Now, the tool is available to the public. The biggest perk of Gemini is that it has Google Search at its core and has the same feel as Google products. Therefore, if you are an avid Google user, Gemini might be the best AI chatbot for you.\nWhat is Apple's involvement with OpenAI?\nAt Apple's Worldwide Developer's Conference in June 2024, the company announced a partnership with OpenAI that will integrate ChatGPT with Siri. With the user's permission, Siri can request ChatGPT for help if Siri deems a task is better suited for ChatGPT. This feature has not yet been released to users."
    },
    {
      "url": "https://www.zdnet.com/article/95-of-business-applications-of-ai-have-failed-heres-why/",
      "text": "95% of business applications of AI have failed. Here's why\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways:\n- Just 5% of enterprise customers are profiting from generative AI.\n- A bottom-up versus top-down approach can improve implementation success.\n- AI companies are making big promises in a bubble, most of which are unfulfilled.\nInvestment in generative AI may be booming, but most individual businesses using it have yet to see the payoff. In fact, a new MIT study found that 95% of enterprises attempting to harness the technology aren't seeing measurable results in revenue or growth.\nAlso: Gen AI disillusionment looms, according to Gartner's 2025 Hype Cycle report\nThe study, conducted by MIT's Networked Agents and Decentralized AI (NANDA) project, was based on interviews with over 150 business leaders and an analysis of 300 business deployments of generative AI.\n\"Just 5% of integrated AI pilots are extracting millions in value, while the vast majority remain stuck with no measurable P&L impact,\" the authors write in the report.\nIt paints a stark contrast between promises and reality: while tech developers are selling AI tools like agents as productivity boosters, NANDA's new report indicates that for all but a vanishingly small minority, the technology is having little to no effect on businesses' bottom lines. What accounts for the huge disparity?\nWhat isn't working - and what could\nIt largely boils down to a matter of bureaucratic inefficiency. Generative AI tools can provide efficiency gains in the hands of competent individuals, but when business leaders attempt to integrate them into existing, company-wide operations and workflows, they tend to throw a wrench into the organizational machinery.\nAlso: 71% of Americans fear that AI will put 'too many people out of work permanently'\nThe main reason for this, according to the report, is that the generative AI systems that most businesses are attempting to deploy internally and at scale lack the ability to seamlessly adapt with existing organizational workflows, ultimately making them more of a hindrance than an accelerant.\n\"The core barrier to scaling is not infrastructure, regulation, or talent. It is learning,\" the authors write. \"Most GenAI systems do not retain feedback, adapt to context, or improve over time.\" While an ability to remember past interactions, customize outputs to different contexts, and learn over time are all key traits of AI, the authors are specifically referring to the context of the technology's use within enterprise-scale operations.\nOne of the implications of the new study therefore seems to be that in order for businesses to make the most of generative AI, they'd do well to take a bottom-up (allowing employees to experiment and discover their optimal mode of human-AI collaboration) as opposed to a top-down approach (forcing all employees to use a particular tool in a manner that's tightly controlled by executives and supervisors).\nAlso: Stop using AI for these 9 work tasks - here's why\nAnother trend that emerged from the study was flawed prioritization in the application of generative AI. Many businesses that were failing to profit from the technology were using it for marketing and sales, while the 5% that were using it successfully tended to do so through the automation of more fine-grained and mundane \"back-office\" tasks.\nBased on their study, the authors predict that future success will belong to those businesses that deploy agentic and adaptable models in the right places, while those that choose a general, top-down approach will continue to be frustrated.\n\"The next wave of adoption will be won not by the flashiest models,\" they write, \"but by the systems that learn and remember and/or by systems that are custom built for a specific process.\"\nAI hype and cultural pressure\nOn its surface, the NANDA study seems to lend support to the belief that generative AI is nothing but a massive hype bubble that will soon pop, not unlike the short-lived corporate rush into the metaverse that preceded it. If such a massive proportion of businesses aren't seeing results, then surely that means the technology is being pedaled on empty promises, right?\nTime will tell. For now, companies across the board are doubling down on their investments in AI, promising customers and investors that the rise of more agentic systems will usher in a golden age of prosperity, creativity, and leisure. At the same time -- and on the heels of a GPT-5 launch that received mixed reviews -- OpenAI CEO Sam Altman himself said he sees an AI bubble taking shape.\nAlso: 5 ways automation can speed up your daily workflow - and implementation is easy\nMeanwhile, the widespread cultural embrace of AI means that companies are facing huge pressure to integrate the technology quickly -- or risk looking like dinosaurs. As NANDA's study indicates, this rush is, in many cases, apparently taking place at the expense of any kind of well-calculated plan, and as a result, investments in generative AI are leading many companies nowhere.\nEven at the individual level, generative AI can be counterproductive in the long-term -- even while boosting productivity in the present. A recent study conducted by Workday, for example, found a correlation between heavy use of AI at work and employee burnout, while other studies find evidence that AI use degrades critical thinking skills."
    }
  ],
  "argos_summary": "The article argues that most enterprise AI pilots fail not because of technology limits but due to flawed strategy and integration. It highlights that only about 5% of pilots deliver measurable value, stressing the need for narrow, value\u2011driven use cases that employees already use as \"shadow AI\". Successful projects focus on productivity amplification, continuous learning, and embedding AI into existing workflows rather than broad, top\u2011down cost\u2011cutting initiatives. The piece urges leaders to study grassroots AI adoption, align pilots with clear KPIs, and build scalable, knowledge\u2011capturing systems to turn AI into lasting business value.",
  "argos_id": "IEHDJPMX7"
}