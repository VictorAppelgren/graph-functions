{
  "url": "https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
  "authorsByline": "Mark Hachman",
  "articleId": "c1a9190f64744d4faa79aca20601d8ed",
  "source": {
    "domain": "pcworld.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "CA",
      "county": "San Francisco",
      "city": "San Francisco",
      "coordinates": {
        "lat": 37.7790262,
        "lon": -122.419906
      }
    }
  },
  "imageUrl": "https://www.pcworld.com/wp-content/uploads/2025/08/kevin-ache-2JJ3wBHu4_0-unsplash-derez.jpg?quality=50&strip=all&w=1024",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-21T18:12:30+00:00",
  "addDate": "2025-08-21T18:18:41.885085+00:00",
  "refreshDate": "2025-08-21T18:18:41.885087+00:00",
  "score": 1.0,
  "title": "How much power and water does AI use? Google, Mistral weigh in",
  "description": "Google published an impact report detailing how much water, energy, and carbon dioxide a typical Gemini AI query produces, following a similar report from Mistral in July.",
  "content": "How badly does AI harm the environment? We now have some answers to that question, as both Google and Mistral have published their own self-assessments of the environmental impact of an AI query.\n\nIn July, Mistral, which publishes its own AI models, published a self-evaluation of the environmental impact of training and querying its model in terms of the amount of carbon dioxide (CO2) produced, the amount of water consumed, and the amount of material consumed. Google took a slightly different approach, publishing the amount of power and water a Gemini query consumes, as well as how much CO2 it produces.\n\nOf course, there are caveats: Each report was self-generated, and not performed by an outside auditor. Also, training a model consumes vastly more resources than inferencing, or the day-to-day tasks users assign a chatbot each time they query it. Still, the reports provide some context for how much AI taxes the environment, even though they exclude the effects of AI training and inferencing by OpenAI and other competitors.\n\nOn Thursday, Google said its estimate for the resources consumed by a \u201cmedian\u201d Gemini query consumes 0.24Wh of energy and 0.26 milliliters (five drops) of water, and generates the equivalent of 0.03 grams of carbon dioxide \u2014 the equivalent of 9 seconds of watching TV. Mistral\u2019s report slightly differed: For a \u201cLe Chat\u201d response generating a page of text (400 tokens), Mistral consumes 50 milliliters of water, produces the equivalent of 1.14 grams of carbon dioxide, and consumes the equivalent of 0.2 milligrams of non-renewable resources.\n\nGoogle said \u201ccomparative models\u201d typically are a bit more lenient, and only look at the impacts of active TPU and GPU consumption. Put this way, the median Gemini text prompt uses 0.10Wh of energy, consumes 0.12ml of water, and emits the equivalent of 0.02 grams of carbon dioxide.\n\nGoogle did not release any assessments of the impact of training its Gemini models. Mistral did: In January 2025, training its Large 2 model produced the equivalent of 20.4 kilotons of carbon dioxide, consumed 281,000 cubic meters of water, and consumed 650 kilograms of resources. That\u2019s about 112 Olympic-sized swimming pools of water consumption. Using the EPA\u2019s estimate that an average car produces 4.6 metric tons of carbon dioxide annually, that works out to the annual CO2 production of 4,435 cars, too.\n\nThe environmental impact assessments assume that energy is produced via means that actually produce carbon dioxide, such as coal. \u201cClean\u201d energy, like solar, lowers that value.\n\nLikewise, the amount of water \u201cconsumed\u201d typically assumes the use of evaporative cooling, where heat is transferred from the chip or server (possibly being cooled by water as well) to what\u2019s known as an evaporative cooler. The evaporative cooler transfers heat efficiently, in the same manner as your body cools itself after a workout. As you sweat, the moisture evaporates, an endothermic reaction that pulls heat from your body. An evaporative cooler performs the same function, wicking heat from a server farm but also evaporating that water back into the atmosphere.\n\nGoogle said that it uses a holistic approach toward managing energy, such as more efficient models, optimized inferencing though models like Flash-Lite, custom-built TPUs, efficient data centers, and efficient idling of CPUs that aren\u2019t being used. Clean energy generation \u2014 such as a planned nuclear reactor \u2014 can help lower the impact numbers, too.\n\n\u201cToday, as AI becomes increasingly integrated into every layer of our economy, it is crucial for developers, policymakers, enterprises, governments, and citizens to better understand the environmental footprint of this transformative technology,\u201d Mistral\u2019s own report adds. \u201cAt Mistral AI, we believe that we share a collective responsibility with each actor of the value chain to address and mitigate the environmental impacts of our innovations.\u201d\n\nHow much water and electricity does ChatGPT consume?\n\nThe reports from Mistral and Google haven\u2019t been duplicated by other companies. EpochAI estimates that the average GPT-4o query on ChatGPT consumes about 0.3Wh of energy, based upon its estimates of the types of servers OpenAI uses.\n\nHowever, the amount of resources AI consumes can vary considerably, and even AI energy scores are rudimentary at best.\n\n\u201cIn reality, the type and size of the model, the type of output you\u2019re generating, and countless variables beyond your control\u2014like which energy grid is connected to the data center your request is sent to and what time of day it\u2019s processed\u2014can make one query thousands of times more energy-intensive and emissions-producing than another,\u201d an MIT Technology Review study found. Its estimates of 15 queries a day plus 10 images plus three 5-second videos would consume 2.9kWh of electricity, it found.\n\nStill, Mistral\u2019s study authors note that its own estimates point the way toward a \u201cscoring system\u201d where buyers and users could use these studies as a way to choose AI models with the least environmental impact. It also called upon other AI model makers to follow its lead.\n\nWhether AI is \u201cbad\u201d for the environment is still up for discussion, but the reports from Google and Mistral provide a foundation for a more reasoned discussion.",
  "medium": "Article",
  "links": [
    "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
    "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://www.reuters.com/sustainability/boards-policy-regulation/google-announces-tennessee-site-small-modular-nuclear-reactor-2025-08-18/&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
    "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://epoch.ai/gradient-updates/how-much-energy-does-chatgpt-use&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
    "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
    "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
    "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://huggingface.co/spaces/AIEnergyScore/Leaderboard&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI models",
      "weight": 0.09126505
    },
    {
      "name": "Mistral AI",
      "weight": 0.086170875
    },
    {
      "name": "other AI model makers",
      "weight": 0.08548273
    },
    {
      "name": "carbon dioxide",
      "weight": 0.07305506
    },
    {
      "name": "models",
      "weight": 0.07207009
    },
    {
      "name": "energy",
      "weight": 0.07145683
    },
    {
      "name": "water consumption",
      "weight": 0.0712938
    },
    {
      "name": "Clean energy generation",
      "weight": 0.06978857
    },
    {
      "name": "water",
      "weight": 0.069630675
    },
    {
      "name": "AI training",
      "weight": 0.06602351
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/People & Society/Social Issues & Advocacy/Green Living & Environmental Issues",
      "score": 0.94970703125
    },
    {
      "name": "/News/Technology News",
      "score": 0.91943359375
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.7197265625
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.54833984375
    },
    {
      "name": "/Business & Industrial/Energy & Utilities/Other",
      "score": 0.50390625
    },
    {
      "name": "/Business & Industrial/Energy & Utilities/Renewable & Alternative Energy",
      "score": 0.415283203125
    }
  ],
  "sentiment": {
    "positive": 0.12779143,
    "negative": 0.3304961,
    "neutral": 0.54171246
  },
  "summary": "Both Google and Mistral have published their self-assessments of the environmental impact of AI queries, with each report being self-generated and not performed by an outside auditor. Mistral, which publishes its own AI models, published a self-evaluation of its model's carbon dioxide (CO2) produced, water consumed, and material consumed. Google published the amount of power and water a Gemini query consumes, as well as how much CO2 it produces. However, these reports exclude the effects of AI training and inferencing by OpenAI and other competitors. Google did not release any assessments of the impact of training its Gemini models.",
  "shortSummary": "Google and Mistral publish self-assessments of the environmental impact of AI queries, highlighting significant impacts on energy consumption, water consumption, and CO2 production.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "186d55a3cffb4f08a40fb5294350a6ac",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
      "text": "How much energy does Google\u2019s AI use? We did the math\nAmin Vahdat\nVP/GM, AI & Infrastructure, Google Cloud\nJeff Dean\nChief Scientist, Google DeepMind and Google Research\nAI is unlocking scientific breakthroughs, improving healthcare and education, and could add trillions to the global economy. Understanding AI\u2019s footprint is crucial, yet thorough data on the energy and environmental impact of AI inference \u2014 the use of a trained AI model to make predictions or generate text or images \u2014 has been limited. As more users use AI systems, the importance of inference efficiency rises.\nThat\u2019s why we\u2019re releasing a technical paper detailing our comprehensive methodology for measuring the energy, emissions, and water impact of Gemini prompts. Using this methodology, we estimate the median Gemini Apps text prompt uses 0.24 watt-hours (Wh) of energy, emits 0.03 grams of carbon dioxide equivalent (gCO2e), and consumes 0.26 milliliters (or about five drops) of water1 \u2014 figures that are substantially lower than many public estimates. The per-prompt energy impact is equivalent to watching TV for less than nine seconds.\nAt the same time, our AI systems are becoming more efficient through research innovations and software and hardware efficiency improvements. For example, over a recent 12 month period, the energy and total carbon footprint of the median Gemini Apps text prompt dropped by 33x and 44x, respectively, all while delivering higher quality responses. These results are built on our latest data center energy emissions reductions and our work to advance carbon-free energy and water replenishment. While we\u2019re proud of the innovation behind our efficiency gains so far, we\u2019re committed to continuing substantial improvements. Here\u2019s a closer look at these ongoing efforts.\nCalculating the environmental footprint of AI at Google\nDetailed measurement lets us compare across different AI models, and the hardware and energy they run on, while enabling system-wide efficiency optimizations \u2014 from hardware and data centers to the models themselves. By sharing our methodology, we hope to increase industry-wide consistency in calculating AI\u2019s resource consumption and efficiency.\nMeasuring the footprint of AI serving workloads isn\u2019t simple. We developed a comprehensive approach that considers the realities of serving AI at Google\u2019s scale, which include:\n-\nFull system dynamic power: This includes not just the energy and water used by the primary AI model during active computation, but also the actual achieved chip utilization at production scale, which can be much lower than theoretical maximums.\n-\nIdle machines: To ensure high availability and reliability, production systems require a degree of provisioned capacity that is idle but ready to handle traffic spikes or failover at any given moment. The energy consumed by these idle chips must be factored into the total energy footprint.\n-\nCPU and RAM: AI model execution doesn't happen solely in ML accelerators like TPUs and GPUs. The host CPU and RAM also play a crucial role in serving AI, and use energy.\n-\nData center overhead: The energy consumed by the IT equipment running AI workloads is only part of the story. The infrastructure supporting these computations \u2014 cooling systems, power distribution, and other data center overhead \u2014 also consumes energy. Overhead energy efficiency is measured by a metric called Power Usage Effectiveness (PUE).\n-\nData center water consumption: To reduce energy consumption and associated emissions, data centers often consume water for cooling. As we optimize our AI systems to be more energy-efficient, this naturally decreases their overall water consumption as well.\nMany current AI energy consumption calculations only include active machine consumption, overlooking several of the critical factors discussed above. As a result, they represent theoretical efficiency instead of true operating efficiency at scale. When we apply this non-comprehensive methodology that only considers active TPU and GPU consumption, we estimate the median Gemini text prompt uses 0.10 Wh of energy, emits 0.02 gCO2e, and consumes 0.12 mL of water. This is an optimistic scenario at best and substantially underestimates the real operational footprint of AI.\nOur comprehensive methodology\u2019s estimates (0.24 Wh of energy, 0.03 gCO2e, 0.26 mL of water) account for all critical elements of serving AI globally. We believe this is the most complete view of AI\u2019s overall footprint.\nOur full-stack approach to AI \u2014 and AI efficiency\nGemini\u2019s dramatic efficiency gains stem from Google\u2019s full-stack approach to AI development \u2014 from custom hardware and highly efficient models, to the robust serving systems that make these models possible. We\u2019ve built efficiency into every layer of AI, including:\n-\nMore efficient model architectures: Gemini models are built on the Transformer model architecture developed by Google researchers, which provide a 10-100x efficiency boost over the previous state-of-the-art architectures for language modeling. We design models with inherently efficient structures like Mixture-of-Experts (MoE) and hybrid reasoning. MoE models, for example, allow us to activate a small subset of a large model specifically required to respond to a query, reducing computations and data transfer by a factor of 10-100x.\n-\nEfficient algorithms and quantization: We continuously refine the algorithms that power our models with methods like Accurate Quantized Training (AQT) to maximize efficiency and reduce energy consumption for serving, without compromising response quality.\n-\nOptimized inference and serving: We constantly improve AI model delivery for responsiveness and efficiency. Technologies like speculative decoding serve more responses with fewer chips by allowing a smaller model to make predictions that are then quickly verified by a larger model, which is more efficient than having the larger model make many sequential predictions on its own. Techniques like distillation create smaller, more efficient models (Gemini Flash and Flash-Lite) for serving that use our larger, more capable models as teachers. Faster machine learning hardware and models enable us to use more efficient larger batch sizes when handling requests, while still meeting our latency targets.\n-\nCustom-built hardware: We\u2019ve been designing our TPUs from the ground up for over a decade to maximize performance per watt. We also co-design our AI models and TPUs, ensuring our software takes full advantage of our hardware \u2014 and that our hardware is able to efficiently run our future AI software when both are ready. Our latest-generation TPU, Ironwood, is 30x more energy-efficient than our first publicly-available TPU and far more power-efficient than general-purpose CPUs for inference.\n-\nOptimized idling: Our serving stack makes highly efficient use of CPUs and minimizes TPU idling by dynamically moving models based on demand in near-real-time, rather than using a \u201cset it and forget\u201d approach.\n-\nML software stack: Our XLA ML compiler, Pallas kernels, and Pathways systems enable model computations expressed in higher-level systems like JAX to run efficiently on our TPU serving hardware.\n-\nUltra-efficient data centers: Google\u2019s data centers are among the industry\u2019s most efficient, operating at a fleet-wide average PUE of 1.09.\n-\nResponsible data center operations: We continue to add clean energy generation in pursuit of our 24/7 carbon-free ambition, while advancing our aim to replenish 120% of the freshwater we consume on average across our offices and data centers. We also optimize our cooling systems, balancing the local trade-off between energy, water, and emissions, by conducting science-backed watershed health assessments, to guide cooling type selection and limit water use in high-stress locations.\nOur commitment to efficient AI\nGemini\u2019s efficiency gains are the result of years of work, but this is just the beginning. Recognizing that AI demand is growing, we're heavily investing in reducing the power provisioning costs and water required per prompt. By sharing our findings and methodology, we aim to drive industry-wide progress toward more efficient AI. This is essential for responsible AI development.\n1. A point-in-time analysis quantified the energy consumed per median Gemini App text-generation prompt, considering data from May 2025. Emissions per prompt was estimated based on energy per prompt, and applying Google\u2019s 2024 average fleetwide grid carbon intensity. Water consumption per prompt was estimated based on energy per prompt, and applying Google\u2019s 2024 average fleetwide water usage effectiveness. These findings do not represent the specific environmental impact for all Gemini App text-generation prompts nor are they indicative of future performance.\n2. The results of the above analysis from May 2025 were compared to baseline data from the median Gemini App text-generation prompt in May 2024. Energy per median prompt is subject to change as new models are added, AI model architecture evolves, and AI chatbot user behavior develops. The data and claims have not been verified by an independent third-party."
    },
    {
      "url": "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
      "text": "Our contribution to a global environmental standard for AI\nAt Mistral AI, our mission is to bring artificial intelligence in everyone\u2019s hands. For this purpose, we have consistently advocated for openness in AI, with a unique focus on empowering organizations that want to own their AI future.\nToday, as AI becomes increasingly integrated into every layer of our economy, it is crucial for developers, policymakers, enterprises, governments and citizens to better understand the environmental footprint of this transformative technology. At Mistral AI, we believe that we share a collective responsibility with each actor of the value chain to address and mitigate the environmental impacts of our innovations.\nEven though some recent initiatives have been taken, such as the Coalition for Sustainable AI, launched during the Paris AI Action Summit in February 2025, the work to achieve here remains important. Without more transparency, it will be impossible for public institutions, enterprises and even users to compare models, take informed purchasing decisions, fill enterprises' extra-financial obligations or reduce the impacts associated with their use of AI.\nIn this context, we have conducted a first-of-its-kind comprehensive study to quantify the environmental impacts of our LLMs. This report aims to provide a clear analysis of the environmental footprint of AI, contributing to set a new standard for our industry.\nThe first comprehensive lifecycle analysis of an AI models\nAfter less than 18 months of existence, we have initiated the first comprehensive lifecycle analysis (LCA) of an AI model, in collaboration with Carbone 4, a leading consultancy in CSR and sustainability, and the French ecological transition agency (ADEME). To ensure robustness, this study was also peer-reviewed by Resilio and Hubblo, two consultancies specializing in environmental audits in the digital industry.\nIn addition to complying with the most rigorous standards*, the aim of this analysis was to quantify the environmental impacts of developing and using LLMs across three impact categories: greenhouse gas emissions (GHG), water use, and resource depletion**. Today, we are disclosing two important metrics for our industry in the long term:\n-\nthe environmental footprint of training Mistral Large 2: as of January 2025, and after 18 months of usage, Large 2 generated the following impacts:\n-\n20,4 ktCO\u2082e,\n-\n281 000 m3 of water consumed,\n-\nand 660 kg Sb eq (standard unit for resource depletion).\n-\nthe marginal impacts of inference, more precisely the use of our AI assistant Le Chat for a 400-token response - excluding users\u2019 terminals:\n-\n1.14 gCO\u2082e,\n-\n45 mL of water,\n-\nand 0.16 mg of Sb eq.\nThese figures reflect the scale of computation involved in Gen AI, requiring numerous GPUs, often in regions with carbon-intensive electricity and sometimes water stress. They also include \u201cupstream emissions\u201d \u2013 impacts from manufacturing servers for instance, and not just energy use.\nLearnings and limits of this study\nGiven the results of our study, we are convinced that the following three indicators are of great importance for users, AI developers and policy makers to fully understand and manage the environmental impacts of LLMs:\n-\nthe absolute impacts of training a model,\n-\nthe marginal impacts of inference,\n-\nand the ratio of total inference to total life-cycle impacts.\nIndicators 1 and 2 could be mandatory figures to report in order to inform the public on impacts, while indicator 3 can act as an internal indicator with optional disclosure. The latter indicator is key to grasp a complete vision of lifecycle impacts, and to ensure that models\u2019 training phases are amortized, and not wasted.\nOur study also shows a strong correlation between a model\u2019s size and its footprint. Benchmarks have shown impacts are roughly proportional to model size: a model 10 times bigger will generate impacts one order of magnitude larger than a smaller model for the same amount of generated tokens. This highlights the importance of choosing the right model for the right use case.\nIt is worth noting that this study is a first approximation, with the difficulty to make precise calculations in such an exercise when no standards exist for LLM environment accountability, and the absence of publicly available impact factors. For instance, a reliable life-cycle inventory of GPUs is yet to be made, as their embodied impacts had to be approximated but account for a significant portion of total impacts.\nTo comply with the GHG Protocol Product Standard, future audits made in the industry may follow this study\u2019s principles of using location-based approach for electricity emissions and including all significant upstream impacts\u2014 i.e., not only those from GPU electricity use, but also all other electricity consumptions (CPUs, cooling devices, etc.) and manufacturing of hardware.\nA path toward a global environmental standard for AI\nThese results point to two levers to reduce the environmental impact of LLMs.\n-\nFirst, to improve transparency and comparability, AI companies ought to publish the environmental impacts of their models using standardized, internationally recognized frameworks. Where needed, specific standards for the AI sector could be developed to ensure consistency. This could enable the creation of a scoring system, helping buyers and users identify the least carbon-, water- and material-intensive models.\n-\nSecond, from the user side, encouraging the research for efficiency practices can make a significant difference:\n-\ndeveloping AI literacy to help people use GenAI in the most optimal way,\n-\nchoosing the model size that is best adapted to users\u2019 needs,\n-\ngrouping queries to limit unnecessary computing,\nFor public institutions in particular, integrating model size and efficiency into procurement criteria could send a strong signal to the market.\nConclusion\nMoving forward, we are committed to updating our environmental impact reports in the future and participating in discussions around the development of international industry standards. We will advocate for greater transparency across the entire AI value chain and work to help AI adopters make informed decisions about the solutions that best suit their needs. The results will later be available via ADEME\u2019s Base Empreinte database, setting a new standard for future reference for transparency in the AI sector.\nBy encouraging sufficiency and efficiency practices and publishing standardized environmental impact reports, we can collectively work towards aligning the AI sector with global climate goals. This study is a humble contribution towards a more accessible and sustainable future for AI.\n\u2014\n* This study was carried out following the Frugal AI methodology developed by AFNOR and is compliant with international standards, including the Green House Gas (GHG) Protocol Product Standard and ISO 14040/44.\n** The environmental impacts were assessed using standard indicators common in Lifecycle Analyses (LCAs): greenhouse gas emissions measured by Global Warming Potential over 100 years (GWP100), water consumption measured by Water Consumption Potential (WCP), and material consumption measured by Abiotic Resource Depletion (ADP).\nADP quantifies the depletion of non-renewable resources (such as metals and minerals) by considering both the current extraction rates and the estimated reserves of each material. These values are standardized relative to Antimony\u2019s ADP, providing a uniform unit since Antimony is a scarce resource.\nFor example, extracting 1 kg of gold corresponds to an ADP of 2.35 kg Sb eq, whereas extracting 1 kg of copper corresponds to 0.000000161 kg Sb eq."
    },
    {
      "url": "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
      "text": "We did the math on AI\u2019s energy footprint. Here\u2019s the story you haven\u2019t heard.\nThe emissions from individual AI text, image, and video queries seem small\u2014until you add up what the industry isn\u2019t tracking and consider where it\u2019s heading next.\nAI\u2019s integration into our lives is the most significant shift in online life in more than a decade. Hundreds of millions of people now regularly turn to chatbots for help with homework, research, coding, or to create images and videos. But what\u2019s powering all of that?\nToday, new analysis by MIT Technology Review provides an unprecedented and comprehensive look at how much energy the AI industry uses\u2014down to a single query\u2014to trace where its carbon footprint stands now, and where it\u2019s headed, as AI barrels towards billions of daily users.\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\nWe spoke to two dozen experts measuring AI\u2019s energy demands, evaluated different AI models and prompts, pored over hundreds of pages of projections and reports, and questioned top AI model makers about their plans. Ultimately, we found that the common understanding of AI\u2019s energy consumption is full of holes.\nWe started small, as the question of how much a single query costs is vitally important to understanding the bigger picture. That\u2019s because those queries are being built into ever more applications beyond standalone chatbots: from search, to agents, to the mundane daily apps we use to track our fitness, shop online, or book a flight. The energy resources required to power this artificial-intelligence revolution are staggering, and the world\u2019s biggest tech companies have made it a top priority to harness ever more of that energy, aiming to reshape our energy grids in the process.\nMeta and Microsoft are working to fire up new nuclear power plants. OpenAI and President Donald Trump announced the Stargate initiative, which aims to spend $500 billion\u2014more than the Apollo space program\u2014to build as many as 10 data centers (each of which could require five gigawatts, more than the total power demand from the state of New Hampshire). Apple announced plans to spend $500 billion on manufacturing and data centers in the US over the next four years. Google expects to spend $75 billion on AI infrastructure alone in 2025.\nThis isn\u2019t simply the norm of a digital world. It\u2019s unique to AI, and a marked departure from Big Tech\u2019s electricity appetite in the recent past. From 2005 to 2017, the amount of electricity going to data centers remained quite flat thanks to increases in efficiency, despite the construction of armies of new data centers to serve the rise of cloud-based online services, from Facebook to Netflix. In 2017, AI began to change everything. Data centers started getting built with energy-intensive hardware designed for AI, which led them to double their electricity consumption by 2023. The latest reports show that 4.4% of all the energy in the US now goes toward data centers.\nThe carbon intensity of electricity used by data centers was 48% higher than the US average.\nGiven the direction AI is headed\u2014more personalized, able to reason and solve complex problems on our behalf, and everywhere we look\u2014it\u2019s likely that our AI footprint today is the smallest it will ever be. According to new projections published by Lawrence Berkeley National Laboratory in December, by 2028 more than half of the electricity going to data centers will be used for AI. At that point, AI alone could consume as much electricity annually as 22% of all US households.\nMeanwhile, data centers are expected to continue trending toward using dirtier, more carbon-intensive forms of energy (like gas) to fill immediate needs, leaving clouds of emissions in their wake. And all of this growth is for a new technology that\u2019s still finding its footing, and in many applications\u2014education, medical advice, legal analysis\u2014might be the wrong tool for the job or at least have a less energy-intensive alternative.\nTallies of AI\u2019s energy use often short-circuit the conversation\u2014either by scolding individual behavior, or by triggering comparisons to bigger climate offenders. Both reactions dodge the point: AI is unavoidable, and even if a single query is low-impact, governments and companies are now shaping a much larger energy future around AI\u2019s needs.\nWe\u2019re taking a different approach with an accounting meant to inform the many decisions still ahead: where data centers go, what powers them, and how to make the growing toll of AI visible and accountable.\nChatGPT is now estimated to be the fifth-most visited website in the world, just after Instagram and ahead of X.\nThat\u2019s because despite the ambitious AI vision set forth by tech companies, utility providers, and the federal government, details of how this future might come about are murky. Scientists, federally funded research facilities, activists, and energy companies argue that leading AI companies and data center operators disclose too little about their activities. Companies building and deploying AI models are largely quiet when it comes to answering a central question: Just how much energy does interacting with one of these models use? And what sorts of energy sources will power AI\u2019s future?\nThis leaves even those whose job it is to predict energy demands forced to assemble a puzzle with countless missing pieces, making it nearly impossible to plan for AI\u2019s future impact on energy grids and emissions. Worse, the deals that utility companies make with the data centers will likely transfer the costs of the AI revolution to the rest of us, in the form of higher electricity bills.\nIt\u2019s a lot to take in. To describe the big picture of what that future looks like, we have to start at the beginning.\nPart One: Making the model\nBefore you can ask an AI model to help you with travel plans or generate a video, the model is born in a data center.\nRacks of servers hum along for months, ingesting training data, crunching numbers, and performing computations. This is a time-consuming and expensive process\u2014it\u2019s estimated that training OpenAI\u2019s GPT-4 took over $100 million and consumed 50 gigawatt-hours of energy, enough to power San Francisco for three days. It\u2019s only after this training, when consumers or customers \u201cinference\u201d the AI models to get answers or generate outputs, that model makers hope to recoup their massive costs and eventually turn a profit.\n\u201cFor any company to make money out of a model\u2014that only happens on inference,\u201d says Esha Choukse, a researcher at Microsoft Azure who has studied how to make AI inference more efficient.\nAs conversations with experts and AI companies made clear, inference, not training, represents an increasing majority of AI\u2019s energy demands and will continue to do so in the near future. It\u2019s now estimated that 80\u201390% of computing power for AI is used for inference.\nAll this happens in data centers. There are roughly 3,000 such buildings across the United States that house servers and cooling systems and are run by cloud providers and tech giants like Amazon or Microsoft, but used by AI startups too. A growing number\u2014though it\u2019s not clear exactly how many, since information on such facilities is guarded so tightly\u2014are set up for AI inferencing.\nAt each of these centers, AI models are loaded onto clusters of servers containing special chips called graphics processing units, or GPUs, most notably a particular model made by Nvidia called the H100.\nThis chip started shipping in October 2022, just a month before ChatGPT launched to the public. Sales of H100s have soared since, and are part of why Nvidia regularly ranks as the most valuable publicly traded company in the world.\nOther chips include the A100 and the latest Blackwells. What all have in common is a significant energy requirement to run their advanced operations without overheating.\nA single AI model might be housed on a dozen or so GPUs, and a large data center might have well over 10,000 of these chips connected together.\nWired close together with these chips are CPUs (chips that serve up information to the GPUs) and fans to keep everything cool.\nSome energy is wasted at nearly every exchange through imperfect insulation materials and long cables in between racks of servers, and many buildings use millions of gallons of water (often fresh, potable water) per day in their cooling operations.\nDepending on anticipated usage, these AI models are loaded onto hundreds or thousands of clusters in various data centers around the globe, each of which have different mixes of energy powering them.\nThey\u2019re then connected online, just waiting for you to ping them with a question.\nPart Two: A Query\nIf you\u2019ve seen a few charts estimating the energy impact of putting a question to an AI model, you might think it\u2019s like measuring a car\u2019s fuel economy or a dishwasher\u2019s energy rating: a knowable value with a shared methodology for calculating it. You\u2019d be wrong.\nIn reality, the type and size of the model, the type of output you\u2019re generating, and countless variables beyond your control\u2014like which energy grid is connected to the data center your request is sent to and what time of day it\u2019s processed\u2014can make one query thousands of times more energy-intensive and emissions-producing than another.\nAnd when you query most AI models, whether on your phone within an app like Instagram or on the web interface for ChatGPT, much of what happens after your question is routed to a data center remains a secret. Factors like which data center in the world processes your request, how much energy it takes to do so, and how carbon-intensive the energy sources used are tend to be knowable only to the companies that run the models.\nThis is true for most of the name-brand models you\u2019re accustomed to, like OpenAI\u2019s ChatGPT, Google\u2019s Gemini, and Anthropic\u2019s Claude, which are referred to as \u201cclosed.\u201d The key details are held closely by the companies that make them, guarded because they\u2019re viewed as trade secrets (and also possibly because they might result in bad PR). These companies face few incentives to release this information, and so far they have not.\n\u201cThe closed AI model providers are serving up a total black box,\u201d says Boris Gamazaychikov, head of AI sustainability at Salesforce, who has led efforts with researchers at Hugging Face, an AI platform provider of tools, models, and libraries for individuals and companies, to make AI\u2019s energy demands more transparent. Without more disclosure from companies, it\u2019s not just that we don\u2019t have good estimates\u2014we have little to go on at all.\nWithout more disclosure from companies, it\u2019s not just that we don\u2019t have good estimates\u2014we have little to go on at all.\nSo where can we turn for estimates? So-called open-source models can be downloaded and tweaked by researchers, who can access special tools to measure how much energy the H100 GPU requires for a given task. Such models are also incredibly popular; Meta announced in April that its Llama models have been downloaded more than 1.2 billion times, and many companies use open-source models when they want more control over outputs than they can get using something like ChatGPT.\nBut even if researchers can measure the power drawn by the GPU, that leaves out the power used up by CPUs, fans, and other equipment. A 2024 paper by Microsoft analyzed energy efficiencies for inferencing large language models and found that doubling the amount of energy used by the GPU gives an approximate estimate of the entire operation\u2019s energy demands.\nSo for now, measuring leading open-source models (and adding estimates for all these other pieces) gives us the best picture we have of just how much energy is being used for a single AI query. However, keep in mind that the ways people use AI today\u2014to write a grocery list or create a surrealist video\u2014are far simpler than the ones we\u2019ll use in the autonomous, agentic future that AI companies are hurling us toward. More on that later.\nHere\u2019s what we found.\nText models\nLet\u2019s start with models where you type a question and receive back a response in words. One of the leading groups evaluating the energy demands of AI is at the University of Michigan, led by PhD candidate Jae-Won Chung and associate professor Mosharaf Chowdhury, who publish energy measurements on their ML.Energy leaderboard. We worked with the team to focus on the energy demands of one of the most widely adopted open-source models, Meta\u2019s Llama.\nThe smallest model in our Llama cohort, Llama 3.1 8B, has 8 billion parameters\u2014essentially the adjustable \u201cknobs\u201d in an AI model that allow it to make predictions. When tested on a variety of different text-generating prompts, like making a travel itinerary for Istanbul or explaining quantum computing, the model required about 57 joules per response, or an estimated 114 joules when accounting for cooling, other computations, and other demands. This is tiny\u2014about what it takes to ride six feet on an e-bike, or run a microwave for one-tenth of a second.\nThe largest of our text-generation cohort, Llama 3.1 405B, has 50 times more parameters. More parameters generally means better answers but more energy required for each response. On average, this model needed 3,353 joules, or an estimated 6,706 joules total, for each response. That\u2019s enough to carry a person about 400 feet on an e-bike or run the microwave for eight seconds.\nSo model size is a huge predictor of energy demand. One reason is that once a model gets to a certain size, it has to be run on more chips, each of which adds to the energy required. The largest model we tested has 405 billion parameters, but others, such as DeepSeek, have gone much further, with over 600 billion parameters. The parameter counts for closed-source models are not publicly disclosed and can only be estimated. GPT-4 is estimated to have over 1 trillion parameters.\nBut in all these cases, the prompt itself was a huge factor too. Simple prompts, like a request to tell a few jokes, frequently used nine times less energy than more complicated prompts to write creative stories or recipe ideas.\nGenerating an image\nAI models that generate images and videos work with a different architecture, called diffusion. Rather than predicting and generating words, they learn how to transform an image of noise into, let\u2019s say, a photo of an elephant. They do this by learning the contours and patterns of pictures in their training data and storing this information across millions or billions of parameters. Video-generator models learn how to do this across the dimension of time as well.\nThe energy required by a given diffusion model doesn\u2019t depend on your prompt\u2014generating an image of a skier on sand dunes requires the same amount of energy as generating one of an astronaut farming on Mars. The energy requirement instead depends on the size of the model, the image resolution, and the number of \u201csteps\u201d the diffusion process takes (more steps lead to higher quality but need more energy).\nGenerating a standard-quality image (1024 x 1024 pixels) with Stable Diffusion 3 Medium, the leading open-source image generator, with 2 billion parameters, requires about 1,141 joules of GPU energy. With diffusion models, unlike large language models, there are no estimates of how much GPUs are responsible for the total energy required, but experts suggested we stick with the \u201cdoubling\u201d approach we\u2019ve used thus far because the differences are likely subtle. That means an estimated 2,282 joules total. Improving the image quality by doubling the number diffusion steps to 50 just about doubles the energy required, to about 4,402 joules. That\u2019s equivalent to about 250 feet on an e-bike, or around five and a half seconds running a microwave. That\u2019s still less than the largest text model.\nThis might be surprising if you imagined generating images to require more energy than generating text. \u201cLarge [text] models have a lot of parameters,\u201d says Chung, who performed the measurements on open-source text and image generators featured in this story. \u201cEven though they are generating text, they are doing a lot of work. \u201d Image generators, on the other hand, often work with fewer parameters.\nMaking a video\nVideos generated by CogVideoX, an open-source model.\nLast year, OpenAI debuted Sora, its dazzling tool for making high-fidelity videos with AI. Other closed-source video models have come out as well, like Google Veo2 and Adobe\u2019s Firefly.\nGiven the eye-watering amounts of capital and content it takes to train these models, it\u2019s no surprise that free-to-use, open-source models generally lag behind in quality. Still, according to researchers at Hugging Face, one of the best is CogVideoX, made by a Chinese AI startup called Zhipu AI and researchers from Tsinghua University in Beijing.\nSasha Luccioni, an AI and climate researcher at Hugging Face, tested the energy required to generate videos with the model using a tool called Code Carbon.\nAn older version of the model, released in August, made videos at just eight frames per second at a grainy resolution\u2014more like a GIF than a video. Each one required about 109,000 joules to produce. But three months later the company launched a larger, higher-quality model that produces five-second videos at 16 frames per second (this frame rate still isn\u2019t high definition; it\u2019s the one used in Hollywood\u2019s silent era until the late 1920s). The new model uses more than 30 times more energy on each 5-second video: about 3.4 million joules, more than 700 times the energy required to generate a high-quality image. This is equivalent to riding 38 miles on an e-bike, or running a microwave for over an hour.\nIt\u2019s fair to say that the leading AI video generators, creating dazzling and hyperrealistic videos up to 30 seconds long, will use significantly more energy. As these generators get larger, they\u2019re also adding features that allow you to tweak particular elements of videos and stitch multiple shots together into scenes\u2014all of which add to their energy demands. A note: AI companies have defended these numbers saying that generative video has a smaller footprint than the film shoots and travel that go into typical video production. That claim is hard to test and doesn\u2019t account for the surge in video generation that might follow if AI videos become cheap to produce.\nAll in a day\u2019s prompt\nSo what might a day\u2019s energy consumption look like for one person with an AI habit?\nLet\u2019s say you\u2019re running a marathon as a charity runner and organizing a fundraiser to support your cause. You ask an AI model 15 questions about the best way to fundraise.\nThen you make 10 attempts at an image for your flyer before you get one you are happy with, and three attempts at a five-second video to post on Instagram.\nYou\u2019d use about 2.9 kilowatt-hours of electricity\u2014enough to ride over 100 miles on an e-bike (or around 10 miles in the average electric vehicle) or run the microwave for over three and a half hours.\nThere is a significant caveat to this math. These numbers cannot serve as a proxy for how much energy is required to power something like ChatGPT 4o. We don\u2019t know how many parameters are in OpenAI\u2019s newest models, how many of those parameters are used for different model architectures, or which data centers are used and how OpenAI may distribute requests across all these systems. You can guess, as many have done, but those guesses are so approximate that they may be more distracting than helpful.\n\u201cWe should stop trying to reverse-engineer numbers based on hearsay,\u201d Luccioni says, \u201cand put more pressure on these companies to actually share the real ones.\u201d Luccioni has created the AI Energy Score, a way to rate models on their energy efficiency. But closed-source companies have to opt in. Few have, Luccioni says.\nPart Three: Fuel and emissions\nNow that we have an estimate of the total energy required to run an AI model to produce text, images, and videos, we can work out what that means in terms of emissions that cause climate change.\nFirst, a data center humming away isn\u2019t necessarily a bad thing. If all data centers were hooked up to solar panels and ran only when the sun was shining, the world would be talking a lot less about AI\u2019s energy consumption. That\u2019s not the case. Most electrical grids around the world are still heavily reliant on fossil fuels. So electricity use comes with a climate toll attached.\n\u201cAI data centers need constant power, 24-7, 365 days a year,\u201d says Rahul Mewawalla, the CEO of Mawson Infrastructure Group, which builds and maintains high-energy data centers that support AI.\nThat means data centers can\u2019t rely on intermittent technologies like wind and solar power, and on average, they tend to use dirtier electricity. One preprint study from Harvard\u2019s T.H. Chan School of Public Health found that the carbon intensity of electricity used by data centers was 48% higher than the US average. Part of the reason is that data centers currently happen to be clustered in places that have dirtier grids on average, like the coal-heavy grid in the mid-Atlantic region that includes Virginia, West Virginia, and Pennsylvania. They also run constantly, including when cleaner sources may not be available.\nData centers can\u2019t rely on intermittent technologies like wind and solar power, and on average, they tend to use dirtier electricity.\nTech companies like Meta, Amazon, and Google have responded to this fossil fuel issue by announcing goals to use more nuclear power. Those three have joined a pledge to triple the world\u2019s nuclear capacity by 2050. But today, nuclear energy only accounts for 20% of electricity supply in the US, and powers a fraction of AI data centers\u2019 operations\u2014natural gas accounts for more than half of electricity generated in Virginia, which has more data centers than any other US state, for example. What\u2019s more, new nuclear operations will take years, perhaps decades, to materialize.\nIn 2024, fossil fuels including natural gas and coal made up just under 60% of electricity supply in the US. Nuclear accounted for about 20%, and a mix of renewables accounted for most of the remaining 20%.\nGaps in power supply, combined with the rush to build data centers to power AI, often mean shortsighted energy plans. In April, Elon Musk\u2019s X supercomputing center near Memphis was found, via satellite imagery, to be using dozens of methane gas generators that the Southern Environmental Law Center alleges are not approved by energy regulators to supplement grid power and are violating the Clean Air Act.\nThe key metric used to quantify the emissions from these data centers is called the carbon intensity: how many grams of carbon dioxide emissions are produced for each kilowatt-hour of electricity consumed. Nailing down the carbon intensity of a given grid requires understanding the emissions produced by each individual power plant in operation, along with the amount of energy each is contributing to the grid at any given time. Utilities, government agencies, and researchers use estimates of average emissions, as well as real-time measurements, to track pollution from power plants.\nThis intensity varies widely across regions. The US grid is fragmented, and the mixes of coal, gas, renewables, or nuclear vary widely. California\u2019s grid is far cleaner than West Virginia\u2019s, for example.\nTime of day matters too. For instance, data from April 2024 shows that California\u2019s grid can swing from under 70 grams per kilowatt-hour in the afternoon when there\u2019s a lot of solar power available to over 300 grams per kilowatt-hour in the middle of the night.\nThis variability means that the same activity may have very different climate impacts, depending on your location and the time you make a request. Take that charity marathon runner, for example. The text, image, and video responses they requested add up to 2.9 kilowatt-hours of electricity. In California, generating that amount of electricity would produce about 650 grams of carbon dioxide pollution on average. But generating that electricity in West Virginia might inflate the total to more than 1,150 grams.\nAI around the corner\nWhat we\u2019ve seen so far is that the energy required to respond to a query can be relatively small, but it can vary a lot, depending on the type of query and the model being used. The emissions associated with that given amount of electricity will also depend on where and when a query is handled. But what does this all add up to?\nChatGPT is now estimated to be the fifth-most visited website in the world, just after Instagram and ahead of X. In December, OpenAI said that ChatGPT receives 1 billion messages every day, and after the company launched a new image generator in March, it said that people were using it to generate 78 million images per day, from Studio Ghibli\u2013style portraits to pictures of themselves as Barbie dolls.\nGiven the direction AI is headed\u2014more personalized, able to reason and solve complex problems on our behalf, and everywhere we look\u2014it\u2019s likely that our AI footprint today is the smallest it will ever be.\nOne can do some very rough math to estimate the energy impact. In February the AI research firm Epoch AI published an estimate of how much energy is used for a single ChatGPT query\u2014an estimate that, as discussed, makes lots of assumptions that can\u2019t be verified. Still, they calculated about 0.3 watt-hours, or 1,080 joules, per message. This falls in between our estimates for the smallest and largest Meta Llama models (and experts we consulted say that if anything, the real number is likely higher, not lower).\nOne billion of these every day for a year would mean over 109 gigawatt-hours of electricity, enough to power 10,400 US homes for a year. If we add images and imagine that generating each one requires as much energy as it does with our high-quality image models, it\u2019d mean an additional 35 gigawatt-hours, enough to power another 3,300 homes for a year. This is on top of the energy demands of OpenAI\u2019s other products, like video generators, and that for all the other AI companies and startups.\nBut here\u2019s the problem: These estimates don\u2019t capture the near future of how we\u2019ll use AI. In that future, we won\u2019t simply ping AI models with a question or two throughout the day, or have them generate a photo. Instead, leading labs are racing us toward a world where AI \u201cagents\u201d perform tasks for us without our supervising their every move. We will speak to models in voice mode, chat with companions for 2 hours a day, and point our phone cameras at our surroundings in video mode. We will give complex tasks to so-called \u201creasoning models\u201d that work through tasks logically but have been found to require 43 times more energy for simple problems, or \u201cdeep research\u201d models that spend hours creating reports for us. We will have AI models that are \u201cpersonalized\u201d by training on our data and preferences.\nThis future is around the corner: OpenAI will reportedly offer agents for $20,000 per month and will use reasoning capabilities in all of its models moving forward, and DeepSeek catapulted \u201cchain of thought\u201d reasoning into the mainstream with a model that often generates nine pages of text for each response. AI models are being added to everything from customer service phone lines to doctor\u2019s offices, rapidly increasing AI\u2019s share of national energy consumption.\n\u201cThe precious few numbers that we have may shed a tiny sliver of light on where we stand right now, but all bets are off in the coming years,\u201d says Luccioni.\nEvery researcher we spoke to said that we cannot understand the energy demands of this future by simply extrapolating from the energy used in AI queries today. And indeed, the moves by leading AI companies to fire up nuclear power plants and create data centers of unprecedented scale suggest that their vision for the future would consume far more energy than even a large number of these individual queries.\n\u201cThe precious few numbers that we have may shed a tiny sliver of light on where we stand right now, but all bets are off in the coming years,\u201d says Luccioni. \u201cGenerative AI tools are getting practically shoved down our throats and it\u2019s getting harder and harder to opt out, or to make informed choices when it comes to energy and climate.\u201d\nTo understand how much power this AI revolution will need, and where it will come from, we have to read between the lines.\nPart four: The future ahead\nA report published in December by the Lawrence Berkeley National Laboratory, which is funded by the Department of Energy and has produced 16 Nobel Prizes, attempted to measure what AI\u2019s proliferation might mean for energy demand.\nIn analyzing both public and proprietary data about data centers as a whole, as well as the specific needs of AI, the researchers came to a clear conclusion. Data centers in the US used somewhere around 200 terawatt-hours of electricity in 2024, roughly what it takes to power Thailand for a year. AI-specific servers in these data centers are estimated to have used between 53 and 76 terawatt-hours of electricity. On the high end, this is enough to power more than 7.2 million US homes for a year.\nIf we imagine the bulk of that was used for inference, it means enough electricity was used on AI in the US last year for every person on Earth to have exchanged more than 4,000 messages with chatbots. In reality, of course, average individual users aren\u2019t responsible for all this power demand. Much of it is likely going toward startups and tech giants testing their models, power users exploring every new feature, and energy-heavy tasks like generating videos or avatars.\nData centers in the US used somewhere around 200 terawatt-hours of electricity in 2024, roughly what it takes to power Thailand for a year.\nBy 2028, the researchers estimate, the power going to AI-specific purposes will rise to between 165 and 326 terawatt-hours per year. That\u2019s more than all electricity currently used by US data centers for all purposes; it\u2019s enough to power 22% of US households each year. That could generate the same emissions as driving over 300 billion miles\u2014over 1,600 round trips to the sun from Earth.\nThe researchers were clear that adoption of AI and the accelerated server technologies that power it has been the primary force causing electricity demand from data centers to skyrocket after remaining stagnant for over a decade. Between 2024 and 2028, the share of US electricity going to data centers may triple, from its current 4.4% to 12%.\nThis unprecedented surge in power demand for AI is in line with what leading companies are announcing. SoftBank, OpenAI, Oracle, and the Emirati investment firm MGX intend to spend $500 billion in the next four years on new data centers in the US. The first has started construction in Abilene, Texas, and includes eight buildings that are each the size of a baseball stadium. In response to a White House request for information, Anthropic suggested that the US build an additional 50 gigawatts of dedicated power by 2027.\nAI companies are also planning multi-gigawatt constructions abroad, including in Malaysia, which is becoming Southeast Asia\u2019s data center hub. In May OpenAI announced a plan to support data-center buildouts abroad as part of a bid to \u201cspread democratic AI.\u201d Companies are taking a scattershot approach to getting there\u2014inking deals for new nuclear plants, firing up old ones, and striking massive deals with utility companies.\nMIT Technology Review sought interviews with Google, OpenAI, and Microsoft about their plans for this future, and for specific figures on the energy required to inference leading AI models. OpenAI declined to provide figures or make anyone available for an interview but provided a statement saying that it prioritizes efficient use of computing resources and collaborates with partners to support sustainability goals, and that AI might help discover climate solutions. The company said early sites for its Stargate initiative will be natural gas and solar powered and that the company will look to include nuclear and geothermal wherever possible.\nMicrosoft discussed its own research on improving AI efficiencies but declined to share specifics of how these approaches are incorporated into its data centers.\nGoogle declined to share numbers detailing how much energy is required at inference time for its AI models like Gemini and features like AI Overviews. The company pointed to information about its TPUs\u2014Google\u2019s proprietary equivalent of GPUs\u2014and the efficiencies they\u2019ve gained.\nThe Lawrence Berkeley researchers offered a blunt critique of where things stand, saying that the information disclosed by tech companies, data center operators, utility companies, and hardware manufacturers is simply not enough to make reasonable projections about the unprecedented energy demands of this future or estimate the emissions it will create. They offered ways that companies could disclose more information without violating trade secrets, such as anonymized data-sharing arrangements, but their report acknowledged that the architects of this massive surge in AI data centers have thus far not been transparent, leaving them without the tools to make a plan.\n\u201cAlong with limiting the scope of this report, this lack of transparency highlights that data center growth is occurring with little consideration for how best to integrate these emergent loads with the expansion of electricity generation/transmission or for broader community development,\u201d they wrote. The authors also noted that only two other reports of this kind have been released in the last 20 years.\nWe heard from several other researchers who say that their ability to understand the emissions and energy demands of AI are hampered by the fact that AI is not yet treated as its own sector. The US Energy Information Administration, for example, makes projections and measurements for manufacturing, mining, construction, and agriculture, but detailed data about AI is simply nonexistent.\n\u201cWhy should we be paying for this infrastructure? Why should we be paying for their power bills?\u201d\nIndividuals may end up footing some of the bill for this AI revolution, according to new research published in March. The researchers, from Harvard\u2019s Electricity Law Initiative, analyzed agreements between utility companies and tech giants like Meta that govern how much those companies will pay for power in massive new data centers. They found that discounts utility companies give to Big Tech can raise the electricity rates paid by consumers. In some cases, if certain data centers fail to attract the promised AI business or need less power than expected, ratepayers could still be on the hook for subsidizing them. A 2024 report from the Virginia legislature estimated that average residential ratepayers in the state could pay an additional $37.50 every month in data center energy costs.\n\u201cIt\u2019s not clear to us that the benefits of these data centers outweigh these costs,\u201d says Eliza Martin, a legal fellow at the Environmental and Energy Law Program at Harvard and a coauthor of the research. \u201cWhy should we be paying for this infrastructure? Why should we be paying for their power bills?\u201d\nWhen you ask an AI model to write you a joke or generate a video of a puppy, that query comes with a small but measurable energy toll and an associated amount of emissions spewed into the atmosphere. Given that each individual request often uses less energy than running a kitchen appliance for a few moments, it may seem insignificant.\nBut as more of us turn to AI tools, these impacts start to add up. And increasingly, you don\u2019t need to go looking to use AI: It\u2019s being integrated into every corner of our digital lives.\nCrucially, there\u2019s a lot we don\u2019t know; tech giants are largely keeping quiet about the details. But to judge from our estimates, it\u2019s clear that AI is a force reshaping not just technology but the power grid and the world around us.\nWe owe a special thanks to Jae-Won Chung, Mosharaf Chowdhury, and Sasha Luccioni, who shared their measurements of AI\u2019s energy use for this project.\nThis story was supported by a grant from the Tarbell Center for AI Journalism.\nRead these next:\n- 1\n- 2\n- 3"
    },
    {
      "url": "https://go.skimresources.com?id=111346X1569483&xs=1&url=https://epoch.ai/gradient-updates/how-much-energy-does-chatgpt-use&xcust=2-1-2885721-1-0-0-0-0&sref=https://www.pcworld.com/article/2885721/how-much-power-and-water-does-ai-use-google-mistral-weigh-in.html",
      "text": "How much energy does ChatGPT use?\nPublished\nResources\nCredit to Alex Erben and Ege Erdil for substantial help with research and calculations. In this issue, \u201cwe\u201d refers to our collective judgment.\nA commonly-cited claim is that powering an individual ChatGPT query requires around 3 watt-hours of electricity, or 10 times as much as a Google search.1 This is often brought up to express concern over AI\u2019s impact on the environment, climate change, or the electric grid.\nHowever, we believe that this figure of 3 watt-hours per query is likely an overestimate. In this issue, we revisit this question using a similar methodology, but using up-to-date facts and clearer assumptions. We find that typical ChatGPT queries using GPT-4o likely consume roughly 0.3 watt-hours, which is ten times less than the older estimate. This difference comes from more efficient models and hardware compared to early 2023, and an overly pessimistic estimate of token counts in the original estimate.\nFor context, 0.3 watt-hours is less than the amount of electricity that an LED lightbulb or a laptop consumes in a few minutes. And even for a heavy chat user, the energy cost of ChatGPT will be a small fraction of the overall electricity consumption of a developed-country resident. The average US household2 uses 10,500 kilowatt-hours of electricity per year, or over 28,000 watt-hours per day.3\nThis estimate of 0.3 watt-hours is actually relatively pessimistic (i.e. erring towards high energy costs), and it is possible that many or most queries are actually much cheaper still. However, this will vary with use case\u2014queries with long input lengths or with longer outputs (e.g. using reasoning models) may be substantially more energy-intensive than 0.3 watt-hours. And training and inference for future models may consume much more energy than using ChatGPT today.\nSee this spreadsheet for sources for everyday electricity uses.\nEstimating the energy cost of a query\nChatGPT and other chatbots are powered by large language models (LLMs). Running these models (also known as inference) requires compute, and the chips and data centers that process that compute require electricity, roughly in proportion to the amount of compute required.\nBelow, I\u2019ll walk through a summary of how to estimate the compute and energy cost of a ChatGPT query. You can find a more detailed version with arguments for every assumption in the appendix.\n-\nChatGPT actually runs on several different models, but let\u2019s use GPT-4o as a reference model, because this is still OpenAI\u2019s leading general-purpose model. OpenAI\u2019s new reasoning models (o1, o3-mini, and the upcoming o3) likely require more energy, but are probably less popular, at least right now. In particular, OpenAI\u2019s new Deep Research product, which is rate-limited and requires their $200/month subscription tier, is certainly far more compute-intensive than a simple ChatGPT query. Meanwhile, GPT 4o-mini is smaller and cheaper than GPT-4o. We\u2019ll discuss these other models in more detail in a later section.\n-\nLLMs generate outputs in units called tokens\u2014for OpenAI, a token represents 0.75 words on average. Floating-point operations (FLOP) is a standard unit for measuring compute, and generating a token requires approximately two FLOP for every active parameter in the model. We previously estimated that GPT-4o has roughly 200 billion total parameters (likely between 100 and 400 billion). It is also most likely a mixture-of-experts model, meaning not all of these parameters are activated at once. Pessimistically taking the high estimate of total parameters, and assuming \u00bc are activated at a time suggests 100 billion active parameters. This means that 2 * 100 billion = 200 billion FLOP are needed to generate one token.\n-\nI assume that a typical number of output tokens per query is 500 tokens (~400 words, or roughly a full page of typed text). This is somewhat pessimistic\u2014for example, Chiang et al. found an average response length of 261 tokens in a dataset of chatbot conversations. This is also assuming text-based conversations\u2014it\u2019s unclear how many tokens are needed in conversations with GPT-4o\u2019s advanced voice mode, and we don\u2019t consider the cost of generating images here.\n-\nThis leads to 500 * 2 * 100 billion = 1e14 FLOP for a GPT-4o query with 500 output tokens. There is also an additional cost for queries with lengthy inputs, which I discuss below.\nNext, we can find the energy cost of this compute based on the power consumption of the necessary AI chips:\n-\nI assume that OpenAI uses Nvidia H100 GPUs for ChatGPT inference. These have a power rating of 700 watts, but H100 clusters can consume up to ~1500 W per GPU due to the overhead costs of servers and data centers.4\n-\nH100s can perform up to 989 trillion (9.89e14) FLOP per second. At that rate, it takes 1e14 / 9.89e14 ~= 0.1 seconds of H100-time to process a query (though in reality, many queries are processed in parallel in batches, so end-to-end generation time is much longer than this).\n-\nHowever, GPUs can\u2019t actually achieve their max FLOP/second output in practice. I use 10% as a rough estimate of typical utilization rates for inference clusters. This increases the number of GPUs needed to process queries by 10x, so roughly one second of H100-time is required to process a ChatGPT query.\n-\nOn the flip side, average power consumption per GPU will be less than 1500 W in practice. One estimate from the literature suggests that inference GPUs may consume around 70% of peak power on average.5\nPutting this together: one second of H100-time per query, 1500 watts per H100, and a 70% factor for power utilization gets us 1050 watt-seconds of energy, which is around 0.3 watt-hours per query. This is around 10 times lower than the widely cited 3 watt-hour estimate!\nOne last factor to account for is the cost of processing input tokens. This is negligible for simple questions that are manually typed, but when the inputs are very long, processing inputs becomes significant relative to the cost of generating outputs. Some chatbot use cases involve uploading large documents, codebases, or other forms of data, and GPT-4o supports an input window of up to 128,000 tokens.\nIn the appendix, we estimate that a long input of 10k tokens (similar to a short paper or long magazine article) would substantially increase the cost per query to around 2.5 watt-hours, and a very long input length of 100k tokens (equal to roughly 200 pages of text) would require almost 40 watt-hours of energy. Note that this input processing is an upfront cost, so if you have an extended chat exchange after uploading a long document, this cost is not incurred with each message. Additionally, it is almost certainly possible to dramatically improve how the cost of processing inputs scales with very long inputs, but we\u2019re not sure if OpenAI has done this yet.6\nWhy is our estimate different from others?\nThe original three watt-hour estimate, which has been widely cited by many different researchers and media outlets, comes from Alex de Vries (2023).\nThe most important reason our estimate differs is that we use a more realistic assumption for the number of output tokens in typical chatbot usage. We also base our estimate on a newer and more efficient chip (NVIDIA H100 vs A100), and a model with somewhat fewer active parameters.\nIn the original estimate, De Vries cites a February 2023 estimate from SemiAnalysis of the compute cost of inference for ChatGPT. This calculation assumed 175B parameters for GPT-3.5 (vs our assumed active parameter count of 100B for GPT-4o), running on A100 HGX servers (less efficient than the more modern H100), and most importantly, assumed 4000 input tokens and 2000 output tokens per query. This is equivalent to 1500 words, which is likely quite unrepresentative of typical queries (for context, it is about half as long as this newsletter issue, besides the appendix). De Vries then converts this compute cost to energy using the A100 server\u2019s max power capacity of 800 W per GPU, while we assume servers consume 70% of peak power.\nIn addition, Luccioni et. al. measured a ~4 Wh energy consumption per query for BLOOM-176B, a model comparable in size to GPT-4o. However, this was for a model deployed for research purposes in 2022: their cluster handled a relatively low volume of requests, likely with low utilization, and didn\u2019t use batching, a basic inference optimization that is standard in any commercial deployment of LLMs (see the appendix for more details on this paper).\nWhat about other models besides GPT-4o?\nWhile GPT-4o serves as our reference model, there are numerous other models available both within ChatGPT and in chatbot products from other companies. OpenAI offers GPT-4o-mini, which, based on its API pricing at under one-tenth the cost of GPT-4o, likely has a significantly smaller parameter count7 and therefore energy cost. In contrast, OpenAI\u2019s o1 and o3 reasoning models may consume substantially more energy to operate.\no1\u2019s parameter count (and cost per-token) is unclear, but its compute and energy costs per query are most likely higher due to the lengthy chain-of-thought that it generates. This cost is even higher with the $200/month o1 pro, which uses the same o1 model but scales up inference compute even further.\nThe newly-released o3-mini deserves special attention here, because it is powerful while also being fast enough that it could significantly displace GPT-4o in usage in the coming months, especially if it receives new product features like image/file upload and voice mode. o3-mini also has an undisclosed parameter count, though it is advertised as a \u201csmall\u201d model and its per-token cost on the API is only 44% as much as GPT-4o\u2019s. So there is a good chance that o3-mini has a lower per-token energy cost than GPT-4o, but this could easily be outweighed by long reasoning chains.\nI informally tested o3-mini and o1 using OpenAI\u2019s API8, and found that they both generate around 2.5x as many tokens as GPT-4o. This was based on a small sample of my own chats and a more rigorous analysis would be useful here.9 Overall, it is currently unclear if o3-mini queries consume more or less energy than GPT-4o queries, but o1 queries most likely require more energy.\nToday, the \u201co\u201d-series models are almost certainly much less popular than 4o and 4o-mini, which are available for free,10 are faster, and have more features (e.g. o1 and o3-mini do not have PDF upload or voice mode as of writing). Also, o1\u2019s superior reasoning abilities are unnecessary for most of today\u2019s chatbot use cases, like answering simple questions or drafting emails. It is possible that a shift to reasoning models significantly drives up energy costs for the average chatbot user, but if that happens, that probably means that AI usage has shifted towards much more difficult or complex problems compared to how they are used today. OpenAI\u2019s new, o3-powered Deep Research product is an early sign of this.\nBeyond OpenAI and ChatGPT, other major chatbot products include Meta\u2019s AI assistant (likely powered by Llama 3.2 11B or 90B since Meta\u2019s product is multimodal), Anthropic\u2019s Claude (powered by Claude 3.5 Sonnet, which we estimate has 400B parameters), and Google\u2019s Gemini (powered by Gemini \u201cFlash\u201d and \u201cPro\u201d models, with undisclosed parameter counts). We don\u2019t have full details on all of these models, but they are probably all roughly comparable to either GPT-4o or 4o-mini in energy costs. Finally, DeepSeek-V3, which we wrote about recently, and its R1 reasoning variant, offers strong performance with just 37B active parameters (out of 671B total parameters), so there is a good chance that it is less energy-intensive per token than GPT-4o.\nMoving forward, it\u2019s unclear how energy costs for AI chatbots will evolve\u2014it could easily go up or down over time, and this could diverge dramatically by use case. Holding capabilities constant, language models will become more energy-efficient over time due to both hardware and algorithmic improvements. The AI and tech industry have been developing more efficient hardware, continually improving the capabilities of smaller models, and inventing inference optimizations like multi-token prediction, all of which drive energy costs down.11 However, if consumers shift to increasingly powerful chatbots and assistants that do increasingly complex tasks, this may eat these efficiency gains through larger models or models that generate increasingly large numbers of reasoning tokens.12\nTraining and other upstream costs\nAnother thing to consider is the upstream energy costs in producing ChatGPT and other LLM products, in addition to the cost of using them.\nThe most obvious one is the energy required to train the models. The training runs for current generation models that are comparable to GPT-4o13 consumed around 20-25 megawatts of power each, lasting around three months. This is enough to power around 20,000 American homes. Since ChatGPT has 300 million users, the energy spent on GPT-4o\u2019s training run is not very significant on a per-user basis. The same article also mentions that ChatGPT users send 1 billion messages per day: using our 0.3 Wh estimate, this suggests that overall ChatGPT inference requires ~12.5 MW of power. This is comparable to the power (temporarily) required to train GPT-4o, but language models are typically used for longer than they are trained (GPT-4o has been available for nine months).\nOne might also wonder about the upstream energy costs of constructing the GPUs and other hardware, also known as \u201cembodied\u201d energy, in addition to the direct energy cost of running them. A full accounting here would be difficult, but these embodied energy costs are likely much smaller than the direct energy costs.\nLuccioni et. al. looked at the embodied carbon emissions of the servers and GPUs used to train BLOOM, a 176 billion parameter LLM, estimating that the embodied emissions (amortized over the length of the training run) were ~11.2 tonnes of CO2. This is less than half of the 24.7 tonnes of CO2 emitted by training the model.14 However, the ratio of embodied energy to training energy is even lower, because those training emissions came from clean French electricity, which is around 5 to 10 times less carbon-intensive than the electricity in most countries.15 So the embodied energy cost was likely a small fraction of the direct energy cost.\nAnother data point is that TSMC, which manufactures a large majority of the world\u2019s AI chips, consumed a total of 24 billion kWh in 2023, which is equivalent to an average power consumption of 2.7 gigawatts. This is in the same ballpark as the power consumption of the GPUs that TSMC produced last year (very roughly, 1 kW per GPU over several million GPUs produced per year). However, GPUs last for multiple years, so the total stock of GPUs probably consumes more than TSMC\u2019s fabs put together. Additionally, much of the 2.7 GW consumed by TSMC actually goes to producing non-AI chips. Overall, his means that it probably takes much less energy to manufacture AI chips than it takes to run them.\nRigorously measuring all these upstream costs would be difficult, but given the available information, they are probably comparable to or less than the direct cost of inference compute for ChatGPT. And in the case of training, this cost is an upfront cost that doesn\u2019t affect the marginal energy cost of using ChatGPT, since AI models don\u2019t wear out after they are trained.\nDiscussion\nWith reasonable and somewhat pessimistic assumptions, a GPT-4o query consumes around 0.3 watt-hours for a typical text-based question, though this increases substantially to 2.5 to 40 watt-hours for queries with very long inputs. As shown in the charts below, this is somewhere between a negligible to a small portion of everyday electricity usage.\nThere is a lot of uncertainty here around both parameter count, utilization, and other factors\u2014you can find more estimates with a wider range of assumptions in this spreadsheet, as well as sources for the other everyday uses of electricity. I\u2019ve tried to err on the side of pessimism (higher energy costs) with every assumption, but different assumptions about parameter counts, utilization, and token output can bring the cost into the 1 to 4 watt-hour range, or down to around 0.1 watt-hours. And again, maxing out GPT-4o\u2019s context window could bring the energy cost into the tens of watt-hours, though this input scaling can be improved with better algorithms.\nMore transparency from OpenAI and other major AI companies would help produce a better estimate. Ideally we could use empirical data from the actual data centers that run ChatGPT and other popular AI products. This may be difficult for AI companies to reveal due to trade secrets, but it seems to me that public confusion on this topic (including many exaggerated impressions of the energy cost of using AI today) is also not in AI developers\u2019 interests.\nTaking a broader view, by some estimates AI could reach fairly eye-popping levels of energy usage by 203016, on the order of 10% of US electricity.17 For this reason, I don\u2019t dismiss concerns about AI\u2019s overall impact on the environment and energy, especially in the longer run. However, it\u2019s still an important fact that the current, marginal cost of using a typical LLM-powered chatbot is fairly low by the standards of other ordinary uses of electricity.\nAppendix\n1. Compute cost\nTo find the compute cost of inference for an LLM, we have to consider the size of the model and the tokens generated. LLMs generate text in units called tokens (for OpenAI, 1 token represents 0.75 words on average). Generating a token requires two floating-point operations (FLOP) for every parameter in the model (plus some compute required to process inputs, which we\u2019ll discuss below). Note that we are ignoring advanced techniques like speculative decoding or multi-token prediction that could reduce this token generation, because we don\u2019t know whether or not OpenAI uses them.\nHow many parameters do ChatGPT models have? As of writing, ChatGPT users can choose between several different models, including GPT-4o, GPT-4o mini, o1, and o1-mini, and o3-mini.\nGPT-4o is the full-featured model for paid subscribers, and GPT-4o-mini is a cheaper and less powerful variant (free users have limited access to GPT-4o and more access to 4o-mini). Meanwhile, o1 and o3 are OpenAI\u2019s new \u201creasoning\u201d models, which likely require much more compute and energy, but they are also probably much less popular among typical users.\nI\u2019ll use GPT-4o as a reference for typical ChatGPT use, though GPT-4o mini might be more popular while requiring much less compute.18 Unfortunately, we don\u2019t know the exact parameter count for GPT-4o or the other models, but we previously estimated that it has 200 billion total parameters, which could be off by roughly a factor of two.\nIt\u2019s also likely that GPT-4o is a mixture-of-experts (MoE) model, meaning not all of these parameters are activated at once. This is likely because MoE seems to be the standard approach for frontier models due to its compute efficiency: for example, GPT-4 is reportedly an MoE model, Gemini 1.5 Pro is an MoE, and nearly all DeepSeek models including V3 and R1 are MoE.\nThe ratio of total parameters to active parameters in MoE models can range from roughly 3.6 to 1 for Mistral\u2019s Mixtral 8x7B and 8x22B, to almost 20 to 1 for DeepSeek-V3, which has 37B parameters against 671B total. So we\u2019ll use a 4 to 1 ratio and assume that GPT-4o has 100B active parameters against 400B total parameters. If GPT-4o turned out to be dense, this would also be consistent with the lower end of our estimate for GPT-4o\u2019s total parameters.19 Overall, this is more pessimistic than our best guess for GPT-4o\u2019s active parameters, though there is substantial uncertainty here.\nNext, we need to multiply this by the number of tokens that ChatGPT generates per query. This can vary widely by use case, but Chiang et al. found an average response length of 269 tokens in a large dataset of LLM chats. We\u2019ll pessimistically bump this up to 500 tokens, which is equivalent to ~400 words or about half a page of typed single-space text. If your ChatGPT usage tends to produce much longer or shorter responses than this, you can adjust our final estimate in proportion to understand your own footprint.\nPutting this together, we get 500 * 2 * 100 billion = 1e14 FLOP for a query with 500 output tokens.\n2. Energy cost of compute\nNext, we need to find the energy cost of producing this much compute.\nThe leading AI chip for companies other than Google is NVIDIA\u2019s H100 GPU, so I\u2019ll assume ChatGPT inference uses H100s. OpenAI may still use some older and less efficient A100s20, but they will also transition to more efficient Blackwell chips sooner or later. The H100 has a power rating21 of 700 watts, but H100 clusters can consume up to ~1500 W per GPU due to the overhead costs of servers and data centers. For example, a DGX server with 8 H100s, a common server configuration for H100s, has a max power usage of 10.2 kW (1275 W per GPU), and there is an additional 10-20% overhead at the data center level.\nH100s can produce up to 989 trillion (9.89e14) FLOP per second. This assumes 16-bit FLOP without sparsity, though many inference providers for open models use 8-bit inference, and H100s can produce twice as many 8-bit FLOP as 16-bit FLOP. Note that NVIDIA\u2019s spec sheet for the H100 reports sparse FLOP/s, so we divide all those number by 2.\nIf the H100s running ChatGPT achieve this peak output, then the 1e14 FLOP required to answer a query would take 2e14 / 9.89e14 = 0.1 seconds of H100-time (in reality, servers process many parallel requests in batches, so generation takes much longer than that end-to-end).\nMultiplying 0.1 seconds by 1500 watts yields an energy cost of 150 watt-seconds, or ~0.041 watt hours, which is much lower than the common-cited estimate of 3 watt hours. However, this is too optimistic, because GPUs don\u2019t produce 100% of max output in practice. We need to adjust for compute utilization, which is actual compute output divided by the theoretical maximum. Lower utilization increases how many GPUs are required to process queries, which increases energy costs.\nHow much utilization is achieved during LLM inference? It\u2019s hard to know for sure, but 10% is a reasonable, though rough assumption. We know that utilization is lower for inference than it is when training AI models (often 30-50%), due to memory bandwidth bottlenecks in inference.22\nAnother line of evidence is the price per token for open-weight models. Open models have known parameter counts, and hosting APIs for open models is a competitive business (since anyone can download an open model and offer an API for it), so API provider margins are likely low, and their prices are evidence of the true cost of serving the models.\nThis makes it possible to estimate utilization based on token prices, assuming a 0% profit margin:\n- Llama 3.1 405B has 405 billion parameters, so 2 * 405B = 810B (8e11) FLOP are needed per token, and 8e17 FLOP per one million tokens.\n- An H100 can perform 2000 teraFLOP/s at peak output (in FP8 FLOP, which is standard for open model providers), or 7.2e18 FLOP per hour. H100s cost around $2 per hour to rent, so at 100% utilization, API providers could achieve 3.6e18 FLOP per dollar.\n- This means that if their GPUs ran at 100% utilization, the compute cost of generating a million Llama 3.1 405B tokens would be around 8e17 / 3.6e18 dollars, which is 22 cents.\n- In fact, according to prices compiled by Artificial Analysis, Llama 3.1 405B costs $3.50 per million output tokens on average, which is more than 10 times more expensive.\nThe cost of renting GPUs is almost certainly the biggest cost of running these APIs, so this suggests that providers need about 10x as many GPUs compared to if they achieved 100% utilization. This means that utilization is roughly 10%, increasing the GPUs required, and hence our energy estimate, by 10x.\nOne final consideration is that a GPU cluster\u2019s average power consumption will be lower than peak consumption, especially given the low compute utilization during inference. We can\u2019t know the actual power consumption of OpenAI\u2019s servers absent specific, real-world data, but there is some literature on measuring inference power consumption for other models.\nPatel et. al., a team of Microsoft Azure23 researchers, did experiments on inference power consumption for several LLMs and found that GPU power consumption is typically around 60-80% of thermal design power, spiking up to 100% when processing input tokens during prefill (Figure 6). Luccioni et. al. also measured inference power consumption, finding that mean power consumption was only about 25% of TDP. However, their setup was very different from ChatGPT\u2019s; for example, they only handled 558 requests per hour, which were processed without batching. By contrast, Patel et. al. sent a \u201csteady stream\u201d of inference requests intended to maximize power utilization, so their result of ~70% may be more representative of large-scale deployments, if somewhat pessimistic. Using this figure means we are pessimistically ignoring the possibility of GPUs being idled on occasion due to fluctuations in user demand, which is a plausible consequence of 10% FLOP throughput, and would reduce power consumption further.\nA compute utilization of 10% and a power utilization of 70% would increase our earlier native energy estimate by 7x (first divide by 10%, i.e. multiply by 10x, and then multiply by 70%). Applying this to the earlier result, we get 0.041 * 7 ~= 0.3 watt-hours for a GPT-4o query with 500 output tokens. This is almost 10 times lower than the widely-cited 3 Wh estimate!\n3. Energy cost of input tokens\nProcessing long inputs can be expensive for language models. Transformer models have an attention mechanism describing the relationships between all of the tokens in the input. In the prefill phase, the model processes the input and computes the values of two matrices used in attention (known as the key and value matrices, or K and V), which are then stored in a KV cache.\nThis cost of processing attention scales proportionally to the product of the model dimension, the model depth, and the square of the input length. Given an assumed parameter count for GPT-4o, we can find reasonable estimates for the model depth and the dimension of each layer that would produce this parameter count, which allows us to estimate the FLOP cost of attention calculations for any given input length. There is also an added cost of generating tokens given a long input, but this cost is relatively minor for very long inputs.\nBecause prefill cost scales quadratically with input length for large inputs, this methodology leads to high compute and energy costs for long inputs. For an input of 10k tokens and an output of 500 tokens, the total cost increases to around 2.4 watt-hours, and 100k input tokens and 500 output tokens would cost around 40 watt-hours of energy. Note that because of the KV cache, this is an upfront cost, and a multi-turn chat conversation that begins with a very long input such as an uploaded document will not incur this cost multiple times.\nYou can find the full calculation in this Colab notebook.\nImportantly, quadratic scaling would also imply very high compute and energy costs for inputs of 1 million tokens or more\u2014processing 1M input tokens would be around 100 times more costly in total than processing 100k tokens. However, Google DeepMind has been offering 1 to 2 million token context windows starting with Gemini 1.5, and this was almost certainly enabled by innovations that improve on quadratic scaling. Additionally, more efficient attention mechanisms have been proposed in the literature. So we know it is possible to improve how input costs scale with input length, but we don\u2019t know which innovations, if any, OpenAI in particular has adopted.\nNotes\n-\nThe comparison with Google searches comes from a very old estimate from Google in 2009 that each search consumes 0.3 Wh. Estimating Google search\u2019s energy cost today is outside the scope of this post. It could easily be lower today due to increased chip and data center efficiency, or higher if Google searches have become more complex over time (and/or because Google search now has AI features like AI Overviews). \u21a9\n-\nHousehold usage is lower than total usage, since the energy used to e.g. manufacture the goods you own are not incurred inside your house. \u21a9\n-\nYou can read this essay from Andy Masley for more useful context on how much energy this is, as well as discussion on AI\u2019s water usage. \u21a9\n-\nA DGX server with 8 H100s, a common server configuration for H100s, has a max power usage of 10.2 kW (1275 W per GPU). And there is an additional 10-20% overhead at the data center level. \u21a9\n-\nThis pessimistically assumes a constant stream of inference requests, ignoring the possibility of GPUs idling due to demand fluctuations. \u21a9\n-\nGoogle DeepMind has almost certainly improved on input scaling, in order to enable 2 million token input windows for its Gemini models, though we\u2019re not sure whether OpenAI has adopted these sorts of innovations yet. \u21a9\n-\nParameter count is not necessarily proportional to API prices, since OpenAI could charge different profit margins for different models, and it is possible to serve models more cheaply by reducing token generation speed (GPT-4o-mini is relatively slow for a \u201cmini\u201d model). \u21a9\n-\nThis was necessary because the full reasoning chain is hidden from users, but the API tells you how many tokens were generated. \u21a9\n-\nI asked GPT-4o, o1, and o3-mini five questions from my chat history and found an average response length of 1374 tokens for o3 and 1392 tokens for o1, versus 540 tokens for GPT-4o. (I used reasoning effort=medium, which is the default in ChatGPT, though I didn\u2019t use ChatGPT\u2019s system prompt). So o1 and o3-mini both generated just over 2.5x as many tokens. Meanwhile DeepSeek-R1 returned an average of 1794 tokens for these questions. This is just a rough sanity check that reasoning models can generate much more tokens, and should not be taken as a representative average. \u21a9\n-\no3-mini is also available for free, likely with serious rate limits. \u21a9\n-\nSee our earlier issue on LLM model sizes for more discussion on this point. \u21a9\n-\nSome possible signs of the growing popularity of reasoning models are DeepSeek R1, which is DeepSeek\u2019s new reasoning model, reaching the top of Apple\u2019s app store, and the fact that o3-mini is available to free users. \u21a9\n-\nWe don\u2019t know how many GPUs were used to train GPT-4o but we do know this for models we believe to be of comparable scale, such as Llama 3.1 and the original GPT-4. \u21a9\n-\nThis is training, not inference, but because they divide the total embodied emissions over the computer equipment\u2019s lifetime by the length of the training run, the conclusion isn\u2019t dramatically different compared if you compare embodied emissions amortized over the very short time period where a cluster serves an inference query, versus the energy cost of that query. \u21a9\n-\nMost of France\u2019s electricity comes from nuclear power. Luccioni et. al. assume a carbon intensity of 57 grams CO2 per kWh for BLOOM training, which is around 5-10x cleaner than the electricity in most other countries, and the embodied emissions presumably mostly came from outside France. \u21a9\n-\nThis is from a starting point of around 0.3% today: Goldman Sachs estimated that AI data centers would consume about 11 terawatt-hours of electricity in 2024, which is about 0.3% of the US\u2019s total consumption of 4 trillion kWh (calculation here). \u21a9\n-\nThese are both roughly consistent with our projection of electricity supply constraints for AI. \u21a9\n-\nGPT-4o mini is substantially smaller, given that it is over 10x cheaper on OpenAI\u2019s API. But I don\u2019t know whether 4o-mini is actually more typical than 4o because the message limits for free users are opaque. \u21a9\n-\nThese two claims (whether or not GPT-4o is an MoE and its total number of parameters) are not completely independent, since we know GPT-4o was a highly capable model by mid-2024 standards, and this is harder to accomplish as the active parameter count goes down. \u21a9\n-\nNVIDIA sales have grown dramatically since the H100 was introduced in late 2022, so the H100 is likely much more common. \u21a9\n-\nOfficially called \u201cthermal design power\u201d, which is roughly equal to the GPU\u2019s peak power draw. \u21a9\n-\nMicrosoft is OpenAI\u2019s main compute provider, though this paper did not test OpenAI models, much less a commercial deployment for ChatGPT. \u21a9"
    }
  ],
  "argos_summary": "The article examines the growing energy footprint of AI, noting that AI data centers are projected to consume up to 10% of U.S. electricity by 2030, yet the marginal cost of a typical LLM-powered chatbot like ChatGPT remains low\u2014around 0.3\u202fWh per text query, rising to 2.5\u201340\u202fWh for very long inputs. It highlights uncertainty in parameter counts, utilization, and input scaling, and calls for greater transparency from AI firms to refine estimates. While AI\u2019s long\u2011term environmental impact is a concern, current usage is relatively modest compared to everyday electricity consumption.",
  "argos_id": "IXSFQOZS6"
}