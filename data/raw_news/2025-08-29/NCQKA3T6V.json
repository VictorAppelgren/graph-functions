{
  "url": "http://www.techtimes.com/articles/311747/20250829/ai-driven-innovation-infrastructure-quality-vision-impact-varun-mukka.htm",
  "authorsByline": "",
  "articleId": "41cc9a28b7814de7b0ca347662abebd2",
  "source": {
    "domain": "techtimes.com",
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://d.techtimes.com/en/full/459043/ai-driven-innovation-infrastructure-quality.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-29T00:42:35-04:00",
  "addDate": "2025-08-29T04:43:53.823420+00:00",
  "refreshDate": "2025-08-29T04:43:53.823421+00:00",
  "score": 1.0,
  "title": "AI-Driven Innovation in Infrastructure Quality: The Vision and Impact of Varun Mukka",
  "description": "In an era where software systems grow ever more complex and distributed, ensuring quality at every layer of infrastructure has become both critical and challenging. Forward-thinking leaders are turning to artificial intelligence (AI) and machine learning (ML) to meet this challenge. One such",
  "content": "In an era where software systems grow ever more complex and distributed, ensuring quality at every layer of infrastructure has become both critical and challenging. Forward-thinking leaders are turning to artificial intelligence (AI) and machine learning (ML) to meet this challenge.\n\nOne such innovator is Varun Mukka, an Engineering Architect in Infrastructure Quality known for integrating AI/ML into test automation and continuous verification. Mukka's approach exemplifies how intelligent automation can transform traditional quality assurance from a slow, reactive task into a proactive, data-driven discipline.\n\nAt a time when fewer than half of companies have adopted AI in their testing workflows, he stands at the cutting edge of a major industry shift.\n\nMukka was an Engineering Architect in Infrastructure Quality at Okta, specializing in Infrastructure Quality. He has extensive experience in software quality engineering and has pioneered the use of AI and ML to enhance testing processes.\n\nMukka leads efforts to embed machine learning into test automation frameworks and to champion continuous verification practices that ensure systems remain reliable even as they scale. By blending classical quality engineering with emerging AI techniques, he has driven innovative changes in how organizations verify complex cloud infrastructure.\n\nThe field of infrastructure quality assurance itself has evolved rapidly alongside DevOps. Traditionally, testing ensured new code met requirements, but modern systems demand continuous verification\u2014the ongoing validation of system behavior in real-time.\n\nContinuous verification (CV) has been described as a discipline, contrasting with older reactive testing methods. This proactive philosophy, combined with AI's ability to detect patterns and anomalies, is gaining momentum across the industry.\n\nAccording to Capgemini's World Quality Report 2023, 72% of organizations are actively exploring AI for QA initiatives by 2024. Against this backdrop, Mukka's work in marrying AI with infrastructure quality positions him as a visionary leader in quality engineering.\n\nAs an Engineering Architect, Mukka early on recognized the untapped potential of AI to improve how we ensure infrastructure reliability. He began experimenting with machine learning in test automation well before it was common practice.\n\n\"I realized early on that AI could transform how we approach quality assurance, turning a traditionally reactive process into a proactive one,\" he says, reflecting on the motivation behind his initiatives. \"By leveraging machine learning to analyze patterns and predict issues, we can address potential failures before they occur and fundamentally improve reliability.\"\n\nEmbracing this mindset required challenging the status quo of manual testing. Mukka championed the idea that AI algorithms could sift through log data, user behaviors, and system metrics far faster than humans, uncovering subtle issues in complex cloud environments.\n\nHis forward-looking perspective set the stage for significant changes in his organization's testing culture. Mukka's pioneering efforts came at a time when, today, only a minority of companies actively use AI for test automation, yet those who do report improved defect detection and efficiency.\n\n\"We started small by introducing AI-driven checks in areas where traditional scripts struggled,\" Mukka explains. \"When those early projects caught critical bugs that we would have missed otherwise, it validated our approach.\"\n\n\"It wasn't about replacing engineers\u2014it was about augmenting our capabilities to deliver a more robust infrastructure.\" Indeed, industry surveys now confirm that AI in testing is gaining traction, with adoption rising from 7% in 2023 to 16% in 2025 as organizations witness the benefits.\n\nBy leading this change early, Mukka positioned his team ahead of the curve in infrastructure quality innovation.\n\nOne of Mukka's core focus areas is continuous verification\u2014continuously checking that systems behave as expected even after deployment. He stresses that in today's fast-paced DevOps pipelines, testing cannot stop at release.\n\n\"Continuous verification means we're always validating our infrastructure and services in real-time,\" Mukka explains. \"Instead of testing once and assuming things will remain good, we use live data and AI models to constantly check system health.\"\n\n\"It's about being proactive: catching issues due to environment changes or integrations before users notice anything wrong.\" In contrast to traditional QA that validates known requirements, continuous verification employs ongoing monitoring and experimentation to uncover unknown failure modes.\n\nMukka implemented this by integrating testing hooks into production and staging environments\u2014for example, using ML algorithms to watch metrics for anomalies post-deployment. This approach aligns with modern SRE (Site Reliability Engineering) practices where success isn't just deploying without errors, but ensuring the software stays error-free under real-world conditions.\n\nThe importance of continuous verification is increasingly recognized in the industry as systems grow more complex. Small failures can hide in the labyrinth of microservices, containers, and APIs.\n\nMukka recounts how continuous verification, enhanced with AI, gives his team an edge in maintaining high uptime. \"We've tied our testing systems into our monitoring stack, so if a new build causes even a slight uptick in error rates, the machine learning models flag it immediately,\" he says.\n\n\"This lets us initiate rollbacks or fixes within minutes. In the past, it might have taken hours or days for a human to notice these subtle issues\u2014now our AI not only notices but also helps diagnose the cause.\"\n\nHis experience is backed by emerging tools in the DevOps world. For instance, now leverage ML to decide when to roll back changes based on live metrics, significantly reducing the risk of a bad deployment.\n\nBy embracing continuous verification, Mukka ensures that quality assurance is not a one-time gate but a continuous guardian. This proactive strategy is essential when growing system complexity could soon make manual oversight impossible.\n\nContinuous verification, especially when augmented by AI, offers a way to manage this complexity and maintain trust in the infrastructure around the clock.\n\nIntegrating machine learning into test automation has profoundly changed Mukka's testing workflows. One major impact has been on the speed and scope of testing.\n\nTraditional automated tests had to be scripted manually and often checked only the scenarios the writers anticipated. With ML, Mukka's team can generate and prioritize test cases dynamically.\n\n\"Machine learning brought a paradigm shift for us in testing,\" Mukka notes. \"We use ML models to analyze past outages and user reports, which helps us generate new test cases targeting high-risk areas.\"\n\n\"It's not just running more tests\u2014it's running smarter tests. We've seen test coverage and depth improve without proportional increases in manual effort.\"\n\nIn practice, his team built systems that learn from each test run, identifying which parts of the infrastructure are most failure-prone and adjusting the test focus accordingly. This intelligence means the team catches issues that previously went unnoticed.\n\nThe result is a faster feedback loop to developers and fewer surprises in production. Industry reports show that 39% of teams have experienced efficiency gains in test automation thanks to AI assistance.\n\nMukka's initiatives mirror these gains: by letting ML handle the tedious pattern recognition, his engineers concentrate on creative problem-solving and edge cases. The use of AI/ML has also improved product quality and reliability for Mukka's organization.\n\nAn AI-enhanced testing regime can run continuously and adapt to changes, which translates into more stable releases. \"Since adopting ML in our quality process, our key quality metrics have all trended upward,\" Mukka observes.\n\n\"We're catching roughly 30% more issues before code hits production and our test suites run significantly faster thanks to intelligent pruning of redundant tests. The net effect is faster delivery of features with confidence that we haven't broken anything critical.\"\n\nThese outcomes align with broader industry findings. The World Quality Report 2023\u201324 found that 75% of organizations now invest in AI to optimize QA, with 65% citing higher productivity as the main benefit.\n\nSimilarly, a majority of companies using generative AI in testing report faster automation and improved efficiency. Mukka's on-the-ground results provide a concrete example: his team accelerated their deployment cycle while simultaneously reducing post-release incidents.\n\nBy transforming their testing with machine learning, they have achieved a level of software quality and speed that would have been very hard to reach with manual methods alone. This demonstrates the real-world impact of AI-driven innovation in infrastructure quality.\n\nWhile the benefits of AI in testing are clear, Mukka is candid that the journey was not without challenges. Introducing AI/ML into a legacy testing process required overcoming skepticism, technical hurdles, and initial failures.\n\nOne major challenge was ensuring the AI models had enough high-quality data to learn from. \"In the beginning, our ML models were only as good as the data we fed them,\" Mukka recalls.\n\n\"We had to invest time in gathering and cleaning years of test results and production incident logs. There were moments when the AI gave us false alarms or missed obvious bugs because it was learning.\"\n\n\"We realized we needed to continuously train and fine-tune the models. It taught us that AI in testing isn't a fire-and-forget solution\u2014it's like raising a child, requiring patience and good guidance.\"\n\nHis experience underscores the importance of data preparation. Industry experts agree that training AI models on high-quality, representative data is critical; otherwise, the predictions and results can be unreliable.\n\nMukka's team addressed this by curating datasets and periodically retraining their ML algorithms as systems evolved. Another challenge Mukka faced was the human factor\u2014getting his QA engineers and developers comfortable with AI assistance.\n\nChange can be daunting, and there were concerns about trust and job relevance. \"At first, some team members were wary that an AI might replace their role or lead us astray with opaque decisions,\" he notes.\n\n\"We tackled this by making the AI's suggestions transparent and involving the team in interpreting the results. We treated the AI as an assistant, not an oracle.\"\n\n\"Over time, as everyone saw the AI catching tricky bugs or saving hours of repetitive work, trust grew and the skepticism turned into enthusiasm.\" Additionally, Mukka emphasizes training and upskilling.\n\n\"We encouraged our QA folks to learn the basics of how the ML models work. This demystified the technology and empowered them to improve our AI tools.\"\n\nSuch upskilling is now widely recommended in the industry; reports highlight that organizations need to invest in developing AI expertise in QA teams to leverage these tools fully. By navigating these technical and cultural challenges\u2014from data quality to team education\u2014Mukka successfully integrated AI into the workflow.\n\nHis experience provides a roadmap for others: start with small pilot projects, involve the team early, and incrementally build trust in the AI by demonstrating tangible wins.\n\nTo illustrate the power of AI in infrastructure quality, Mukka shared a striking example from his work. His team developed an AI-driven monitoring and test system to safeguard their platform's reliability.\n\n\"One of our biggest wins was an internally developed anomaly detection tool that watches our infrastructure 24/7,\" Mukka says. \"We trained a machine learning model on our application logs and metrics. It learned what 'normal' looks like across our services\u2014things like typical memory usage, response times, and database queries. Not long after we deployed it, the model caught a subtle memory leak in a new microservice within an hour of rollout.\"\n\n\"No traditional test had flagged it. Thanks to the alert, we were able to roll back and fix the issue before any customers were affected.\"\n\nThis incident proved the value of AI-driven continuous verification in a real-world scenario. The ML system effectively acted as an ever-vigilant tester in production, finding a needle-in-a-haystack problem that could have grown into a major outage.\n\nThe outcome was improved confidence in deployments and a faster remediation cycle, as the team didn't have to wait for user reports or scheduled manual checks to discover critical issues. Mukka's success story is reminiscent of approaches used by leading tech companies to maintain quality at scale.\n\nFor example, Facebook (Meta) deployed an intelligent testing tool, Sapienz, which \"automatically designs, runs, and reports the results of tens of thousands of test cases every day\" on their mobile app, catching issues within minutes of code being written.\n\nSimilarly, many organizations are adopting anomaly detection and self-healing systems in their operations. Mukka points out that these examples inspired his work.\n\n\"Seeing companies like Netflix and Facebook use AI to push quality and reliability gave us confidence. Our context at Okta was different, but the principle was the same\u2014use smart algorithms to handle scale and complexity.\"\n\n\"Now, whenever we roll out a major change, our AI monitors are effectively doing a mini 'health check' every few seconds. It's like having an extra team member who never sleeps.\"\n\nThe broader industry is moving in this direction as well: Gartner predicts that \"by 2026, over 60% of enterprises will operationalize AI-driven monitoring tools\" (Gartner, 2023). In Mukka's case, the AI-driven solution has become a cornerstone of their infrastructure quality strategy.\n\nIt demonstrates significant impact: faster detection of problems, automatic prevention of incidents, and ultimately higher trust from the business and users that the system will behave as expected.\n\nBy combining AI-driven testing with continuous verification, Mukka has markedly enhanced both the reliability of his company's systems and the efficiency of the engineering process. One clear benefit is a reduction in production issues and downtime.\n\nWith tests running continuously and AI models predicting points of failure, problems are identified and resolved much earlier in the development cycle. \"Our failure rate after releases went down dramatically,\" Mukka reports. \"In the past, we might have discovered an issue hours or days after a deployment. Now, our pipeline's continuous tests and verifications often catch those issues immediately.\"\n\n\"It means we fix things before they escalate\u2014customers never see most of the glitches anymore.\" Early detection of defects is indeed a hallmark benefit of continuous testing in DevOps.\n\nBy shifting quality checks to happen throughout development and after deployment, his team reduces the risk of a small bug snowballing into a major outage. This has improved service reliability metrics like uptime and error rates.\n\nMukka's approach aligns with industry best practices: continuous testing and verification are known to enhance overall software quality and safety by ensuring each code change is vetted in real conditions. In other words, quality isn't an afterthought\u2014it's built and maintained continuously, which for an identity and security platform like Okta, is essential.\n\nEfficiency gains have come hand-in-hand with reliability. Automating tests with AI and running them continuously means engineers get faster feedback, enabling quicker iterations and releases.\n\nMukka notes that their deployment frequency increased once the new practices settled in. \"We were able to speed up our release cycles because the team spends less time in lengthy manual test phases,\" he says. \"AI helps prioritize the most important tests, and continuous verification gives us the confidence to deploy more often. When an issue does arise, it's usually caught and fixed in the same cycle.\"\n\n\"This agility was hard to imagine in the old days of big, infrequent releases.\" The faster time-to-market is a direct result of streamlining quality checks\u2014a benefit widely reported by DevOps adopters.\n\nMukka also emphasizes that efficiency isn't just about speed, but smarter resource use. Engineers are freed from tedious regression testing and can focus on designing better features and improving the tests themselves.\n\nAdditionally, by automatically pruning redundant tests and using AI to target risky areas, they avoid wasting computing resources on running thousands of unnecessary test cases. The combination of these factors\u2014fewer production issues, quicker delivery, and optimized testing workload\u2014shows how AI-driven continuous verification can achieve the often elusive goal of\n\nIt's a win-win that Mukka and his team have demonstrated, serving as a model for modern quality engineering.\n\nLooking ahead, Mukka envisions an exciting future for infrastructure quality engineering, heavily influenced by AI and automation. He believes we are only at the early stages of what AI/ML will do for quality assurance.\n\n\"In the next few years, I see AI becoming an even more integral part of the toolset for quality engineers,\" Mukka predicts. \"We'll have intelligent systems that not only detect issues but also fix some of them automatically. Imagine a deployment pipeline where an AI finds a misconfiguration and immediately applies a safe correction\u2014that's where we're headed. I also expect testing will become more autonomous.\"\n\n\"We're starting to experiment with AI agents that can generate test scenarios on the fly and even simulate user behavior in complex workflows without being explicitly told what to do.\" This vision extends the current trends to a more autonomous QA paradigm, where AI acts like a co-pilot to engineers.\n\nIndustry trends support this direction: respondents to a recent Gartner survey expect that generative AI and other advanced tools will significantly influence testing in the next three years.\n\nThe concept of \u2014autonomous AI systems that make decisions and take actions\u2014is gaining traction, with analysts predicting that such AI could handle at least 15% of routine work decisions by 2028. In the context of quality engineering, that could mean AI-driven testing agents handling a substantial portion of day-to-day validation tasks.\n\nMukka is quick to add that human expertise will remain vital, even as AI's role expands. He foresees the role of QA professionals evolving rather than disappearing.\n\n\"Engineers will focus more on defining quality goals, interpreting AI findings, and handling the creative and complex aspects of quality that AI can't easily grasp,\" he says. \"The mundane stuff\u2014running countless test variations, monitoring every little metric\u2014that can be offloaded to AI.\"\n\n\"But the insight to understand a novel failure, or to design a clever test strategy, that will always need human intuition.\" He encourages quality engineers to embrace AI as a collaborator.\n\nThe future he paints is one of human-machine collaboration: AI doing the heavy lifting and humans providing direction and critical thinking. This outlook is optimistic and empowering.\n\nIt aligns with the broader industry sentiment that AI will amplify human capabilities in software development rather than replace them. As AI technology matures, Mukka expects continuous verification to become smarter and more self-sufficient, possibly evolving into self-adaptive systems that automatically adjust quality checks as a system changes.\n\nUltimately, his vision of infrastructure quality is one where AI-driven tools ensure software reliability in the background, enabling development teams to move at high velocity without sacrificing stability. It's a future where quality is by intelligent automation, guided by the strategic oversight of engineers\u2014a future Mukka is actively helping to create.\n\nGiven his experience, Mukka often advises other engineers and organizations on how to successfully adopt AI in their testing and infrastructure quality practices. A key piece of advice he offers is to start small and focused.\n\n\"Don't try to overhaul everything in one go,\" Mukka advises. \"Pick one or two areas where AI can make an immediate impact\u2014maybe it's an AI tool to generate test data, or a machine learning model to prioritize your regression tests.\"\n\n\"Implement it, learn from the results, and iterate. Early quick wins will build the confidence and justification for expanding AI's role.\" This incremental approach is echoed by industry best practices: experts recommend integrating AI into specific testing scenarios first, rather than attempting a wholesale replacement of existing processes.\n\nBy starting with a manageable pilot project, teams can work out kinks in the technology and build trust in the outcomes. Mukka's own journey began with such pilots, which helped demonstrate the value of AI to his stakeholders.\n\nHe also emphasizes setting clear success criteria (for example, reducing test execution time by X% or catching certain categories of bugs) so that the benefits of the AI experiment can be objectively measured. Another crucial piece of guidance from Mukka is to invest in people and processes around the AI tools.\n\n\"AI in testing isn't magic\u2014your team needs to understand it and your processes might need adjustment,\" he notes. \"Upskill your testers so they are comfortable with data and basic AI concepts. Encourage collaboration between developers, QA, and data scientists if you have them. And establish feedback loops:\n\n\"If an AI recommendation is wrong, treat it as a learning opportunity to improve the model.\" Maintaining human oversight and continuously refining the AI models ensures that the technology remains effective and aligned with quality goals.\n\nMukka also stresses the importance of keeping test assets and data well-organized\u2014since AI tools often rely on parsing test cases, logs, and results, having them in machine-readable formats makes a big difference in outcomes. Finally, he tells teams not to lose sight of the fundamental testing principles.\n\n\"AI can augment your testing, but it won't replace good test design and analysis. Use AI to handle scale and complexity, but always validate its findings with your expertise,\" he says.\n\nThis balanced approach ensures that AI is used as a powerful tool within a robust quality engineering strategy. With these practical pieces of advice, Mukka's experience becomes a playbook for others aiming to bring AI-driven innovation to their own infrastructure quality and testing practices.\n\nMukka's journey illustrates the profound impact that AI-driven innovation can have on infrastructure quality and testing. From the introduction of machine learning to catch hidden issues, to the establishment of continuous verification as a guardrail for software in production, Mukka has shown how traditional QA can evolve into a smarter, faster, and more proactive practice.\n\nUnder his leadership, AI and ML moved from buzzwords to tangible results: higher reliability, faster releases, and a culture of continuous improvement in quality engineering. His vision for the future\u2014where autonomous testing agents work alongside human engineers\u2014offers a glimpse of how the next generation of software quality assurance might look.\n\nIt's an optimistic vision in which quality is assured not by slowing down change, but by innovating the way we verify and validate at every step. As organizations everywhere grapple with the demands of complex systems and rapid delivery, Mukka's story serves as both inspiration and blueprint.\n\nThe takeaway is clear: embracing AI in infrastructure quality is not just a theoretical idea, but a practical path to robust systems and efficient engineering. And with pioneers like Mukka leading the way, the software industry is poised to make quality assurance smarter and more resilient than ever before.",
  "medium": "Article",
  "links": [
    "https://www.harness.io/blog/importance-of-continuous-verification#:~:text=Connecting%20observability%20and%20logging%20tools,reduce%20the%20risk%20of%20change",
    "https://www.opencredo.com/blogs/what-is-continuous-verification#:~:text=Continuous%20Verification%20is%20a%20term,tooling%20that%20verifies%20system%20behaviors",
    "https://gbej.org/tariff-turmoil-u-s-automotive-industry-faces-uncertain-future-amid-protectionist-policies/",
    "https://www.cm-alliance.com/cybersecurity-blog/ai-in-software-testing-what-is-it-and-how-to-use-it#:~:text=2,to%2C%20the%20better%20it%20becomes",
    "https://palify.io/articles/view-article/agentic-ai-the-next-wave-in-testing?id=c8465657-6ff9-4efc-b2ae-385753c8992f&userId=#:~:text=The%20rise%20of%20AI%20in,just%20a%20few%20years%20ago",
    "https://www.cm-alliance.com/cybersecurity-blog/ai-in-software-testing-what-is-it-and-how-to-use-it#:~:text=1,testing%20or%20regression%20suite%20analysis",
    "https://testlio.com/blog/test-automation-statistics/#:~:text=AI%20testing%20adoption%20has%20increased,automation%2C%20defect%20prediction%2C%20and%20analytics",
    "https://devops.com/continuous-testing-the-key-to-quality-assurance-in-the-devops-era/#:~:text=Continuous%20testing%20is%20a%20critical,making",
    "https://www.tricentis.com/blog/5-ai-trends-shaping-software-testing-in-2025#:~:text=So%20why%20the%20excitement%20now%3F,quality%20outcome%20for%20using%20AI",
    "https://engineering.fb.com/2018/05/02/developer-tools/sapienz-intelligent-automated-software-testing-at-scale/#:~:text=Sapienz%20now%20automatically%20designs%2C%20runs%2C,without%20compromising%20stability%20or%20performance",
    "https://devops.com/continuous-testing-the-key-to-quality-assurance-in-the-devops-era/#:~:text=There%20are%20several%20benefits%20of,continuous%20testing%20in%20DevOps%2C%20including",
    "https://successquarterly.com/blueprint-for-success-blue-elephant-cnc-guides-entrepreneurs-in-machining-business/",
    "https://www.opencredo.com/blogs/what-is-continuous-verification#:~:text=The%20inherent%20complexity%20of%20these,Many",
    "https://www.functionize.com/blog/gartner-top-strategic-trends-2025#:~:text=Gartner%C2%AE%20Top%20Strategic%20Trends%20of,from%20zero%20percent%20in",
    "https://testlio.com/blog/test-automation-statistics/#:~:text=23.%2068,12",
    "https://qentelli.com/thought-leadership/insights/the-future-of-continuous-testing-integrating-agile-and-devops-strategies#:~:text=The%20Future%20of%20Continuous%20Testing%3A,marking%20a%20significant%20increase",
    "https://www.gartner.com/peer-community/oneminuteinsights/omi-automated-software-testing-adoption-trends-7d6#:~:text=Automated%20Software%20Testing%20Adoption%20and,will%20impact%20automated%20software%20testing",
    "https://www.softwaretestingmagazine.com/knowledge/ai-powered-automation-the-future-of-smarter-faster-testing/#:~:text=,and%20upskilling%20of%20QA%20team",
    "https://testlio.com/blog/test-automation-statistics/#:~:text=21.%20Less%20Than%2050,3",
    "https://testlio.com/blog/test-automation-statistics/#:~:text=36.%2039,1",
    "https://www.softwaretestingmagazine.com/knowledge/ai-powered-automation-the-future-of-smarter-faster-testing/#:~:text=the%20finance%2C%20healthcare%2C%20and%20insurance"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI models",
      "weight": 0.07883276
    },
    {
      "name": "autonomous AI systems",
      "weight": 0.07808738
    },
    {
      "name": "AI tools",
      "weight": 0.07688449
    },
    {
      "name": "AI",
      "weight": 0.07424314
    },
    {
      "name": "AI algorithms",
      "weight": 0.073065534
    },
    {
      "name": "AI technology",
      "weight": 0.072833754
    },
    {
      "name": "such AI",
      "weight": 0.07253079
    },
    {
      "name": "AI agents",
      "weight": 0.07241552
    },
    {
      "name": "AI expertise",
      "weight": 0.0721351
    },
    {
      "name": "AI findings",
      "weight": 0.0719529
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.876953125
    },
    {
      "name": "/News/Technology News",
      "score": 0.74658203125
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.7109375
    },
    {
      "name": "/Computers & Electronics/Enterprise Technology/Other",
      "score": 0.53076171875
    },
    {
      "name": "/Business & Industrial/Business Operations/Management",
      "score": 0.347412109375
    },
    {
      "name": "/Computers & Electronics/Enterprise Technology/Data Management",
      "score": 0.337890625
    }
  ],
  "sentiment": {
    "positive": 0.51030606,
    "negative": 0.12548406,
    "neutral": 0.36420986
  },
  "summary": "Varun Mukka, an Engineering Architect in Infrastructure Quality at Okta, is known for integrating AI/ML into test automation and continuous verification. Mukka's approach has been instrumental in transforming traditional quality assurance into a proactive, data-driven discipline. He has led efforts to embed machine learning into test automations and champion continuous verification practices that ensure systems remain reliable even as they scale. His innovative approach has led to significant changes in the field of infrastructure quality assurance. According to Capgemini's World Quality Report 2023, 72% of organizations are actively exploring AI for QA initiatives by 2024. Industry surveys confirm that AI in testing is gaining traction, with adoption rising from 7% to 16% in 2025.",
  "shortSummary": "Varun Mukka's AI-driven innovation in Infrastructure Quality integrates AI/ML technology into continuous verification practices, transforming traditional quality assurance into a proactive, data-driven discipline amid rapid growth in complexity and distributed systems.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "b85b0b12ad4a4273a15ee884880a4e09",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.tricentis.com/blog/5-ai-trends-shaping-software-testing-in-2025#:~:text=So%20why%20the%20excitement%20now%3F,quality%20outcome%20for%20using%20AI",
      "text": "5 AI trends shaping software testing in 2025\nDiscover 5 AI trends revolutionizing software testing and SDLC in 2025, from test suite optimization to agentic AI. Boost efficiency with AI testing solutions.\nThanks to AI, a few people might be starting the new year with bright, shiny smiles. The technology has exploded in popularity and augmented almost everything, including a toothbrush and app combo that uses AI to optimize your dental hygiene habits.\nTeeth brushers aren\u2019t the only ones grinning due to the AI explosion. Enterprise leaders have been cheerful about the advantages AI can bring to their company\u2019s testing efforts and the software development lifecycle (SDLC). To speed up the SDLC, 61% of organizations prefer to use generative AI for code generation and auto-completion.\nTricentis\u2019 own research has shown that 80% of software teams will use AI next year. That\u2019s a staggering adoption rate that hasn\u2019t been seen since maybe the smartphone explosion in the 2010s. Simply put \u2013 if you\u2019re not using AI in your personal or professional life, there\u2019s a likely chance that 2025 is the year that changes.\nAI will become a necessity for QA leaders and engineers to remain competitive as it becomes embedded in every facet of software quality, influencing product development cycles, customer experiences, and business operations. So where will the technology go from here? In this blog, we\u2019ll share the AI trends that will likely enhance software testing in the months ahead and set us up for success for years into the future.\nTrend 1: AI fears will turn into excitement\nYou know that feeling when you\u2019re at an amusement park looking at the crazy ride that will take you upside down and twirl you around? Your initial fear can quickly turn into a rush of excitement. That\u2019s the feeling right now around AI. The positive perceptions about AI have now surpassed the negatives. Years of uncertainties that AI can\u2019t be trusted or is taking over our jobs have given way to what it can do for software testing.\nThe job outlook is positive. The U.S. Bureau of Labor Statistics predicts jobs for software developers, quality assurance analysts, and testers will grow at a \u201cmuch faster\u201d rate than the average of all occupations from 2023 through 2033. In its report, the agency credited AI, in part, for driving the increase. Some of the ways testing teams implement AI are to identify what needs to be tested based on requirements, to generate test cases for those areas much more quickly, and to simplify test case maintenance with self-healing capabilities.\nSo why the excitement now? Because enterprises are seeing results from their AI investments. AI is helping to increase cost efficiency, shorten time to market, and improve quality. The World Quality Report 2023-24 found that 75% of organizations consistently invest in AI and utilize it to optimize QA processes. Almost two-thirds (65%) of organizations say higher productivity is the primary quality outcome for using AI.\nTrend 2: AI will drastically shrink test execution timelines\nGenerative AI is being used to speed up test case creation by auto generating tests and analyzing requirements (user stories and epics) using natural language. Using certain technologies, testers can generate test cases in seconds that include a title, preconditions, and description for each one, plus the informative descriptions and the expected test result. The effect in many cases will be to significantly grow test suites \u2013 which can be both a boon and a burden.\nAs test suites grow, another class of AI-augmented technologies called quality intelligence can help focus them. Quality intelligence pinpoints the impact of code changes and risks up front to minimize the redundancy in test suites. By prioritizing only the necessary tests based on what\u2019s actually at risk, teams can cut test cycle time and costs and minimize the risk of production errors.\nShortening test execution timelines with this type of technology \u2013 sometimes by as much as 80% \u2013 means a faster delivery pipeline and less reliance on resources. By prioritizing only the necessary tests based on what\u2019s actually at risk, teams can cut test cycle time and costs and minimize the risk of production errors.\nTrend 3: Developer productivity and output will skyrocket\nIf 2023 was the year of the emergence of the term \u201cAI copilot\u201d and 2024 was when it seemed that every company in the universe launched a copilot (present company included!), then this year we expect to see how well all those copilots can so-called \u201cland the plane.\u201d\nThe behemoth among the copilots is Microsoft 365 Copilot. A recent Morgan Stanley study found that 94% of CIOs expect to adopt Microsoft generative AI products over the next year, up from 63% in Q4 2023. However, according to Forrester, despite adopting AI copilots, business leaders are still waiting to see their payoff and true ROI beyond the benefits of better employee experiences.\nThe probable exception to the AI disbelievers? Testing. In a survey of more than 400 IT leaders at U.S. and U.K.-based organizations, more than two-thirds of respondents indicated high levels of trust in tool performance. Just 1 in 10 organizations aren\u2019t convinced AI tools will improve testing efficiency and effectiveness.\nAs AI-powered software testing gains traction, developer productivity is expected to soar with the help of copilots and other AI assistants to auto generate customized coded test steps or optimize an existing test portfolio by finding unused, unlinked, or duplicate test assets. According to the Tricentis and Techstrong Research survey, writing code (58%) and testing (42.5%) are the two leading areas where respondents use AI copilots today. The survey reveals that AI copilot functionality will be available for use in close to 100% of the roles across the SDLC by the end of 2025.\nTrend 4: AI tools will become increasingly multimodal\nIt\u2019s New Year\u2019s Eve, and you\u2019re listening to Auld Lang Syne while hearting your Insta friends\u2019 party pics and reading the celebration headlines from around the world \u2013 congrats, you\u2019re essentially a multimodal AI machine. Multimodal AI can simultaneously process information from multiple data types, like text, images, audio, and video, versus unimodal AI, which relies on a single data type.\nAccording to a report by MIT Technology Review, the global multimodal AI market is expected to grow at an annual average rate of 32.2% between 2019 and 2030 to reach US$8.4 billion. Multimodal AI tools are already being used in testing, especially for areas where having an improved understanding of visual elements, user interactions, and contextual information helps to ensure test accuracy and coverage. For instance, a UI tester could leverage multimodal AI to analyze user interface screenshots and user interaction data.\nTrend 5: AI agents will work collaboratively alongside humans\nDictionary.com chose \u201cdemure\u201d as its 2024 Word of the Year. If we had to predict the buzzword in tech circles for 2025, it would be \u201cagentic AI\u201d (okay, that\u2019s two words). Agentic AI will not be demure-ish but will confidently move to the front of the AI tool collection.\nAgentic AI happens when autonomous \u201cagents\u201d make decisions, plan actions, or solve problems independently, with little to no human interaction. Agentic AI is goal-oriented and focused on outcomes. Recent agentic AI launches include Salesforce Agentforce and ServiceNow AI Agents embedded in its Now Platform.\nAccording to Gartner, by 2028, 33% of enterprise software applications will include agentic AI, up from less than 1% in 2024, enabling 15% of day-to-day work decisions to be made more autonomously. Agentic AI is still evolving, and Gartner expects software developers to be some of the first to use it as existing AI coding assistants gain maturity, and organizations look for ways to increase the number of automatable testing tasks and workflows.\nThe advantage of agentic AI for software developers is its ability to automate multiple steps in the SDLC based on context and objectives. Agentic AI has the potential to write code and review code for errors as well as take on some of the tedious and repetitive tasks like bug fixes, freeing up developers to focus on more business-critical activities. But there\u2019s one major caveat: Regardless of how autonomous AI becomes, it\u2019s our opinion that a certain level of human oversight will always be required.\nAI will continue to transform software development and testing\nFor the second consecutive year, AI tops the list (58%) as the most important technology in the year ahead in the annual IEEE survey of global technology leaders. AI isn\u2019t one of those tech trends that come and go. AI has staying power, and as it continues to evolve and innovate, we predict an ongoing trend of it helping enterprises and their testing teams succeed in 2025 and beyond.\nIf you would like to learn more about AI and testing, read our 4 ways generative AI will transform the way you manage testing guide and discover how generative AI can boost your IT and quality engineering teams\u2019 testing efficiency."
    },
    {
      "url": "https://devops.com/continuous-testing-the-key-to-quality-assurance-in-the-devops-era/#:~:text=There%20are%20several%20benefits%20of,continuous%20testing%20in%20DevOps%2C%20including",
      "text": "Continuous testing has emerged as a critical aspect of quality assurance in the DevOps era. In recent years, there has been a rapid transformation in software development methodologies. With the advent of DevOps, there has been a significant shift toward automation and continuous integration and delivery (CI/CD). The traditional approaches to quality assurance are no longer adequate to meet the needs of modern software development practices. Continuous testing is the key to ensuring that the software is delivered on time, with quality and reliability.\nDefinition of Continuous Testing\nContinuous testing is a process of executing automated tests continuously throughout the software development life cycle to provide rapid feedback on the quality of the code. It is an essential component of DevOps that ensures that the software is always ready for release. Continuous testing allows for the detection of defects and errors at an early stage, which reduces the risk of production issues and enhances the overall quality of the software.\nImportance of Continuous Testing in DevOps\nTo understand the importance of continuous testing in DevOps, it is essential to understand the evolution of quality assurance in software development.\nTraditional Waterfall Method\nThe traditional waterfall model of software development had a sequential approach to quality assurance. Testing was only performed after the development phase was completed. This approach often led to delays and increased costs due to detecting defects later.\nAgile Methodology\nThe Agile methodology introduced an innovative approach to quality assurance. Testing became an integral part of the software development lifecycle, and the focus was on continuous feedback and improvement. However, even with Agile, testing was still done in batches, and there was a significant amount of manual testing involved, which could be time-consuming and prone to errors.\nDevOps Model\nThe DevOps model took the concept of continuous feedback and improvement to a new level. With DevOps, testing became automated, and continuous integration and delivery became the norm. This approach significantly reduced the time to market, increased collaboration between development and operations teams, and enhanced the overall quality of the software.\nThe Role of Continuous Testing in DevOps\nDefinition of DevOps\nDevOps is a software development methodology that emphasizes collaboration and communication between development and operations teams to deliver software products rapidly and reliably. It involves the integration of development, testing and operations into a continuous process, with automation as a key enabler.\nImportance of Continuous Testing in DevOps\nContinuous testing is a critical component of DevOps. It enables the testing of software continuously throughout the development lifecycle, which helps to identify defects and issues early on.\nThis early detection of issues is essential in reducing the risk of production issues, and it allows for timely feedback, which facilitates faster decision-making.\nBenefits of Continuous Testing in DevOps\nThere are several benefits of continuous testing in DevOps, including:\n\u00b7 Faster time to market: Continuous testing enables the rapid release of software, reducing the time to market significantly.\n\u00b7 Improved collaboration: DevOps emphasizes collaboration between development and operations teams, and continuous testing facilitates this collaboration by providing continuous feedback on the quality of the code.\n\u00b7 Enhanced quality: Continuous testing enables the early detection of defects and issues, which reduces the risk of production issues and enhances the overall quality of the software.\n\u00b7 Reduced costs: Continuous testing can help to reduce costs by identifying defects and issues early on, which reduces the cost of fixing issues later in the development cycle.\nImplementing Continuous Testing in DevOps\nChoosing the Right Test Automation Tools\nThe selection of the right test automation tools is critical to the success of continuous testing in DevOps. Some of the popular test automation tools for DevOps include Selenium, Appium, Cucumber and JUnit.\nIt is essential to choose a tool that is easy to use, integrates well with other DevOps tools, and has strong community support.\nBuilding an Effective Test Automation Framework\nAn effective test automation framework is essential to implementing continuous testing in DevOps. A test automation framework is a set of guidelines and standards that help in the creation and execution of automated tests.\nA good framework should be modular, flexible, and scalable. It should also be easy to maintain and extend as the project grows.\nIntegrating Continuous Testing Into the DevOps Pipeline\nTo implement continuous testing in DevOps, testing should be integrated into the DevOps pipeline. The DevOps pipeline is a series of steps that automate the software development process, from code commit to production release.\nContinuous testing should be integrated into every stage of the pipeline, from unit testing to acceptance testing.\nBest Practices for Continuous Testing in DevOps\nTo achieve the full benefits of continuous testing in DevOps, it is important to follow best practices. Some of the best practices for continuous testing in DevOps include:\n\u00b7 Test early and often: Testing should begin early in the development cycle and continue throughout the development process.\n\u00b7 Automate everything: Automation is essential for continuous testing in DevOps. All tests should be automated, including unit tests, integration tests and acceptance tests.\n\u00b7 Integrate testing into the DevOps pipeline: Testing should be integrated into every stage of the DevOps pipeline, from code commit to production release.\n\u00b7 Use the right test data: Test data should be relevant and representative of real-world scenarios.\n\u00b7 Collaborate across teams: Collaboration between development, testing and operations teams is essential for the success of continuous testing in DevOps.\nChallenges and Solutions for Continuous Testing in DevOps\nThere are several challenges to implementing continuous testing in DevOps, including:\n\u00b7 Test data management: It can be challenging to manage and maintain relevant test data that accurately represent real-world scenarios.\n\u00b7 Test environment management: Ensuring that the test environment is consistent and reliable can be a challenge, especially in complex applications.\n\u00b7 Test automation maintenance: Test automation scripts can become complex and challenging to maintain, especially as the application evolves.\nStrategies for Overcoming Continuous Testing Challenges\nThere are several strategies for overcoming the challenges of continuous testing in DevOps, including:\n\u00b7 Test data management: Creating synthetic test data can help ensure that the data is relevant and representative of real-world scenarios.\n\u00b7 Test environment management: Using containerization technology such as Docker can help ensure a consistent and reliable test environment.\n\u00b7 Test automation maintenance: Regular maintenance and refactoring of test automation scripts can help keep them up-to-date and relevant.\nConclusion\nContinuous testing is the key to ensuring quality in the DevOps era. It enables the rapid release of software, reduces the risk of production issues and enhances the overall quality of the software. Implementing continuous testing in DevOps requires choosing the right test automation tools, building an effective test automation framework, integrating testing into the DevOps pipeline and following DevOps best practices. Although there are challenges to implementing continuous testing in DevOps, these challenges can be overcome with the right strategies and practices. By implementing continuous testing in DevOps, organizations can achieve faster time to market, improved collaboration, enhanced quality and reduced costs."
    },
    {
      "url": "https://www.functionize.com/blog/gartner-top-strategic-trends-2025#:~:text=Gartner%C2%AE%20Top%20Strategic%20Trends%20of,from%20zero%20percent%20in",
      "text": "Gartner\u00ae Top Strategic Trends of 2025\nLearn how Agentic AI, named a Top Strategic Trend for 2025 by Gartner\u00ae, is transforming automation with intelligent agents capable of autonomous decision-making.\nAgentic AI was just named as one of Gartner\u00ae Top Strategic Trends of 2025. If you think you\u2019ve been hearing Agentic a lot lately, there\u2019s a good reason why.\nThe tech industry is rapidly incorporating AI and automation to create technologies capable of fully executing common jobs and tasks. These Enterprise Agents are highly trained to perform work functions autonomously instead of scripted tasks or fixed automation. This is fueled, in large part, by the quantum leap with LLMs and Machine Learning. Importantly, a key point in Gartner\u00ae research is that not all Agentic tools are as capable, as illustrated below.\nThe \u201cAI Agency gap\u201d highlights how many tools are not truly Agentic, though many vendors using some form of AI will make that claim. Gartner\u00ae predicts that \u201cby 2028, at least 15% of day-to-day work decisions will be made autonomously through agentic AI, up from zero percent in 2024.\u201d We believe, this mirrors what we at Functionize are seeing in the market.\nThere are two simultaneous things happening in the market:\n1. a general disillusionment with many tools that vendors promised would create high value and\n2. a renewed search for software products that actually do create value in process automation.\nCome see how Functionize provides real Agentic value today and get a demo today!\nGartner, Top Strategic Technology Trends for 2025: Agentic AI, By Tom Coshow, Arnold Gao, Lawrence Pingree, Anushree Verma, Don Scheibenreif, Haritha Khandabattu, Gary Olliffe, 21 October 2024.\nGARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved.\nThis graphic was published by Gartner, Inc. as part of a larger research document and should be evaluated in the context of the entire document. The Gartner document is available upon request from Functionize.\nGartner does not endorse any vendor, product or service depicted in its research publications and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner\u2019s Research & Advisory organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose."
    },
    {
      "url": "https://www.harness.io/blog/importance-of-continuous-verification#:~:text=Connecting%20observability%20and%20logging%20tools,reduce%20the%20risk%20of%20change",
      "text": "Connecting observability and logging tools to your production release process is powerful. With machine learning, you can identify when error rates or metrics are unsatisfactory, quickly initiating a rollback. This approach can significantly reduce the risk of change.\nAn existential question for any engineer with deployment responsibility is \u201cwhen is your deployment over?\u201d. Digging into that question, an immediate follow up would be \u201cwhat are you deploying?\u201d. In the Kubernetes world, some might draw the line as when the Readiness Probe is complete. Application centric views might be when the first set of transactions are successful or their transactional boundaries are crossed. Certain architectural thought is that the deployment is over when the next deployment goes in.\nRegardless where you fall in that spectrum, there is a common thread that binds all of the approaches. There is some sort of validation that has to occur. From an engineer hitting their newly updated endpoint to validate that the change has gone in, to SREs interrogating the observability stack, validation has to occur to make the judgment call.\nThe go or no-go decision has become more complex in modern software delivery teams. This is because of usually a lack of an absolute failure. If there was an absolute failure, for example the deployment failed to start, the call to action can be pretty quick; a rollback would ensue almost immediately. Though with the scrutiny of modern software delivery and rise in more atomic microservices, deployments in the form of the application just starting can be expected to be initially successful in production. This lack of a major event makes finding the root cause in line with the adage \u201cthere is no root cause\u201d.\nSite Reliability Engineering is about finding the needle in the haystack, before the needle was placed in the haystack. Closely interrogating the myriad of observability, monitoring, and logging tools looking for trending towards failure is not an easy process. Having a dedicated SRE for every deployment analyzing this data can be costly. This is why Harness has created Continuous Verification (CV) to do this analysis on your behalf.\nContinuous Verification is Harness\u2019s deployment verification. Included as part of your free or paid Harness Continuous Delivery subscription, Continuous Verification validates your deployments with your observability, monitoring, and logging tools and platforms.\nBy adding a Verify Step to your Harness Pipeline, you can allow Harness to query a Health Source or Health Sources such as Prometheus, Datadog, Splunk, AppDynamics, or even a Custom Health Source, to help make a judgment call on your behalf. Can look at the more exhaustive list in the CV documentation.\nContinuous Verification is integrated into your Harness Pipeline. You do have the ability to allow Harness to take automatic action on your behalf or enact a Failure Strategy that is supported by the Harness Pipeline such as a manual intervention.\nContinuous Verification can work in conjunction with your deployment strategy. For example if executing a Canary Deployment, CV can analyze the canary phases before proceeding on.\nGetting Continuous Verification enabled on your Pipeline is an important step to achieving resiliency goals with your deployments.\nCombining two engineering sayings, first \u201cquality is everyone\u2019s responsibility\u201d and \u201cslowness is the new down\u201d will produce a third saying, \u201creliability is everyone\u2019s responsibility\u201d. Your Continuous Delivery Pipeline is a culmination of expertise across multiple disciplines. Having an SRE manually verify every deployment is not scalable thus having a system to assist with the most arduous tasks in validation will allow you to scale. SRE expertise can be consistently and systematically applied in your Delivery Pipeline with Continuous Verification.\nChange inherently brings risk and can be a risk to reliability. Even slight blips in reliability can cause reputation damage and revenue loss. Though innovation requires change and balancing innovation and reliability is a paradox many engineers face. With Continuous Verification, adding a layer of systemic verification and validation allows for change to push forward with the ability to baseline what is or was normal.\nHaving verification and validation as part of your pipeline is crucial. Even if you are not using Harness for your Delivery Pipelines, leveraging something like Spinnaker\u2019s Kayenta or Argo\u2019s Rollout Analysis should be leveraged if using either of those two tools. Harness\u2019s Continuous Verification is designed for ease of use and flexibility no matter which deployment strategy you take, trying CV out is a great first step.\nTo get started with Harness Continuous Verification, check out this tutorial on Harness Developer Hub which goes through verifying a Kubernetes deployment using Prometheus. Since \u201creliability is everyone\u2019s responsibility\u201d, adding a Verification Step to your Harness Pipeline is a prudent step and included as part of your Harness CD subscription (Sign Up Here). Take a look at the tutorial and get further on the reliability journey, today."
    },
    {
      "url": "https://www.cm-alliance.com/cybersecurity-blog/ai-in-software-testing-what-is-it-and-how-to-use-it#:~:text=1,testing%20or%20regression%20suite%20analysis",
      "text": "AI in Software Testing: What Is It and How To Use It?\nDate: 22 September 2024\nSoftware testing is a critical process that is used in the software development life cycle. It aids in the confirmation of the application functionality as well as the quality of meeting the user\u2019s needs. However testing can be very much time consuming and requires a lot of resources. It is here that artificial intelligence (AI) can play a role.\nWhat is AI testing software? AI is revolutionizing software testing in exciting ways. It is making testing faster, smarter, and more efficient. AI-based software testing tools can automatically generate test cases, execute tests, detect defects, and even fix bugs with minimal human intervention. As a result, AI is helping overcome key pain points in testing like time constraints, skill shortages, complex test maintenance and more. AI plays an unprecedented role in enhancing both software testing and content creation processes. While AI algorithms boost software testing through predictable efficiency, in realms like content management, selecting the best AI paraphrasing tool becomes vital to bypass detection systems without compromising quality. These sophisticated tools not only make paraphrasing faster but also ensure the uniqueness and integrity of output\u2014elemental for creators and strategists dealing with large data sets.\nWhat is AI in Software Testing\nArtificial intelligence in software testing refers to test automation solutions that leverage AI/machine learning algorithms, including generative ai development services, instead of predefined rules. These intelligent systems can:\n- Learn from existing data sets.\n- Adapt to new test scenarios.\n- Make predictions and decisions on their own.\n- Improve over time with more data and usage.\nIn essence, AI introduces self-learning capabilities to software testing. This enables dynamic and autonomous test execution without explicit programming.\nHow to use AI in software testing? AI testing tools can process software code, UIs, logs, defect reports and operational data to uncover insights. This data can then be used to train machine learning models to automate various testing tasks, significantly enhancing the efficiency and effectiveness of AI software development services.\nKey Capabilities of AI in Testing\nHere are some of the most popular applications of AI QA testing in software:\n1. Automated Test Case Generation\nTest case generation can be done automatically with the help of AI by analyzing the code of the application, requirement documents and test history. This is far easier and less time consuming than having to manually create the test case on one\u2019s own.\nOther advantages of Smart AI algorithms include the ability to identify the gaps in the current test suites and produce new test cases to cover those gaps. This makes it possible to test all aspects of the design with little repetition.\n2. Automated Test Execution\nExecuting test cases requires setting up test data, following pre-defined steps and comparing expected vs actual outcomes. AI-based testing platforms can automate these repetitive tasks without human intervention.\nSome AI testing tools even support computer vision and image recognition capabilities for automated UI testing. This is extremely useful for cross-browser and cross-device testing.\n3. Intelligent Test Maintenance\nOver time, test suites can become unoptimized and redundant as the application evolves. AI algorithms can analyze these suites and recommend modifications like:\n- Removing obsolete test cases.\n- Updating impacted test cases.\n- Generating fresh test cases.\nThis \u201cself-healing\u201d minimizes outdated and duplicate test scripts.\n4. Defect Detection and Logging\nAI testing tools can monitor system behaviour during test execution to detect failures, crashes or anomalies. The defects get automatically logged with relevant execution details, screenshots and environment information.\nSome AI solutions can even classify defects by criticality and root cause (e.g., application crash, UI flaw, database error, etc.). This accelerates debugging and correction.\n5. Predictive Analytics\nHistorical testing data holds valuable insights that can be uncovered through machine learning. AI algorithms can crunch this data to:\n- Predict areas and modules most prone to errors.\n- Forecast defects that may arise from a code change.\n- Prescribe additional test coverage for high-risk aspects.\n- Estimate optimal testing timelines and effort.\nThese insights enhance test planning and resource allocation.\nKey Benefits of Leveraging AI in Testing\nAdopting AI for software testing provides multifaceted benefits ranging from productivity gains to better software quality.\n- Increased testing efficiency. AI automation handles time-intensive tasks like test case design, test execution, data setup, defect logging etc. This reduces the testing time and effort by over 50% as per Capgemini research. Teams get more productive and can run more test cycles.\n- Enhanced test coverage. AI algorithms enhance test coverage by generating additional test cases for unhandled scenarios. This minimizes the risk of undiscovered defects in production.\n- Rapid feedback cycles. With agile teams delivering smaller changes more frequently, the regression testing needs are growing exponentially. AI allows running these repetitive regression test cycles rapidly without extensive manual intervention.\n- Superior test accuracy. Unlike manual testing prone to human errors, AI automation provides reliable and consistent testing devoid of emotional, physical or mental influences.\n- Effective utilization of SMEs. AI handles tedious testing tasks, while subject matter experts can focus on creative test analysis and design. This ensures optimal utilization of high-skilled engineering resources.\n- Proactive defect prevention. AI provides predictive insights to estimate application reliability even before testing begins. Teams can address potentially problematic areas upfront through focused code reviews or static analysis.\nChallenges in Adopting AI for Software Testing\nWhile AI innovation is accelerating test automation, some key challenges need consideration:\n- Lack of skilled resources. AI/ML adoption involves data science skills which testing teams, in their traditional sense, do not possess. This means that organizations require to retrain testers or recruit new AI-specialized employees. It might also be a good idea to leverage nearshore software development services to access skilled AI professionals at a lower cost while maintaining close collaboration and efficient project management.\n- Integration with existing tools. Almost all testing teams already employ well-established tools such as for test automation. Another challenge is the technical issue of how to incorporate these tools with the new generation of AI testing environments. Leveraging an AI API can bridge this gap by enabling seamless integration between legacy systems and modern AI-powered testing platforms.\n- Initial data acquisition. AI models need considerable data to begin providing value. Most companies do not readily have the required volumes of structured test data.\n- Data privacy and security concerns. Testing data often includes sensitive customer information, which raises data security and privacy needs. Ethical AI practices are integral to preventing data misuse.\n- Lack of explainability. Unlike rule-based systems, AI testing outcomes maybe opaque and not explainable. This lack of transparency hinders adoption.\nBest Practices for Leveraging AI in Testing\nHere are some tips to maximize the value of AI test automation tools:\n- Start small. Do not try to replace all the test automation at once, integrate AI into some particular scenarios, for instance, UI testing or regression suite analysis.\n- Monitor and tweak continuously. Continuously monitor the effectiveness of the AI tool and modify the algorithms whenever necessary. The more usage it is subjected to, the better it becomes.\n- Maintain machine-readable test assets. Understand that test cases, test data, logs and defect reports need to be formatted to be easily understandable by the AI tool.\n- Analyze AI testing results. Analyse the testing outcomes such as the defect detection patterns, test recommendations etc. and draw useful insights for the improvement of the testing process.\n- Retrain the models. In case the AI models are no longer accurate or efficient in the long run, the models should be retrained using the latest test data in order to rectify the predictions.\n- AI should be complemented by human intelligence. Do not over rely on AI capabilities while testing but also do not make manual testing too exhaustive. This offers a perfect and fail-safe strategy.\nThe Future of AI in Software Testing\nAI innovation in testing is still evolving with immense scope for growth. Recent research shows that over 71% of respondents want to integrate AI into application development and SDLC management procedures.\nHere are some futuristic AI capabilities expected to widen in testing:\n- Conversational testing. This is because testing teams could use conversational interfaces to inquire about the status of the tests, report bugs or even initiate test runs through natural language.\n- Holistic automation. Automated testing from the requirements level, test script generation, test running, defect reporting and tracking within one smart system.\n- Autonomous testing. Automated testing systems that challenge progressively developing software on their own with little or no interference.\n- Crowdtesting. AI could complement crowd-testing through proper assignment of test cases to human testers depending on their abilities, their gadgets, and previous performance.\n- Instant feedback. AI could offer predictions in real time during development on possible defects, technical debt and reliability problems that would allow agile teams to correct direction in real time.\nWhen you combine the creative power of human intelligence with the scale, speed and accuracy of Artificial Intelligence, the possibilities seem endless. This opens up an exciting future for software test professionals who want to elevate QA practices and evolve into strategic \u201cquality advisors.\u201d\nConclusion\nAI innovation is rapidly advancing, and its testing applications are still evolving. As algorithms get more sophisticated, AI and software testing will become increasingly intertwined. Future possibilities include AI-enabled code reviews for defect prediction, automated code remediation, intelligent test environment simulation and automated root cause analysis. To stay ahead, teams should explore emerging AI-driven tools that simplify testing workflows, enhance accuracy, and accelerate development cycles. Keeping pace with these advances will be key to harnessing their full potential.\nOrganisations also need to nurture partnerships between their AI and QA teams to complement algorithmic capabilities with testing domain expertise. This human-machine collaboration can elevate software quality and efficiency to new heights."
    },
    {
      "url": "https://www.cm-alliance.com/cybersecurity-blog/ai-in-software-testing-what-is-it-and-how-to-use-it#:~:text=2,to%2C%20the%20better%20it%20becomes",
      "text": "AI in Software Testing: What Is It and How To Use It?\nDate: 22 September 2024\nSoftware testing is a critical process that is used in the software development life cycle. It aids in the confirmation of the application functionality as well as the quality of meeting the user\u2019s needs. However testing can be very much time consuming and requires a lot of resources. It is here that artificial intelligence (AI) can play a role.\nWhat is AI testing software? AI is revolutionizing software testing in exciting ways. It is making testing faster, smarter, and more efficient. AI-based software testing tools can automatically generate test cases, execute tests, detect defects, and even fix bugs with minimal human intervention. As a result, AI is helping overcome key pain points in testing like time constraints, skill shortages, complex test maintenance and more. AI plays an unprecedented role in enhancing both software testing and content creation processes. While AI algorithms boost software testing through predictable efficiency, in realms like content management, selecting the best AI paraphrasing tool becomes vital to bypass detection systems without compromising quality. These sophisticated tools not only make paraphrasing faster but also ensure the uniqueness and integrity of output\u2014elemental for creators and strategists dealing with large data sets.\nWhat is AI in Software Testing\nArtificial intelligence in software testing refers to test automation solutions that leverage AI/machine learning algorithms, including generative ai development services, instead of predefined rules. These intelligent systems can:\n- Learn from existing data sets.\n- Adapt to new test scenarios.\n- Make predictions and decisions on their own.\n- Improve over time with more data and usage.\nIn essence, AI introduces self-learning capabilities to software testing. This enables dynamic and autonomous test execution without explicit programming.\nHow to use AI in software testing? AI testing tools can process software code, UIs, logs, defect reports and operational data to uncover insights. This data can then be used to train machine learning models to automate various testing tasks, significantly enhancing the efficiency and effectiveness of AI software development services.\nKey Capabilities of AI in Testing\nHere are some of the most popular applications of AI QA testing in software:\n1. Automated Test Case Generation\nTest case generation can be done automatically with the help of AI by analyzing the code of the application, requirement documents and test history. This is far easier and less time consuming than having to manually create the test case on one\u2019s own.\nOther advantages of Smart AI algorithms include the ability to identify the gaps in the current test suites and produce new test cases to cover those gaps. This makes it possible to test all aspects of the design with little repetition.\n2. Automated Test Execution\nExecuting test cases requires setting up test data, following pre-defined steps and comparing expected vs actual outcomes. AI-based testing platforms can automate these repetitive tasks without human intervention.\nSome AI testing tools even support computer vision and image recognition capabilities for automated UI testing. This is extremely useful for cross-browser and cross-device testing.\n3. Intelligent Test Maintenance\nOver time, test suites can become unoptimized and redundant as the application evolves. AI algorithms can analyze these suites and recommend modifications like:\n- Removing obsolete test cases.\n- Updating impacted test cases.\n- Generating fresh test cases.\nThis \u201cself-healing\u201d minimizes outdated and duplicate test scripts.\n4. Defect Detection and Logging\nAI testing tools can monitor system behaviour during test execution to detect failures, crashes or anomalies. The defects get automatically logged with relevant execution details, screenshots and environment information.\nSome AI solutions can even classify defects by criticality and root cause (e.g., application crash, UI flaw, database error, etc.). This accelerates debugging and correction.\n5. Predictive Analytics\nHistorical testing data holds valuable insights that can be uncovered through machine learning. AI algorithms can crunch this data to:\n- Predict areas and modules most prone to errors.\n- Forecast defects that may arise from a code change.\n- Prescribe additional test coverage for high-risk aspects.\n- Estimate optimal testing timelines and effort.\nThese insights enhance test planning and resource allocation.\nKey Benefits of Leveraging AI in Testing\nAdopting AI for software testing provides multifaceted benefits ranging from productivity gains to better software quality.\n- Increased testing efficiency. AI automation handles time-intensive tasks like test case design, test execution, data setup, defect logging etc. This reduces the testing time and effort by over 50% as per Capgemini research. Teams get more productive and can run more test cycles.\n- Enhanced test coverage. AI algorithms enhance test coverage by generating additional test cases for unhandled scenarios. This minimizes the risk of undiscovered defects in production.\n- Rapid feedback cycles. With agile teams delivering smaller changes more frequently, the regression testing needs are growing exponentially. AI allows running these repetitive regression test cycles rapidly without extensive manual intervention.\n- Superior test accuracy. Unlike manual testing prone to human errors, AI automation provides reliable and consistent testing devoid of emotional, physical or mental influences.\n- Effective utilization of SMEs. AI handles tedious testing tasks, while subject matter experts can focus on creative test analysis and design. This ensures optimal utilization of high-skilled engineering resources.\n- Proactive defect prevention. AI provides predictive insights to estimate application reliability even before testing begins. Teams can address potentially problematic areas upfront through focused code reviews or static analysis.\nChallenges in Adopting AI for Software Testing\nWhile AI innovation is accelerating test automation, some key challenges need consideration:\n- Lack of skilled resources. AI/ML adoption involves data science skills which testing teams, in their traditional sense, do not possess. This means that organizations require to retrain testers or recruit new AI-specialized employees. It might also be a good idea to leverage nearshore software development services to access skilled AI professionals at a lower cost while maintaining close collaboration and efficient project management.\n- Integration with existing tools. Almost all testing teams already employ well-established tools such as for test automation. Another challenge is the technical issue of how to incorporate these tools with the new generation of AI testing environments. Leveraging an AI API can bridge this gap by enabling seamless integration between legacy systems and modern AI-powered testing platforms.\n- Initial data acquisition. AI models need considerable data to begin providing value. Most companies do not readily have the required volumes of structured test data.\n- Data privacy and security concerns. Testing data often includes sensitive customer information, which raises data security and privacy needs. Ethical AI practices are integral to preventing data misuse.\n- Lack of explainability. Unlike rule-based systems, AI testing outcomes maybe opaque and not explainable. This lack of transparency hinders adoption.\nBest Practices for Leveraging AI in Testing\nHere are some tips to maximize the value of AI test automation tools:\n- Start small. Do not try to replace all the test automation at once, integrate AI into some particular scenarios, for instance, UI testing or regression suite analysis.\n- Monitor and tweak continuously. Continuously monitor the effectiveness of the AI tool and modify the algorithms whenever necessary. The more usage it is subjected to, the better it becomes.\n- Maintain machine-readable test assets. Understand that test cases, test data, logs and defect reports need to be formatted to be easily understandable by the AI tool.\n- Analyze AI testing results. Analyse the testing outcomes such as the defect detection patterns, test recommendations etc. and draw useful insights for the improvement of the testing process.\n- Retrain the models. In case the AI models are no longer accurate or efficient in the long run, the models should be retrained using the latest test data in order to rectify the predictions.\n- AI should be complemented by human intelligence. Do not over rely on AI capabilities while testing but also do not make manual testing too exhaustive. This offers a perfect and fail-safe strategy.\nThe Future of AI in Software Testing\nAI innovation in testing is still evolving with immense scope for growth. Recent research shows that over 71% of respondents want to integrate AI into application development and SDLC management procedures.\nHere are some futuristic AI capabilities expected to widen in testing:\n- Conversational testing. This is because testing teams could use conversational interfaces to inquire about the status of the tests, report bugs or even initiate test runs through natural language.\n- Holistic automation. Automated testing from the requirements level, test script generation, test running, defect reporting and tracking within one smart system.\n- Autonomous testing. Automated testing systems that challenge progressively developing software on their own with little or no interference.\n- Crowdtesting. AI could complement crowd-testing through proper assignment of test cases to human testers depending on their abilities, their gadgets, and previous performance.\n- Instant feedback. AI could offer predictions in real time during development on possible defects, technical debt and reliability problems that would allow agile teams to correct direction in real time.\nWhen you combine the creative power of human intelligence with the scale, speed and accuracy of Artificial Intelligence, the possibilities seem endless. This opens up an exciting future for software test professionals who want to elevate QA practices and evolve into strategic \u201cquality advisors.\u201d\nConclusion\nAI innovation is rapidly advancing, and its testing applications are still evolving. As algorithms get more sophisticated, AI and software testing will become increasingly intertwined. Future possibilities include AI-enabled code reviews for defect prediction, automated code remediation, intelligent test environment simulation and automated root cause analysis. To stay ahead, teams should explore emerging AI-driven tools that simplify testing workflows, enhance accuracy, and accelerate development cycles. Keeping pace with these advances will be key to harnessing their full potential.\nOrganisations also need to nurture partnerships between their AI and QA teams to complement algorithmic capabilities with testing domain expertise. This human-machine collaboration can elevate software quality and efficiency to new heights."
    },
    {
      "url": "https://www.opencredo.com/blogs/what-is-continuous-verification#:~:text=The%20inherent%20complexity%20of%20these,Many",
      "text": "What is Continuous Verification?\nContinuous Verification is a term that is starting to pop up from time-to-time\u2026 but what does it mean? Well\u2026 according to Nora Jones and Casey Rosenthal, authors of O\u2019Reilly\u2019s Chaos Engineering books,\u201cContinuous verification (CV) is a discipline of proactive experimentation, implemented as tooling that verifies system behaviors.\nThis stands in contrast to prior common practices in software quality assurance, which favor reactive testing, implemented as methodologies that validate known properties of software. This isn\u2019t to say that prior common practices are invalid or should be deprecated. Alerting, testing, code reviews, monitoring, SRE practices, and the like\u2014these are all great practices and should be encouraged\u201dOver the course of this post, we will unpack this statement: to understand what is behind it and what it might mean for your development process.\nThe delivery of software has been transformed in recent years by increased adoption of Continuous Integration (CI) and Continuous Delivery & Deployment (CD) processes, and the introduction of the DevOps approach to infrastructure management.To provide a, somewhat simplistic, summary of these processes, we could say that they are primarily concerned with the automated validation and delivery of application and infrastructure deliverables.\nWe seek to ensure that we have a single pipeline that automatically ensures that our deliverables meet both functional and non-functional requirements and are delivered efficiently and reliably through to production.The focus here is on ensuring each individual delivery is evaluated against quality criteria as defined by the delivery team and business, so that we can have confidence that the risk of deployment is low. We further reduce risk by ensuring that deliveries are small, incremental and can be reversed if the delivery is proved problematic.These methods are fundamentally important for the delivery of modern software and can grow to become sophisticated with full test deployment to environments which replicate production and use of service virtualisation to model system behaviour.So, what shortfall does Continuous Verification fulfil?\nComplexity\nFollowing alongside DevOps and CI/CD, the adoption of cloud, distributed NoSql databases, microservices and Kubernetes, has generated an explosion in the complexity of IT systems. Systems which previously had 3 layers (presentation, application and persistence) may now have hundreds of moving parts.\nThe inherent complexity of these systems has been pushed from the monolith into multiple components composing the system architecture and infrastructure. This is reflected in studies and surveys, for example 76% of chief information officers surveyed think growing information technology complexity may soon make it impossible to efficiently manage digital performance.Many systems have reached the point where it is no longer possible for a single person to fully understand its architecture and interactions.\nOthers are following along closely behind. These systems are complex: their behavior can no longer be reasonably inferred from their properties. So, from a CI/CD perspective, we cannot reason about how a deliverable will behave within the production environment based only on quality guarantees provided by the CI/CD pipeline.\nThis is pretty scary stuff for enterprise IT which has long operated in a command-and-control manner (despite attempts to adopt more Agile methods) and, for which, CI/CD itself is often a relatively novel methodology. The control we have demanded is now impossible in the traditional sense. We must now think about our IT systems as a whole rather than reduce them to individual components.\nOvercoming Complexity\nSo how do we manage quality in a complex system?In order to make some inroads here, we might look to some of the basic methods used by science for studying complex systems like biology. By performing experiments on the system we can learn about the behaviour of the system and whether this behaviour lines up with our expectations.\n- Generate a hypothesis about expected behaviour: as per the problem of induction, we actually require a null hypothesis - namely that nothing will change.\n- Configure a measurement for a dependent variable, the thing that will tell us whether something has changed.\n- Alter an independent variable and see whether changes occur in measurements for the dependent variable which would invalidate the null hypothesis.\nThis approach is, of course, the principles of chaos restated. By performing experiments we learn how chaos manifests in our system and whether the system behaves as we expect it to.So, we need to generate hypotheses about how the system works. There are many people involved in the development and operation of an IT system: developers, SREs, managers and architects. We can expect each of these to only have a partial mental model about how a system works. So, we must bring together all stakeholders so that we can get a broad perspective and use the power of the collective group to generate effective hypotheses.With our hypotheses we can then experiment with the system to confirm/fail to refute our beliefs about the system - and in the process gain an understanding about how the system will behave under certain conditions or following particular deliveries.\nContinuous Verification\nContinuous Verification (CV) is an extension of the CI/CD process that is concerned with verifying the system as a whole. For the cynical, it might be tempting to consider it a rebranding of Chaos Engineering. To turn this around, Chaos Engineering is simply one tool in the arsenal of CV: it includes, and depends on, a whole array of tooling - from canary deployments to observability to FinOps.\nThe critical element is that we are verifying that the system as a whole behaves as we expect it to, in addition to our existing tests for the behaviour of component parts.We must test in production - in other environments (dev/uat) there are different sets of conditions in play which may produce different results.\nThis means that we must have discipline in the deployment process to ensure that tests can be delivered to samples of traffic and rolled back effectively if they cause significant issues.Taking a broad view, Continuous Verification may grow to include all methods and tools which allow us to understand complex IT systems in more detail. There is a significant historical body of work for resilience engineering, safety engineering and complex systems modelling.\nOver time we would expect that elements of these disciplines will be trialed and adapted to see what is useful in the IT context, with those suitable to the more general cases becoming mainstream.This blog is written exclusively by the OpenCredo team. We do not accept external contributions.\nRelated articles\nStay up to date with our latest blogs posts."
    },
    {
      "url": "https://www.opencredo.com/blogs/what-is-continuous-verification#:~:text=Continuous%20Verification%20is%20a%20term,tooling%20that%20verifies%20system%20behaviors",
      "text": "What is Continuous Verification?\nContinuous Verification is a term that is starting to pop up from time-to-time\u2026 but what does it mean? Well\u2026 according to Nora Jones and Casey Rosenthal, authors of O\u2019Reilly\u2019s Chaos Engineering books,\u201cContinuous verification (CV) is a discipline of proactive experimentation, implemented as tooling that verifies system behaviors.\nThis stands in contrast to prior common practices in software quality assurance, which favor reactive testing, implemented as methodologies that validate known properties of software. This isn\u2019t to say that prior common practices are invalid or should be deprecated. Alerting, testing, code reviews, monitoring, SRE practices, and the like\u2014these are all great practices and should be encouraged\u201dOver the course of this post, we will unpack this statement: to understand what is behind it and what it might mean for your development process.\nThe delivery of software has been transformed in recent years by increased adoption of Continuous Integration (CI) and Continuous Delivery & Deployment (CD) processes, and the introduction of the DevOps approach to infrastructure management.To provide a, somewhat simplistic, summary of these processes, we could say that they are primarily concerned with the automated validation and delivery of application and infrastructure deliverables.\nWe seek to ensure that we have a single pipeline that automatically ensures that our deliverables meet both functional and non-functional requirements and are delivered efficiently and reliably through to production.The focus here is on ensuring each individual delivery is evaluated against quality criteria as defined by the delivery team and business, so that we can have confidence that the risk of deployment is low. We further reduce risk by ensuring that deliveries are small, incremental and can be reversed if the delivery is proved problematic.These methods are fundamentally important for the delivery of modern software and can grow to become sophisticated with full test deployment to environments which replicate production and use of service virtualisation to model system behaviour.So, what shortfall does Continuous Verification fulfil?\nComplexity\nFollowing alongside DevOps and CI/CD, the adoption of cloud, distributed NoSql databases, microservices and Kubernetes, has generated an explosion in the complexity of IT systems. Systems which previously had 3 layers (presentation, application and persistence) may now have hundreds of moving parts.\nThe inherent complexity of these systems has been pushed from the monolith into multiple components composing the system architecture and infrastructure. This is reflected in studies and surveys, for example 76% of chief information officers surveyed think growing information technology complexity may soon make it impossible to efficiently manage digital performance.Many systems have reached the point where it is no longer possible for a single person to fully understand its architecture and interactions.\nOthers are following along closely behind. These systems are complex: their behavior can no longer be reasonably inferred from their properties. So, from a CI/CD perspective, we cannot reason about how a deliverable will behave within the production environment based only on quality guarantees provided by the CI/CD pipeline.\nThis is pretty scary stuff for enterprise IT which has long operated in a command-and-control manner (despite attempts to adopt more Agile methods) and, for which, CI/CD itself is often a relatively novel methodology. The control we have demanded is now impossible in the traditional sense. We must now think about our IT systems as a whole rather than reduce them to individual components.\nOvercoming Complexity\nSo how do we manage quality in a complex system?In order to make some inroads here, we might look to some of the basic methods used by science for studying complex systems like biology. By performing experiments on the system we can learn about the behaviour of the system and whether this behaviour lines up with our expectations.\n- Generate a hypothesis about expected behaviour: as per the problem of induction, we actually require a null hypothesis - namely that nothing will change.\n- Configure a measurement for a dependent variable, the thing that will tell us whether something has changed.\n- Alter an independent variable and see whether changes occur in measurements for the dependent variable which would invalidate the null hypothesis.\nThis approach is, of course, the principles of chaos restated. By performing experiments we learn how chaos manifests in our system and whether the system behaves as we expect it to.So, we need to generate hypotheses about how the system works. There are many people involved in the development and operation of an IT system: developers, SREs, managers and architects. We can expect each of these to only have a partial mental model about how a system works. So, we must bring together all stakeholders so that we can get a broad perspective and use the power of the collective group to generate effective hypotheses.With our hypotheses we can then experiment with the system to confirm/fail to refute our beliefs about the system - and in the process gain an understanding about how the system will behave under certain conditions or following particular deliveries.\nContinuous Verification\nContinuous Verification (CV) is an extension of the CI/CD process that is concerned with verifying the system as a whole. For the cynical, it might be tempting to consider it a rebranding of Chaos Engineering. To turn this around, Chaos Engineering is simply one tool in the arsenal of CV: it includes, and depends on, a whole array of tooling - from canary deployments to observability to FinOps.\nThe critical element is that we are verifying that the system as a whole behaves as we expect it to, in addition to our existing tests for the behaviour of component parts.We must test in production - in other environments (dev/uat) there are different sets of conditions in play which may produce different results.\nThis means that we must have discipline in the deployment process to ensure that tests can be delivered to samples of traffic and rolled back effectively if they cause significant issues.Taking a broad view, Continuous Verification may grow to include all methods and tools which allow us to understand complex IT systems in more detail. There is a significant historical body of work for resilience engineering, safety engineering and complex systems modelling.\nOver time we would expect that elements of these disciplines will be trialed and adapted to see what is useful in the IT context, with those suitable to the more general cases becoming mainstream.This blog is written exclusively by the OpenCredo team. We do not accept external contributions.\nRelated articles\nStay up to date with our latest blogs posts."
    },
    {
      "url": "https://www.softwaretestingmagazine.com/knowledge/ai-powered-automation-the-future-of-smarter-faster-testing/#:~:text=the%20finance%2C%20healthcare%2C%20and%20insurance",
      "text": "In this article, Priya Yesare explains why AI driven software testing is faster, more efficient and more reliable. AI addresses the limitations of traditional test automation by incorporating machine learning, large language models (LLM), natural language processing and predictive analysis to automate complex tasks with improved accuracy.\nAuthor : Priya Yesare, Principal SQA Engineer, https://www.linkedin.com/in/priya-yesare-8075053b/\nSoftware testing is one of the most important phases in the software development lifecycle as it ensures a quality and reliable product reaches the end user. A strong testing process improves user experience, ensures compliance with the requirements and helps build trust in the application. Traditional automation testing methods are efficient to help automate repetitive tests thus improving test execution time and eliminating the possibility of human errors in the process.\nHowever, automation comes with high costs, as it requires coding skills and requires continuous maintenance of scripts to adapt as the system under test evolves. The latest breakthrough in test automation is AI-powered testing. AI driven testing is faster, more efficient and more reliable. AI addresses the limitations for traditional automation testing by incorporating machine learning, natural language processing and predictive analysis to automate complex tasks with improved accuracy.\nThis article discusses the use of AI powered testing frameworks through efficiency, cost effectiveness, and adaptability. AI will transform software quality by exploring real world applications and help improving test coverage, reducing defect leakage, and improving testing efficiency. It\u2019s a force that will change the software quality assurance.\nArtificial Intelligence (AI) has impacted much of the world including the software testing and Quality Assurance. As the software releases get faster, the complexity also increase; traditional software testing approaches that are either manual or partially automated find it difficult to mirror the rate of releases.\nLet\u2019s look at how AI plays a role in automation testing and benefits in being transformative.\n- Test Case Generation\nThe use of the automation testing has experienced significant revolutionizing in software testing, aided with AI driven automation testing, in terms of its speed, efficiency, and flexibility. AI powered automation tools have the capability to create and increase test cases at random.\nTraditional automation is based on the script that is predefined and therefore can be stiff and must be updated often. But machine learning (ML) and natural language processing (NLP) make use of AI powered tools to develop and continually refine test cases automatically and also reduce effort in run time.\nThe use of Machine learning algorithms enhance AI-driven automation by analysing the existing codebase to identify critical areas for testing as well as generating test cases against certain software functionalities. This helps improve efficiency and coverage.\nAutomating the generation of test cases not only offers quality test coverage but also guarantees adaptability to the changes that occur on the requirements of software, positively impacting the agility of the process of development.\nSecondly, predictive analytics can analyse past defect data to prioritize test cases by putting their focus on high-risk components. By optimizing test cases using AI, redundancy is lessened, execution becomes more effective, and ultimately, software reliability is increased.\n- Defect Prediction\nTraditional testing follows a reactive approach, i.e. it identifies defects after they occur. AI shifts this to a proactive approach in which defect prediction is more accurate. AI driven models can predict potential failure points in the software applications even before they occur based on analysis of historical test data, test results and code changes to identify patterns and high risk areas.\nThis predictive capability allows development teams to find treatment for vulnerabilities within the software lifecycle, reducing the maintenance cost and making the end product more reliable. ML algorithms are used as AI based defect prediction models to relate the past software problems to the current code changes and enable useful information on the areas of higher risk.\nThis approach also improves efficiency of detecting defects and reduces the dependence on manual debugging. Cloud-based AI testing helps predict defects more accurately by using scalable computing power to run automated tests in real time. It also provides instant feedback allowing faster detection and resolution of defects.\nBelow we can see that with the help of machine learning, an AI based test automation framework can be built in Selenium using which we can predict the failing test case with historical data.\nUsing this script, one can train a machine learning model to predict the test cases that will probably fail based on execution time and history of test failures. AI will then determine if it should run or skip a test, reducing unnecessary failures and minimizing the execution time.\n- Self-Healing Automation\nOne of the recent innovations in using an AI for automation testing is the development of self-healing AI frameworks. Changes in software behaviour often necessitate frequent updating of traditional automated testing scripts which then becomes a maintenance challenge itself.\nThis problem is addressed by AI powered self-healing mechanisms which identifies and modifies the test scripts dynamically with respect to the software modifications. These frameworks use machine learning for anomaly detection and automatic script updates, ensuring test execution is not interrupted by software updates.\nSelf-healing test automation is one of the major breakthroughs in AI driven testing, where test scripts adapt to changes in application\u2019s user interface (UI), even when not anticipated during script development. Testim and Applitools use AI to check for changes in UI elements and then adjust the test scripts accordingly. Moreover, AI driven test suites match historical test results to predict possible failure points as well as order test execution.\n- Cost Optimization\nAI driven testing breaks through the barriers in the speed of defect management and efficiency in overall budgeting bringing in a transformative change. Integration of AI based automation framework in CI/CD pipeline leads to huge productivity gain and faster time to market for organizations.\nModern software development has seen the rise of AI driven automation testing, providing the ability for faster execution times, better adaptability, more efficient defect detection and offers the creator the ability for better assistance in scripting. With organizations widely embracing DevOps and CI/CD methodologies, AI driven tests automation in software testing has significantly improved reliability while minimizing costs.\n- Challenges\nDespite its numerous advantages, there are few challenges that we have to be overcome before the mainstream adoption of AI driven automation testing.\n- The biggest challenge is training the AI models to be dependent on high-quality data. If trained with incorrect or biased data, the test predictions and automation results can be unreliable and unfair. Ensuring a broad and representative dataset is used to train AI models is critical for AI driven testing outcomes.\n- Software testing needs skilled personnel who possess expertise in AI and software testing required to integrate AI into the existing testing frameworks. To carry out AI driven testing methodologies, organizations need to invest in training and upskilling of QA team.\n- Regulatory compliance and security in AI based testing environment is of utmost importance in the sectors like the finance, healthcare, and insurance.\nAddressing data dependency, integration complexity and the need for expertise are the main challenges when working with AI in order to achieve the maximum potential of AI driven testing automation. Further research can enhance the AI models, enhance the automation framework as well as come up with novel techniques to push the frontiers of innovation in software testing practice with organizations adopting AI based QA strategies.\nConclusion\nAI-driven software testing is transforming defect management and cost optimization in software development. The biggest advantage for organizations integrating AI-based automation into their CI/CD pipelines is the ability to accelerate time-to-market while maintaining high productivity and efficiency.\nAI based automation testing has become as a game changer in the modern software development, achieving faster execution time, adaptability and defect detection to a great extent. Just like DevOps and CI/CD, businesses are increasingly relying on automation testing to streamline software releases. As organizations embrace these methodologies, AI-driven test automation will become essential to ensure software reliability while keeping costs low.\nWhile AI automates many tasks, human testers will play a crucial role in test strategy, exploratory testing and validating AI generated test results. AI should enhance testing and not replace critical human decision making in areas requiring judgement, intuition or domain expertise.\nAbout the Author\nPriya Yesare is a principal SQA Engineer and AI enthusiast with 20+ years of experience in manual and automation testing, specializing in Java, Selenium, Playwright, and TypeScript. She is a skilled in architecting QA frameworks, leading full-stack teams, and streamlining testing processes. A strong mentor and collaborator, ensuring seamless integration, innovation, and high-quality software delivery."
    },
    {
      "url": "https://www.gartner.com/peer-community/oneminuteinsights/omi-automated-software-testing-adoption-trends-7d6#:~:text=Automated%20Software%20Testing%20Adoption%20and,will%20impact%20automated%20software%20testing",
      "text": "Automated Software Testing Adoption and Trends\nAutomated software testing reduces human error and speeds up eicient delivery, but how integrated is it in modern software engineering organizations, and which trends stand to impact it? Find out.\nOne minute insights:\n- Respondents commonly automate API testing and use automated testing continuously throughout the development cycle\n- Many respondents\u2019 organizations chose to automate testing to improve product quality and increase deployment speed\n- Respondents report higher test accuracy after automating testing, but some struggle with implementation\n- Tech leaders predict generative AI will impact the future of automated testing and that we will see a change in QA headcount and daily responsibilities\nA range of automated software testing types and cadences are in place\n40% of respondents automate software testing continuously during their development cycle. Other common timing options included at set milestones (38%), like prior to 40% a commit, and at set intervals (32%), such as a specific number of days.\nQuestion: What are your final thoughts on automated software testing?\nAutomated testing is necessary to find bugs and security issues in a timely, efficient manner. We would be much slower to release new software without it.\nThis technology is still in its infancy. I\u2019m anxiously looking forward to the coming advances in automated testing in the coming months and years.\nOrganizations automate software testing to improve product quality and increase deployment speed\n60% of respondents said improving product quality was among their organization\u2019s reasons to automate software testing. 58% said their 60% organization was influenced by a desire to increase deployment speed.\nOther common reasons for automating testing included improving departmental agility (41%), facilitating CI/CD (36%) and alleviating the QA team\u2019s workload (29%).\nQuestion: What are your final thoughts on automated software testing?\n[Automated software testing is] essential to ensure software development can scale, that changes can be released rapidly and to be sure many tests are covered despite human restrictions on headcount, working hours and accuracy.\nIt\u2019s easily one of the most important hygiene practices in any Software Development Environment. Without some form of automated testing, you\u2019re essentially flying completely blind to your most important workflows breaking.\nLeaders report higher test accuracy after automating testing, but some struggle with implementation\nHigher test accuracy (43%), increased agility (42%) and wider test coverage (40%) are among the most significant benefits respondents have seen at their organizations since automating testing.\nQuestion: What are your final thoughts on automated software testing?\nThere is an increasing need to have automated testing. The challenge is the upfront investment to implement such framework and to maintain it. AI can positively impact the capability of automated software testing.\nThe practice has lost a lot of its sophistication and eective tooling over the past decade or two. The push towards developer-led testing has reduced the availability of qualified resources who can help implement truly effective testing practices. We are struggling to find people and even find ideas of how to implement effective testing.\nTech leaders predict generative AI will impact the future of automated testing and that we will see a change in QA headcount and daily responsibilities\nThinking only of generative AI\u2019s potential impacts on automated software testing in the next three years, respondents believe it will predict common issues or bugs (57%), analyze test results (52%) and suggest error solutions (46%).\nConsidering automated software testing\u2019s potential impact on the QA department or roles, respondents believe that, in the next three years, we will see a reduction in QA headcount (40%) and a fundamental change to QA\u2019s daily responsibilities (40%).\nQuestion: What are your final thoughts on automated software testing?\nThe [automated software testing] process still has a way to go to completely replace individuals, but within 5 years and certainly 10 it may completely take over the function entirely.\nAutomated testing is going to fundamentally change the role of QA testing in [the] SDLC process and make the process a lot more robust and agile in the short run. In the long run, in combination with Generative AI, the level of automation may not be only limited to testing execution but will expand to script generation and test result analysis.\nWant more insights like this from leaders like yourself?\nClick here to explore the revamped, retooled and reimagined Gartner Peer Community. You'll get access to synthesized insights and engaging discussions from a community of your peers."
    },
    {
      "url": "https://devops.com/continuous-testing-the-key-to-quality-assurance-in-the-devops-era/#:~:text=Continuous%20testing%20is%20a%20critical,making",
      "text": "Continuous testing has emerged as a critical aspect of quality assurance in the DevOps era. In recent years, there has been a rapid transformation in software development methodologies. With the advent of DevOps, there has been a significant shift toward automation and continuous integration and delivery (CI/CD). The traditional approaches to quality assurance are no longer adequate to meet the needs of modern software development practices. Continuous testing is the key to ensuring that the software is delivered on time, with quality and reliability.\nDefinition of Continuous Testing\nContinuous testing is a process of executing automated tests continuously throughout the software development life cycle to provide rapid feedback on the quality of the code. It is an essential component of DevOps that ensures that the software is always ready for release. Continuous testing allows for the detection of defects and errors at an early stage, which reduces the risk of production issues and enhances the overall quality of the software.\nImportance of Continuous Testing in DevOps\nTo understand the importance of continuous testing in DevOps, it is essential to understand the evolution of quality assurance in software development.\nTraditional Waterfall Method\nThe traditional waterfall model of software development had a sequential approach to quality assurance. Testing was only performed after the development phase was completed. This approach often led to delays and increased costs due to detecting defects later.\nAgile Methodology\nThe Agile methodology introduced an innovative approach to quality assurance. Testing became an integral part of the software development lifecycle, and the focus was on continuous feedback and improvement. However, even with Agile, testing was still done in batches, and there was a significant amount of manual testing involved, which could be time-consuming and prone to errors.\nDevOps Model\nThe DevOps model took the concept of continuous feedback and improvement to a new level. With DevOps, testing became automated, and continuous integration and delivery became the norm. This approach significantly reduced the time to market, increased collaboration between development and operations teams, and enhanced the overall quality of the software.\nThe Role of Continuous Testing in DevOps\nDefinition of DevOps\nDevOps is a software development methodology that emphasizes collaboration and communication between development and operations teams to deliver software products rapidly and reliably. It involves the integration of development, testing and operations into a continuous process, with automation as a key enabler.\nImportance of Continuous Testing in DevOps\nContinuous testing is a critical component of DevOps. It enables the testing of software continuously throughout the development lifecycle, which helps to identify defects and issues early on.\nThis early detection of issues is essential in reducing the risk of production issues, and it allows for timely feedback, which facilitates faster decision-making.\nBenefits of Continuous Testing in DevOps\nThere are several benefits of continuous testing in DevOps, including:\n\u00b7 Faster time to market: Continuous testing enables the rapid release of software, reducing the time to market significantly.\n\u00b7 Improved collaboration: DevOps emphasizes collaboration between development and operations teams, and continuous testing facilitates this collaboration by providing continuous feedback on the quality of the code.\n\u00b7 Enhanced quality: Continuous testing enables the early detection of defects and issues, which reduces the risk of production issues and enhances the overall quality of the software.\n\u00b7 Reduced costs: Continuous testing can help to reduce costs by identifying defects and issues early on, which reduces the cost of fixing issues later in the development cycle.\nImplementing Continuous Testing in DevOps\nChoosing the Right Test Automation Tools\nThe selection of the right test automation tools is critical to the success of continuous testing in DevOps. Some of the popular test automation tools for DevOps include Selenium, Appium, Cucumber and JUnit.\nIt is essential to choose a tool that is easy to use, integrates well with other DevOps tools, and has strong community support.\nBuilding an Effective Test Automation Framework\nAn effective test automation framework is essential to implementing continuous testing in DevOps. A test automation framework is a set of guidelines and standards that help in the creation and execution of automated tests.\nA good framework should be modular, flexible, and scalable. It should also be easy to maintain and extend as the project grows.\nIntegrating Continuous Testing Into the DevOps Pipeline\nTo implement continuous testing in DevOps, testing should be integrated into the DevOps pipeline. The DevOps pipeline is a series of steps that automate the software development process, from code commit to production release.\nContinuous testing should be integrated into every stage of the pipeline, from unit testing to acceptance testing.\nBest Practices for Continuous Testing in DevOps\nTo achieve the full benefits of continuous testing in DevOps, it is important to follow best practices. Some of the best practices for continuous testing in DevOps include:\n\u00b7 Test early and often: Testing should begin early in the development cycle and continue throughout the development process.\n\u00b7 Automate everything: Automation is essential for continuous testing in DevOps. All tests should be automated, including unit tests, integration tests and acceptance tests.\n\u00b7 Integrate testing into the DevOps pipeline: Testing should be integrated into every stage of the DevOps pipeline, from code commit to production release.\n\u00b7 Use the right test data: Test data should be relevant and representative of real-world scenarios.\n\u00b7 Collaborate across teams: Collaboration between development, testing and operations teams is essential for the success of continuous testing in DevOps.\nChallenges and Solutions for Continuous Testing in DevOps\nThere are several challenges to implementing continuous testing in DevOps, including:\n\u00b7 Test data management: It can be challenging to manage and maintain relevant test data that accurately represent real-world scenarios.\n\u00b7 Test environment management: Ensuring that the test environment is consistent and reliable can be a challenge, especially in complex applications.\n\u00b7 Test automation maintenance: Test automation scripts can become complex and challenging to maintain, especially as the application evolves.\nStrategies for Overcoming Continuous Testing Challenges\nThere are several strategies for overcoming the challenges of continuous testing in DevOps, including:\n\u00b7 Test data management: Creating synthetic test data can help ensure that the data is relevant and representative of real-world scenarios.\n\u00b7 Test environment management: Using containerization technology such as Docker can help ensure a consistent and reliable test environment.\n\u00b7 Test automation maintenance: Regular maintenance and refactoring of test automation scripts can help keep them up-to-date and relevant.\nConclusion\nContinuous testing is the key to ensuring quality in the DevOps era. It enables the rapid release of software, reduces the risk of production issues and enhances the overall quality of the software. Implementing continuous testing in DevOps requires choosing the right test automation tools, building an effective test automation framework, integrating testing into the DevOps pipeline and following DevOps best practices. Although there are challenges to implementing continuous testing in DevOps, these challenges can be overcome with the right strategies and practices. By implementing continuous testing in DevOps, organizations can achieve faster time to market, improved collaboration, enhanced quality and reduced costs."
    },
    {
      "url": "https://www.softwaretestingmagazine.com/knowledge/ai-powered-automation-the-future-of-smarter-faster-testing/#:~:text=,and%20upskilling%20of%20QA%20team",
      "text": "In this article, Priya Yesare explains why AI driven software testing is faster, more efficient and more reliable. AI addresses the limitations of traditional test automation by incorporating machine learning, large language models (LLM), natural language processing and predictive analysis to automate complex tasks with improved accuracy.\nAuthor : Priya Yesare, Principal SQA Engineer, https://www.linkedin.com/in/priya-yesare-8075053b/\nSoftware testing is one of the most important phases in the software development lifecycle as it ensures a quality and reliable product reaches the end user. A strong testing process improves user experience, ensures compliance with the requirements and helps build trust in the application. Traditional automation testing methods are efficient to help automate repetitive tests thus improving test execution time and eliminating the possibility of human errors in the process.\nHowever, automation comes with high costs, as it requires coding skills and requires continuous maintenance of scripts to adapt as the system under test evolves. The latest breakthrough in test automation is AI-powered testing. AI driven testing is faster, more efficient and more reliable. AI addresses the limitations for traditional automation testing by incorporating machine learning, natural language processing and predictive analysis to automate complex tasks with improved accuracy.\nThis article discusses the use of AI powered testing frameworks through efficiency, cost effectiveness, and adaptability. AI will transform software quality by exploring real world applications and help improving test coverage, reducing defect leakage, and improving testing efficiency. It\u2019s a force that will change the software quality assurance.\nArtificial Intelligence (AI) has impacted much of the world including the software testing and Quality Assurance. As the software releases get faster, the complexity also increase; traditional software testing approaches that are either manual or partially automated find it difficult to mirror the rate of releases.\nLet\u2019s look at how AI plays a role in automation testing and benefits in being transformative.\n- Test Case Generation\nThe use of the automation testing has experienced significant revolutionizing in software testing, aided with AI driven automation testing, in terms of its speed, efficiency, and flexibility. AI powered automation tools have the capability to create and increase test cases at random.\nTraditional automation is based on the script that is predefined and therefore can be stiff and must be updated often. But machine learning (ML) and natural language processing (NLP) make use of AI powered tools to develop and continually refine test cases automatically and also reduce effort in run time.\nThe use of Machine learning algorithms enhance AI-driven automation by analysing the existing codebase to identify critical areas for testing as well as generating test cases against certain software functionalities. This helps improve efficiency and coverage.\nAutomating the generation of test cases not only offers quality test coverage but also guarantees adaptability to the changes that occur on the requirements of software, positively impacting the agility of the process of development.\nSecondly, predictive analytics can analyse past defect data to prioritize test cases by putting their focus on high-risk components. By optimizing test cases using AI, redundancy is lessened, execution becomes more effective, and ultimately, software reliability is increased.\n- Defect Prediction\nTraditional testing follows a reactive approach, i.e. it identifies defects after they occur. AI shifts this to a proactive approach in which defect prediction is more accurate. AI driven models can predict potential failure points in the software applications even before they occur based on analysis of historical test data, test results and code changes to identify patterns and high risk areas.\nThis predictive capability allows development teams to find treatment for vulnerabilities within the software lifecycle, reducing the maintenance cost and making the end product more reliable. ML algorithms are used as AI based defect prediction models to relate the past software problems to the current code changes and enable useful information on the areas of higher risk.\nThis approach also improves efficiency of detecting defects and reduces the dependence on manual debugging. Cloud-based AI testing helps predict defects more accurately by using scalable computing power to run automated tests in real time. It also provides instant feedback allowing faster detection and resolution of defects.\nBelow we can see that with the help of machine learning, an AI based test automation framework can be built in Selenium using which we can predict the failing test case with historical data.\nUsing this script, one can train a machine learning model to predict the test cases that will probably fail based on execution time and history of test failures. AI will then determine if it should run or skip a test, reducing unnecessary failures and minimizing the execution time.\n- Self-Healing Automation\nOne of the recent innovations in using an AI for automation testing is the development of self-healing AI frameworks. Changes in software behaviour often necessitate frequent updating of traditional automated testing scripts which then becomes a maintenance challenge itself.\nThis problem is addressed by AI powered self-healing mechanisms which identifies and modifies the test scripts dynamically with respect to the software modifications. These frameworks use machine learning for anomaly detection and automatic script updates, ensuring test execution is not interrupted by software updates.\nSelf-healing test automation is one of the major breakthroughs in AI driven testing, where test scripts adapt to changes in application\u2019s user interface (UI), even when not anticipated during script development. Testim and Applitools use AI to check for changes in UI elements and then adjust the test scripts accordingly. Moreover, AI driven test suites match historical test results to predict possible failure points as well as order test execution.\n- Cost Optimization\nAI driven testing breaks through the barriers in the speed of defect management and efficiency in overall budgeting bringing in a transformative change. Integration of AI based automation framework in CI/CD pipeline leads to huge productivity gain and faster time to market for organizations.\nModern software development has seen the rise of AI driven automation testing, providing the ability for faster execution times, better adaptability, more efficient defect detection and offers the creator the ability for better assistance in scripting. With organizations widely embracing DevOps and CI/CD methodologies, AI driven tests automation in software testing has significantly improved reliability while minimizing costs.\n- Challenges\nDespite its numerous advantages, there are few challenges that we have to be overcome before the mainstream adoption of AI driven automation testing.\n- The biggest challenge is training the AI models to be dependent on high-quality data. If trained with incorrect or biased data, the test predictions and automation results can be unreliable and unfair. Ensuring a broad and representative dataset is used to train AI models is critical for AI driven testing outcomes.\n- Software testing needs skilled personnel who possess expertise in AI and software testing required to integrate AI into the existing testing frameworks. To carry out AI driven testing methodologies, organizations need to invest in training and upskilling of QA team.\n- Regulatory compliance and security in AI based testing environment is of utmost importance in the sectors like the finance, healthcare, and insurance.\nAddressing data dependency, integration complexity and the need for expertise are the main challenges when working with AI in order to achieve the maximum potential of AI driven testing automation. Further research can enhance the AI models, enhance the automation framework as well as come up with novel techniques to push the frontiers of innovation in software testing practice with organizations adopting AI based QA strategies.\nConclusion\nAI-driven software testing is transforming defect management and cost optimization in software development. The biggest advantage for organizations integrating AI-based automation into their CI/CD pipelines is the ability to accelerate time-to-market while maintaining high productivity and efficiency.\nAI based automation testing has become as a game changer in the modern software development, achieving faster execution time, adaptability and defect detection to a great extent. Just like DevOps and CI/CD, businesses are increasingly relying on automation testing to streamline software releases. As organizations embrace these methodologies, AI-driven test automation will become essential to ensure software reliability while keeping costs low.\nWhile AI automates many tasks, human testers will play a crucial role in test strategy, exploratory testing and validating AI generated test results. AI should enhance testing and not replace critical human decision making in areas requiring judgement, intuition or domain expertise.\nAbout the Author\nPriya Yesare is a principal SQA Engineer and AI enthusiast with 20+ years of experience in manual and automation testing, specializing in Java, Selenium, Playwright, and TypeScript. She is a skilled in architecting QA frameworks, leading full-stack teams, and streamlining testing processes. A strong mentor and collaborator, ensuring seamless integration, innovation, and high-quality software delivery."
    },
    {
      "url": "https://engineering.fb.com/2018/05/02/developer-tools/sapienz-intelligent-automated-software-testing-at-scale/#:~:text=Sapienz%20now%20automatically%20designs%2C%20runs%2C,without%20compromising%20stability%20or%20performance",
      "text": "Shipping code updates to the Facebook app, which is used every day by hundreds of millions of people, requires extensive testing to ensure stability and performance. At Facebook\u2019s scale, this process requires checking hundreds of important interactions across numerous types of devices and operating systems for both correctness and speed. Traditionally, this has largely been a manual test design process, during which engineers devote time and resources to designing test cases. But at Facebook, we have developed an intelligent software testing tool called Sapienz to efficiently and effectively design many of the test cases we need. There are undoubtedly still test cases best designed by human engineers, because of their superior domain knowledge compared with a machine\u2019s. But our work with Sapienz shows it is possible to automate much of the tedious, time-consuming process and accelerate the deployment of new features.\nSapienz technology, which grew out of decades of work by the software engineering research community, leverages automated test design to make the testing process faster, more comprehensive, and more effective. This is the first time a search-based automated test design technology has been deployed at this scale into continuous integration. Its rapid deployment, successful intervention rates, and reporting precision have pleasantly surprised both the engineers who designed it and those who are using it for friction-free fault finding.\nSapienz samples the space of all possible tests, using intelligent computational search and an approach called search-based software testing. An important design principle is that Sapienz tests through the UI, so issues Sapienz reports to engineers can be found through the UI. This avoids the false positives that bedevil many test design approaches, but it makes it more challenging to provide guidance to the computational search.\nAs Sapienz searches, it builds a model of the system under test, through the UI interactions, and harvests good tests for subsequent reuse. Sapienz uses a hybrid approach to search, combining lower-level individual events with higher-level patterns of related events (known as motif events), that captures a connection of related lower-level events into a higher-level motif gene.\nWe first deployed Sapienz at Facebook in September 2017, and have since scaled it to meet the challenge of testing millions of lines of code in Facebook\u2019s Android app.\nSapienz now automatically designs, runs, and reports the results of tens of thousands of test cases every day on the Facebook Android app. In the first few months since its deployment, the technology has allowed engineers to fix issues within hours (sometimes within minutes) of the code being written. This has had a positive impact on hundreds of millions of people around the world, bringing them new features and enhancements as quickly as possible, without compromising stability or performance.\nIn addition to speeding up the testing process, Sapienz has delivered an extremely low false-positive rate. Our engineers find that 75 percent of Sapienz reports are actionable, resulting in fixes. Sapienz uses Facebook\u2019s One World platform to scale to many hundreds (and even thousands) of emulators at any one time, in continuous deployment 24 hours a day, seven days a week.\nEngineers have benefited from the friction-free way in which Sapienz finds code issues. It checks new code nearly in real time, reports straight into Facebook\u2019s continuous integration system, Phabricator, and adds comments just as a human engineer would. It designs tests to expose potential faults without the testing engineer\u2019s having to intervene at all, making Sapienz unique in its level of friction-free code-checking.\nWe are just at the beginning of our journey with Sapienz at Facebook and expect over the next few years to implement and share new applications and feature enhancements. We also expect that search-based software engineering will find many new exciting and innovative applicators, both here at Facebook and in the wider tech sector as a whole.\nThe Sapienz team is strongly committed to increasing engagement between fundamental scientific work and practical industrial-scale software engineering. The tool originated as part of a research paper but moved to Facebook-scale deployment in just 17 months, illustrating how academic researchers and engineers can collaborate to rapidly improve real-world applications used by billions of people.\nWe are currently extending Sapienz to test other Android and iOS apps in the Facebook product family."
    }
  ],
  "argos_summary": "The article highlights how AI and machine learning are transforming software testing, enabling proactive continuous verification, automated test case generation, defect prediction, and self\u2011healing scripts. It showcases Okta\u2019s Engineering Architect Varun Mukka\u2019s work integrating AI into infrastructure quality, and Facebook\u2019s Sapienz system, which automatically designs and runs thousands of UI tests daily with a low false\u2011positive rate. These innovations accelerate release cycles, improve reliability, and reduce manual testing effort across large, complex codebases.",
  "argos_id": "NCQKA3T6V"
}