{
  "url": "https://techxplore.com/news/2025-08-robots-tools.html",
  "authorsByline": "Cheng Zhu",
  "articleId": "9cc286d7f35d41f18637a85fdd155b44",
  "source": {
    "domain": "techxplore.com",
    "location": null
  },
  "imageUrl": "https://scx2.b-cdn.net/gfx/news/hires/2025/robots-can-now-learn-t.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-23T09:10:01-04:00",
  "addDate": "2025-08-23T13:20:35.080584+00:00",
  "refreshDate": "2025-08-23T13:20:35.080587+00:00",
  "score": 1.0,
  "title": "Robots can now learn to use tools\u2014just by watching us",
  "description": "Despite decades of progress, most robots are still programmed for specific, repetitive tasks. They struggle with the unexpected and can't adapt to new situations without painstaking reprogramming. But what if they could learn to use tools as naturally as a child does by watching videos?",
  "content": "This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:\n\nDespite decades of progress, most robots are still programmed for specific, repetitive tasks. They struggle with the unexpected and can't adapt to new situations without painstaking reprogramming. But what if they could learn to use tools as naturally as a child does by watching videos?\n\nI still remember the first time I saw one of our lab's robots flip an egg in a frying pan. It wasn't pre-programmed. No one was controlling it with a joystick. The robot had simply watched a video of a human doing it, and then did it itself. For someone who has spent years thinking about how to make robots more adaptable, that moment was thrilling.\n\nOur team at the University of Illinois Urbana-Champaign, together with collaborators at Columbia University and UT Austin, has been exploring that very question. Could robots watch someone hammer a nail or scoop a meatball, and then figure out how to do it themselves, without costly sensors, motion capture suits, or hours of remote teleoperation?\n\nThat idea led us to create a new framework we call \"Tool-as-Interface,\" currently available on the arXiv preprint server. The goal is straightforward: teach robots complex, dynamic tool-use skills using nothing more than ordinary videos of people doing everyday tasks. All it takes is two camera views of the action, something you could capture with a couple of smartphones.\n\nHere's how it works. The process begins with those two video frames, which a vision model called MASt3R uses to reconstruct a three-dimensional model of the scene. Then, using a rendering method known as 3D Gaussian splatting\u2014think of it as digitally painting a 3D picture of the scene\u2014we generate additional viewpoints so the robot can \"see\" the task from multiple angles.\n\nBut the real magic happens when we digitally remove the human from the scene. With the help of \"Grounded-SAM,\" our system isolates just the tool and its interaction with the environment. It is like telling the robot, \"Ignore the human, and only pay attention to what the tool is doing.\"\n\nThis \"tool-centric\" perspective is the secret ingredient. It means the robot isn't trying to copy human hand motions, but is instead learning the exact trajectory and orientation of the tool itself. This allows the skill to transfer between different robots, regardless of how their arms or cameras are configured.\n\nWe tested this on five tasks: hammering a nail, scooping a meatball, flipping food in a pan, balancing a wine bottle, and even kicking a soccer ball into a goal. These are not simple pick-and-place jobs; they require speed, precision, and adaptability. Compared to traditional teleoperation methods, Tool-as-Interface achieved 71% higher success rates and gathered training data 77% faster.\n\nOne of my favorite tests involved a robot scooping meatballs while a human tossed in more mid-task. The robot didn't hesitate, it just adapted. In another, it flipped a loose egg in a pan, a notoriously tricky move for teleoperated robots.\n\n\"Our approach was inspired by the way children learn, which is by watching adults,\" said my colleague and lead author Haonan Chen. \"They don't need to operate the same tool as the person they're watching; they can practice with something similar. We wanted to know if we could mimic that ability in robots.\"\n\nThese results point toward something bigger than just better lab demos. By removing the need for expert operators or specialized hardware, we can imagine robots learning from smartphone videos, YouTube clips, or even crowdsourced footage.\n\n\"Despite a lot of hype around robots, they are still limited in where they can reliably operate and are generally much worse than humans at most tasks,\" said Professor Katie Driggs-Campbell, who leads our lab.\n\n\"We're interested in designing frameworks and algorithms that will enable robots to easily learn from people with minimal engineering effort.\"\n\nOf course, there are still challenges. Right now, the system assumes the tool is rigidly fixed to the robot's gripper, which isn't always true in real life. It also sometimes struggles with 6D pose estimation errors, and synthesized camera views can lose realism if the angle shift is too extreme.\n\nIn the future, we want to make the perception system more robust, so that a robot could, for example, watch someone use one kind of pen and then apply that skill to pens of different shapes and sizes.\n\nEven with these limitations, I think we're seeing a profound shift in how robots can learn, away from painstaking programming and toward natural observation. Billions of cameras are already recording how humans use tools. With the right algorithms, those videos could become training material for the next generation of adaptable, helpful robots.\n\nThis research, which was honored with the Best Paper Award at the ICRA 2025 Workshop on Foundation Models and Neural-Symbolic (NeSy) AI for Robotics, is a critical step toward unlocking that potential, transforming the vast ocean of human recorded video into a global training library for robots that can learn and adapt as naturally as a child does.\n\nThis story is part of Science X Dialog, where researchers can report findings from their published research articles. Visit this page for information about Science X Dialog and how to participate.\n\nCheng Zhu is second author of Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning, UIUC BS Computer Engineering, UPenn MSE ROBO",
  "medium": "Article",
  "links": [
    "https://sciencex.com/help/dialog/",
    "https://techxplore.com/tags/soccer+ball/",
    "https://sciencex.com/news/dialog/",
    "https://arxiv.org/abs/2504.04612",
    "https://techxplore.com/journals/arxiv/",
    "https://dx.doi.org/10.48550/arxiv.2504.04612",
    "https://techxplore.com/tags/robot/",
    "http://arxiv.org/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Robots",
      "weight": 0.097195566
    },
    {
      "name": "robots",
      "weight": 0.097195566
    },
    {
      "name": "different robots",
      "weight": 0.09667245
    },
    {
      "name": "most robots",
      "weight": 0.09645104
    },
    {
      "name": "Robot Policies",
      "weight": 0.09565136
    },
    {
      "name": "teleoperated robots",
      "weight": 0.09372284
    },
    {
      "name": "Human Tool Usage",
      "weight": 0.07497756
    },
    {
      "name": "tools",
      "weight": 0.06171467
    },
    {
      "name": "human recorded video",
      "weight": 0.06098324
    },
    {
      "name": "Science X Dialog",
      "weight": 0.058806583
    }
  ],
  "topics": [],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.93408203125
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.83203125
    }
  ],
  "sentiment": {
    "positive": 0.14345315,
    "negative": 0.40508404,
    "neutral": 0.45146278
  },
  "summary": "The University of Illinois Urbana-Champaign and Columbia University have developed a framework called \"Tool-as-Interface\" that teaches robots complex tool-use skills using ordinary videos of people doing everyday tasks. The system removes the human from the scene, allowing the skill to transfer between different robots. It achieved 71% higher success rates and gathered training data 77% faster than traditional teleoperation methods. The researchers tested five tasks including hammering a nail, scooping a meatball, flipping food in a pan, and even kicking a soccer ball into a goal. Despite limitations, the researchers believe this shift away from painstaking programming and observation could shift towards natural observation and natural observation.",
  "shortSummary": "Robots can learn to use tools using video clips, using \"Tool-as-Interface\" technology, overcoming traditional programming limitations and improving adaptability among robots.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "edfc5895a2784c97bf83b19200f036a0",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://sciencex.com/help/dialog/",
      "text": "What is Science X Dialog?\nScience X Dialog is where researchers can share news and information about their own published journal articles. Science X Dialog articles are written for general science-minded readers, and provide an opportunity to clarify and explain complex scientific data in a way that Science X\u2019s audience of 10 million readers can understand and share. Coverage in Science X Dialog may also help author-researchers in obtaining future grants or positions.\nHow a Dialog story differs from a press release\nA press release is structured like a traditional news story. By contrast, Dialog seeks a direct level of engagement with the audience\u2014imagine that you are writing a TED talk and speaking to professionals and students who may not be experts in your specific field.\nThat connection is direct and explanatory\u2014and it is best suited for a first-person voice in which you express your own thoughts and ideas about the subject of the article. A Dialog story does not have the requirement of strict formality that might characterize a press release or a news story.\nScience X Dialog content guidelines:\n- Your article should be at least 400 words, written for a general science-minded audience in standard American or Global English.\n- Your article must be based on a study published in a highly ranked science journal or presented at a major conference within the past six months.\n- Your article should be original. The Dialog is not meant for press release distribution.\n- It should be clearly written and satisfy our standards for language and quality. See our guide: How to write a good news article.\n- Your article will be submitted for our editorial review. Science X editors may edit your work or ask for corrections or clarifications. Science X Dialog requires the active involvement of content authors in the editorial process to correct or clarify where necessary.\n- You must provide at least one image or illustration (required aspect ratio 5:3 or close to it; 1280 x 1024 pixels or larger) with clear caption and credit. It should be public domain, under CC license, or you must own the copyright or arrange all necessary permissions for third-party publication. YouTube, Vimeo or video files are also welcome.\n- You must provide a direct link to online publications, a journal citation or the DOI of your published work. Prepress publications are welcome, provided ample information for references are supplied.\nYour draft article will be reviewed by Science X editors. The editors will communicate their suggestions for improvements, if needed. Once both parties are satisfied with the quality, the article goes for publication in Science X Dialog.\nTo publish your research, first submit the short application form below. Please describe your research in 100 to 150 words, provide a journal reference and a brief bio of yourself. Our editor will contact you with the details of the submission process. We try to respond promptly, but it may take us several business days to get back to you."
    },
    {
      "url": "https://arxiv.org/abs/2504.04612",
      "text": "Computer Science > Robotics\n[Submitted on 6 Apr 2025]\nTitle:Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning\nView PDF HTML (experimental)Abstract:Tool use is critical for enabling robots to perform complex real-world tasks, and leveraging human tool-use data can be instrumental for teaching robots. However, existing data collection methods like teleoperation are slow, prone to control delays, and unsuitable for dynamic tasks. In contrast, human natural data, where humans directly perform tasks with tools, offers natural, unstructured interactions that are both efficient and easy to collect. Building on the insight that humans and robots can share the same tools, we propose a framework to transfer tool-use knowledge from human data to robots. Using two RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting for novel view augmentation, employs segmentation models to extract embodiment-agnostic observations, and leverages task-space tool-action representations to train visuomotor policies. We validate our approach on diverse real-world tasks, including meatball scooping, pan flipping, wine bottle balancing, and other complex tasks. Our method achieves a 71\\% higher average success rate compared to diffusion policies trained with teleoperation data and reduces data collection time by 77\\%, with some tasks solvable only by our framework. Compared to hand-held gripper, our method cuts data collection time by 41\\%. Additionally, our method bridges the embodiment gap, improves robustness to variations in camera viewpoints and robot configurations, and generalizes effectively across objects and spatial setups.\nCurrent browse context:\ncs.RO\nReferences & Citations\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."
    },
    {
      "url": "http://arxiv.org/",
      "text": "arXiv is a free distribution service and an open-access archive for nearly 2.4 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics. Materials on this site are not peer-reviewed by arXiv.\nPhysics\n- Astrophysics (astro-ph new, recent, search) Astrophysics of Galaxies; Cosmology and Nongalactic Astrophysics; Earth and Planetary Astrophysics; High Energy Astrophysical Phenomena; Instrumentation and Methods for Astrophysics; Solar and Stellar Astrophysics\n- Condensed Matter (cond-mat new, recent, search) Disordered Systems and Neural Networks; Materials Science; Mesoscale and Nanoscale Physics; Other Condensed Matter; Quantum Gases; Soft Condensed Matter; Statistical Mechanics; Strongly Correlated Electrons; Superconductivity\n- General Relativity and Quantum Cosmology (gr-qc new, recent, search)\n- High Energy Physics - Experiment (hep-ex new, recent, search)\n- High Energy Physics - Lattice (hep-lat new, recent, search)\n- High Energy Physics - Phenomenology (hep-ph new, recent, search)\n- High Energy Physics - Theory (hep-th new, recent, search)\n- Mathematical Physics (math-ph new, recent, search)\n-\nNonlinear Sciences\n(nlin\nnew,\nrecent,\nsearch)\nincludes: Adaptation and Self-Organizing Systems; Cellular Automata and Lattice Gases; Chaotic Dynamics; Exactly Solvable and Integrable Systems; Pattern Formation and Solitons - Nuclear Experiment (nucl-ex new, recent, search)\n- Nuclear Theory (nucl-th new, recent, search)\n-\nPhysics\n(physics\nnew,\nrecent,\nsearch)\nincludes: Accelerator Physics; Applied Physics; Atmospheric and Oceanic Physics; Atomic and Molecular Clusters; Atomic Physics; Biological Physics; Chemical Physics; Classical Physics; Computational Physics; Data Analysis, Statistics and Probability; Fluid Dynamics; General Physics; Geophysics; History and Philosophy of Physics; Instrumentation and Detectors; Medical Physics; Optics; Physics and Society; Physics Education; Plasma Physics; Popular Physics; Space Physics - Quantum Physics (quant-ph new, recent, search)\nMathematics\n-\nMathematics\n(math\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Algebraic Geometry; Algebraic Topology; Analysis of PDEs; Category Theory; Classical Analysis and ODEs; Combinatorics; Commutative Algebra; Complex Variables; Differential Geometry; Dynamical Systems; Functional Analysis; General Mathematics; General Topology; Geometric Topology; Group Theory; History and Overview; Information Theory; K-Theory and Homology; Logic; Mathematical Physics; Metric Geometry; Number Theory; Numerical Analysis; Operator Algebras; Optimization and Control; Probability; Quantum Algebra; Representation Theory; Rings and Algebras; Spectral Theory; Statistics Theory; Symplectic Geometry\nComputer Science\n-\nComputing Research Repository\n(CoRR\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Artificial Intelligence; Computation and Language; Computational Complexity; Computational Engineering, Finance, and Science; Computational Geometry; Computer Science and Game Theory; Computer Vision and Pattern Recognition; Computers and Society; Cryptography and Security; Data Structures and Algorithms; Databases; Digital Libraries; Discrete Mathematics; Distributed, Parallel, and Cluster Computing; Emerging Technologies; Formal Languages and Automata Theory; General Literature; Graphics; Hardware Architecture; Human-Computer Interaction; Information Retrieval; Information Theory; Logic in Computer Science; Machine Learning; Mathematical Software; Multiagent Systems; Multimedia; Networking and Internet Architecture; Neural and Evolutionary Computing; Numerical Analysis; Operating Systems; Other Computer Science; Performance; Programming Languages; Robotics; Social and Information Networks; Software Engineering; Sound; Symbolic Computation; Systems and Control\nQuantitative Biology\n-\nQuantitative Biology\n(q-bio\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Biomolecules; Cell Behavior; Genomics; Molecular Networks; Neurons and Cognition; Other Quantitative Biology; Populations and Evolution; Quantitative Methods; Subcellular Processes; Tissues and Organs\nQuantitative Finance\n-\nQuantitative Finance\n(q-fin\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Computational Finance; Economics; General Finance; Mathematical Finance; Portfolio Management; Pricing of Securities; Risk Management; Statistical Finance; Trading and Market Microstructure\nStatistics\n-\nStatistics\n(stat\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Applications; Computation; Machine Learning; Methodology; Other Statistics; Statistics Theory\nElectrical Engineering and Systems Science\n-\nElectrical Engineering and Systems Science\n(eess\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Audio and Speech Processing; Image and Video Processing; Signal Processing; Systems and Control\nEconomics\n-\nEconomics\n(econ\nnew,\nrecent,\nsearch)\nincludes: (see detailed description): Econometrics; General Economics; Theoretical Economics"
    },
    {
      "url": "https://sciencex.com/news/dialog/",
      "text": "Science X Dialog\nScience X Dialog is where researchers can share news and information about their own published journal articles.\nHow to apply\nDialog / Robots can now learn to use tools\u2014just by watching us\nDespite decades of progress, most robots are still programmed for specific, repetitive tasks. They struggle with the unexpected and can't adapt to new situations without painstaking reprogramming. But what if they could learn ...\nDialog / Informal e-waste recycling in Pakistan: A hidden environmental crisis\nWhen I began my research on electronic waste in Pakistan, I quickly realized how deeply it touches both our environment and our daily lives. We live in an age where technology evolves faster than ever\u2014phones, laptops, and ...\nDialog / How human protein ACE2 modulation could stop the entry of coronavirus\nEarly in the pandemic, most research, including our own, focused on designing drugs that could block the virus's spike protein. This was a logical first step, but as we've seen, the virus is a moving target. It was rapidly ...\nDialog / More cameras, more problems? Why deep learning still struggles with 3D human sensing\nAccurately estimating human pose was among the first tasks addressed by deep learning. Early models like OpenPose focused on localizing human joints as 2D keypoints in image coordinates. Later, Google came up with Mediapipe, ...\nDialog / Rethinking imperfections: How defects are powering brighter perovskite emissions\nIn materials science, defects are usually seen as problems, unwanted microscopic features that degrade performance, reduce efficiency or shorten the lifespan of devices. But a recent breakthrough published in Advanced Materials ...\nDialog / Uncovering the hidden world of parasites inside ticks\nWhen I tell people I study parasites that live inside ticks, I usually get one of two reactions: a shudder of disgust or a puzzled look that says, \"Don't ticks already carry enough diseases?\"\nDialog / Bacterial detection using glycan-targeting nanoparticles in Raman spectroscopy\nIn a study published in the Gold Bulletin journal by a group of researchers from the Center for Applied Physics and Advanced Technology (CFATA) and National School of Higher Education (ENES), Leon, both from the National ...\nDialog / Unexpected survival strategies of plankton in alpine lakes have global relevance\nAlpine lakes are among the most harsh aquatic environments on Earth. Located at high elevation, they are exposed to intense ultraviolet radiation (UVR), wide temperature and light fluctuations, and low nutrient availability. ...\nDialog / When space becomes time: A new look inside the BTZ black hole\nExploring the BTZ black hole in (2+1)-dimensional gravity took me down a fascinating rabbit hole, connecting ideas I never expected\u2014like black holes and topological phases in quantum matter! When I swapped the roles of ...\nDialog / Deep geothermal energy: How gas bubbles can unlock Earth's hidden energy\nImagine a power plant fueled by heat generated deep beneath your feet, silently providing renewable energy day and night, independent of weather or sunlight. Enhanced geothermal systems (EGS) promise exactly this, tapping ...\nDialog / Precision medicine starts with who we study and who we've missed: What a hidden heart mutation in Dominicans taught me\nPrecision medicine promises to tailor health care to the individual. But what happens when entire communities are left out of the data that drives that tailoring?\nDialog / Democratizing AI-powered sentiment analysis\nArtificial intelligence is accelerating at breakneck speed, with larger models dominating the scene\u2014more parameters, more data, more power. But here is the real question: Do we really need bigger to be better? We challenged ..."
    },
    {
      "url": "https://dx.doi.org/10.48550/arxiv.2504.04612",
      "text": "Computer Science > Robotics\n[Submitted on 6 Apr 2025]\nTitle:Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning\nView PDF HTML (experimental)Abstract:Tool use is critical for enabling robots to perform complex real-world tasks, and leveraging human tool-use data can be instrumental for teaching robots. However, existing data collection methods like teleoperation are slow, prone to control delays, and unsuitable for dynamic tasks. In contrast, human natural data, where humans directly perform tasks with tools, offers natural, unstructured interactions that are both efficient and easy to collect. Building on the insight that humans and robots can share the same tools, we propose a framework to transfer tool-use knowledge from human data to robots. Using two RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting for novel view augmentation, employs segmentation models to extract embodiment-agnostic observations, and leverages task-space tool-action representations to train visuomotor policies. We validate our approach on diverse real-world tasks, including meatball scooping, pan flipping, wine bottle balancing, and other complex tasks. Our method achieves a 71\\% higher average success rate compared to diffusion policies trained with teleoperation data and reduces data collection time by 77\\%, with some tasks solvable only by our framework. Compared to hand-held gripper, our method cuts data collection time by 41\\%. Additionally, our method bridges the embodiment gap, improves robustness to variations in camera viewpoints and robot configurations, and generalizes effectively across objects and spatial setups.\nCurrent browse context:\ncs.RO\nReferences & Citations\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."
    }
  ],
  "argos_summary": "Researchers at UIUC, Columbia, and UT Austin introduced Tool-as-Interface, a framework that teaches robots complex tool\u2011use skills by watching simple two\u2011camera videos of humans. The system reconstructs a 3D scene, generates novel viewpoints, isolates the tool with segmentation, and trains visuomotor policies that transfer across robot embodiments. Tested on tasks such as hammering, scooping, flipping, balancing, and kicking, it outperformed teleoperation by 71\u202f% in success rate and reduced data collection time by 77\u202f%. The approach eliminates the need for expensive sensors or expert operators, enabling robots to learn from ubiquitous smartphone or YouTube footage. The work received the Best Paper Award at the ICRA\u202f2025 Workshop on Foundation Models and Neural\u2011Symbolic AI for Robotics.",
  "argos_id": "CSXMF2WQ8"
}