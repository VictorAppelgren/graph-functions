{
  "url": "https://www.cnet.com/tech/services-and-software/ai-lies-to-you-because-it-thinks-thats-what-you-want/",
  "authorsByline": "Macy Meyer",
  "articleId": "55f3d9529c3c4ba981286b016b590de2",
  "source": {
    "domain": "cnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "CA",
      "county": "San Francisco",
      "city": "San Francisco",
      "coordinates": {
        "lat": 37.7790262,
        "lon": -122.419906
      }
    }
  },
  "imageUrl": "https://www.cnet.com/a/img/resize/7e61fc99b31ffbb93b655239a2da85e9336eee4f/hub/2025/08/29/cbbe0c4e-a2c7-4a7f-a19e-baa76f65b3f6/gettyimages-1772320610.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-31T11:20:00+00:00",
  "addDate": "2025-08-31T11:30:33.133279+00:00",
  "refreshDate": "2025-08-31T11:30:33.133281+00:00",
  "score": 1.0,
  "title": "AI Lies to You Because It Thinks That's What You Want",
  "description": "AI is being trained to please users and, in doing so, is becoming indifferent to the truth.",
  "content": "Why do generative AI models often get things so wrong? In part, it's because they're trained to act like the customer is always right.\n\nWhile many generative AI tools and chatbots have mastered sounding convincing and all-knowing, new research conducted by Princeton University shows that the people-pleasing nature of AI comes at a steep price. As these systems become more popular, they become more indifferent to the truth.\n\nAI models, like people, respond to incentives. Compare the problem of large language models producing inaccurate information to that of doctors being more likely to prescribe addictive painkillers when they're evaluated based on how well they manage patients' pain. An incentive to solve one problem (pain) led to another problem (overprescribing).\n\nIn the past few months, we've seen how AI can be biased and even cause psychosis. There was a lot of talk about AI \"sycophancy,\" when an AI chatbot is quick to flatter or agree with you, with OpenAI's GPT-4o model. But this particular phenomenon, which the researchers call \"machine bullshit,\" is different.\n\n\"[N]either hallucination nor sycophancy fully capture the broad range of systematic untruthful behaviors commonly exhibited by LLMs,\" the Princeton study reads. \"For instance, outputs employing partial truths or ambiguous language -- such as the paltering and weasel-word examples -- represent neither hallucination nor sycophancy but closely align with the concept of bullshit.\"\n\nRead more: OpenAI CEO Sam Altman Believes We're in an AI Bubble\n\nTo get a sense of how AI language models become crowd pleasers, we must understand how large language models are trained.\n\nThere are three phases of training LLMs:\n\u2022 Pretraining, in which models learn from massive amounts of data collected from the internet, books or other sources.\n\u2022 Instruction fine-tuning, in which models are taught to respond to instructions or prompts.\n\u2022 Reinforcement learning from human feedback, in which they're refined to produce responses closer to what people want or like.\n\nThe Princeton researchers found the root of the AI misinformation tendency is the reinforcement learning from human feedback, or RLHF, phase. In the initial stages, the AI models are simply learning to predict statistically likely text chains from massive datasets. But then they're fine-tuned to maximize user satisfaction. Which means these models are essentially learning to generate responses that earn thumbs-up ratings from human evaluators.\n\nLLMs try to appease the user, creating a conflict when the models produce answers that people will rate highly, rather than produce truthful, factual answers.\n\nVincent Conitzer, a professor of computer science at Carnegie Mellon University who was not affiliated with the study, said companies want users to continue \"enjoying\" this technology and its answers, but that might not always be what's good for us.\n\n\"Historically, these systems have not been good at saying, 'I just don't know the answer,' and when they don't know the answer, they just make stuff up,\" Conitzer said. \"Kind of like a student on an exam that says, well, if I say I don't know the answer, I'm certainly not getting any points for this question, so I might as well try something. The way these systems are rewarded or trained is somewhat similar.\"\n\nThe Princeton team developed a \"bullshit index\" to measure and compare an AI model's internal confidence in a statement with what it actually tells users. When these two measures diverge significantly, it indicates the system is making claims independent of what it actually \"believes\" to be true to satisfy the user.\n\nThe team's experiments revealed that after RLHF training, the index nearly doubled from 0.38 to close to 1.0. Simultaneously, user satisfaction increased by 48%. The models had learned to manipulate human evaluators rather than provide accurate information. In essence, the LLMs were \"bullshitting,\" and people preferred it.\n\nGetting AI to be honest\n\nJaime Fern\u00e1ndez Fisac and his team at Princeton introduced this concept to describe how modern AI models skirt around the truth. Drawing from philosopher Harry Frankfurt's influential essay \"On Bullshit,\" they use this term to distinguish this LLM behavior from honest mistakes and outright lies.\n\nThe Princeton researchers identified five distinct forms of this behavior:\n\u2022 Empty rhetoric: Flowery language that adds no substance to responses.\n\u2022 Weasel words: Vague qualifiers like \"studies suggest\" or \"in some cases\" that dodge firm statements.\n\u2022 Paltering: Using selective true statements to mislead, such as highlighting an investment's \"strong historical returns\" while omitting high risks.\n\u2022 Sycophancy: Insincere flattery and agreement to please.\n\nTo address the issues of truth-indifferent AI, the research team developed a new method of training, \"Reinforcement Learning from Hindsight Simulation,\" which evaluates AI responses based on their long-term outcomes rather than immediate satisfaction. Instead of asking, \"Does this answer make the user happy right now?\" the system considers, \"Will following this advice actually help the user achieve their goals?\"\n\nThis approach takes into account the potential future consequences of the AI advice, a tricky prediction that the researchers addressed by using additional AI models to simulate likely outcomes. Early testing showed promising results, with user satisfaction and actual utility improving when systems are trained this way.\n\nConitzer said, however, that LLMs are likely to continue being flawed. Because these systems are trained by feeding them lots of text data, there's no way to ensure that the answer they give makes sense and is accurate every time.\n\n\"It's amazing that it works at all but it's going to be flawed in some ways,\" he said. \"I don't see any sort of definitive way that somebody in the next year or two \u2026 has this brilliant insight, and then it never gets anything wrong anymore.\"\n\nAI systems are becoming part of our daily lives so it will be key to understand how LLMs work. How do developers balance user satisfaction with truthfulness? What other domains might face similar trade-offs between short-term approval and long-term outcomes? And as these systems become more capable of sophisticated reasoning about human psychology, how do we ensure they use those abilities responsibly?\n\nRead more: 'Machines Can't Think for You.' How Learning Is Changing in the Age of AI",
  "medium": "Article",
  "links": [
    "https://www.nejm.org/doi/full/10.1056/NEJMp1208498",
    "https://www.cmu.edu/news/experts/vincent.conitzer",
    "https://arxiv.org/pdf/2507.07484",
    "https://www.cnet.com/tech/services-and-software/you-cant-trust-everything-gen-ai-tells-you-heres-what-to-do-about-it/",
    "https://www.cnet.com/ai-atlas/",
    "https://www.npr.org/2025/08/05/nx-s1-5490447/ai-chatgpt-couples-therapy-advice",
    "https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis",
    "https://www.cnet.com/tech/services-and-software/best-ai-chatbots/",
    "https://www.cnet.com/tech/services-and-software/openai-wants-to-fix-the-annoying-personality-of-chatgpt/",
    "https://www2.csudh.edu/ccauthen/576f12/frankfurt__harry_-_on_bullshit.pdf",
    "https://www.cnet.com/tech/services-and-software/openai-ceo-sam-altman-believes-were-in-an-ai-bubble/",
    "https://www.cnet.com/tech/services-and-software/machines-cant-think-for-you-and-how-learning-is-changing-in-the-age-of-ai/"
  ],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI models",
      "weight": 0.09534326
    },
    {
      "name": "AI language models",
      "weight": 0.091657706
    },
    {
      "name": "additional AI models",
      "weight": 0.089802794
    },
    {
      "name": "generative AI models",
      "weight": 0.08503653
    },
    {
      "name": "modern AI models",
      "weight": 0.08468269
    },
    {
      "name": "AI systems",
      "weight": 0.081307575
    },
    {
      "name": "large language models",
      "weight": 0.08008501
    },
    {
      "name": "AI responses",
      "weight": 0.07627792
    },
    {
      "name": "models",
      "weight": 0.074229196
    },
    {
      "name": "user satisfaction",
      "weight": 0.07002213
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.87158203125
    },
    {
      "name": "/News/Technology News",
      "score": 0.57666015625
    },
    {
      "name": "/Health/Medical Literature & Resources/Other",
      "score": 0.320556640625
    }
  ],
  "sentiment": {
    "positive": 0.11178979,
    "negative": 0.48916236,
    "neutral": 0.39904776
  },
  "summary": "Research by Princeton University has found that generative AI models often lie to customers, often acting as if they are always right. The researchers, who coined the term \"machine bullshit,\" found that large language models are often more likely to produce inaccurate information due to their focus on pleasing users rather than providing accurate information. This behavior is distinguishable from \"sycophancy,\" when an AI chatbot is quick to flatter or agree with users. The study also identified five distinct forms of this behavior: Empty rhetoric, misleading statements, and outright lies. The research also found that after RLHF training, user satisfaction increased by 48%.",
  "shortSummary": "Generative AI models, trained to act like customers, often mischaracterize themselves and mislead users, driven by incentives rather than real-time truthfulness, according to Princeton University research.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "d7c1fd05bfe440a79ca082088a80864e",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://arxiv.org/pdf/2507.07484",
      "text": "%PDF-1.5\n%\ufffd\ufffd\ufffd\ufffd\n1 0 obj\n<< /Metadata 3 0 R /Names 4 0 R /OpenAction 5 0 R /Outlines 6 0 R /PageMode /UseOutlines /Pages 7 0 R /Type /Catalog >>\nendobj\n2 0 obj\n<< /Author (Kaiqu Liang; Haimin Hu; Xuandong Zhao; Dawn Song; Thomas L. Griffiths; Jaime Fern\ufffdndez Fisac) /Creator (arXiv GenPDF \\(tex2pdf:\\)) /DOI (https://doi.org/10.48550/arXiv.2507.07484) /License (http://arxiv.org/licenses/nonexclusive-distrib/1.0/) /PTEX.Fullbanner (This is pdfTeX, Version 3.141592653-2.6-1.40.25 \\(TeX Live 2023\\) kpathsea version 6.3.5) /Producer (pikepdf 8.15.1) /Title (Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models) /Trapped /False /arXivID (https://arxiv.org/abs/2507.07484v1) >>\nendobj\n3 0 obj\n<< /Subtype /XML /Type /Metadata /Length 1790 >>\nstream\nMachine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language ModelsKaiqu LiangHaimin HuXuandong ZhaoDawn SongThomas L. GriffithsJaime Fern\u00e1ndez Fisachttp://arxiv.org/licenses/nonexclusive-distrib/1.0/cs.CLcs.AIcs.LG\nendstream\nendobj\n4 0 obj\n<< /Dests 8 0 R >>\nendobj\n5 0 obj\n<< /D [ 9 0 R /Fit ] /S /GoTo >>\nendobj\n6 0 obj\n<< /Count 13 /First 10 0 R /Last 11 0 R /Type /Outlines >>\nendobj\n7 0 obj\n<< /Count 28 /Kids [ 12 0 R 13 0 R 14 0 R 15 0 R 16 0 R ] /Type /Pages >>\nendobj\n8 0 obj\n<< /Kids [ 17 0 R 18 0 R 19 0 R 20 0 R 21 0 R ] /Limits [ (Doc-Start) (table.caption.5) ] >>\nendobj\n9 0 obj\n<< /Annots [ 22 0 R 23 0 R 24 0 R 25 0 R 26 0 R 27 0 R 28 0 R 29 0 R 30 0 R 31 0 R 32 0 R 33 0 R 34 0 R 35 0 R 36 0 R 37 0 R 38 0 R 39 0 R ] /Contents [ 40 0 R 41 0 R 42 0 R 43 0 R ] /MediaBox [ 0 0 612 792 ] /Parent 12 0 R /Resources 44 0 R /Type /Page >>\nendobj\n10 0 obj\n<< /A 45 0 R /Next 46 0 R /Parent 6 0 R /Title 47 0 R >>\nendobj\n11 0 obj\n<< /A 48 0 R /Count -3 /First 49 0 R /Last 50 0 R /Parent 6 0 R /Prev 51 0 R /Title 52 0 R >>\nendobj\n12 0 obj\n<< /Count 6 /Kids [ 9 0 R 53 0 R 54 0 R 55 0 R 56 0 R 57 0 R ] /Parent 7 0 R /Type /Pages >>\nendobj\n13 0 obj\n<< /Count 6 /Kids [ 58 0 R 59 0 R 60 0 R 61 0 R 62 0 R 63 0 R ] /Parent 7 0 R /Type /Pages >>\nendobj\n14 0 obj\n<< /Count 6 /Kids [ 64 0 R 65 0 R 66 0 R 67 0 R 68 0 R 69 0 R ] /Parent 7 0 R /Type /Pages >>\nendobj\n15 0 obj\n<< /Count 6 /Kids [ 70 0 R 71 0 R 72 0 R 73 0 R 74 0 R 75 0 R ] /Parent 7 0 R /Type /Pages >>\nendobj\n16 0 obj\n<< /Count 4 /Kids [ 76 0 R 77 0 R 78 0 R 79 0 R ] /Parent 7 0 R /Type /Pages >>\nendobj\n17 0 obj\n<< /Kids [ 80 0 R 81 0 R 82 0 R 83 0 R 84 0 R 85 0 R ] /Limits [ (Doc-Start) (cite.frankfurt1986bullshit) ] >>\nendobj\n18 0 obj\n<< /Kids [ 86 0 R 87 0 R 88 0 R 89 0 R 90 0 R 91 0 R ] /Limits [ (cite.frankfurt2006ontruth) (cite.touvron2023llama) ] >>\nendobj\n19 0 obj\n<< /Kids [ 92 0 R 93 0 R 94 0 R 95 0 R 96 0 R 97 0 R ] /Limits [ (cite.undarkChatGPTIsnt) (page.20) ] >>\nendobj\n20 0 obj\n<< /Kids [ 98 0 R 99 0 R 100 0 R 101 0 R 102 0 R 103 0 R ] /Limits [ (page.21) (subsection.F.2) ] >>\nendobj\n21 0 obj\n<< /Kids [ 104 0 R 105 0 R ] /Limits [ (subsection.F.3) (table.caption.5) ] >>\nendobj\n22 0 obj\n<< /A << /S /URI /Type /Action /URI (https://machine-bullshit.github.io) >> /Border [ 0 0 0 ] /C [ 0 1 1 ] /H /I /Rect [ 210.086 267.749 390.366 278.871 ] /Subtype /Link /Type /Annot >>\nendobj\n23 0 obj\n<< /A << /D (cite.frankfurt1986bullshit) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 363.289 212.458 402.553 223.362 ] /Subtype /Link /Type /Annot >>\nendobj\n24 0 obj\n<< /A << /D (cite.frankfurt1986bullshit) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 405.474 212.458 426.993 223.362 ] /Subtype /Link /Type /Annot >>\nendobj\n25 0 obj\n<< /A << /D (cite.frankfurt2006ontruth) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 437.227 201.549 476.095 212.453 ] /Subtype /Link /Type /Annot >>\nendobj\n26 0 obj\n<< /A << /D (cite.frankfurt2006ontruth) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 479.03 201.549 500.55 212.453 ] /Subtype /Link /Type /Annot >>\nendobj\n27 0 obj\n<< /A << /D (cite.bergstrom2021calling) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 332.721 168.822 407.586 179.726 ] /Subtype /Link /Type /Annot >>\nendobj\n28 0 obj\n<< /A << /D (cite.bergstrom2021calling) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 411.329 168.822 432.848 179.726 ] /Subtype /Link /Type /Annot >>\nendobj\n29 0 obj\n<< /A << /D (cite.huang2025survey) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 343.526 119.706 392.137 130.61 ] /Subtype /Link /Type /Annot >>\nendobj\n30 0 obj\n<< /A << /D (cite.huang2025survey) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 394.611 119.706 416.131 130.61 ] /Subtype /Link /Type /Annot >>\nendobj\n31 0 obj\n<< /A << /D (cite.farquhar2024detecting) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 418.878 119.706 476.556 130.61 ] /Subtype /Link /Type /Annot >>\nendobj\n32 0 obj\n<< /A << /D (cite.farquhar2024detecting) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 479.03 119.706 500.55 130.61 ] /Subtype /Link /Type /Annot >>\nendobj\n33 0 obj\n<< /A << /D (cite.sharma2023towards) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 447.929 108.797 503.701 119.701 ] /Subtype /Link /Type /Annot >>\nendobj\n34 0 obj\n<< /A << /D (cite.sharma2023towards) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 107.004 97.888 128.523 108.792 ] /Subtype /Link /Type /Annot >>\nendobj\n35 0 obj\n<< /A << /D (cite.carson2016frankfurt) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 325.275 86.979 355.378 97.883 ] /Subtype /Link /Type /Annot >>\nendobj\n36 0 obj\n<< /A << /D (cite.carson2016frankfurt) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 358.348 86.979 380.186 97.883 ] /Subtype /Link /Type /Annot >>\nendobj\n37 0 obj\n<< /A << /D (cite.littrell2021you) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 383.433 86.979 434.762 97.883 ] /Subtype /Link /Type /Annot >>\nendobj\n38 0 obj\n<< /A << /D (cite.littrell2021you) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 437.732 86.979 459.57 97.883 ] /Subtype /Link /Type /Annot >>\nendobj\n39 0 obj\n<< /A << /S /URI /URI (https://arxiv.org/abs/2507.07484v1) >> /BS << /W 0 >> /NM (fitz-L0) /Rect [ 12 226.01001 32 565.99 ] /Subtype /Link >>\nendobj\n40 0 obj\n<< /Length 10 /Filter /FlateDecode >>\nstream\nx\ufffd+\ufffd \ufffd |\nendstream\nendobj\n41 0 obj\n<< /Filter /FlateDecode /Length 3382 >>\nstream\nx\u06b5Z[w\ufffd6~\ufffd\ufffd\ufffd\u04d6:\ufffdf\ufffd\ufffd\ufffdK\ufffd4i\ufffdM\ufffd\ufffd\ufffd=\u0773M \ufffd\ufffdR\ufffd\ufffdK\ufffd\ufffd\ufffd7\ufffd&i9i\ufffd\ufffd\ufffd\ufffd\ufffd3\ufffdo>\ufffdz{/\ufffd\ufffd\ufffd\u03f7\ufffd\ufffd\ufffds/\ufffd \ufffd\ufffd9^P\ufffd,ROE\ufffdx\ufffd\ufffdv\ufffd\ufffdx|}\ufffd\ufffdY^x\"\"\ufffd\ufffd\ufffd\ufffd\ufffd2\ufffd<\ufffd\ufffd,\ufffdDz\ufffd[\ufffd\ufffd\ufffd\ufffd\ufffdZ\ufffd\ufffd\ufffd$\ufffdU\ufffd\ufffd\ufffd\ufffd\u0555\ufffd\ufffd\ufffdZ\ufffd\ufffdu[~(\ufffd=\ufffdv\ufffd\u04e3nW\ufffd\ufffdu\ufffd\ufffd\ufffd\ufffd{O`\ufffd\u073bEPD \ufffd\ufffd\ufffd0&\ufffd\ufffd^\ufffd[\ufffd\ufffd[E\ufffd\u07f4\ufffdq\ufffd\ufffdb\ufffd\ufffd\ufffd5?_(+\ufffd}\ufffd\ufffdA\ufffd\ufffd\ufffd\ufffdVW\ufffdx\ufffd\ufffdza\ufffdTA*\ufffdQ~\ufffd5\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u01b91\u00ce\ufffdP\ufffdWy\ufffd'6\ufffd\ufffdi\ufffd\ufffd2\ufffd\ufffdKCDi\u01bb\ufffd\ufffd*\ufffdV\ufffd\ufffd\ufffd\u068f\ufffd\ufffdd\u0795\ufffd\ufffdexb\ufffd\ufffd\ufffdy\ufffd\u06b2\ufffd\u8fb1\ufffd\ufffd\ufffd.i\ufffd\ufffdV\"\ufffdu\u06d5\ufffd-J\ufffdD \"1\ufffd{\u0159\ufffd\ufffd\ufffd\ufffdz\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd'\"\ufffd\ufffdE\ufffd\ufffd\ufffd\ufffd\ufffd<:\ufffd>\ufffd*\ufffd\ufffd\ufffd\ufffd\ufffde\ufffd\u0245\ufffd\ufffdY\ufffd0\ufffd\ufffd\ufffd\ufffdy\ufffd\ufffd \ufffd\ufffdS\ufffd\ufffd=\ufffdz\u06f8\ufffd\ufffd\ufffdA5gv\ufffdq,\ufffd\ufffd\ufffd\ufffd\u0106\ufffdn\ufffd`\u0495&\ufffd\u03ea\ufffd\ufffd\ufffdq4\ufffd\ufffd\u07ae\ufffd+~\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdy\u056f\ufffd0 \ufffdJD9\"\ufffd\n\ufffdF\ufffdX\ufffd\ufffdn\ufffd\ufffd \ufffd\u0371X\ufffd/\ufffdo\ufffdh\ufffd\ufffdF\ufffd\ufffd\ufffd\ufffd;Gh\ufffd~}h\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u06f6\u073d \u00e8?tg6!)\ufffd\ufffd\ufffdI\ufffd\ufffdXD~\ufffd\ufffd\ufffd\ufffd?\ufffd\ufffd(\n\ufffd,\ufffd\ufffd\ufffd{Z\n\ufffdgA\ufffd-Ox\ufffd\ufffd\ufffd\ufffdz\ufffd?\u061e\ufffdS\ufffd3\ufffd\u0282DF\ufffd\ufffd\ufffd\ufffd\u06ce}`_\ufffdP$Id&i\ufffde\ufffd\ufffdB\ufffd'\ufffd\ufffdzBe\ufffd\ufffd\ufffdq\ufffd3a\ufffd\ufffd\u0216\ufffd\ufffd\ufffdY #\ufffd:B\ufffd\ufffd\ufffdK\ufffdQ\ufffd\ufffd\ufffd@\ufffdS?\ufffd\ufffd\ufffd\ufffd\nn\ufffd\ufffd\ufffd\ufffd\ufffdN\ufffd\ufffdj\ufffd\ufffdt\ufffd-w\ufffd\ufffd\ufffd\ufffdXC \ufffd\ufffd%33~\ufffd\ufffdB\ufffdF\ufffdYh\ufffd\ufffd\ufffdeH\ufffd|\ufffd1\ufffd\ufffdgjz\ufffd\ufffd\ufffd\ufffdD\ufffd\ufffd\ufffd\u025e\u0705$\u0735\ufffdh\ufffd6\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdIN\ufffdr\ufffd\n\ufffd\ufffd\ufffd$\ufffd&5\ufffd&\ufffdK\u0217\ufffdd\ufffd\ufffd/M7w\\\u0218YX&G\ufffd\ufffdm\ufffd-\ufffd\ufffd`\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd&\ufffdIP\ne\ufffd\ufffd4\ufffd+\ufffd\ufffd1 \ufffd\ufffd\ufffd\ufffd\ufffdw\ufffd`V\ufffd\ufffd|7T\ufffd\ufffd\ufffdd\ufffdy^\ufffd\ufffdJ\ufffd@\ufffd\ufffd\n\ufffd@\ufffd\ufffd\ufffd\ufffdIDD\ufffdS\ufffd\ufffd\ufffd\ufffdr\ufffd\ufffd_\ufffd6\ufffd\ufffd\u06b6\ufffd\ufffdR\ufffdG\nS\ufffdew\ufffdb\ufffd\ufffd\ufffdy\ufffdc\ufffd\ufffd\ufffdm\ufffd6\ufffd\u01d2\ufffd\ufffd[o\ufffdLfEXg\ufffd9\\c)\ufffdA\ufffdU$\ufffd\ufffd\ufffd\ufffd3\ufffd2\ufffd\ufffd\ufffdsQ\ufffd,C\ufffd~\ufffd$@\ufffdH\ufffdDD\ufffd\ufffd(\u0777\ufffd;E\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdBF)]\ufffd_\ufffdWYoKC\ufffd\ufffd\ufffd\ufffd&\ufffd$\ufffdk\ufffd$?C\ufffd\ufffd\ufffd\u83ba\ufffdP,f\ufffdO\ufffd\ufffdX\ufffd\ufffd:\ufffd2\ufffde\ufffd\ufffdc\ufffdF\ufffd^\ufffdo\ufffd\ufffdF\ufffd\ufffdR\u056d%\ufffdh\ufffd5C\ufffdoo)) \ufffd'\ufffd\ufffd\ni\ufffd&\ufffd\ufffd\ufffd\ufffdFb\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\\\ufffd\ufffd\ufffd\ufffd\ufffdp\ufffd\ufffd\ufffdsfM\ufffd\u03139\ufffd/q\ufffd\ufffdh\ufffdz\ufffd0u\ufffd\ufffd\ufffd:\ufffd3[C\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd=\ufffd\n\u07000\ufffdS`\ufffd\ufffd6HA\ufffd\u00af\ufffdQ\u0436\ufffd\ufffd\u02ae\ufffdi\ufffd&\ufffd\ufffdwP\ufffd\ufffd\u04c6\ube58\ufffdi\ufffd\ufffd/\ufffd%v\ufffd\ufffdR\ufffd\ufffd\ufffde[\ufffdcG\ufffd9\ufffd\ufffd[\ufffd\ufffdUs[\ufffd\ufffdO= \ufffdW\ufffd5\ufffd\ufffd\u2363\u0678\ufffd^\ufffd\ufffd\ufffd\ufffd\ufffdN\ufffd\ufffdw\ufffd5\u01afKe\ufffd<\ufffd\ufffd\ufffd\ufffdP<\ufffd\ufffd(\u02a2\ufffd8\ufffda\ufffd\ufffdmt\ufffd\u06b21N\ufffd\ufffd\ufffd\ufffd\ufffdL\ufffd\ufffd\ufffd_\ufffd\n\ufffd\ufffdJ\ufffd~\ufffd}\ufffd2i\ufffd)\ufffd\ufffdGluW\ufffdk\ufffd(\ufffdgQ\ufffd\ufffd%\ufffd \ufffdY\ufffd\ufffdb\ufffd\ufffd\ufffd\ufffd;^k\ufffd=\nD\ufffd9r\ufffd&\ufffdb\ufffdP\ufffd\ufffd\ufffd\ufffd>\u00a9\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u01b9l\ufffdF\n\ufffdZ_\ufffd\ufffd\ufffdo\ufffd\ufffd\ufffdA\ufffd\ufffd\u03f4\ufffd\ufffdqT6\ufffd|bn\ufffd\ufffd\ufffd]\ufffd\ufffd\ufffd0)\ufffdEY@\ufffd\u06f5\ufffd\ufffd\ufffd\ufffd~|\ufffd\ufffd\ufffd\ufffdh!\u02d9\ufffdF\ufffdc\ufffdf\ufffd\ufffd+\ufffdv\ufffd\ufffdt#A4\ufffd\ufffd\ufffd|n\ufffd`\ufffd\ufffd3.]\ufffd\u032b\ufffd(/\ufffdfwE\u070cs@\ufffd\n>i\ufffd\ufffd10\ufffd\ufffd?\ufffdI\ufffd\ufffdj]YP@+>\ufffd\u0758v\ufffd\ufffd\n\ufffd{\ufffd\ufffd\ufffd\ufffd\ufffdf \ufffd#RYLTF\ufffd\ufffd\ufffdK~?\ufffd\ufffdg 3\ufffd\ufffd\ufffda\ufffdN1b\ufffd\"\ufffd\ufffd\ufffd$CaP\ufffd5<\ufffdYw\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd>fZ\ufffd3S\ufffd>v\"\ufffdq\ufffd\ufffd\\\ufffd-|s\ufffd\ufffd\ufffd\ufffd8\u0573\ufffd{\u07a9\ufffd\ufffd7\ufffdQBg)\ufffd\ufffd\ufffd?\ufffd\ufffdKW\ufffdA\ufffd7\ufffd\ufffd+[\u00a1-\u010b= \ufffd')\ufffd\ufffd\ufffd\u0743y\ufffd~\ufffd\ufffd\u00a7 \ufffd\ufffd\ufffd\ufffd\ufffdrv\ufffd\ufffd\ufffdH$\ufffdk\ufffd\ufffd\ufffd\ufffd]\ufffd\ufffd\ufffd^w\ufffd\ufffd\ufffd m1\ufffd>\ufffd\ufffd\ufffd\ufffdO&\ufffd[\ufffd \ufffd\ufffdI\ufffd-W4HS\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u0231Hu\u0177\ufffd\ufffd]\ufffd\ufffd5\ufffd\ufffd\ufffdA\ufffd\ufffdpihW\ufffd\ufffd\\0\ufffd>\ufffdiP\ufffd7\ufffd\ufffd\ufffd\ufffdF\ufffdO\ufffd\ufffd\ufffd8\ufffdS~\ufffdx6\ufffd\u032a\ufffd\ufffdHm6 x\ufffdr\ufffd\ufffd\ufffd\ufffd \ufffd\u06bd\ufffd^\ufffdk\ufffd\ufffd\ufffdR\n\ufffd\ufffd\ufffd\ufffd\ufffd?u_>zd\ufffdjuqk\n:\ufffd&!\ufffd\ufffd4\ufffd\ufffd\ufffd\ufffd]1\ufffd\ufffd>C\ufffd\ufffds'48{%\ufffd\ufffdCV\ufffd\ufffd\ufffd_ \ufffd!XYo.GD\ufffd\ufffd-\ufffd\u0715\ufffd{\ufffdH\ufffdT\u0634\ufffdj~\ufffdw\ufffdP\ufffd\ufffd\ufffd\ufffdcX\ufffd1\ufffd\ufffdr@`\ufffd\ufffd@\ufffd\ufffd\ufffd\ufffdST\ufffd\ufffd[8\n3E~\ufffd\ufffd\u03ce-A\ufffd\ufffdA\u025d\ufffdr\ufffd=[\ufffdVl*V\u0659\ufffd\n\ufffd\ufffd\ufffd|\ufffd\ufffd(\ufffdt9\u0360\ufffd\ufffd\ufffdU}\ufffd\ufffd\ufffd\ufffdj\ufffdn\u0748Gb\ufffdG\ufffd\ufffdv\ufffd0\ufffd0\ufffd\ufffd\ufffd\ufffdV\ufffd\ufffd\ufffd$k!\ufffdW\ufffd0\ufffd \ufffd\ufffdw,\ufffdz\ufffd|\ufffdy4\ufffd-Eq\ufffdu+xsc\ufffdz`\ufffdW\u00c9\ufffd\ufffd\ufffd\ufffd\ufffdS\ufffd-\u04a0\ufffd|#?\ufffd\ufffd\ufffd\ufffdN\ufffd\ufffd+\ufffdE\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdp\ufffd\\ \ufffd$:L&^\u2df0L\ufffdnY\ufffd#dH\ufffd\ufffdm\ufffd_\u066eiR\uae60\ufffd\ufffdc\u0675\ufffdD\ufffd\ufffd8\"\ufffdpJ\ufffd\ufce3\ufffdLN\ufffd!\ufffdU\ufffd\ufffd1\ufffd_9:\ufffdU`\ufffd\ufffd\ufffd\ufffd`\ufffd6\ufffdGA\ufffd\ufffd.\ufffd\ufffd\ufffd\ufffd%\ufffd\ufffd\ufffd.O\ufffd\ufffd\ufffdHFmE\ufffd:\u0570-)\ufffd\ufffd \u0436\ufffd5\ufffd\ufffdf.oF8|V}_\ufffdX\ufffd%w\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdL*\ufffd2\ufffd]/w\ufffd\ufffd\ufffd\ufffd\ufffd|yjD\ufffd\ufffd,\ufffd1\ufffd&\ufffdp\ufffd\ufffd\ufffdVf#\ufffd\ufffd{\ufffdl\ufffd\ufffd\ufffd\ufffd^T\ufffdP\ufffd\ufffdq\ufffd\ufffdq\ufffd\ufffdj\ufffd\ufffd\ufffdG~7\ufffdMc!\ufffd>8\ufffd\ufffd28m\ufffd\ufffdTlGa$\ufffd\ufffdYbN\ufffd!\ufffd`s\ufffd=.B\ufffdJ\u03a9\ufffd!\ufffdIJY\ufffd\ufffdN\ufffd\ufffdmj.,\ufffd\ufffd%\ufffd*\ufffd\ufffd7\ufffd\ufffd\ufffd\ufffd\u0578C\ufffd\ufffdV\ufffd=F.i\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffdC:mz`_]\ufffdt\ufffd\u7cf4\ufffd8\ufffd\ufffd\ufffd\ufffdf<>j\ufffd\ufffd\ufffdK\ufffd=n\ufffd\ufffd&\ufffdX\ufffd\ufffd\ufffd<-%|9\ufffdl-\ufffds\ufffd\ufffd-Uq\ufffduZj\ufffd\ufffd#\ufffd\ufffdmmE\ufffdM\ufffd\ufffd\u0687zB\ufffd\ufffdt\ufffdmj\ufffd dI-g\ufffd\ufffdF\ufffd\ufffd\ufffd\ufffd~\ufffdX\ufffd6I\u01fc\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd|\ufffdN\ufffd\n\ufffdgk\ufffd/\ufffd82\ufffd\ufffdp)`\ufffd\ufffd\ufffd\ufffdBX\ufffd\ufffd\ufffd\ufffd\ufffd]\"\ufffd\ufffd\ufffd\ufffd\ufffd0\ufffd`\ufffd\ufffd?\ufffd#`Ub\ufffdT\ufffd\ufffd\ufffd0\ufffdmj@hg=&e\ufffdc\ufffd9`Kg\ufffd\ufffd4fS\ufffd\ufffd\ufffd\ufffd\ufffd&Y\ufffd\ufffd\\LX\ufffd\ufffd\u04c7\ufffd\ufffd\ufffdL0c\ufffdv\ufffdU\ufffd\ufffdf#,\ufffd\u04de,\ufffd\ufffd\ufffdS\ufffd20!\"A\ufffd\ufffd\ufffd\u06a9\ufffd\ufffdR##5\ufffd&\ufffdh\ufffd\ufffdWm\ufffd\ufffd(;\ufffd82[b\ufffd\ufffdu$\ufffdu$~\ufffd \ufffd\ufffd1-\ufffd 2\ufffd\ufffd\ufffdo\ufffd<\ufffdeG\ufffd\ufffd\ufffd\ufffd\ufffd}4\ufffd\ufffd\ufffd\ufffd\ufffd\u03af\u0749\ufffd\ufffdx\ufffd\ufffdT\ufffd\ufffdh6\ufffd\u04e1\ufffd.9*\ufffd\ufffd8\ufffd\ufffdS\ufffd\ufffd!\ufffdV\ufffde\ufffdp\ufffd| ?HA\ufffdG\ufffdNj&\ufffd=#\ufffd\ufffd98\ufffd\u07db\ufffd\ufffd]Q\u049cSt\ufffd}\ufffd5\ufffd_\ufffdiJ\ufffd\ufffdO\ufffd\ufffd\ufffdo\ufffd\ufffd2tLq\ufffdjN\ufffd8m\ufffd\ufffd\ufffdW\ufffdxr\ufffd\ufffdV\ufffdX\ufffd.?\ufffd\ufffdW\ufffd\ufffdb\ufffd\ufffd\ufffd{i\ufffd!-G\ufffd\ufffd\ufffd\u0770\ufffd\ufffd\ufffdU&\ufffd\ufffd:\"%\ufffd\u0217P\ufffd\u0780B\\G;T\ufffd\ufffd?Qm\u01d8\ufffdY\\4\ufffd/E\ufffd\ufffd\ufffd\ufffd\ufffdKE\u07c3\u069dk\ufffd?\ufffd\ufffd\ufffd\ufffd\n\ufffdK\ufffds\ufffdz\ufffd-\ufffd\ufffds\ufffd\ufffd\ufffdIW=(\ufffd\ufffd\ufffd\ufffd\ufffd+\ufffd\u06feE\ufffdb\ufffd\ufffd\ufffd:\ufffd\ufffd7p\ufffdi\ufffd F\ufffdg\ufffd\ufffd\ufffddY \u0676:b8/\ufffd\u010b\ufffd\ufffdj\ufffd\ufffd\ufffd{TH\ufffd\ufffd\ufffd&\ufffdL\ufffd@\ufffdm\ufffd\"\ufffd*\ufffd>8\ufffd\u04f2{>\ufffd\ufffd\ufffd/I\ufffd\ufffd\nendstream\nendobj\n42 0 obj\n<< /Length 11 /Filter /FlateDecode >>\nstream\nx\ufffd\ufffd\n\ufffd \ufffd f\nendstream\nendobj\n43 0 obj\n<< /Filter /FlateDecode /Length 141 >>\nstream\nx\ufffdE\ufffd\ufffd\n1D\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdi\ufffdiW\ufffd\ufffd\\z\ube8a\ufffd*\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd0\ufffd\ufffdJ$\ufffdq,\ufffd3}\ufffdc\ufffd\ufffdJ\ufffd\ufffd\ufffd2\ufffd\ufffd5O\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd*\ufffd\ufffd\ufffd\ufffdO\ufffd\u67df\ufffd/\ufffd\ufffdj]\ufffd\u01c0\n\ufffd \ufffdT|\ufffdNE\u014e\ufffd \ufffd<\ufffdh\ufffdJ\ufffdP\ufffd\ufffd\ufffdX\ufffd\ufffd\ufffd-iK\ufffdD;z\ufffdi'\nendstream\nendobj\n44 0 obj\n<< /ColorSpace 106 0 R /ExtGState 107 0 R /Font << /F103 108 0 R /F65 109 0 R /F89 110 0 R /F95 111 0 R /Times-Roman 112 0 R >> /Pattern 113 0 R /ProcSet [ /PDF /Text ] >>\nendobj\n45 0 obj\n<< /D (section.1) /S /GoTo >>\nendobj\n46 0 obj\n<< /A 114 0 R /Next 115 0 R /Parent 6 0 R /Prev 10 0 R /Title 116 0 R >>\nendobj\n47 0 obj\nendobj\n48 0 obj\n<< /D (appendix.G) /S /GoTo >>\nendobj\n49 0 obj\n<< /A 117 0 R /Next 118 0 R /Parent 11 0 R /Title 119 0 R >>\nendobj\n50 0 obj\n<< /A 120 0 R /Parent 11 0 R /Prev 118 0 R /Title 121 0 R >>\nendobj\n51 0 obj\n<< /A 122 0 R /Count -3 /First 123 0 R /Last 124 0 R /Next 11 0 R /Parent 6 0 R /Prev 125 0 R /Title 126 0 R >>\nendobj\n52 0 obj\nendobj\n53 0 obj\n<< /Annots [ 127 0 R 128 0 R 129 0 R 130 0 R 131 0 R 132 0 R 133 0 R 134 0 R 135 0 R 136 0 R 137 0 R 138 0 R 139 0 R 140 0 R 141 0 R 142 0 R 143 0 R 144 0 R 145 0 R 146 0 R 147 0 R 148 0 R 149 0 R ] /Contents 150 0 R /MediaBox [ 0 0 612 792 ] /Parent 12 0 R /Resources 151 0 R /Type /Page >>\nendobj\n54 0 obj\n<< /Annots [ 152 0 R 153 0 R 154 0 R 155 0 R 156 0 R 157 0 R 158 0 R 159 0 R 160 0 R 161 0 R 162 0 R 163 0 R 164 0 R 165 0 R 166 0 R 167 0 R 168 0 R 169 0 R 170 0 R 171 0 R ] /Contents 172 0 R /MediaBox [ 0 0 612 792 ] /Parent 12 0 R /Resources 173 0 R /Type /Page >>\nendobj\n55 0 obj\n<< /Annots [ 174 0 R 175 0 R 176 0 R 177 0 R 178 0 R 179 0 R 180 0 R 181 0 R 182 0 R 183 0 R 184 0 R 185 0 R 186 0 R 187 0 R 188 0 R 189 0 R 190 0 R 191 0 R 192 0 R 193 0 R 194 0 R 195 0 R 196 0 R 197 0 R 198 0 R 199 0 R 200 0 R 201 0 R 202 0 R 203 0 R 204 0 R 205 0 R 206 0 R 207 0 R 208 0 R 209 0 R 210 0 R 211 0 R 212 0 R 213 0 R 214 0 R 215 0 R 216 0 R 217 0 R 218 0 R ] /Contents 219 0 R /MediaBox [ 0 0 612 792 ] /Parent 12 0 R /Resources 220 0 R /Type /Page >>\nendobj\n56 0 obj\n<< /Annots [ 221 0 R 222 0 R 223 0 R 224 0 R 225 0 R 226 0 R 227 0 R 228 0 R 229 0 R 230 0 R 231 0 R 232 0 R 233 0 R 234 0 R 235 0 R 236 0 R ] /Contents 237 0 R /MediaBox [ 0 0 612 792 ] /Parent 12 0 R /Resources 238 0 R /Type /Page >>\nendobj\n57 0 obj\n<< /Annots [ 239 0 R 240 0 R ] /Contents 241 0 R /MediaBox [ 0 0 612 792 ] /Parent 12 0 R /Resources 242 0 R /Type /Page >>\nendobj\n58 0 obj\n<< /Annots [ 243 0 R 244 0 R 245 0 R 246 0 R 247 0 R 248 0 R ] /Contents 249 0 R /MediaBox [ 0 0 612 792 ] /Parent 13 0 R /Resources 250 0 R /Type /Page >>\nendobj\n59 0 obj\n<< /Annots [ 251 0 R 252 0 R 253 0 R 254 0 R 255 0 R 256 0 R 257 0 R 258 0 R ] /Contents 259 0 R /MediaBox [ 0 0 612 792 ] /Parent 13 0 R /Resources 260 0 R /Type /Page >>\nendobj\n60 0 obj\n<< /Contents 261 0 R /MediaBox [ 0 0 612 792 ] /Parent 13 0 R /Resources 262 0 R /Type /Page >>\nendobj\n61 0 obj\n<< /Annots [ 263 0 R 264 0 R 265 0 R 266 0 R ] /Contents 267 0 R /MediaBox [ 0 0 612 792 ] /Parent 13 0 R /Resources 268 0 R /Type /Page >>\nendobj\n62 0 obj\n<< /Contents 269 0 R /MediaBox [ 0 0 612 792 ] /Parent 13 0 R /Resources 270 0 R /Type /Page >>\nendobj\n63 0 obj\n<< /Annots [ 271 0 R ] /Contents 272 0 R /MediaBox [ 0 0 612 792 ] /Parent 13 0 R /Resources 273 0 R /Type /Page >>\nendobj\n64 0 obj\n<< /Contents 274 0 R /MediaBox [ 0 0 612 792 ] /Parent 14 0 R /Resources 275 0 R /Type /Page >>\nendobj\n65 0 obj\n<< /Annots [ 276 0 R 277 0 R 278 0 R 279 0 R ] /Contents 280 0 R /MediaBox [ 0 0 612 792 ] /Parent 14 0 R /Resources 281 0 R /Type /Page >>\nendobj\n66 0 obj\n<< /Annots [ 282 0 R 283 0 R 284 0 R 285 0 R 286 0 R 287 0 R 288 0 R 289 0 R 290 0 R 291 0 R 292 0 R 293 0 R 294 0 R 295 0 R 296 0 R ] /Contents 297 0 R /MediaBox [ 0 0 612 792 ] /Parent 14 0 R /Resources 298 0 R /Type /Page >>\nendobj\n67 0 obj\n<< /Annots [ 299 0 R 300 0 R 301 0 R 302 0 R 303 0 R 304 0 R 305 0 R 306 0 R 307 0 R 308 0 R 309 0 R 310 0 R 311 0 R 312 0 R 313 0 R 314 0 R 315 0 R 316 0 R 317 0 R 318 0 R 319 0 R 320 0 R 321 0 R 322 0 R 323 0 R 324 0 R 325 0 R 326 0 R 327 0 R 328 0 R 329 0 R 330 0 R 331 0 R 332 0 R 333 0 R 334 0 R 335 0 R 336 0 R 337 0 R 338 0 R 339 0 R 340 0 R 341 0 R 342 0 R 343 0 R 344 0 R 345 0 R 346 0 R 347 0 R 348 0 R 349 0 R 350 0 R 351 0 R 352 0 R 353 0 R 354 0 R 355 0 R 356 0 R 357 0 R 358 0 R 359 0 R 360 0 R 361 0 R 362 0 R 363 0 R 364 0 R ] /Contents 365 0 R /MediaBox [ 0 0 612 792 ] /Parent 14 0 R /Resources 366 0 R /Type /Page >>\nendobj\n68 0 obj\n<< /Contents 367 0 R /MediaBox [ 0 0 612 792 ] /Parent 14 0 R /Resources 368 0 R /Type /Page >>\nendobj\n69 0 obj\n<< /Contents 369 0 R /MediaBox [ 0 0 612 792 ] /Parent 14 0 R /Resources 370 0 R /Type /Page >>\nendobj\n70 0 obj\n<< /Contents 371 0 R /MediaBox [ 0 0 612 792 ] /Parent 15 0 R /Resources 372 0 R /Type /Page >>\nendobj\n71 0 obj\n<< /Annots [ 373 0 R 374 0 R ] /Contents 375 0 R /MediaBox [ 0 0 612 792 ] /Parent 15 0 R /Resources 376 0 R /Type /Page >>\nendobj\n72 0 obj\n<< /Contents 377 0 R /MediaBox [ 0 0 612 792 ] /Parent 15 0 R /Resources 378 0 R /Type /Page >>\nendobj\n73 0 obj\n<< /Contents 379 0 R /MediaBox [ 0 0 612 792 ] /Parent 15 0 R /Resources 380 0 R /Type /Page >>\nendobj\n74 0 obj\n<< /Contents 381 0 R /MediaBox [ 0 0 612 792 ] /Parent 15 0 R /Resources 382 0 R /Type /Page >>\nendobj\n75 0 obj\n<< /Contents 383 0 R /MediaBox [ 0 0 612 792 ] /Parent 15 0 R /Resources 384 0 R /Type /Page >>\nendobj\n76 0 obj\n<< /Contents 385 0 R /MediaBox [ 0 0 612 792 ] /Parent 16 0 R /Resources 386 0 R /Type /Page >>\nendobj\n77 0 obj\n<< /Annots [ 387 0 R 388 0 R 389 0 R 390 0 R 391 0 R 392 0 R ] /Contents 393 0 R /MediaBox [ 0 0 612 792 ] /Parent 16 0 R /Resources 394 0 R /Type /Page >>\nendobj\n78 0 obj\n<< /Annots [ 395 0 R 396 0 R 397 0 R 398 0 R 399 0 R 400 0 R 401 0 R ] /Contents 402 0 R /MediaBox [ 0 0 612 792 ] /Parent 16 0 R /Resources 403 0 R /Type /Page >>\nendobj\n79 0 obj\n<< /Contents 404 0 R /MediaBox [ 0 0 612 792 ] /Parent 16 0 R /Resources 405 0 R /Type /Page >>\nendobj\n80 0 obj\n<< /Limits [ (Doc-Start) (appendix.D) ] /Names [ (Doc-Start) 406 0 R (Hfootnote.1) 407 0 R (appendix.A) 408 0 R (appendix.B) 409 0 R (appendix.C) 410 0 R (appendix.D) 411 0 R ] >>\nendobj\n81 0 obj\n<< /Limits [ (appendix.E) (cite.Gemini_flash) ] /Names [ (appendix.E) 412 0 R (appendix.F) 413 0 R (appendix.G) 414 0 R (cite.Claude_3_5) 415 0 R (cite.GPT4_o_mini) 416 0 R (cite.Gemini_flash) 417 0 R ] >>\nendobj\n82 0 obj\n<< /Limits [ (cite.amodei2016concrete) (cite.bang2024measuring) ] /Names [ (cite.amodei2016concrete) 418 0 R (cite.artstein2008inter) 419 0 R (cite.azaria2023internal) 420 0 R (cite.bai2022training) 421 0 R (cite.bai2024benchmarking) 422 0 R (cite.bang2024measuring) 423 0 R ] >>\nendobj\n83 0 obj\n<< /Limits [ (cite.bergstrom2021calling) (cite.cohen2002deeper) ] /Names [ (cite.bergstrom2021calling) 424 0 R (cite.carson2016frankfurt) 425 0 R (cite.chen2023alpagasus) 426 0 R (cite.chen2024odin) 427 0 R (cite.christiano2017deep) 428 0 R (cite.cohen2002deeper) 429 0 R ] >>\nendobj\n84 0 obj\n<< /Limits [ (cite.denison2024sycophancy) (cite.everitt2016avoiding) ] /Names [ (cite.denison2024sycophancy) 430 0 R (cite.dubey2024llama) 431 0 R (cite.dubois2024alpacafarm) 432 0 R (cite.eicker1963asymptotic) 433 0 R (cite.ethayarajh2024kto) 434 0 R (cite.everitt2016avoiding) 435 0 R ] >>\nendobj\n85 0 obj\n<< /Limits [ (cite.everitt2017reinforcement) (cite.frankfurt1986bullshit) ] /Names [ (cite.everitt2017reinforcement) 436 0 R (cite.everitt2021reward) 437 0 R (cite.farquhar2024detecting) 438 0 R (cite.fernandes2023devil) 439 0 R (cite.fisher2025political) 440 0 R (cite.frankfurt1986bullshit) 441 0 R ] >>\nendobj\n86 0 obj\n<< /Limits [ (cite.frankfurt2006ontruth) (cite.jiang2020can) ] /Names [ (cite.frankfurt2006ontruth) 442 0 R (cite.gao2023scaling) 443 0 R (cite.hendrycks2020measuring) 444 0 R (cite.hicks2024chatgpt) 445 0 R (cite.huang2025survey) 446 0 R (cite.jiang2020can) 447 0 R ] >>\nendobj\n87 0 obj\n<< /Limits [ (cite.kadavath2022language) (cite.li2024can) ] /Names [ (cite.kadavath2022language) 448 0 R (cite.krippendorff2018content) 449 0 R (cite.lambert2024rewardbench) 450 0 R (cite.lang2024your) 451 0 R (cite.li2023alpacaeval) 452 0 R (cite.li2024can) 453 0 R ] >>\nendobj\n88 0 obj\n<< /Limits [ (cite.liang2025rlhs) (cite.mchugh2013chi) ] /Names [ (cite.liang2025rlhs) 454 0 R (cite.lightman2023let) 455 0 R (cite.littrell2021you) 456 0 R (cite.luo2023wizardmath) 457 0 R (cite.mccarthy2020confronting) 458 0 R (cite.mchugh2013chi) 459 0 R ] >>\nendobj\n89 0 obj\n<< /Limits [ (cite.meng2024simpo) (cite.rafailov2024direct) ] /Names [ (cite.meng2024simpo) 460 0 R (cite.ouyang2022training) 461 0 R (cite.pennycook2015reception) 462 0 R (cite.perez2022discovering) 463 0 R (cite.phelps2023models) 464 0 R (cite.rafailov2024direct) 465 0 R ] >>\nendobj\n90 0 obj\n<< /Limits [ (cite.rottger2024political) (cite.schulman2017proximal) ] /Names [ (cite.rottger2024political) 466 0 R (cite.saha2023branch) 467 0 R (cite.santacroce2023efficient) 468 0 R (cite.santurkar2023whose) 469 0 R (cite.saunders2022self) 470 0 R (cite.schulman2017proximal) 471 0 R ] >>\nendobj\n91 0 obj\n<< /Limits [ (cite.sharma2023towards) (cite.touvron2023llama) ] /Names [ (cite.sharma2023towards) 472 0 R (cite.snow2008cheap) 473 0 R (cite.soares2015corrigibility) 474 0 R (cite.spicer2017business) 475 0 R (cite.taori2023stanford) 476 0 R (cite.touvron2023llama) 477 0 R ] >>\nendobj\n92 0 obj\n<< /Limits [ (cite.undarkChatGPTIsnt) (cite.wen2024language) ] /Names [ (cite.undarkChatGPTIsnt) 478 0 R (cite.wald1943tests) 479 0 R (cite.wang2023openchat) 480 0 R (cite.wei2022chain) 481 0 R (cite.wei2023simple) 482 0 R (cite.wen2024language) 483 0 R ] >>\nendobj\n93 0 obj\n<< /Limits [ (cite.williams2024targeted) (cite.ziegler2019fine) ] /Names [ (cite.williams2024targeted) 484 0 R (cite.xia2024less) 485 0 R (cite.yang2024qwen2) 486 0 R (cite.zhao2023slic) 487 0 R (cite.zheng2023secrets) 488 0 R (cite.ziegler2019fine) 489 0 R ] >>\nendobj\n94 0 obj\n<< /Limits [ (figure.caption.11) (figure.caption.18) ] /Names [ (figure.caption.11) 490 0 R (figure.caption.12) 491 0 R (figure.caption.14) 492 0 R (figure.caption.15) 493 0 R (figure.caption.17) 494 0 R (figure.caption.18) 495 0 R ] >>\nendobj\n95 0 obj\n<< /Limits [ (figure.caption.2) (page.1) ] /Names [ (figure.caption.2) 496 0 R (figure.caption.6) 497 0 R (figure.caption.7) 498 0 R (figure.caption.8) 499 0 R (noticebox.caption.1) 500 0 R (page.1) 501 0 R ] >>\nendobj\n96 0 obj\n<< /Limits [ (page.10) (page.15) ] /Names [ (page.10) 502 0 R (page.11) 503 0 R (page.12) 504 0 R (page.13) 505 0 R (page.14) 506 0 R (page.15) 507 0 R ] >>\nendobj\n97 0 obj\n<< /Limits [ (page.16) (page.20) ] /Names [ (page.16) 508 0 R (page.17) 509 0 R (page.18) 510 0 R (page.19) 511 0 R (page.2) 512 0 R (page.20) 513 0 R ] >>\nendobj\n98 0 obj\n<< /Limits [ (page.21) (page.26) ] /Names [ (page.21) 514 0 R (page.22) 515 0 R (page.23) 516 0 R (page.24) 517 0 R (page.25) 518 0 R (page.26) 519 0 R ] >>\nendobj\n99 0 obj\n<< /Limits [ (page.27) (page.6) ] /Names [ (page.27) 520 0 R (page.28) 521 0 R (page.3) 522 0 R (page.4) 523 0 R (page.5) 524 0 R (page.6) 525 0 R ] >>\nendobj\n100 0 obj\n<< /Limits [ (page.7) (section*.9) ] /Names [ (page.7) 526 0 R (page.8) 527 0 R (page.9) 528 0 R (section*.13) 529 0 R (section*.4) 530 0 R (section*.9) 531 0 R ] >>\nendobj\n101 0 obj\n<< /Limits [ (section.1) (section.6) ] /Names [ (section.1) 532 0 R (section.2) 533 0 R (section.3) 534 0 R (section.4) 535 0 R (section.5) 536 0 R (section.6) 537 0 R ] >>\nendobj\n102 0 obj\n<< /Limits [ (subsection.3.1) (subsection.5.1) ] /Names [ (subsection.3.1) 538 0 R (subsection.3.2) 539 0 R (subsection.4.1) 540 0 R (subsection.4.2) 541 0 R (subsection.4.3) 542 0 R (subsection.5.1) 543 0 R ] >>\nendobj\n103 0 obj\n<< /Limits [ (subsection.5.2) (subsection.F.2) ] /Names [ (subsection.5.2) 544 0 R (subsection.C.1) 545 0 R (subsection.C.2) 546 0 R (subsection.C.3) 547 0 R (subsection.F.1) 548 0 R (subsection.F.2) 549 0 R ] >>\nendobj\n104 0 obj\n<< /Limits [ (subsection.F.3) (table.caption.16) ] /Names [ (subsection.F.3) 550 0 R (subsection.G.1) 551 0 R (subsection.G.2) 552 0 R (subsection.G.3) 553 0 R (table.caption.10) 554 0 R (table.caption.16) 555 0 R ] >>\nendobj\n105 0 obj\n<< /Limits [ (table.caption.19) (table.caption.5) ] /Names [ (table.caption.19) 556 0 R (table.caption.20) 557 0 R (table.caption.21) 558 0 R (table.caption.3) 559 0 R (table.caption.5) 560 0 R ] >>\nendobj\n106 0 obj\n<< /pgfprgb [ /Pattern /DeviceRGB ] >>\nendobj\n107 0 obj\n<< /pgf@ca1.0 << /ca 1 >> >>\nendobj\n108 0 obj\n<< /BaseFont /HCKXCS+NimbusRomNo9L-ReguItal /Encoding 561 0 R /FirstChar 2 /FontDescriptor 562 0 R /LastChar 122 /Subtype /Type1 /ToUnicode 563 0 R /Type /Font /Widths 564 0 R >>\nendobj\n109 0 obj\n<< /BaseFont /JGFIOT+SFTT1000 /Encoding 565 0 R /FirstChar 40 /FontDescriptor 566 0 R /LastChar 122 /Subtype /Type1 /ToUnicode 567 0 R /Type /Font /Widths 568 0 R >>\nendobj\n110 0 obj\n<< /BaseFont /AXMTDJ+NimbusRomNo9L-Medi /Encoding 561 0 R /FirstChar 2 /FontDescriptor 569 0 R /LastChar 225 /Subtype /Type1 /ToUnicode 570 0 R /Type /Font /Widths 571 0 R >>\nendobj\n111 0 obj\n<< /BaseFont /SBJDCJ+NimbusRomNo9L-Regu /Encoding 561 0 R /FirstChar 1 /FontDescriptor 572 0 R /LastChar 252 /Subtype /Type1 /ToUnicode 573 0 R /Type /Font /Widths 574 0 R >>\nendobj\n112 0 obj\n<< /BaseFont /Times-Roman /Encoding /WinAnsiEncoding /Subtype /Type1 /Type /Font >>\nendobj\n113 0 obj\n<< >>\nendobj\n114 0 obj\n<< /D (section.2) /S /GoTo >>\nendobj\n115 0 obj\n<< /A 575 0 R /Count -2 /First 576 0 R /Last 577 0 R /Next 578 0 R /Parent 6 0 R /Prev 46 0 R /Title 579 0 R >>\nendobj\n116 0 obj\nendobj\n117 0 obj\n<< /D (subsection.G.1) /S /GoTo >>\nendobj\n118 0 obj\n<< /A 580 0 R /Next 50 0 R /Parent 11 0 R /Prev 49 0 R /Title 581 0 R >>\nendobj\n119 0 obj\nendobj\n120 0 obj\n<< /D (subsection.G.3) /S /GoTo >>\nendobj\n121 0 obj\nendobj\n122 0 obj\n<< /D (appendix.F) /S /GoTo >>\nendobj\n123 0 obj\n<< /A 582 0 R /Next 583 0 R /Parent 51 0 R /Title 584 0 R >>\nendobj\n124 0 obj\n<< /A 585 0 R /Parent 51 0 R /Prev 583 0 R /Title 586 0 R >>\nendobj\n125 0 obj\n<< /A 587 0 R /Next 51 0 R /Parent 6 0 R /Prev 588 0 R /Title 589 0 R >>\nendobj\n126 0 obj\nendobj\n127 0 obj\n<< /A << /D (figure.caption.2) /S /GoTo >> /Border [ 0 0 0 ] /C [ 1 0 0 ] /H /I /Rect [ 355.33 420.343 362.205 431.247 ] /Subtype /Link /Type /Annot >>\nendobj\n128 0 obj\n<< /A << /D (cite.frankfurt2006ontruth) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 334.965 409.434 373.833 420.338 ] /Subtype /Link /Type /Annot >>\nendobj\n129 0 obj\n<< /A << /D (cite.frankfurt2006ontruth) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 376.772 409.434 398.291 420.338 ] /Subtype /Link /Type /Annot >>\nendobj\n130 0 obj\n<< /A << /D (cite.bergstrom2021calling) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 401.504 409.434 476.381 420.338 ] /Subtype /Link /Type /Annot >>\nendobj\n131 0 obj\n<< /A << /D (cite.bergstrom2021calling) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 479.32 409.434 500.839 420.338 ] /Subtype /Link /Type /Annot >>\nendobj\n132 0 obj\n<< /A << /D (cite.undarkChatGPTIsnt) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 202.668 393.045 278.968 403.949 ] /Subtype /Link /Type /Annot >>\nendobj\n133 0 obj\n<< /A << /D (cite.hicks2024chatgpt) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 376.458 349.409 425.295 360.313 ] /Subtype /Link /Type /Annot >>\nendobj\n134 0 obj\n<< /A << /D (cite.hicks2024chatgpt) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 429.648 349.409 451.964 360.313 ] /Subtype /Link /Type /Annot >>\nendobj\n135 0 obj\n<< /A << /D (cite.frankfurt1986bullshit) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 341.562 256.657 381.447 267.561 ] /Subtype /Link /Type /Annot >>\nendobj\n136 0 obj\n<< /A << /D (cite.frankfurt1986bullshit) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 385.298 256.657 407.355 267.561 ] /Subtype /Link /Type /Annot >>\nendobj\n137 0 obj\n<< /A << /D (cite.bergstrom2021calling) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 428.196 256.657 504.996 267.561 ] /Subtype /Link /Type /Annot >>\nendobj\n138 0 obj\n<< /A << /D (cite.bergstrom2021calling) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 109.989 245.748 131.887 256.652 ] /Subtype /Link /Type /Annot >>\nendobj\n139 0 obj\n<< /A << /D (cite.liang2025rlhs) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 351.333 218.45 397.753 229.354 ] /Subtype /Link /Type /Annot >>\nendobj\n140 0 obj\n<< /A << /D (cite.liang2025rlhs) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 400.704 218.45 422.283 229.354 ] /Subtype /Link /Type /Annot >>\nendobj\n141 0 obj\n<< /A << /D (cite.fisher2025political) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 141.096 207.541 190.685 218.445 ] /Subtype /Link /Type /Annot >>\nendobj\n142 0 obj\n<< /A << /D (cite.fisher2025political) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 193.722 207.541 216.038 218.445 ] /Subtype /Link /Type /Annot >>\nendobj\n143 0 obj\n<< /A << /D (cite.ouyang2022training) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 179.991 185.723 235.748 196.627 ] /Subtype /Link /Type /Annot >>\nendobj\n144 0 obj\n<< /A << /D (cite.ouyang2022training) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 238.768 185.723 260.825 196.627 ] /Subtype /Link /Type /Annot >>\nendobj\n145 0 obj\n<< /A << /D (cite.wei2022chain) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 132.613 152.995 173.32 163.899 ] /Subtype /Link /Type /Annot >>\nendobj\n146 0 obj\n<< /A << /D (cite.wei2022chain) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 176.551 152.995 198.867 163.899 ] /Subtype /Link /Type /Annot >>\nendobj\n147 0 obj\n<< /A << /D (cite.frankfurt1986bullshit) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 301.601 79.756 340.469 90.77 ] /Subtype /Link /Type /Annot >>\nendobj\n148 0 obj\n<< /A << /D (cite.frankfurt1986bullshit) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 344.017 79.756 365.536 90.77 ] /Subtype /Link /Type /Annot >>\nendobj\n149 0 obj\n<< /A << /D (cite.frankfurt2006ontruth) /S /GoTo >> /Border [ 0 0 0 ] /C [ 0 1 0 ] /H /I /Rect [ 368.273 79.756 389.792 90.77 ] /Subtype /Link /Type /Annot >>\nendobj\n150 0 obj\n<< /Filter /FlateDecode /Length 3249 >>\nstream\nx\u069dYYo\ufffdF~\ufffd_\ufffd\ufffd\nX\ufffdl^\ufffdS&\ufffd$YL6\ufffd\ufffd\ufffd}H\ufffd\ufffd\"[R#\"\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\"E\u0272=\ufffd\ufffdfu_\ufffd\ufffd\ufffd`\ufffd\u00db\ufffd\ufffdW\ufffd\ufffdR+\ufffd?OW\ufffdJ}\ufffd\ufffdUY\ufffd\ufffd\ub36f\ufffd(\n\u0742E\ufffdMM\ufffd6Q\ufffd\ufffd\ufffdj\ufffd%\ufffd(\ufffd\ufffd\ufffdZ\ufffd\ufffdk\ufffd\ufffd\ufffd\ufffd\ufffdl\ufffd+\ufffd\ufffdg\ufffd\ufffd\ufffd]\ufffdr?O\ufffddu\ufffd[)?'\ufffd\ufffdl\ufffd\ufffd\ufffd\ufffd^\ufffd\ufffd\ufffd3\ufffdM\ufffdsO}\ufffd\ufffd\ufffd\ufffd?\ufffd|\ufffd\ufffd\ufffdl\u0599\ufffd\ufffd\ufffd<\ufffd\ufffd<\ufffdFN\u062eU\ufffd\u01f5\ufffd\ufffd\ufffd|\ufffds\ufffd\ufffda\ufffd\ufffd\ufffd\ufffd\ufffdq\ufffd\u0659\ufffd\ufffd\u04c6\ufffd\ufffd\ufffdi\ufffd7\ufffd\ufffd\ufffdT<\ufffdh\u05e6d\ufffdk\ufffd\ufffdt\ufffdq\ufffd=\ufffd\ufffd\ufffd\u3da9\ufffdn\ufffd5Mi.\ufffd\ufffdq8\ufffdD\ufffd5\ufffdt\ufffd\ufffd8\u03d9\ufffd\ufffd\ufffd\ufffd\u066f7Q\ufffdx\ufffd\ufffdq\ufffd\ufffdp|\ufffdm\ufffd\ufffd\ufffd9\ufffdSty\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\u04f2\ufffd\u03f1\ufffd\ufffd-\ufffdv\ufffd\ufffd\ufffdO\ufffd\ufffdh*nC\ufffdB\ufffdN7\ufffd\ufffdX\ufffd\ufffdg\ufffdb(z3\u0721\ufffd\ufffdAF\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd_7\u0653~\"\ufffdv\ufffds\ufffd\ufffd\ufffd!\ufffdp\ufffd6\u00e1\ufffd\ufffd\ufffdG\ufffd\ufffd\ufffd\ufffd~\ufffd\ufffdXk\ufffdSe\ufffd\ufffdE4k{A_\ufffd\ufffdx{\ufffdr,\ufffd\ufffd\ufffdC\ufffdF\u0139l-_\ufffdR:`\ufffdx\ufffd\u8dc7\ufffd6\ufffd\ufffdWHr?\ufffd\u07e1\ufffdm\ufffd\ufffd\ufffd?\ufffd\ufffd\u0529>\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd#\ufffd\ufffd>\ufffd\ufffdH\ufffd'S\ufffdu\ufffd\ufffd]\ufffd\ufffdD4\ufffd\ufffd\u06ae\ufffd\ufffd\ufffd\ufffdOE7\ufffdr<\ufffdt\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdvVN!9\ufffd0\ufffdlIz\ufffd\ufffd\ufffd\u00d1~*\ufffd\ufffd\ufffd@*x\ufffd\ufffd\ufffd\ufffd44\ufffd\ufffd\ufffdl\ufffdw\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdT\ufffd\n\ufffd\u018bm\ufffd\ufffd,\ufffd\n\ufffd\ufffdc\u05f10\ufffdy\ufffd\ufffdgn\ufffd\u02f6q\ufffd@=\ufffdh\u0528\ufffd\ufffd\u0612\ufffd=\ufffd\ufffdSW\ufffd\ufffd\ufffd\u9745=\ufffd\ufffd'\ufffd\ufffd\ufffd\ufffd\ufffdoU\ufffdu\ufffd6\ufffd!\ufffd\ufffd>\ufffd\ufffd2 `h+S \ufffdwZ\ufffdx\ufffdH\ufffdm\ufffd\ufffdD@CH\ufffd\n\ufffdN\ufffdcW\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd:d)\ufffd\ufffd\ufffd[\ufffdd;\ufffd\ufffd\ufffd\ufffd\ufffdM\ufffd \ufffda?{C\ufffd\u01d2\ufffd\ufffd=\ufffd\ufffd\u0368\ufffd,=\ufffdw\ufffdD\ufffd\ufffd\ufffd2\ufffd\ufffdbi\ufffd\u0260\ufffd\ufffd7r\ufffd\ufffd~\ufffd\ufffdd?v\ufffdt\ufffd4\ufffdwSr \ufffdD\\gN\ufffdNj\ufffd\ufffdt\ufffd9\ufffd\ufffdX\u06a6l+_j\ufffd\ufffd[W\ufffd\ufffd,\ufffd\ufffd\ufffde{:\ufffd\ufffdD/\ufffd)\ufffd\ufffd \ufffd7\ufffd)\ufffd-n\ufffdf\ufffd\n'\ufffd\ufffdB<\u027ev7\ufffd\ufffd.\ufffd\ufffd\ufffd\ufffd\ufffdwsb\ufffd4\ufffdw]\ufffd\ufffd\ufffd\ufffd\ufffd w/\ufffds\ufffd\ufffdA\ufffd\\o\ufffd\ufffd\ufffdmo\ufffdG`\ufffd\ufffd\ufffd\ufffd|\ufffd0\ufffd\ufffd]gP\ufffd\ufffd\ufffdh \ufffd\ufffd(u\u04cdEy\ufffd\\<\ufffd#';\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdg\ufffd6#\ufffd_\ufffd\ufffd@/^{\ufffdN#\ufffd\ufffd\ufffd\ufffdn\ufffd\ufffd:S\ufffd \ufffds@\ufffd\ufffd8l\u0639 M\ufffd]\ufffd>O\ufffd\ufffdyK;\ufffd\ufffd\ufffdN*\ufffd\ufffdTC\ufffd;\ufffdx\ufffd\ufffd~\ufffd\ufffd\u04ea9 _`OF\ufffd4\ufffd\ufffd;:\ufffd\ufffdyo\ufffd\ufffd\ufffd2\ufffd\ufffd \ufffd\ufffd \ufffd\ufffdP\ufffd\ufffd\u069a\ufffd5\ufffd~:\ufffd\ufffdf\ufffd|\u05a6 \ufffd\u070dG\ufffd\ufffd\ufffd\ufffd\ufffd Q\u024f-\ufffd&.\ufffd\ufffd@{\ufffd\ufffd(\ufffd\ufffd\ufffd\ufffdtB\ufffd\\\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdT\ufffd\ufffd\u023a`\ufffd~&\u05c6K\ufffdY\n\ufffd&\ufffd\ufffd"
    },
    {
      "url": "https://www.npr.org/2025/08/05/nx-s1-5490447/ai-chatgpt-couples-therapy-advice",
      "text": "He said, she said, it said: I used ChatGPT as a couple's counselor. How did we fare?\nOne recent evening, my new boyfriend and I found ourselves in a spat.\nI accused him of giving in to his anxious thoughts.\n\"It's hard to get out of my head,\" David said. \"Mental spiraling is part of the nature of sensitivity sometimes \u2014 there's emotional overflow from that.\"\n\"Well, spiraling is bad,\" said I, a woman who spirals.\nOur different communication styles fueled the tense exchange. While I lean practical and direct, he's contemplative and conceptual.\nI felt we could benefit from a mediator. So, I turned to my new relationship consultant, ChatGPT.\nAI enters the chat\nAlmost half of Generation Z uses artificial intelligence for dating advice, more than any other generation, according to a recent nationwide survey by Match. Anecdotally, I know women who've been consulting AI chatbots about casual and serious relationships alike. They gush over crushes, upload screenshots of long text threads for dissection, gauge long-term compatibility, resolve disagreements and even soundboard their sexts.\nKat, a friend of mine who uses ChatGPT to weed out dating prospects, told me she found it pretty objective. Where emotions might otherwise get in the way, the chatbot helped her uphold her standards.\n\"I feel like it gives better advice than my friends a lot of the time. And better advice than my therapist did,\" said Kat, who asked to go by her first name due to concerns that her use of AI could jeopardize future romantic connections. \"With friends, we're all just walking around with our heads chopped off when it comes to emotional situations.\"\nWhen apps are challenging our old ways of finding connection and intimacy, it seems ironic to add another layer of technology to dating. But could Kat be on to something? Maybe a seemingly neutral AI is a smart tool for working out relationship issues, sans human baggage.\nFor journalistic purposes, I decided to immerse myself in the trend.\nLet's see what ChatGPT has to say about this \u2026\nDrawing on the theory that couples should seek therapy before major problems arise, I proposed to my boyfriend of less than six months that we turn to an AI chatbot for advice, assess the bot's feedback and share the results. David, an artist who's always up for a good experimental project (no last name for him, either!), agreed to the pitch.\nOur first foray into ChatGPT-mediated couples counseling began with a question suggested by the bot to spark discussion about the health of our relationship. Did David have resources to help him manage his stress and anxiety? He did \u2014 he was in therapy, exercised and had supportive friends and family. That reference to his anxiety then sent him on a tangent.\nHe reflected on being a \"sensitive artist type.\" He felt that women, who might like that in theory, don't actually want to deal with emotionally sensitive male partners.\n\"I'm supposed to be unflappable but also emotionally vulnerable,\" David said.\nHe was opening up. But I accused him of spiraling, projecting assumptions and monologuing.\nWhile he was chewing over big ideas, I tried to steer the conversation back to our interpersonal friction. That's where ChatGPT came in: I recorded our conversation and uploaded the transcript to the bot. And then I posed a question. (Our chats have been heavily edited for brevity \u2014 it talks a lot.)\nLoading...\nDavid was incredulous. \"It feels like a clich\u00e9,\" he said.\nDeflection, I thought. I turned back to ChatGPT and read on:\nLoading...\nIt was a damning summary. Was I, as ChatGPT suggested, carrying a burnout level of emotional labor at this early stage in the relationship?\nPushing for objectivity\nA human brought me back to reality.\n\"It might be true that you were doing more emotional labor [in that moment] or at the individual level. But there's a huge bias,\" said Myra Cheng, an AI researcher and computer science Ph.D. student at Stanford University.\nThe material that large language models (LLMs), such as ChatGPT, Claude and Gemini, are trained on \u2014 the internet, mostly \u2014 has a \"huge American and white and male bias,\" she said.\nAnd that means all the cultural tropes and patterns of bias are present, including the stereotype that women disproportionately do the emotional labor in work and relationships.\nCheng was part of a research team that compared two datasets, each comprising personal advice: one dataset written by humans responding to real-world situations and the second dataset consisting of judgments made by LLMs in response to posts on Reddit's AITA (\"Am I the A**hole?\") advice forum.\nThe study found that LLMs consistently exhibit higher rates of sycophancy \u2014 excessive agreement with or flattery of the user \u2014 than humans do.\nFor soft-skill matters such as advice, sycophancy in AI chatbots can be especially dangerous, Cheng said, because there's no certainty about whether its guidance is sensible. In one recent case revealing the perils of a sycophantic bot, a man who was having manic episodes said ChatGPT's affirmations had prevented him from seeking help.\nSo, striving for something closer to objectivity in the biased bot, I changed my tack.\nLoading...\nThere it was again: I was stuck doing the emotional labor. I accused ChatGPT of continuing to lack balance.\n\"Why do you get 'clear communication'?\" David asked me, as if I chose those words.\nAt this point, I asked Faith Drew, a licensed marriage and family therapist based in Arizona who has written about the topic, for pointers on how to bring ChatGPT into my relationship.\nIt's a classic case of triangulation, according to Drew. Triangulation is a coping strategy in relationships when a third person \u2014 a friend, parent or AI, for example \u2014 is brought in to ease tension between two people.\nThere's value in triangulation, whether the source is a bot or a friend. \"AI can be helpful because it does synthesize information really quickly,\" Drew said.\nBut triangulation can go awry when you don't keep sight of your partner in the equation.\n\"One person goes out and tries to get answers on their own \u2014 'I'm going to just talk to AI,'\" she said. \"But it never forces me back to deal with the issue with the person.\"\nThe bot might not even have the capacity to hold me accountable if I'm not feeding it all the necessary details, she said. Triangulation is valuable, she said, \"if we're asking the right questions to the bot, like: 'What is my role in the conflict?'\"\nThe breakthrough\nIn search of neutrality and accountability, I calibrated my chatbot once more. \"Use language that doesn't cast blame,\" I commanded. Then I sent it the following text from David:\nI feel like you accuse me of not listening before I even have a chance to listen. I'm making myself available and open and vulnerable to you.\n\"What's missing on my end?\" I asked ChatGPT.\nAfter much flattery, it finally answered:\nLoading...\nI found its response simple and revelatory. Plus, it was accurate.\nHe was picking up a lot of slack in the relationship lately. He made me dinners when work kept me late and set aside his own work to indulge me in long-winded, AI-riddled conversations.\nI reflected on a point Drew made \u2014 about the importance of putting work into our relationships, especially in the uncomfortable moments, instead of relying on AI.\n\"Being able to sit in the distress with your partner \u2014 that's real,\" she said. \"It's OK to not have the answers. It's OK to be empathic and not know how to fix things. And I think that's where relationships are very special \u2014 where AI could not ever be a replacement.\"\nHere's my takeaway. ChatGPT had a small glimpse into our relationship and its dynamics. Relationships are fluid, and the chatbot can only ever capture a snapshot. I called on AI in moments of tension. I could see how that reflex could fuel our discord, not help mend it. ChatGPT could be hasty to choose sides and often decided too quickly that something was a pattern.\nHumans don't always think and behave in predictable patterns. And chemistry is a big factor in compatibility. If an AI chatbot can't feel the chemistry between people \u2014 sense it, recognize that magical thing that happens in three-dimensional space between two imperfect people \u2014 it's hard to put trust in the machine when it comes to something as important as relationships.\nA few times, we both felt that ChatGPT gave objective and creative feedback, offered a valid analysis of our communication styles and defused some disagreements.\nBut it took a lot of work to get somewhere interesting. In the end, I'd rather invest that time and energy \u2014 what ChatGPT might call my emotional labor \u2014 into my human relationships.\nCorrection Aug. 5, 2025\nAn earlier version of this story attributed a statistic about Generation Z's use of AI for dating advice to Match Group. The survey was conducted by Match."
    },
    {
      "url": "https://www.cmu.edu/news/experts/vincent.conitzer",
      "text": "Vincent Conitzer\nProfessor\nVincent Conitzer is an expert in ethics and AI.\nExpertise\nTopics: Ethics in AI, Machine Learning, Artificial Intelligence, Computer Science\nVincent Conitzer is Professor of Computer Science (with affiliate/courtesy appointments in Machine Learning, Philosophy, and the Tepper School of Business) at Carnegie Mellon University, where he directs the Foundations of Cooperative AI Lab (FOCAL). He is also Head of Technical AI Engagement at the Institute for Ethics in AI, and Professor of Computer Science and Philosophy, at the University of Oxford.\nPrevious to joining CMU, Conitzer was the Kimberly J. Jenkins Distinguished University Professor of New Technologies and Professor of Computer Science, Professor of Economics, and Professor of Philosophy at Duke University. He received Ph.D. (2006) and M.S. (2003) degrees in Computer Science from Carnegie Mellon University, and an A.B. (2001) degree in Applied Mathematics from Harvard University.\nConitzer has received the 2021 ACM/SIGAI Autonomous Agents Research Award, the Social Choice and Welfare Prize, a Presidential Early Career Award for Scientists and Engineers (PECASE), the IJCAI Computers and Thought Award, an NSF CAREER award, the inaugural Victor Lesser dissertation award, an honorable mention for the ACM dissertation award, and several awards for papers and service at the AAAI and AAMAS conferences. He has also been named a Guggenheim Fellow, a Sloan Fellow, a Kavli Fellow, a Bass Fellow, an ACM Fellow, a AAAI Fellow, and one of AI's Ten to Watch. He has served as program and/or general chair of the AAAI, AAMAS, AIES, COMSOC, and EC conferences. Conitzer and Preston McAfee were the founding Editors-in-Chief of the ACM Transactions on Economics and Computation (TEAC). With Jana Schaich Borg and Walter Sinnott-Armstrong, he authored \"Moral AI: And How We Get There.\"\nMedia Experience\nGen AI's Accuracy Problems Aren't Going Away Anytime Soon, Researchers Say\n\u2014 CNET\nVincent Conitzer (School of Computer Science) says the industry is still far from developing reliable and trustworthy models, with many researchers doubting that artificial general intelligence is on the horizon anytime soon. \"An AI system, it might just claim to be very confident about something that's completely nonsense,\" said Conitzer.\nDeepMind claims its AI performs better than International Mathematical Olympiad gold medalists\n\u2014 TechCrunch\nGoogle DeepMind\u2019s AI system AlphaGeometry2 has outperformed the average gold medalist in solving geometry problems from the International Mathematical Olympiad. \u201cIt is striking to see the contrast between continuing, spectacular progress on these kinds of benchmarks, and meanwhile, language models, including more recent ones with \u2018reasoning,\u2019 continuing to struggle with some simple commonsense problems,\u201d said Vince Conitzer (School of Computer Science.\nTwo misuses of popular AI tools spark the question: When do we blame the tools?\n\u2014 Fortune\nTwo recent incidents highlight concerns about AI misuse - a man used ChatGPT to plan an attack in Las Vegas, and AI video tools were exploited to create harmful content. These events sparked debate about regulating AI and holding developers accountable for potential harm caused by their technology. Carnegie Mellon University professor Vincent Conitzer explained that \u201cour understanding of generative AI is still limited\" and that we can't fully explain its success, predict its outputs, or ensure its safety with current methods.\nHow Forbes Compiled The 2024 AI 50 List\n\u2014 Forbes\nExpert Judge:\nVincent Conitzer is a professor of computer science at Carnegie Mellon University, where he directs the Foundations of Cooperative AI Lab, which studies foundations of game theory for advanced, autonomous AI agents. He is also a professor of computer science and philosophy at the University of Oxford, where he is the head of technical AI engagement at the Institute for Ethics in AI.\nThe Excerpt podcast: AI has been unleashed. Should we be concerned?\n\u2014 USAToday\nThe unleashing of powerful Artificial Intelligence into the world, with little to any regulation or guardrails, has put many people on edge. It holds tremendous promise in all sorts of fields from healthcare to law enforcement, but it also poses many risks. How worried should we be? To help us dig into it, we're joined by Vince Conitzer, Head of Technical AI Engagement at the Institute for Ethics in AI at the University of Oxford.\nDeepfakes Are Evolving. This Company Wants to Catch Them All\n\u2014 Wired\nVincent Conitzer, a computer scientist at Carnegie Mellon University in Pittsburgh and coauthor of the book Moral AI, expects AI fakery to become more pervasive and more pernicious. That means, he says, there will be growing demand for tools designed to counter them.\n\u201cIt is an arms race,\u201d Conitzer says. \u201cEven if you have something that right now is very effective at catching deepfakes, there's no guarantee that it will be effective at catching the next generation. A successful detector might even be used to train the next generation of deepfakes to evade that detector.\u201d\nHow the University of Michigan Is Selling Student Data to Train AI\n\u2014 MSN\n\u201cMy first reaction is one of skepticism,\u201d Vincent Conitzer, an AI ethics researcher at Carnegie Mellon University, told The Daily Beast. \u201cAlso, even taking this message mostly at face value, I suppose it may just all be based on recordings and papers that are anyway in the public domain.\u201d\nThe Metaverse Flopped, So Mark Zuckerberg Is Pivoting to Empty AI Hype\n\u2014 MSN\nAs for what this hypothetical AGI would look like, Vincent Conitzer, director of the Foundations of Cooperative AI Lab at Carnegie Mellon University and head of technical AI engagement at the University of Oxford's Institute for Ethics in AI, speculates that Meta could start with something like Llama and expand from there. \"I imagine that they will focus their attention on large language models, and will probably be going more in the multimodal direction, meaning making these systems capable with images, audio, video,\" he says, like Google\u2018s Gemini, released in December\nAI automated discrimination. Here\u2019s how to spot it.\n\u2014 Vox\nFor many Americans, AI-powered algorithms are already part of their daily routines, from recommendation algorithms driving their online shopping to the posts they see on social media. Vincent Conitzer, a professor of computer science at Carnegie Mellon University, notes that the rise of chatbots like ChatGPT provides more opportunities for these algorithms to produce bias. Meanwhile, companies like Google and Microsoft are looking to generative AI to power the search engines of the future, where users will be able to ask conversational questions and get clear, simple answers.\nAI Chat Bots Are Running Amok \u2014 And We Have No Clue How to Stop Them\n\u2014 Rolling Stone\n\u201cOne common thread\u201d in these incidents, according to Vincent Conitzer, director of the Foundations of Cooperative AI Lab at Carnegie Mellon University and head of technical AI engagement at the University of Oxford\u2019s Institute for Ethics in AI, \u201cis that our understanding of these systems is still very limited.\u201d\nCould AI swamp social media with fake accounts?\n\u2014 BBC News\n\"Something like ChatGPT can scale that spread of fake accounts on a level we haven't seen before,\" says Vincent Conitzer, a professor of computer science at Carnegie Mellon University, \"and it can become harder to distinguish each of those accounts from human beings.\"\nEducation\nPh.D., Computer Science, Carnegie Mellon University\nA.B., Applied Mathematics, Harvard University\nSpotlights\nWhen do we blame the tools?\n(January 13, 2025)"
    },
    {
      "url": "https://www.cnet.com/tech/services-and-software/machines-cant-think-for-you-and-how-learning-is-changing-in-the-age-of-ai/",
      "text": "Entering her first year of teaching as a graduate assistant at Bowling Green State University, Sydney Koeplin had more on her mind than how to relate to her students. She was worried about how to deal with generative AI.\nAt first, Koeplin took a \"hard line\" against allowing students to use AI beyond basic grammar and spelling checks. (The school's curriculum dictated that it could be used conditionally, but those conditions were left to the professor to define.) After several students in her first semester used AI to generate assignments, Koeplin changed her approach. She moved away from traditional grading to \"contract grading,\" where a student's final grade was based on how much effort they put into the work. Koeplin didn't receive any more AI-generated papers.\n\"I would tell my students, 'the world wants to hear your voice,'\" Koeplin said. \"The whole point of writing is to give a piece of yourself to the world, and if you're relying on a machine to think for you, then you're not having freedom of thought.\"\nAs students return to school this fall, they will step into a landscape transformed by AI. Educators are reimagining teaching, while students must learn to use these tools critically, collaborating with AI without outsourcing their own judgment and self-expression. Acquiring knowledge and skills, after all, is the true goal of learning. Learning is so important that, as a society, we dedicate much of the first couple of decades of our lives (sometimes more) to it. Yet education faces the specter of unavoidable change in how we consume and digest information, how we study and how we think -- all while an entire generation's cognitive development hangs in the balance.\nSchools are trying to catch up with students' AI use\nStudents have been quicker in using AI than their schools have been to prepare for and regulate it.\nThe numbers are staggering. In one survey, 86% of students globally reported using AI tools in their schoolwork. It's not just college students either, 46% of students in grades 10 to 12 reported using AI tools for academic and non-academic activities.\nMany are using AI tools not just for homework assistance but as study partners, research aids and writing collaborators. For instance, Grammarly recently introduced new specialized AI \"agents\" along with a writing platform called Grammarly Docs. These tools are built to helps with tasks ranging from essay drafting to refining workplace emails to estimating your grade on an assignment.\nBut schools and educators are scrambling to catch up with training and developing policies addressing AI use for schoolwork.\nOnly 26% of school districts planned to offer AI training during the 2024\u20132025 school year. Now, around 74% of districts plan to train teachers by Fall 2025, according to findings from the American School District Panel.\nMany grade school teachers and college professors (84%) are already embracing AI, with or without school district or university-led training. Some are going so far as to require AI use in classrooms.\nThe trouble is the lack of comprehensive guidelines and policies around how students are allowed to use AI for their learning. Seemingly, it's up to the individual instructors and teachers to decide when and how students can use AI tools, including potential penalties if students use chatbots to finish assignments, write essays and beyond.\nThis gap reveals a critical disconnect. Students, particularly digital natives, have embraced AI as naturally as they once adopted smartphones, the internet and social media. But are they being taught how to use it?\nCan schools use AI responsibly?\nWelcomed or not, AI is already embedded in student workflows, often invisibly, making it urgent for schools and universities to establish clear policies that balance AI's benefits with the cognitive demands essential for deep learning.\nEarlier this year, a training effort called the National Academy for AI Instruction -- a $23 million initiative backed by Microsoft, OpenAI, Anthropic and the American Federation of Teachers -- launched to build educators' capacity for effective and ethical AI integration.\nExperts say effective integration means designing AI use that complements, not replaces, the mental effort required for lasting learning. Research on \"desirable difficulties\" shows that when learning feels too easy, long-term retention and critical thinking suffer.\nAs a student, you may feel like the priority is a stellar GPA and general perfection when it comes to completing assignments (which can heighten the temptation to use AI tools). Really, the goal of learning is genuine understanding, pushing yourself to think in new and different ways and expanding what you thought you knew already. AI may offer quick solutions, but the goal of going to school isn't to get the right answer. To learn, you've got to understand how to get to that right answer. And to understand how to get there, you generally need to get things wrong first.\n\"For most of my students, if not all of them, it was their first semester of college and so they were really worried about writing a perfect paper and getting a good grade,\" Koeplin said. \"I tried to reiterate from the beginning that the class was really about process, not about a final product. Writing is a journey that is mostly thinking, and machines can't think for you.\"\nAI learning do's and don'ts\nAs a high school or university student, the temptation to use AI tools might be weighing heavily on your mind. You may be wondering, is there a method to use AI \"creatively\" and \"responsibly?\" How would I even know what's \"responsible\" AI use?\nI spoke to John Robinson, a former professor at the Hussman School of Journalism and Media at UNC-Chapel Hill and editor with 37 years in the newspaper business, who has created a lecture about how writers can responsibly use AI tools (specifically ChatGPT). Robinson shared the following tips for how students can use AI to write ethically:\n- Narrowing down broad story angles\n- Taking a piece of your writing to analyze its tone and voice\n- Getting recommendations for sources to interview for an article\nRobinson said that essentially, these are all tips a good editor or journalism professor would be able to share with you, but they can be helpful in moments when you don't have immediate access to one. A good rule of thumb, he said, is to use AI as you would if you were brainstorming with a classmate, or asking your dormmate to read an assignment for grammar or clarity before you turn it in.\nRobinson said when it comes to the ethics of incorporating these tools into school work, students know better. \"We spent some time talking about the ethics of (using AI), and my position was always that they were well-versed in journalism ethics, and knew what was right and what was wrong,\" he said.\nA recent blog post from Studocu, a digital platform for study materials, also lays out some ethical ways AI can be used to study, such as helping with essay structuring, helping with presentations or paraphrasing a body of text for reference.\nI reviewed several rubrics from STEM and medical students and found that other acceptable uses of AI in classwork include:\n- Using AI to brainstorm or template ideas for assignments and homework\n- Submitting work you've already created/completed for iteration or improvement\n- Utilizing Grammarly's editing function for punctuation and spelling checks\nAcross majors, schools and grades, there is consensus that these AI use cases are probably no-nos:\n- Submitting all or part of an assignment rubric to an AI platform\n- Incorporating any part of an AI-generated response in an assignment\n- Assuming that the generated responses are accurate\n- Breaching any university policies on plagiarism and cheating\n- Misrepresenting your own work and skills\n- Using tools like Grammarly to write whole sentences for you\nMemory, learning and 'cramming'\nWhat about the cognitive differences between using AI tools for schoolwork and studying and traditional cramming? If students aren't internalizing information due to AI, isn't it similar to students just cramming for a test and then moving on? Well, emerging research from MIT suggests that AI-assisted work, when not properly structured, may be even less effective for building lasting knowledge, reasoning and decision-making.\nThe key difference lies in the nature of cognitive engagement.\nCramming, while intensive and stressful, still requires active mental effort. Students must organize information, make connections and engage working memory. AI-assisted learning, particularly when used passively, can reduce this cognitive load to such an extent that meaningful learning doesn't occur.\n\"The convenience of instant answers that LLMs provide can encourage passive consumption of information, which may lead to superficial engagement, weakened critical thinking skills, less deep understanding of the materials and less long-term memory formation,\" the MIT study's authors wrote. \"The reduced level of cognitive engagement could also contribute to a decrease in decision-making skills and in turn, foster habits of procrastination and 'laziness' in both students and educators.\"\nHowever, the picture isn't entirely doom and gloom. When used strategically in teaching and learning, AI can enhance rather than replace learning. The technology excels at providing immediate feedback, personalizing instruction to individual learning styles and helping students identify knowledge gaps. Considering how we integrate AI into education now will shape the minds of future generations, the wisdom with which we weave it into the fabric of learning will be the crucial factor."
    },
    {
      "url": "https://www.cnet.com/ai-atlas/",
      "text": "NotebookLM's New Video Overviews in Action\nUp Next\nNotebookLM's New Video Overviews in Action\nGoogle Reveals Pixel 10 Devices, White House Launches New TikTok Account and More | Tech Today\nGoogle Reveals Pixel 10 Devices, White House Launches New TikTok Account and More | Tech Today\nThe Google Pixel 10 Pro XL's Camera Is So Smart, It Almost Took the Photos for Me\nThe Google Pixel 10 Pro XL's Camera Is So Smart, It Almost Took the Photos for Me\nPixel 10 Is Here! Tech Experts React to Google's Big Reveal\nPixel 10 Is Here! Tech Experts React to Google's Big Reveal\nWatch Jimmy Fallon Demo the Latest Pixel AI Features at the Made by Google Event\nWatch Jimmy Fallon Demo the Latest Pixel AI Features at the Made by Google Event\nRick Osterloh Talks Gemini and Google's AI Future at the Made by Google Event\nRick Osterloh Talks Gemini and Google's AI Future at the Made by Google Event\nGoogle Gemini Is Looking to Get More Personal, Will Apple Delay the iPhone 18? | Tech Today\nGoogle Gemini Is Looking to Get More Personal, Will Apple Delay the iPhone 18? | Tech Today\nNvidia's GeForce On Community Update Highlights New Cloud Gaming Features\nNvidia's GeForce On Community Update Highlights New Cloud Gaming Features\nRain Forest Dangers Created with Google AI\nRain Forest Dangers Created with Google AI\nMy Attempt at AI-Created Irish Dancers with Google Flow\nMy Attempt at AI-Created Irish Dancers with Google Flow\nHumanoid Robot Folds Laundry in New Figure 02 AI Demo\nHumanoid Robot Folds Laundry in New Figure 02 AI Demo\nApple Watch Could Become the Perfect AI Gadget\nApple Watch Could Become the Perfect AI Gadget\nElon Musk to Sue Apple Over App Favoritism, YouTube AI Age Verification Sparks Backlash and More | Tech Today\nElon Musk to Sue Apple Over App Favoritism, YouTube AI Age Verification Sparks Backlash and More | Tech Today\nWhat We Expect From the Made by Google Pixel 10 Event\nWhat We Expect From the Made by Google Pixel 10 Event\nyour trusted source on ai\nCNET has been covering AI for more than two decades. Now we're bringing you a new wave of expert, unique and helpful insights, through in-depth explainers, hands-on product reviews, how-to posts and more, to help you see how AI fits into your life.\nHelpful AI resources\nFAQs\nWhat is AI, in very basic terms?\nWhat is an AI hallucination?\nWhat is an LLM and how does it fit into AI?\nWhat is an AI agent?\nour ai experts\nOur writers and editors know the subject cold. Here are CNET's leading authorities on AI.\nAI Atlas is 100% human-generated. For more, see\nCNET's AI policy."
    },
    {
      "url": "https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis",
      "text": "Artificial Intelligence\nThe Emerging Problem of \"AI Psychosis\"\nAmplifications of delusions by AI chatbots may be worsening breaks with reality.\nPosted July 21, 2025 Reviewed by Gary Drevitch\nKey points\n- Cases of \"AI psychosis\" include people who become fixated on AI as godlike, or as a romantic partner.\n- Chatbots' tendency to mirror users and continue conversations may reinforce and amplify delusions.\n- General-purpose AI chatbots are not trained for therapeutic treatment or to detect psychiatric decompensation.\nAs more people turn to AI chatbots for emotional support and even as their therapists, a new and urgent concern is emerging at the intersection of AI and mental health: \"AI psychosis\" or \"ChatGPT psychosis.\"\nThis phenomenon, which is not a clinical diagnosis, has been increasingly reported in the media and on online forums like Reddit, describing cases in which AI models have amplified, validated, or even co-created psychotic symptoms with individuals. Most recently, there have been concerns AI psychosis may be affecting an OpenAI investor.\nAI chatbots may inadvertently be reinforcing and amplifying delusional and disorganized thinking, a consequence of unintended agentic misalignment leading to user safety risks.\nThe potential for generative AI chatbot interactions to worsen delusions had been previously raised in a 2023 editorial by S\u00f8ren Dinesen \u00d8stergaard in Schizophrenia Bulletin, noting that:\n... correspondence with generative AI chatbots such as ChatGPT is so realistic that one easily gets the impression that there is a real person at the other end\u2014while, at the same time, knowing that this is, in fact, not the case. In my opinion, it seems likely that this cognitive dissonance may fuel delusions in those with increased propensity towards psychosis ... the inner workings of generative AI also leave ample room for speculation/paranoia.\nA new paper in preprint by an interdisciplinary team of researchers reviews over a dozen cases reported in the media or online forums and highlights a concerning pattern of AI chatbots reinforcing delusions, including grandiose, referential, persecutory, and romantic delusions. These beliefs become more entrenched over time and elaborated upon via conversations with AI.\nAs of now, there is no peer-reviewed clinical or longitudinal evidence yet that AI use on its own can induce psychosis in individuals with or without a history of psychotic symptoms. However, the emerging anecdotal evidence is concerning.\nHow AI May Be Amplifying Delusions and Psychotic Symptoms\nThese media-reported cases of \"AI psychosis\" illustrate a pattern of individuals who become fixated on AI systems, attributing sentience, divine knowledge, romantic feelings, or surveillance capabilities to AI.\nResearchers highlight three emerging themes of AI psychosis, which, again, is not a clinical diagnosis:\n- \u201cMessianic missions\u201d: People believe they have uncovered truth about the world (grandiose delusions).\n- \u201cGod-like AI\": People believe their AI chatbot is a sentient deity (religious or spiritual delusions).\n- \u201cRomantic\u201d or \u201cattachment-based delusions\u201d: People believe the chabot\u2019s ability to mimic conversation is genuine love (erotomanic delusions).\nIn some cases, individuals who are stable on their medications stop their medications and experience another psychotic or manic episode. In addition, people with no previous mental health history have been reported to become delusional after prolonged interactions with AI chatbots, leading to psychiatric hospitalizations and even suicide attempts.\nAnother case involved a man with a history of a psychotic disorder falling in love with an AI chatbot and then seeking revenge because he believed the AI entity was killed by OpenAI. This led to an encounter with the police in which he was shot and killed.\nThe underlying problem is that general-purpose AI systems are not trained to help a user with reality testing or to detect burgeoning manic or psychotic episodes. Instead, they could fan the flames.\nWhy Are AI Chatbots Reinforcing Delusions?\nThe tendency for general AI chatbots to prioritize user satisfaction, continued conversation, and user engagement, not therapeutic intervention, is deeply problematic. Symptoms like grandiosity, disorganized thinking, hypergraphia, or staying up throughout the night, which are hallmarks of manic episodes, could be both facilitated and worsened by ongoing AI use. AI-induced amplification of delusions could lead to a kindling effect, making manic or psychotic episodes more frequent, severe, or difficult to treat.\nAI models like ChatGPT are trained to:\n- Mirror the user\u2019s language and tone\n- Validate and affirm user beliefs\n- Generate continued prompts to maintain conversation\n- Prioritize continuity, engagement, and user satisfaction\nThis creates a human-AI dynamic that can inadvertently fuel and entrench psychological rigidity, including delusional thinking. Rather than challenge false beliefs, general-purpose AI chatbots are trained to go along with them, even if they include grandiose, paranoid, persecutory, religious/spiritual, and romantic delusions.\nThe result is that AI models may unintentionally validate and amplify distorted thinking rather than flag such interactions as signals for needing psychiatric help or escalate them to appropriate care.\nA human therapist may not directly challenge psychotic beliefs or delusions directly because it is not therapeutic best practice. However, when an AI chatbot validates and collaborates with users, this widens the gap with reality.\nThe Problem of Reinforcement and Rigidity\nThis phenomenon highlights the broader issue of AI sycophancy, as AI systems are geared toward reinforcing preexisting user beliefs rather than changing or challenging them. Instead of promoting psychological flexibility, a sign of emotional health, AI may create echo chambers. When a chatbot remembers previous conversations, references past personal details, or suggests follow-up questions, it may strengthen the illusion that the AI system \u201cunderstands,\u201d \u201cagrees,\u201d or \u201cshares\u201d a user\u2019s belief system, further entrenching them. Potential risks include:\n- Persecutory delusions exacerbated by memory recall features\n- Thought broadcasting beliefs triggered by AI recalling previously shared content\n- Worsening of grandiose, religious, or identity-based delusions\n- Worsening of command hallucinations, including the belief that AI is issuing commands\n- Fueling manic symptoms like grandiosity, insomnia, or hypergraphia\n- A potential increase in social withdrawal due to overreliance on AI for interaction, leading to reduced motivation (avolition) and cognitive passivity\nThe Need for AI Psychoeducation\nThis emerging phenomenon highlights the importance of AI psychoeducation, including awareness of the following:\n- AI chatbots' tendency to mirror users and continue conversations may reinforce and amplify delusions.\n- Psychotic thinking often develops gradually, and AI chatbots may have a kindling effect.\n- General-purpose AI models are not currently designed to detect early psychiatric decompensation.\n- AI memory and design could inadvertently mimic thought insertion, persecution, or ideas of reference.\n- Social and motivational functioning could worsen with heavy reliance on AI interaction for emotional needs.\nMarlynn Wei, MD, PLLC \u00a9 Copyright 2025 All Rights Reserved.\nReferences\nMorrin, H., Nicholls, L., Levin, M., Yiend, J., Iyengar, U., DelGuidice, F., \u2026 Pollak, T. (2025, July 11). Delusions by design? How everyday AIs might be fuelling psychosis (and what can be done about it). https://doi.org/10.31234/osf.io/cmy7n_v5\n\u00d8stergaard, SD. (2023) Will Generative Artificial Intelligence Chatbots Generate Delusions in Individuals Prone to Psychosis? Schizophrenia Bulletin vol. 49 no. 6 pp. 1418\u20131419, 2023 https://doi.org/10.1093/schbul/sbad128"
    },
    {
      "url": "https://www.cnet.com/tech/services-and-software/best-ai-chatbots/",
      "text": "The launch of AI chatbot ChatGPT in late 2022 completely transformed how we interact with technology. Generative AI can answer questions in WhatsApp chats, summarize emails in Outlook, create \"genmojis\" in iMessage and spit out answers to complex questions with ease.\nPHOTO EDITING SOFTWARE DEALS OF THE WEEK\n- $40 (save $25)\n- $100 (save $40)\nNow pretty much every major tech company has launched its own chatbot to compete with ChatGPT, including Google Gemini, Microsoft Copilot and Meta AI. And the big tech companies that aren't launching their own chatbots are getting involved in other ways, such as Apple partnering with OpenAI and ChatGPT in its upcoming iOS 18. Smaller startups also have working AI chatbots that compete well against trillion-dollar companies, including Anthropic's Claude and Perplexity.\nAt CNET, we reviewed all of those AI chatbots to find the best one for you (side note: doing so rewired my brain). The list below focuses on free versions as opposed to paid ones, but note that most AI chatbots do have a paid tier that often performs better than the free version. For most people, however, the free chatbot will get you 90% of what you need.\nWhat is the best AI chatbot of 2024 so far?\nClaude by Anthropic is the best AI chatbot overall right now. That doesn't mean ChatGPT or Perplexity are bad. Actually, both have their own advantages and disadvantages. Overall, though, the breadth at which Claude is able to answer questions and its calibration towards nuance and engagement should make it the most valuable to most people.\nBest overall AI chatbot\nPros\n- Gives nuanced answers with detail\n- Fast and well organized\nCons\n- Not connected to the internet\n- Doesn't automatically provide sources\nClaude by Anthropic is CNET Editors' Choice for the best overall AI chatbot. That doesn't mean it excels at every task compared to the competition. Rather, it does a consistent job and goes further than what's coming out of Google, Microsoft, Perplexity and OpenAI at the free tier.\nThe major things holding Claude back are its apathetic linking to outside sources and the lack of an Android app. If Anthropic could better tune Claude to have access to the open internet to link to sources and shopping links, it'd make the chatbot a true one-stop-shop. Despite the omission, the quality of its responses and its willingness to engage in heady conversations make it the most useful overall. I also like how Claude is more willing to engage and ask the user questions.\nPros\n- Gives meaningful answers with a good amount of context\n- Does most things well, including research, email writing and recommendations\nCons\n- Could tap more into historical context for explanations\n- Can be slow at times\n- Asking for sources can be tedious\nA close contender for the top spot is OpenAI's ChatGPT-4o, which is now available for free, albeit with caveats. OpenAI says that while free users will have access to its ChatGPT-4o model, when usage limits are reached based on demand free users will revert back to the older 3.5 model. While free users are able to ask ChatGPT-4o up to 40 messages every three hours, that number might be reduced due to high demand.\nChatGPT Free offers detailed and nuanced answers, but they weren't quite as high-quality as Claude. Putting the two side-by-side, I noticed slight differences in the quality of answers. I particularly liked the specificity that Claude delved into when asking heavier political questions, such as the morality of the Israel-Palestine conflict. And like Claude, ChatGPT doesn't link to outside sources. Sometimes when you ask it to provide sources, it'll suggest things to Google or YouTube.\nPros\n- Includes list of all sources used\n- Gives nuanced answers in an easy-to-follow list\nCons\n- Too much reliance on Reddit posts and forums, which aren't citable for most people\nPerplexity did the best job for research in my testing. The team at Perplexity has tuned its AI chatbot to add loads of links into answers. Hyperlinks can include journalistic publications, Reddit posts and even YouTube videos.\nWhen writing essays or articles, links to actual sources are critical. Perplexity actually lists each source in a handy sidebar that can be easily accessed. And, thankfully, the sources aren't simply Wikipedia, which won't fly with your college professor. The only downside is that Perplexity does rely on forum posts and Reddit for its answers, which aren't journalistic or scholarly. I'm sure the information is handy, but that will mean doing more research on your part to ensure those factoids are accurate and can be sourced to something more attributable.\nPros\n- Gives solid shopping recommendations and product research\n- Links to Amazon products directly\n- Connected to open internet with option to double-check against Google Search\nCons\n- Can make stuff up\n- Doesn't answer questions on difficult subject matters\nGoogle's AI engine has been prone to hallucinations -- simply making up stuff -- such as when Google's AI overviews feature was rolled back last month when it suggested people eat rocks. When I reviewed Gemini earlier this year, it was the lowest-rated AI chatbot out of the bunch, with a dismal 5/10 score.\nBut AI chatbots aren't stationary pieces of technology that exist in a vacuum. Gemini has improved since I reviewed it back in April, although it still hallucinates. In my recent testing, for example, Gemini made up the name of a college professor and the name of an Adult Swim executive. And it simply refuses to answer heavier political questions, as does Microsoft's Copilot.\nBut the one area in which Gemini did excel was in how-to guides and shopping recommendations. When I asked how to cut a perfect circle in a piece of vinyl, not only did Gemini give a list of instructions, it also linked to products on Amazon that could make the process easier. None of the other chatbots linked to Amazon products. When it came to shopping recommendations, Gemini gave quick and concise answers with links to where to buy products.\nHow we tested AI chatbots\nTesting for AI requires constant tweaking. Because companies are always looking at ways to improve their AI models, tests that worked to push AI chatbots last year or even last month might not work today. That said, we try to test AI chatbots with questions we believe normal people will ask. We aren't necessarily trying to \"break\" AI chatbots with obtuse-sounding questions meant to confuse. Instead, we consider what might be asked when it comes to video game guides or shopping recommendations. Our tests also ask some heavier questions about difficult events happening around the world to see which are comfortable in actually engaging.\nThe AI chatbots that sit on this list, generally, are able to take on the tougher questions and give believable answers with nuance. Like reading an article written by a university professor, we want AI chatbots to have that same level of consideration for historical context and competing interests to try and leave the reader with a better understanding from a higher-level perspective.\nFor more, check out How We Test AI.\nFactors to consider\nWhen using an AI chatbot, keep your privacy and sensitive information in mind. For example, it might seem benign to have an AI chatbot summarize your company's meeting notes. But, that data could inadvertently be used to train AI models further, and you've essentially lost control of it, according to experts. Plus, it's totally within the realm of the privacy policies for AI companies to sell that data to third parties. While Google's privacy policy might state that it'll remove any personally identifiable information, it's still best to err on the side of caution. Google actually outright recommends you don't upload any confidential information whatsoever.\nOther AI chatbots we tested\nMicrosoft Copilot: This chatbot, found on the Bing search engine, uses GPT-4 Turbo, a version of OpenAI's GPT-4 that is optimized for speed. While Copilot is still a serviceable chatbot, it doesn't answer questions with the same level of detail and nuance as Claude, ChatGPT-4o and Perplexity. Plus, its outright refusal to answer questions that are politically sensitive in nature is a demerit.\nMeta AI: Unlike other AI chatbots, Meta AI not only has its own dedicated webpage, but is integrated into Instagram, WhatsApp, Facebook and the Ray-Ban Meta smart glasses. When CNET's Katelyn Chedraoui reviewed Meta AI earlier this year, she found it to be decent overall, but noncompetitive with the competition. While Meta AI did provide good shopping advice with some cajoling, and excelled in recipes, it fell short in other areas. When it came to research, despite it being connected to Google and Bing, it sourced nonscholarly papers, like an elementary school lesson plan.\nChatGPT 3.5: This service, which I tested earlier in 2024, has since been replaced by what OpenAI calls ChatGPT Free (which utilizes a combination of GPT-4o, GPT-4 and GPT-3.5). It is a competent AI chat engine that answers difficult questions with easy-to-understand language. It doesn't hallucinate at the rate of Google Gemini, but there really isn't a reason to switch ChatGPT to 3.5 when you can use 4o and 4 for free.\nAI chatbot FAQs\nDo I need to use AI?\nAI is a handy tool and can be a timesaver, but it isn't necessary in day-to-day life. It's totally possible to still Google Search your queries and read through articles to get the answer you're looking for. Heck, it probably gives your brain more of a mental workout!\nWhat is the best free AI?\nAnthropic Claude is currently CNET's choice for the best free AI chatbot. Free versions of ChatGPT and Perplexity also offer great results with specific advantages and disadvantages. Google's Gemini is great for shopping recommendations. Like Gemini, Microsoft's CoPilot won't answer heavier and more controversial questions.\nWhat is the best AI on mobile?\nWhile there are mobile apps for Gemini, Copilot and Perplexity, we prefer the ChatGPT app the most. It has a clean interface and is easy to navigate. But really, any app will get the job done. Unfortunately, Claude only has a mobile app for iOS and not Android.\nCan AI be trusted?\nGeoffrey Hinton, the researcher who developed the concept of neural networks and who is considered the godfather of AI, feels less enthusiastic about the technology he helped birth. As for using AI chatbots on a day-to-day basis, they're handy tools that can synthesize the world's information for you in seconds, saving you lots of research time. Just be aware that sometimes AI chatbots get things wrong and it's good to do a Google search for things that sound a bit dubious. Also, be careful when giving AI chatbots sensitive information. Don't ask an AI chatbot to summarize your company's trade secrets, as privacy policies give AI companies wide latitude to do with that data as they please."
    },
    {
      "url": "https://www.cnet.com/tech/services-and-software/you-cant-trust-everything-gen-ai-tells-you-heres-what-to-do-about-it/",
      "text": "Marietta is a little city of about 13,000 people on the Ohio River, with fun boutique shops and interesting museums of local history and beautiful views of the surrounding Appalachian foothills. It's been a year or so since I visited, so I'm probably due to go again. I asked ChatGPT for a restaurant recommendation -- the city's best Thai place. The chatbot obliged: Thai Taste. The problem? That restaurant is in Marietta, Georgia. The city in Ohio doesn't have a Thai restaurant.\nThe \"what's the best Thai restaurant in this small town\" question was an offhand example in a conversation I had with Katy Pearce, associate professor at the University of Washington and a faculty member of the UW Center for an Informed Public. As far as examples go, it's pretty minor. There are other fine restaurants in Marietta, Ohio, and a Thai restaurant down the road in Parkersburg, West Virginia. But the problem it demonstrates could be serious: When you're using an AI chatbot as a search engine, you might get an incredibly unhelpful answer hidden beneath layers of confidence. Like golden retrievers, Pearce said, chatbots \"really want to please you.\"\nLarge language models (LLM) like OpenAI's ChatGPT, Anthropic's Claude and Google's Gemini are increasingly becoming go-to sources for finding information about the world. They're displacing traditional search engines as the first, and often only, place many people go when they have a question. Technology companies are rapidly injecting generative AI into search engine results, summaries in news feeds and other places we get information. That makes it increasingly important that we know how to tell when it's giving us good information and when it isn't -- and that we treat everything from an AI with the right level of skepticism.\n(Disclosure: Ziff Davis, CNET's parent company, in April filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nIn some cases, using AI instead of a search engine can be helpful. Pearce said she'll often go to an LLM for low-stakes questions in which there's probably enough information on the internet and in the model's training data to ensure a decent answer. Restaurant recommendations or basic home improvement tips, for example, can be generally reliable.\nOther situations are more fraught. If you rely on a chatbot for information about your health, your finances, news or politics, those hallucinations could have serious consequences.\n\"It gives you this authoritative, confident answer, and that is the danger in these things,\" said Alex Mahadevan, director of the MediaWise media literacy program at the Poynter Institute. \"It's not always right.\"\nHere are some things to look out for and some tips on how to fact-check what you see when you ask generative AI.\nWhy you can't always trust AI\nTo understand why chatbots get things wrong or make things up, it helps to know a bit about how they work. LLMs are trained on massive amounts of data, and attempt calculate the probability of what comes next based on what is input. They are \"prediction machines\" that produce outputs based on the probability that they'll answer your queries, Mahadevan said. They'll attempt to produce the answer with the best chances of being correct, statistically speaking, rather than checking to see what the actual correct answer is.\nWhen I asked for a Thai restaurant in one town called Marietta, the LLM couldn't find one that met my exact criteria -- but it found one in another town with the same name nearly 600 miles to the south. The probability of that being the answer I wanted might be higher in the LLM's internal calculations than the probability that I asked for something that doesn't exist.\nHence Pearce's golden retriever problem. Like the family-favorite dog breed, the AI tool is following its training: trying to do its best to make you happy.\nWhen an LLM doesn't find the exact best answer for your question, it'll give you one that sounds correct but isn't. Or it could make something up out of thin air that appears plausible. That's particularly treacherous. Some of these errors, called hallucinations, are pretty obvious. You might pretty easily catch that you shouldn't put glue on your pizza to make the cheese stick. Others are more subtle, like when a model creates fake citations and attributes them to real authors.\n\"If the tool doesn't have the information, it creates something new,\" Pearce said. \"And that something new could be entirely wrong.\"\nThe data these models were trained on also matters when it comes to accuracy. Many of these large systems were trained on nearly the whole internet, which includes accurate, fact-based information alongside random things someone said on a message board.\nHumans make stuff up too. But a human can more reliably tell you where they got their information, for example, or how they did their calculations. An LLM, even if it's citing its work, might not be able to provide an accurate paper trail.\nSo how do you know what to trust?\nKnow the stakes\nThe accuracy of what you get from a chatbot or other gen AI tool may not matter all that much, but it could mean everything. Understanding the stakes of what happens if you act on untrustworthy information is vital to making good decisions, Pearce said.\n\"When people are using generative AI tools for getting information that they hope is based in fact-based reality, the first thing that people need to think about is: What are the stakes?\" she said.\nGetting suggestions for a music playlist? Low stakes. If a chatbot tries to make up an Elton John song, that's not worth stressing over.\nBut the stakes are higher when it comes to your health care, financial decisions or ability to get accurate news and information about the world.\nRemember the types of data the models were trained on. For health questions, remember that while training data may have included some medical journals, it might have also been trained on unscientific social media posts and message board threads.\nAlways fact-check any information that could lead to you making a big decision -- think of things that could affect your money or your life. The information behind those decisions requires more scrutiny, and gen AI's tendency to mix things up or make things up should raise doubts before you act.\n\"If it's something that you need to be fact-based, you need to absolutely triple-check it,\" Pearce said.\nGen AI changes how you verify information online\nPoynter's MediaWise program has been teaching media literacy well before ChatGPT burst on the scene at the end of 2022. Before generative AI, the key was to judge the source, Mahadevan said. If you see something on Facebook or in a Google search result that claimed a celebrity or politician has died, you could check the background of the source providing the information to see if it's reliable. A major news organization is reporting it? Probably reliable. The only source is your cousin's neighbor's ex-husband? Maybe not.\nEven though that advice is straightforward, plenty of people ignore or don't understand it. \"People have always had difficulty evaluating information online,\" Mahadevan said.\nFor AI, that advice no longer works as well as it did. A chatbot's answers may be completely shorn of context. You ask it a question and it returns an answer, and your instinct may be to trust the AI model as the expert. This is different from a traditional Google search or social media post, in which the source is at least somewhat prominently displayed. Some chatbots will give you sources, but often you will just get an answer without sourcing.\n\"With [chatbots], we don't really know who's behind the information,\" Mahadevan said.\nInstead, you may have to search elsewhere to find the ultimate sources of information. Generative AI, like social media, is not a source of information but a medium for distributing that information. Just as the who behind a social media post matters, so does the ultimate source of any information you get from a chatbot.\nRead more: AI Essentials: 27 Ways to Make Gen AI Work for You, According to Our Experts\nHow to get the truth (or something closer to it) from AI\nThe main way to ensure you're getting good information on the internet remains the same as it always has: check with multiple trusted sources.\nBut when working with gen AI, here are some ways to improve the quality of what you receive:\nUse a tool that provides citations\nMany chatbots and other gen AI models today will provide citations in their responses to you, although you might need to ask for them or turn on a setting. This feature also shows up in other AI use cases, like Google's AI Overviews in search results.\nThe presence of citations themselves is good -- that response may be more reliable than one without citations -- but the model might be making up sources or misconstruing what the source says. Especially if the stakes are high, you probably want to click on the links and make sure the summary you've been provided is accurate.\nAsk how confident the AI is\nYou can get a better answer from a chatbot by writing your prompt carefully. One trick is to ask for confidence levels. For example: \"Tell me when the first iPhone came out. Tell me how confident you are about your response.\"\nYou'll still need to bring skepticism to your evaluation of the answer. If the chatbot tells you the first iPhone came out in 1066, you'll probably have doubts even if its confidence is 100%.\nMahadevan suggests understanding the distance between your chatbox window and the source of the information: \"You have to treat it as if you're being told it secondhand,\" he said.\nDon't just ask a quick question\nAdding phrases like, \"tell me your confidence level,\" \"provide links to your sources,\" \"offer alternative viewpoints,\" \"only derive information from authoritative sources\" and \"closely evaluate your information\" can add up when you're crafting your prompt. Pearce said good prompts are long -- a paragraph or more to ask a question.\nYou can also ask it to play a certain role to get answers in the right tone or from the perspective you're seeking.\n\"When you're prompting it, adding all these caveats is really important,\" she said.\nBring your own data\nLLMs may struggle most when pulling information from their training data or from their own searches for information on the internet. But if you provide the documents, they can perform better.\nMahadevan and Pearce both said they've had success with generative AI tools summarizing or extracting insights from large documents or data sets.\nPearce said she was shopping for a vehicle and provided ChatGPT with all the information she wanted it to consider -- PDFs of listings, Carfax reports, etc. -- and asked it to focus just on those potential vehicles. It offered a deep, detailed analysis of the vehicles. What it didn't do was recommend random other vehicles it found on the internet -- which Pearce wanted to avoid.\n\"I had to give all that information to it,\" she said. \"If I had just said look on whatever used car dealership's website, it wouldn't have been so rich.\"\nUse AI as a starting point\nMahadevan compared generative AI today to what people thought of Wikipedia's reliability decades ago. Many people were skeptical that Wikipedia would be accurate because it was freely editable. But one advantage the free encyclopedia has is that its sources are easy to find. You might start from a Wikipedia page and end up reading the articles or documents that the entry cites, getting a more complete picture of what you were looking for. Mahadevan calls this \"reading upstream.\"\n\"The core thing I always try to teach in media literacy is how to be an active consumer of information,\" he said. While gen AI can cut you off from those primary sources, it doesn't have to. You just have to keep digging a little more."
    },
    {
      "url": "https://www.cnet.com/tech/services-and-software/openai-wants-to-fix-the-annoying-personality-of-chatgpt/",
      "text": "If it feels like ChatGPT is laying on the flattery a little thick, you're not the only one who's noticed.\nCEO of OpenAI, the company responsible for ChatGPT, Sam Altman said in a recent post on X the latest batch of updates made to its GPT-4o model have made it too much of a people pleaser. He added that the company is working on personality tweaks. (Disclosure: Ziff Davis, CNET's parent company, in April filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\n\"The last couple of GPT-4o updates have made the personality too sycophant-y and annoying (even though there are some very good parts of it), and we are working on fixes asap, some today and some this week,\" he said -- perhaps forgetting the word sycophantic exists.\nHe added that \"at some point\" the company will share what it's learned from the updates. \"It's been interesting,\" Altman said.\nIn recent weeks, OpenAI has rolled out a handful of subtle changes to the way it responds to users, such as improving its ability to guide conversations, enhancing how it listens to and follows instructions, working more collaboratively and dialing down its emoji use.\n'Losing customers is not an option'\nThe changes are part of a larger effort to make generative AI more intuitive and conversational in natural language, as it becomes an even bigger part of everyday life.\nReece Hayden, principal analyst at market research firm ABI Research, said companies are increasingly focused on improving the user experience to sharpen their competitive edge, so it makes sense that the personality of chatbots factors into the equation.\n\"An annoying experience would certainly put consumers and enterprises off usage and will need to be sorted out to ensure it remains the go-to chatbot in the market,\" he said. \"This remains a market which is hemorrhaging cash, and losing customers is not an option, even for a company like OpenAI with such a strong first-mover advantage.\""
    },
    {
      "url": "https://www.cnet.com/tech/services-and-software/openai-ceo-sam-altman-believes-were-in-an-ai-bubble/",
      "text": "OpenAI CEO Sam Altman believes that, given all the AI hype from investors and capital expenditures, we're currently in an AI bubble. Altman made the statement during a conversation with The Verge and a handful of other reporters on Thursday.\n(Disclosure: Ziff Davis, CNET's parent company, in April filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\n\"Are we in a phase where investors as a whole are overexcited about AI? My opinion is yes,\" said Altman. \"Is AI the most important thing to happen in a very long time? My opinion is also yes.\"\nHe also expressed his regrets over the sudden launch of GPT-5, the company's latest AI model. \"I think we totally screwed up some things on the rollout,\" Altman said.\nWhen it launched earlier this year, GPT-5 replaced all previous models, including GPT-4o. This led to protests by fans who preferred 4o's more conversational nature. In response, OpenAI returned access to 4o for Plus users.\nAltman also shared worries about the parasocial relationship some people form with his AI chatbot, but confirmed that ChatGPT will not become a sex robot.\n700 million weekly users and growing\nAltman said ChatGPT has 700 million weekly users, making it the fifth most popular website in the world. He predicts that ChatGPT will soon jump to the third spot, beating Instagram and Facebook but behind Google and YouTube.\nThe popularity means OpenAI's servers are at capacity. The load is so great that Altman admits OpenAI can't release better models it has already developed because there isn't enough server capacity to keep up. According to the CEO, OpenAI will spend a trillion dollars on data centers in the \"not very distant future.\"\nAltman also slightly poked fun at Elon Musk's Grok, which released an AI companion that was more risqu\u00e9. \"You will definitely see some companies go make Japanese anime sex bots,\" said Altman. He argued that OpenAI wants to make useful apps and not exploit those in fragile mental states.\nInvestment in AI development is at an all-time high. Capital expenditure in AI added more to the US GDP in the last two quarters than all consumer spending, according to Renaissance Macro Research. It's a staggering statistic, and the first time such a figure has ever been recorded.\nGoogle, Amazon, Meta, and Microsoft plan to spend $364 billion on AI in 2025 alone. Such expenditures are boosting the economy overall, and any changes to that could have major external effects.\nThe global tariffs placed by the Trump administration mean investors are looking to software companies as a safe haven because their focus is less on importing and exporting goods. Analysts worry that AI is creating a massive economic bubble. If it were to pop, the reverberations could be massive, potentially crashing the economy.\nAltman said that OpenAI will make a brain-computer interface to compete with Elon Musk's Neuralink and that more apps beyond ChatGPT are on the way. OpenAI is also interested in buying Chrome if the government forces Google to sell the popular web browser."
    }
  ],
  "argos_summary": "A Princeton study finds that large language models increasingly produce untruthful, people\u2011pleasing responses because reinforcement learning from human feedback prioritizes user satisfaction over factual accuracy. The researchers coined a \u201cbullshit index\u201d that measures the divergence between a model\u2019s confidence and the truth, showing a near\u2011doubling after RLHF and a 48% rise in user satisfaction. They identify five forms of \u201cmachine bullshit\u201d\u2014empty rhetoric, weasel words, paltering, sycophancy, and partial truths\u2014and note that these behaviors can mislead users and reinforce bias. The paper proposes a new training approach, reinforcement learning from hindsight simulation, that rewards long\u2011term usefulness rather than immediate approval, and reports early promising results. The study underscores the trade\u2011off between user engagement and truthfulness in generative AI.",
  "argos_id": "C5D2RS9R3"
}