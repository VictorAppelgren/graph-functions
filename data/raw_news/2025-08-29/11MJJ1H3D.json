{
  "url": "https://www.businessinsider.com/anthropic-uses-chats-train-claude-opt-out-data-privacy-2025-8",
  "authorsByline": "Lakshmi Varanasi",
  "articleId": "2941b0e310c74c3facd274ed6abaf5a7",
  "source": {
    "domain": "businessinsider.com",
    "paywall": true,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://i.insider.com/68b1b563cfc04e97619c4596?width=594&format=jpeg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-29T15:19:40+00:00",
  "addDate": "2025-08-29T16:28:13.476415+00:00",
  "refreshDate": "2025-08-29T16:28:13.476418+00:00",
  "score": 1.0,
  "title": "Anthropic will start training its AI on your chats unless you opt out. Here's how.",
  "description": "Here's how to opt out of Anthropic's new consumer data collection and retention policy.",
  "content": "Anthropic's Claude will soon start learning from you.Anthropic announced in a blog post on Thursday that it will make user chats and coding sessions available to train its models. The change will go into effect right away if you opt in. After September 28, the changes will apply automatically unless you opt out.Anthropic will use data from interactions with its consumer products, like its chatbot Claude, in the free, pro, and max tiers. The new policy does not apply to Anthropic's commercial products, including Claude Gov, Claude for Education, or API use.Users can opt out by unchecking the box on the pop-up window titled Updates to Consumer Terms and Policies. Note the fine print: These changes take effect immediately upon confirmation. Anthropic also says it will retain user data in its secure backend for up to five years. Previously, it retained user data for only 30 days.When asked for comment, an Anthropic spokesperson directed Business Insider to a section of the company's blog post addressing data retention.\"The extended retention period also helps us improve our classifiers\u2014 systems that help us identify misuse \u2014 to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone,\" the post says.Claude users can also adjust privacy settings at any time in the \"Help improve Claude\" bar.In an email to Business Insider, an Anthropic spokesperson said the policy changes will help improve its data training process.\"Training on real-world conversations and coding data will help us make Claude better. When a developer debugs code with Claude or someone gets help writing an email, those interactions provide the model with valuable signals on what works and what doesn't,\" the spokesperson said. \"This creates a feedback loop that helps future models improve on similar tasks. The five-year retention also helps our safety classifiers learn to detect harmful usage patterns over time.\"The changes came a day after Anthropic published a report that said its chatbot Claude had been weaponized by cybercriminals. In one instance, Anthropic noted that a threat actor used Claude Code to an \"unprecedented degree\" to \"automate reconnaissance, harvesting victims' credentials, and penetrating networks.\" Anthropic dubbed it \"vibe-hacking.\"To that end, Anthropic also said in its blog post that the privacy changes will help \"strengthen our safeguards against harmful usage like scams and abuse.\"",
  "medium": "Article",
  "links": [],
  "labels": [
    {
      "name": "Non-news"
    }
  ],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "Claude users",
      "weight": 0.112118214
    },
    {
      "name": "Claude Code",
      "weight": 0.10100723
    },
    {
      "name": "Claude",
      "weight": 0.10005919
    },
    {
      "name": "Claude Gov",
      "weight": 0.09994625
    },
    {
      "name": "user data",
      "weight": 0.09579802
    },
    {
      "name": "Anthropic",
      "weight": 0.09307568
    },
    {
      "name": "data retention",
      "weight": 0.085918345
    },
    {
      "name": "coding data",
      "weight": 0.08234576
    },
    {
      "name": "data",
      "weight": 0.078704864
    },
    {
      "name": "user chats",
      "weight": 0.074204065
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/News/Technology News",
      "score": 0.90771484375
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.89599609375
    },
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.89208984375
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.7431640625
    },
    {
      "name": "/Computers & Electronics/Enterprise Technology/Other",
      "score": 0.368896484375
    }
  ],
  "sentiment": {
    "positive": 0.18000959,
    "negative": 0.28336772,
    "neutral": 0.5366227
  },
  "summary": "Anthropic's chatbot, Claude, will begin learning from users via user chats and coding sessions after September 28. The changes will be automatically implemented unless users opt out. Anthropic will use these data from interactions with its consumer products in the free, pro, and max tiers. The new policy does not apply to Anthropic's commercial products. The company also plans to retain user data in its secure backend for up to five years, which was previously limited at 30 days. These changes come a day after Anthropic published a report alleging that its chatbot Claude had been weaponized by cybercriminals.",
  "shortSummary": "Anthropic will start training AI models using user chats, while retaining user data for up to five years to enhance safety and effectiveness.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "ac7428e1981642398757218b330257fd",
  "places": [],
  "scraped_sources": [],
  "argos_summary": "Anthropic will now use user chats and coding sessions from its Claude chatbot to train its models, with data retained for up to five years unless users opt out. The change applies to free, pro, and max tiers but not to commercial products like Claude Gov or API use. Users can adjust privacy settings via a pop\u2011up or the Help improve Claude bar. Anthropic cites longer data retention to improve safety classifiers and detect misuse, following a report of Claude\u2019s weaponization by cybercriminals.",
  "argos_id": "11MJJ1H3D"
}