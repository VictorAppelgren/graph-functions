{
  "url": "https://www.theverge.com/ai-artificial-intelligence/770646/switzerland-ai-model-llm-open-apertus",
  "authorsByline": "Elissa Welle",
  "articleId": "905fd07928af43c2a8357da3b8266307",
  "source": {
    "domain": "theverge.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "DC",
      "city": "Washington",
      "coordinates": {
        "lat": 38.8949924,
        "lon": -77.0365581
      }
    }
  },
  "imageUrl": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/gettyimages-2225698490.jpg?quality=90&strip=all&crop=0%2C10.75214828329%2C100%2C78.49570343342&w=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-09-03T20:32:37+00:00",
  "addDate": "2025-09-03T20:34:38.201814+00:00",
  "refreshDate": "2025-09-03T20:34:38.201816+00:00",
  "score": 1.0,
  "title": "Switzerland releases an open-weight AI model",
  "description": "The model\u2019s training data, model weights, and source code are open to the public.",
  "content": "is a NYC-based AI reporter and is currently supported by the Tarbell Center for AI Journalism. She covers AI companies, policies, and products.\n\nPosts from this author will be added to your daily email digest and your homepage feed.\n\nSwitzerland launched an open-source model called Apertus on Monday as an alternative to proprietary models like OpenAI\u2019s ChatGPT or Anthropic\u2019s Claude, reports SWI as spotted by Engadget. The model\u2019s source code, training data, model weights, and detailed development process are available on the AI model platform HuggingFace.\n\nApertus, which is Latin for \u201copen,\u201d was designed to \u201cset a new baseline for trustworthy and globally relevant open models,\u201d according to the developers. The model was trained on over 1,800 languages and comes in two sizes with either 8 billion or 70 billion parameters. Apertus is comparable to the 2024 Llama 3 model from Meta, according to SWI.\n\nThe model was built to adhere to the European Union\u2019s copyright laws and voluntary AI code of practice, which some US-based AI companies have said reluctantly signed while claiming that the regulations will curb AI innovation and deployment. Apertus\u2019 training data was restricted to public sources, while adhering to the AI crawler opt-out requests on certain websites, according to the developers. (No \u201cstealth-crawling.\u201d)\n\nFollow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates.",
  "medium": "Article",
  "links": [
    "https://www.theverge.com/news/718319/perplexity-stealth-crawling-cloudflare-ai-bots-report",
    "https://www.swissinfo.ch/eng/swiss-ai/switzerland-launches-transparent-chatgpt-alternative/89929269",
    "https://www.engadget.com/ai/switzerland-launches-its-own-open-source-ai-model-133051578.html",
    "https://www.theverge.com/news/715710/google-falls-in-line-with-the-eus-ai-plan",
    "https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509",
    "https://github.com/swiss-ai/apertus-tech-report/blob/main/Apertus_Tech_Report.pdf"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "model weights",
      "weight": 0.1176896
    },
    {
      "name": "proprietary models",
      "weight": 0.11368271
    },
    {
      "name": "voluntary AI code",
      "weight": 0.10633924
    },
    {
      "name": "AI companies",
      "weight": 0.08880973
    },
    {
      "name": "the AI model platform",
      "weight": 0.0786048
    },
    {
      "name": "AI innovation",
      "weight": 0.07854581
    },
    {
      "name": "public sources",
      "weight": 0.07668473
    },
    {
      "name": "AI Journalism",
      "weight": 0.07613302
    },
    {
      "name": "an open-weight AI model",
      "weight": 0.06866082
    },
    {
      "name": "detailed development process",
      "weight": 0.06806973
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.958984375
    },
    {
      "name": "/News/Technology News",
      "score": 0.94384765625
    },
    {
      "name": "/News/Business News/Company News",
      "score": 0.40625
    }
  ],
  "sentiment": {
    "positive": 0.29167753,
    "negative": 0.10248849,
    "neutral": 0.60583395
  },
  "summary": "Switzerland has launched an open-source AI model called Apertus, designed to compete with proprietary models like OpenAI's ChatGPT or Anthropic's Claude. The model was trained on over 1,800 languages and comes in two sizes with either 8 billion or 70 billion parameters. It was designed to adhere to the European Union's copyright laws and voluntary AI code of practice, which some US-based AI companies have argued will curb AI innovation and deployment.",
  "shortSummary": "Switzerland launched an open-source AI model called Apertus, offering reliable, scalable models with strict copyright and deployment regulations.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "d7cf5d2504c24145aea5474c14e5327e",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.engadget.com/ai/switzerland-launches-its-own-open-source-ai-model-133051578.html",
      "text": "Switzerland launches its own open-source AI model\nApertus was trained only on publicly available data, it said.\nThere's a new player in the AI race, and it's a whole country. Switzerland has just released Apertus, its open-source national Large Language Model (LLM) that it hopes would be an alternative to models offered by companies like OpenAI. Apertus, Latin for the world \"open,\" was developed by the Swiss Federal Technology Institute of Lausanne (EPFL), ETH Zurich and the Swiss National Supercomputing Centre (CSCS), all of which are public institutions.\n\"Currently, Apertus is the leading public AI model: a model built by public institutions, for the public interest. It is our best proof yet that AI can be a form of public infrastructure like highways, water, or electricity,\" said Joshua Tan, a leading proponent in making AI a public infrastructure.\nThe Swiss institutions designed Apertus to be completely open, allowing users to inspect any part of its training process. In addition to the model itself, they released comprehensive documentation and source code of its training process, as well as the datasets they used. They built Apertus to comply with Swiss data protection and copyright laws, which makes it perhaps one of the better choices for companies that want to adhere to European regulations. The Swiss Bankers Association previously said that a homegrown LLM would have \"great long-term potential,\" since it will be able to better comply with Switzerland's strict local data protection and bank secrecy rules. At the moment, Swiss banks are already using other AI models for their needs, so it remains to be seen whether they'll switch to Apertus.\nAnybody can use the new model: Researchers, hobbyists and even companies are welcome to build upon it and to tailor it for their needs. They can use it to create chatbots, translators and even educational or training tools, for instance. Apertus was trained on 15 trillion tokens across more than 1,000 languages, with 40 percent of the data in languages other than English, including Swiss German and Romansh. Switzerland's announcement says the model was only trained on publicly available data, and its crawlers respected machine-readable opt-out requests when they came across them on websites. To note, AI companies like Perplexity have previously been accused of scraping websites and bypassing protocols meant to block their crawlers. Some AI companies have also been sued by news organizations and creatives for using their content to train their models without permission.\nApertus is currently available in two sizes with 8 billion and 70 billion parameters. It's currently available via Swisscom, a Swiss information and communication technology company, or via Hugging Face.\nIf you buy something through a link in this article, we may earn commission."
    },
    {
      "url": "https://www.theverge.com/news/718319/perplexity-stealth-crawling-cloudflare-ai-bots-report",
      "text": "The AI search startup Perplexity is allegedly skirting restrictions meant to stop its AI web crawlers from accessing certain websites, according to a report from Cloudflare. In the report, Cloudflare claims that when Perplexity encounters a block, the startup will conceal its crawling identity \u201cin an attempt to circumvent the website\u2019s preferences.\u201d\nCloudflare says Perplexity\u2019s AI bots are \u2018stealth crawling\u2019 blocked sites\nPerplexity disguised its AI crawlers and rotated its IPs to get around restrictions, according to Cloudflare.\nPerplexity disguised its AI crawlers and rotated its IPs to get around restrictions, according to Cloudflare.\nThe report only adds to concerns about Perplexity vacuuming up content without permission, as the company got caught barging past paywalls and ignoring sites\u2019 robots.txt files last year. At the time, Perplexity CEO Aravind Srinivas blamed the activity on third-party crawlers used by the site.\nNow, Cloudflare, one of the world\u2019s biggest internet architecture providers, says it received complaints from customers who claimed that Perplexity\u2019s bots still had access to their websites even after putting their preference in their websites\u2019 robots.txt file and by creating Web Application Firewall (WAF) rules to restrict access to the startup\u2019s AI bots.\nTo test this, Cloudflare says it created new domains with similar restrictions against Perplexity\u2019s AI scrapers. It found that the startup will first attempt to access the sites by identifying itself as the names of its crawlers: \u201cPerplexityBot\u201d or \u201cPerplexity-User.\u201d\nBut if the website has restrictions against AI scraping, Cloudflare claims Perplexity will change its user agent \u2014 the bit of information that tells a website what kind of browser and device you\u2019re using, or if the visitor is a bot \u2014 to \u201cimpersonate Google Chrome on macOS.\u201d Cloudflare says this \u201cundeclared crawler\u201d uses \u201crotating\u201d IP addresses that the company doesn\u2019t include on the list of IP addresses used by its bots.\nAdditionally, Cloudflare claims that Perplexity changes its autonomous system networks (ASN), a number used to identify groups of IP networks controlled by a single operator, to get around blocks as well. \u201cThis activity was observed across tens of thousands of domains and millions of requests per day,\u201d Cloudflare writes.\nIn a statement to The Verge, Perplexity spokesperson Jesse Dwyer called Cloudflare\u2019s report a \u201cpublicity stunt,\u201d adding that \u201cthere are a lot of misunderstandings in the blog post.\u201d\nPerplexity has published a response on its website, claiming Cloudflare conflated 20 to 25 million user agent requests with AI scrapers. \u201cUser-driven agents only act when users make specific requests, and they only fetch the content needed to fulfill those requests,\u201d Perplexity says. The startup adds that Cloudflare \u201cconfused\u201d Perplexity with \u201c3-6M daily requests of unrelated traffic from BrowserBase,\u201d a cloud browser for AI agents that Perplexity says it only \u201coccasionally\u201d uses.\nCloudflare has since delisted Perplexity as a verified bot and has rolled out methods to block Perplexity\u2019s \u201cstealth crawling.\u201d\nCloudflare CEO Matthew Prince has been outspoken about AI\u2019s \u201cexistential threat\u201d to publishers. Last month, the company started letting websites ask AI companies to pay to crawl their content, and began blocking AI crawlers by default.\nUpdate, August 5th: Added Perplexity\u2019s response."
    },
    {
      "url": "https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509",
      "text": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but you have to accept the conditions to access its files and content.\nApertus LLM Acceptable Use Policy\n(1.0 | September 1, 2025)\n\"Agreement\" The Swiss National AI Institute (SNAI) is a partnership between the two Swiss Federal Institutes of Technology, ETH Zurich and EPFL.\nBy using the Apertus LLM you agree to indemnify, defend, and hold harmless ETH Zurich and EPFL against any third-party claims arising from your use of Apertus LLM.\nThe training data and the Apertus LLM may contain or generate information that directly or indirectly refers to an identifiable individual (Personal Data). You process Personal Data as independent controller in accordance with applicable data protection law. SNAI will regularly provide a file with hash values for download which you can apply as an output filter to your use of our Apertus LLM. The file reflects data protection deletion requests which have been addressed to SNAI as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from SNAI every six months following the release of the model.\nLog in or Sign Up to review the conditions and access this model content.\nApertus\nTable of Contents\nModel Summary\nApertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models. The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.\nThe model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO.\nKey features\n- Fully open model: open weights + open data + full training details including all data and training recipes\n- Massively Multilingual: 1811 natively supported languages\n- Compliant Apertus is trained while respecting opt-out consent of data owners (even retrospectivey), and avoiding memorization of training data\nFor more details refer to our technical report\nHow to use\nThe modeling code for Apertus is available in transformers v4.56.0\n, so make sure to upgrade your transformers version. You can also load the model with the latest vLLM\nwhich uses transformers as a backend.\npip install -U transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"swiss-ai/Apertus-8B-Instruct-2509\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\n).to(device)\n# prepare the model input\nprompt = \"Give me a brief explanation of gravity in simple terms.\"\nmessages_think = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages_think,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\nWe recommend setting\ntemperature=0.8\nandtop_p=0.9\nin the sampling parameters.\nLong context processing\nApertus by default supports a context length up to 65,536 tokens.\nAgentic Usage\nApertus supports tool use\nvLLM and SGLang\nYou can use vLLM and SGLang to deploy the model in an API compatible with OpenAI format.\nEvaluation\nIn this section, we report the evaluation results of Apertus model.\nBase Pre-Trained Model\nInstruction Model\nTraining\nModel\n- Architecture: Transformer decoder\n- Pretraining tokens: 15T\n- Precision: bfloat16\nSoftware & hardware\n- GPUs: 4096 GH200\n- Training Framework: Megatron-LM\n- ...\nOpen resources\nAll elements used in the training process are made openly available\n- Training data reconstruction scripts: github.com/swiss-ai/pretrain-data\n- The training intermediate checkpoints are available on the different branches of this same repository\nLimitations\nApertus can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nLegal Aspects\nEU AI Act Transparency Documentation and Code of Practice\nData Protection and Copyright Requests\nFor removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly\nOutput Filter for PII\n- Currently no output filter is provided.\n- Please check this site regularly for an output filter that can be used on top of the Apertus LLM. The filter reflects data protection deletion requests which have been addressed to us as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from this site every six months.\nContact\nTo contact us, please send an email to llm-requests@swiss-ai.org\nCitation\n@misc{swissai2025apertus,\ntitle={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}},\nauthor={Apertus Team},\nyear={2025},\nhowpublished={\\url{https://huggingface.co/swiss-ai/Apertus-70B-2509}}\n}\n- Downloads last month\n- 2,868\nModel tree for swiss-ai/Apertus-8B-Instruct-2509\nBase model\nswiss-ai/Apertus-8B-2509"
    },
    {
      "url": "https://www.theverge.com/news/715710/google-falls-in-line-with-the-eus-ai-plan",
      "text": "J\nGoogle falls in line with the EU\u2019s AI plan.\nThe search giant has followed OpenAI in signing the EU\u2019s voluntary AI code of practice, after Meta snubbed the agreement over \u201clegal uncertainties.\u201d Google also has its complaints despite signing, saying in a statement:\n\u201cWe remain concerned that the AI Act and Code risk slowing Europe\u2019s development and deployment of AI. In particular, departures from EU copyright law, steps that slow approvals, or requirements that expose trade secrets could chill European model development and deployment.\u201d\nWe will sign the EU AI Code of Practice.\n[blog.google]\nFollow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates."
    },
    {
      "url": "https://www.swissinfo.ch/eng/swiss-ai/switzerland-launches-transparent-chatgpt-alternative/89929269",
      "text": "Switzerland launches transparent ChatGPT alternative\nSwitzerland has entered the artificial intelligence (AI) race with the launch of a national Large Language Model (LLM) to provide an alternative to the likes of ChatGPT, Llama and DeepSeek.\n+Get the most important news from Switzerland in your inbox\nDesigned by leading Swiss universities, the Apertus LLM is comparable to Meta\u2019s Llama 3 model from 2024. Since then, Meta and other rivals have produced more advanced versions.\nBut the Swiss team insist they are not trying to compete with the multi-billion dollar budgets of United States trailblazers. They are happy to sacrifice the latest frills aimed at general users in favour of a safer and more accessible AI system for scientific researchers and commerce.\n+ How Switzerland strives to steer a safe AI course\n\u201cWe aim to provide a blueprint for how a trustworthy, sovereign and inclusive AI model can be developed,\u201d said Martin Jaggi, professor of machine learning at the Swiss Federal Institute of Technology Lausanne EPFL.\nHallucinations and bias\nThe frenetic pace of AI innovation since the public release of OpenAI\u2019s ChatGPT in 2022 has seen leading companies, such as Anthropic, release a spate of LLMs every year. Outside the US, China\u2019s DeepSeek and Qwen platforms and models from France\u2019s Mistral have deepened the pool of options for AI users.\nMore\nArtificial intelligence explained\nBut the race for AI dominance has also left a trail of problems: machines that hallucinate or magnify human bias and a growing list of copyright infringement lawsuits over the material that companies use to train their LLMs.\nArtificial intelligence inspires both hopes of a better future and fears of inflicting societal damage if left to roam wild without controls.\nOne answer is to produce publicly available, open-source AI systems to slug it out for market share with privately owned rivals.\n+ Swiss Alps supercomputer to leverage AI for science\nCommercial LLMs tend to push the frontiers of innovation by releasing the most advanced platforms. Open-source models are free to use and allow users to inspect their designs to figure out how they generate responses.\nSome models, such as DeepSeek from China, shine a light on how they operate but not how they were programmed in the first place. It\u2019s like watching a chef prepare a dish without knowing what went into the sauce.\nOpen-source milestone\nApertus (from the Latin word meaning \u201copen\u201d) leaves nothing to the imagination in this respect. It promises that every nut and bolt is open for public scrutiny along with its design manual and recipe formula. This is intended to inspire public trust and assuage concerns about the potential ugly side of AI.\nApertus is one of the most ambitious open-source models to date, according to Leandro von Werra, head of research at the open-source AI community Hugging Face.\nMore\nThe ethics of artificial intelligence\n\u201cIt\u2019s not totally unique, but it\u2019s still a trailblazer given its scale and the amount of compute used to train the model,\u201d he told Swissinfo. \u201cIt\u2019s definitely a new milestone in open models.\u201d\nResearchers, programmers, start-ups and the public sector can download a copy of Apertus onto their own servers to build projects. This approach allows users to keep control over their data.\nThe scale and success of applications built on top of Apertus will ultimately reveal the true worth of the Swiss LLM. Scientists are already exploring projects in the fields of health, education and the climate.\nSwiss industry groups have welcomed the home-grown AI initiative, particularly its focus on data security. But they caution it will face stiff competition for commercial attention from a growing range of powerful international rivals.\nCompeting for users\nThe Swiss Bankers Association believes a home-grown LLM has \u201cgreat long-term potential\u201d for the financial industry, especially given the need to comply with local data protection and banking secrecy laws.\nHowever, some Swiss banks are already building AI projects using other LLMs. For example, Switzerland\u2019s largest bank, UBS, is already working with OpenAI and Microsoft.\nSwissmem, which represents the domestic electrical engineering and machine building industries, believes that a Swiss LLM could better serve local companies as it is built to respect European data regulations. But this is no guarantee that Swiss industry will jump onto Apertus en masse.\nPotential users will gauge the speed, accuracy and depth of responses generated by Apertus compared to other LLMs. These results will be weighed up against the advantages of the system\u2019s transparency and their data security requirements.\n\u201cExperience shows that there is no single solution that fits all needs,\u201d Swissmem\u2019s head of digitalisation, innovation and technology, Adam Gontarz, told Swissinfo.\n\u201cEach project must take into account the specific circumstances and requirements of the business,\u201d he added. \u201cIn some cases, international solutions may also be the most effective choice.\u201d\nMore\nEdited by Gabe Bullard/vdv/ts\nIn compliance with the JTI standards\nMore: SWI swissinfo.ch certified by the Journalism Trust Initiative\nYou can find an overview of ongoing debates with our journalists here . Please join us!\nIf you want to start a conversation about a topic raised in this article or want to report factual errors, email us at english@swissinfo.ch."
    }
  ],
  "argos_summary": "Switzerland has launched Apertus, an open\u2011source large language model developed by EPFL, ETH Zurich, and the Swiss National Supercomputing Centre. The 8B and 70B parameter models are trained on 15 trillion tokens across 1,800 languages and fully comply with EU copyright and data\u2011protection rules. All source code, training data, and documentation are publicly available on Hugging Face, positioning Apertus as a transparent alternative to proprietary models like ChatGPT. The Swiss banks and industry see potential for a sovereign AI infrastructure that meets local privacy requirements, though adoption remains to be seen.",
  "argos_id": "NSMHSXG24"
}