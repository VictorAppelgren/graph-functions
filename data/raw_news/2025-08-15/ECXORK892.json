{
  "url": "https://gizmodo.com/open-sourced-ai-models-may-be-more-costly-in-the-long-run-study-finds-2000643685",
  "authorsByline": "Bruce Gil",
  "articleId": "16a7da68b54740439e0166adae953032",
  "source": {
    "domain": "gizmodo.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://gizmodo.com/app/uploads/2024/10/CharacterAI.jpg",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-15T16:00:24+00:00",
  "addDate": "2025-08-15T16:12:48.035299+00:00",
  "refreshDate": "2025-08-15T16:12:48.035302+00:00",
  "score": 1.0,
  "title": "Open-Sourced AI Models May Be More Costly in the Long Run, Study Finds",
  "description": "Open-source AI models tend to use more computing power than closed-source models for the same tasks.",
  "content": "As more businesses adopt AI, picking which model to go with is a major decision. While open-sourced models may seem cheaper initially, a new study warns that those savings can evaporate fast, due to the extra computing power they require.\n\nIn fact, open-source AI models burn through significantly more computing resources than their closed-source rivals when performing the same tasks, according to a study published Thursday by Nous Research.\n\nThe researchers tested dozens of AI models, including closed systems from Google and OpenAI, as well as open-source models from DeepSeek and Magistral. They measured how much computing effort each required to complete identical tasks across three categories: simple knowledge questions, math problems, and logic puzzles.\n\nTo do this, they used the number of tokens each model used to solve and answer questions as for computing resources consumed.\n\n\u201cOpen-weight models use 1.5\u20134\u00d7 more tokens than closed ones\u2014and up to 10\u00d7 for simple knowledge questions\u2014making them sometimes more expensive per query despite lower per-token costs,\u201d the study authors wrote.\n\nIn AI, a token is a piece of text or data\u2014it could be a word, part of a word, or even punctuation\u2014that models use to understand language. Models process and generate text one token at a time, so the more tokens they use, the more computing power and time a task requires.\n\nSince most closed-source models don\u2019t reveal their raw reasoning process or chain of thought (CoT), the researchers measured their computing efficiency by counting the tokens they used instead. Because models are billed by total output tokens used in their reasoning process and outputting the final answer, completion tokens serve as a proxy for the effort needed to produce a response.\n\nThis is an important consideration for companies using AI for many reasons.\n\n\u201cFirst, while hosting open weight models may be cheaper, this cost advantage could be easily offset if they require more tokens to reason about a given problem,\u201d the researchers wrote. \u201cSecond, an increased number of tokens will lead to longer generation times and increased latency.\u201d\n\nThe study found that open models consistently use more tokens than closed models for the same tasks, sometimes three times as many for simple knowledge questions. The gap narrowed to less than twice for math and logic problems.\n\n\u201cClosed models (OpenAI, Grok-4) optimize for fewer tokens to cut costs, while open models (DeepSeek, Qwen) use more tokens, possibly for better reasoning,\u201d the study authors wrote.\n\nAmong open models, llama-3.3-nemotron-super-49b-v1 was the most efficient, while Magistral models were the most inefficient.\n\nOpenAI\u2019s models were standouts as well. Both its o4\u2011mini and the new open-weight gpt\u2011oss models showed impressive token efficiency, especially on math problems.\n\nThe researchers noted that OpenAI\u2019s gpt\u2011oss models, with their concise chain-of-thoughts, could serve as a benchmark for improving token efficiency in other open models.",
  "medium": "Article",
  "links": [
    "https://gizmodo.com/openai-google-and-meta-researchers-warn-we-may-lose-the-ability-to-track-ai-misbehavior-2000630259",
    "https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "open models",
      "weight": 0.13720268
    },
    {
      "name": "open weight models",
      "weight": 0.13278437
    },
    {
      "name": "other open models",
      "weight": 0.12886171
    },
    {
      "name": "Closed models",
      "weight": 0.12691323
    },
    {
      "name": "Models",
      "weight": 0.12350061
    },
    {
      "name": "models",
      "weight": 0.12350061
    },
    {
      "name": "closed models",
      "weight": 0.12320517
    },
    {
      "name": "AI models",
      "weight": 0.12238834
    },
    {
      "name": "Magistral models",
      "weight": 0.121904686
    },
    {
      "name": "more tokens",
      "weight": 0.11598149
    }
  ],
  "topics": [
    {
      "name": "Data"
    },
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.9462890625
    },
    {
      "name": "/News/Technology News",
      "score": 0.921875
    }
  ],
  "sentiment": {
    "positive": 0.22058474,
    "negative": 0.27961218,
    "neutral": 0.49980304
  },
  "summary": "A study by Nous Research has found that open-source AI models may be more costly in the long run due to their high demand for computing power. The study tested dozens of AI models, including closed systems from Google and OpenAI, and open models from DeepSeek and Magistral. The researchers compared these models' computing efficiency to the actual computing effort each required to complete identical tasks across three categories: simple knowledge questions, math problems, and logic puzzles. Open-weight models use 1.5\u20134\u00d7 more tokens than closed ones, making them more expensive per query. The cost advantage may be easily offset if they require more tokens to reason about a given problem, which could lead to longer generation times and increased latency.",
  "shortSummary": "Open-source AI models consume significantly more computing resources than closed ones, despite initial savings, due to increased token usage and longer processing times.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "701503c3d8864010808885a1b3008ac8",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://gizmodo.com/openai-google-and-meta-researchers-warn-we-may-lose-the-ability-to-track-ai-misbehavior-2000630259",
      "text": "Over 40 scientists from the world\u2019s leading AI institutions, including OpenAI, Google DeepMind, Anthropic, and Meta, have come together to call for more research in a particular type of safety monitoring that allows humans to analyze how AI models \u201cthink.\u201d\nThe scientists published a research paper on Tuesday that highlighted what is known as chain of thought (CoT) monitoring as a new yet fragile opportunity to boost AI safety. The paper was endorsed by prominent AI figures like OpenAI co-founders John Schulman and Ilya Sutskever as well as Nobel Prize laureate known as the \u201cGodfather of AI,\u201d Geoffrey Hinton.\nIn the paper, the scientists explained how modern reasoning models like ChatGPT are trained to \u201cperform extended reasoning in CoT before taking actions or producing final outputs.\u201d In other words, they \u201cthink out loud\u201d through problems step by step, providing them a form of working memory for solving complex tasks.\n\u201cAI systems that \u2018think\u2019 in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave,\u201d the paper\u2019s authors wrote.\nThe researchers argue that CoT monitoring can help researchers detect when models begin to exploit flaws in their training, manipulate data, or fall victim to malicious user manipulation. Any issues that are found can then either be \u201cblocked, or replaced with safer actions, or reviewed in more depth.\u201d\nOpenAI researchers have already used this technique in testing to find cases when AI models have had the phrase \u201cLet\u2019s Hack\u201d in their CoT.\nCurrent AI models perform this thinking in human language, but the researchers warn that this may not always be the case.\nAs developers rely more on reinforcement learning, which prioritizes correct outputs rather than how they arrived at them, future models may evolve away from using reasoning that humans can\u2019t easily understand. Additionally, advanced models might eventually learn to suppress or obscure their reasoning if they detect that it\u2019s being monitored.\nIn response, the researchers are urging AI developers to track and evaluate the CoT monitorability of their models and to treat this as a critical component of overall model safety. They even recommend that it become a key consideration when training and deploying new models."
    },
    {
      "url": "https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/",
      "text": "Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark\nLarge Reasoning Models (LRMs) employ a novel paradigm known as test-time scaling, leveraging reinforcement learning to teach the models to generate extended chains of thought (CoT) during reasoning tasks. This enhances their problem-solving capabilities beyond what their base models could achieve independently.\nWhile cost and efficiency trade-off curves (\"the Pareto frontier\") typically focus on model intelligence versus cost per million completion tokens, token efficiency \u2014 the number of tokens used for reasoning relative to the solution \u2014 is a critical factor that is recently receiving more attention.\nAnecdotal evidence suggests open weight models produce significantly more tokens for similar tasks than closed weight models.\nThis report systematically investigates these observations. We confirm this trend to be generally true, but observe significant differences depending on problem domain.\nTL;DR:\nClosed models (OpenAI, Grok-4) optimize for fewer tokens to cut costs, while open models (DeepSeek, Qwen) use more tokens, possibly for better reasoning. Open weight models use 1.5\u20134\u00d7 more tokens than closed ones (up to 10\u00d7 for simple knowledge questions), making them sometimes more expensive per query despite lower per\u2011token costs. OpenAI leads in token efficiency for math. Among open models,\nllama-3.3-nemotron-super-49b-v1\nis most efficient, while Magistral models are outliers with exceptionally high token usage. The release of OpenAI's\ngpt-oss\nopen weight models with extremely short CoT can serve as a reference to optimize token usage in other open weight models.\nWhy is it of interest to measure token efficiency?\nToken efficiency is a critical metric for several practical reasons:\nFirst, while hosting open weight models may be cheaper, this cost advantage could be easily offset if they require more tokens to reason about a given problem. Second, an increased number of tokens will lead to longer generation times and increased latency. Finally, inefficient token usage may exhaust the available context window, limiting the model's ability to process complex queries effectively.\nWe address three questions:\n- Do open weight models systematically require more tokens than closed weight models for comparable tasks?\n- What are the cost implications when token efficiency is factored into total inference expenses?\n- Are there specific task categories where this efficiency gap is more pronounced?\nHow can we measure token efficiency?\nMeasuring the length of the thinking process, the Chain-of-Thought, presents some issues because most recent closed source models will not share their raw reasoning traces. The rationale behind this is to prevent competitors from fine-tuning on their reasoning traces. Instead, they use smaller language models to transcribe the chain of thought into summaries or compressed representations. This means the original reasoning process remains hidden, with only the final answer and a transcribed version of the CoT available for analysis.\nHowever, since models are usually billed by the number of output tokens for the full prompt completion (thinking and final answer output), we can use the number of completion tokens as a proxy for the total effort required to generate an answer.\nTo investigate which models transcribe the CoT and which make them directly available, we analyzed the relationship between total characters for completion (the sum of thinking trace and final output) versus the number of tokens billed for completion across all prompts in this study.\nFigure 1 illustrates the relationship between total characters for completion (combining thinking trace and final output) versus the number of tokens billed for completion across all prompts in this study. Each model shows a linear relationship, but with different slopes depending on the specific model implementation.\nFigure 2 presents the extracted slopes for each model, revealing interesting patterns.\n- Open weight models show a consistent character-to-token ratio of approximately 3-3.4, which is typical for tokenizers. This suggests that no transcription took place for these models.\n- Claude 3.7 Sonnet exhibits a ratio of 2.8, suggesting that the CoT is mostly intact, but the lower ratio may indicate that some filtering took place, possibly through simple word filtering or substitution.\n- Gemini models show a slope of ~1, which suggests a text densification ratio of 3:1. While the original CoT is not available, the output remains proportional to chain of thought length and should reflect the reasoning process.\n- OpenAI and Claude models show a ratio of 0.3, with closer examination of the scatter plot revealing that character count doesn't increase with CoT length, suggesting only a summary of the entire CoT is provided rather than transcription. This behavior seems to be specific to chat completions, as continuous updates are seen in streaming mode.\nSome models also provide information about reasoning tokens as part of the API response. However, we found this number to be unreliable in many cases as some models (Claude) would only return the number of tokens of the transcribed CoT, while others would provide token counts that exceeded the total completion length. We introduced consistency checks to decide how to extract the reasoning token count, as described in the appendix.\nBased on these findings, we use completion tokens to assess overall effort, supplemented by reasoning tokens from APIs where available.\nDataset\nTo systematically evaluate token efficiency across different reasoning domains, we curated a dataset consisting of three categories:\n- Knowledge questions: Can be answered in one sentence from the model's pre-training corpus.\n- Math problems: Math problems should exhibit the most optimized behavior of reasoning models.\n- Logic puzzles: Logic puzzles require both semantic understanding and logical reasoning.\nThe questions were chosen to be solvable within the 30,000 token limit to avoid truncated responses.\nFindings\nKnowledge questions\nThis part of the evaluation consists of 5 trivial knowledge questions that can be answered in one word and that do not require reasoning at all. E.g.\n- What is the capital of Australia?\n- How many days are there in February during a leap year?\nAll models were able to respond to these prompts correctly. The purpose of these questions is to probe for superfluous reasoning. Efficient models should be able to determine that no reasoning is required.\nFigure 3 shows the average number of reasoning and output tokens for each prompt. We can see that, on average, reasoning models spend hundreds of tokens pondering simple knowledge questions.\nResolving this per model reveals stark differences. Figure 4 shows that closed-weight frontier lab models are far more token efficient than open weight models.\nTo better quantify the excess token ratio, we normalized the token count for each prompt relative to the most optimized closed weight models (reference). This approach considers the excess token ratio for all prompts equally, which contrasts with averaging across all completion lengths, which would be dominated by long completions.\nFigure 5 shows the mean excess token\nratio for different LLMs. The recently released Magistral\nmodels are an extreme outlier with up to 10x excess tokens.\nWe also see that most other open weight models show an\nexcess token ratio of around 4, meaning that they use four\ntimes as many tokens as the most optimized closed weight\nmodels. The recently released\ngpt-oss-120b\nby OpenAI leads all\nother models in token efficiency.\nHow does this affect inference costs? Figure 6 shows the mean cost per model for knowledge questions, based on minimum and maximum completion pricing on the OpenRouter API in July 2025. The better token efficiency of closed weight models often compensates for the higher API pricing of those models.\nMath problems\nMost reasoning models are specifically trained to solve mathematical problems. One reason for this is that math problems are usually easily verifiable, which is a key advantage for reinforcement learning. Furthermore, math problems are also an easy benchmark target for reasoning models as there are many widely available problem sets.\nFor this study, we selected a set of six problems to test token efficiency in the math domain. Three problems were sourced from AIME 2025, and one problem was taken from AIME 2023. Easier problems were chosen to prevent models from exceeding the 30,000 token limit. To further investigate the role of memorization in problem-solving, we created two modified problems by changing the variables in one AIME 2025 problem and the AIME 2023 problem. The rationale behind this approach is that unknown problems may require a longer chain of thought, as the model cannot rely on memorized solutions. The AIME 2025 problems are too new to be in the pre-training data of any model, while some may have seen the AIME 2023 problems during pre-training.\nExample\nAIME2025I Problem 2 (Original)\nFind the sum of all positive integers $n$ such that $n+2$ divides the product $3(n+3)(n^2+9).$\nAIME2025I Problem 2 (Modified)\nFind the sum of all positive integers $n$ such that $n+2$ divides the product $3(n+3)(n^2+7).$\nWith a few exceptions, all models were able to solve the math problems correctly Figure 7.\nWe can see that, on average, fewer than 10,000 tokens are required to solve the selected problems (Figure 8). The more complex problems from the AIME2025 set would easily require more than 30,000 tokens in some models and were therefore not used for this evaluation to avoid skewing the distribution due to truncation.\nRemarkably, the number of tokens required to solve the pairs of original and modified problems is almost identical. This could suggest two trends across all LLMs:\n- These math problems are not solved based on memorization, but algorithmically.\n- The number of tokens is identical for similar problems.\nFigure 9 shows the relative excess\ntoken ratio compared to the reference. In contrast to the\ntrends observed for knowledge questions, the relative ratio\nbetween models is much lower.\no4-mini-high-long\nis a notable\noutlier with a surprisingly low token count \u2014 3x fewer\ntokens than other commercial models. This suggests that\no4-mini has been specifically optimized for token efficiency\nin mathematical problems. The same optimizations seem to be\nshared by the OpenAI open weight model\ngpt-oss-120b\n. The recently\nreleased grok-4\nalso seems to\nhave been optimized for token efficiency in math problems.\nThe most efficient open weight model is\nllama-3.3-nemotron-super-49b-v1\n,\nwhich uses fewer tokens than most closed weight models.\nmagistral-small\nand\nmagistral-medium\nremain the\nhighest token count models, but show only 3x the ratio of\nthe reference models.\nGenerally, there is a very clear trend toward higher reasoning token usage for open weight models in math problems, compared to closed weight models. This may suggest an optimization toward benchmarking performance rather than production efficiency.\nExamining completion costs reveals that since token\nconsumption is relatively similar across most models for\nmath problems, those with higher per-token pricing naturally\nincur the highest completion costs. However, the exceptional\ntoken efficiency of\no4-mini-high-long\nand\ngpt-oss-120b\nenables them to\nachieve very competitive total completion costs despite\npotentially higher per-token costs, demonstrating how\noptimization can offset pricing disadvantages.\nLogic puzzles\nLogic puzzles are a curious domain for reasoning models. They require a combination of semantic understanding and logical reasoning, making them an interesting test case for evaluating reasoning capabilities.\nHowever, many well-known logic puzzles are commonly found in pre-training data, which causes models to be over-fitted on specific solutions. Non-reasoning models will often have difficulty recognizing small changes to logic problems and tend to answer them based on memorization of the original problem. The Misguided Attention evaluation showcases this issue. Reasoning models can often overcome the bias of their pre-training data in the CoT and solve modified problems correctly.\nTo explore the effect of memorization, variants of two well-known logic puzzles were selected in addition to a generic logic puzzle (roses problem).\nBridge and torch problem\nThe bridge and torch problem requires finding the minimum time for four people with a torch to cross a bridge at night. The original problem and three variants were included:\n| Problem Variant | Description |\n|---|---|\n| bridge_torch_default | The unmodified problem in its most well-known version with a 17-minute solution |\n| bridge_torch_easy | Ambiguous version where two solutions exist: 10 and 17 minutes |\n| bridge_torch_easy_10m | Simplified version with a 10-minute solution |\n| bridge_torch_impossible | Constraints that make the problem impossible to solve |\nGenerally, we found that even more recent reasoning models struggle with logic problems that have ambiguous or impossible solutions.\nMonty Hall problem\nThe Monty Hall problem is a very well-known and famously unintuitive probability puzzle. All base models used for reasoning models are severely over-fitted on this problem and are biased toward solutions of the Monty Hall problem even for remotely similar scenarios.\nThe original problem and two variants were included:\n| Problem Variant | Description |\n|---|---|\n| monty_hall_default | The unmodified Monty Hall problem |\n| monty_hall_inverse | A modified version of the Monty Hall problem with the opposite solution |\n| monty_appliance_simple | An ambiguous problem that can be easily answered by humans, but is usually answered in the context of the Monty Hall problem by reasoning models |\nFigure 10 shows the success rates across different logic puzzle problems. Unlike the math problems, success rates vary significantly across both models and problem types. Only the default problems were solved by all models, while many models struggle with modified problems, indicating strong influence of pre-existing bias in the model.\nFigure 11 shows the average token consumption for all prompts. Remarkably, the number of reasoning tokens is the lowest for the default problems, while it significantly increases for modified problems. This is unlike the observation for math problems and suggests that pre-existing bias allows solving the problems with a shorter CoT for the default problems. The impossible problem generates the longest CoT as the models tend to try many solutions before giving up.\nSimilar to the patterns observed in other categories,\nFigure 12 shows that open-weight\nmodels use more tokens than closed-weight models for logic\npuzzles. However, the gap is far less pronounced than for\nmath and knowledge questions.\nmagistral-small\nand\nmagistral-medium\nstill present an\nexception for high token usage.\nclaude-opus-4\nuses the fewest\nreasoning tokens, a trend generally observed across problem\ndomains. The extreme token optimization that was observed\nfor o4-mini-high\ncannot be seen\nfor logic puzzles.\nSimilar to the math domain, Nvidia's model\nllama-3.3-nemotron-super-49b-v1\nstands out as the most token efficient open weight model\nbefore the release of\ngpt-oss-120b\n.\nThe cost analysis for the logic puzzle prompts reveals few surprises; since the variation in tokens is modest for logic prompts, the completion cost mostly scales with token pricing.\nChain of Thought Optimization\nSince the chain of thought of\ngpt-oss-120b\nis freely\naccessible, it allows us to inspect some of the length\noptimizations that are likely also part of the other OpenAI\nmodels. Below, we compare the CoT of\ngpt-oss-120b\nwith the CoT of\nDeepseek-R1-0528\nfor selected\nknowledge, math and logic prompts.\nKnowledge\nQuestion\nPROMPT\nWhich continent is Brazil located on?\nRESPONSE DeepSeek R1-0528\nOkay, the user is asking which continent Brazil is located on. This seems like a straightforward geography question. Hmm, I recall that Brazil is the largest country in South America. Let me double-check that fact to be sure. Yes, definitely South America. The user just wants a simple answer without any extra details, so I'll keep it concise. No need to mention neighboring countries or other continents. I wonder if they're asking for a school assignment or just casual curiosity. Either way, the answer is clear. Should I add that it's in the southern hemisphere? No, the question only asks for the continent. Best to stick to what was asked. \"South America\" it is. Short and accurate.\nRESPONSE gpt-oss-120b\nThe user asks: \"Which continent is Brazil located on?\" Answer: South America.\nWe can see that\ngpt-oss-120b\nomits a lot of\nsuperfluous reasoning and directly provides the answer.\nMath\nQuestion (AIME2023II P1)\nPROMPT\nThe numbers of apples growing on each of six apple trees form an arithmetic sequence where the greatest number of apples growing on any of the six trees is double the least number of apples growing on any of the six trees. The total number of apples growing on all six trees is $990.$ Find the greatest number of apples growing on any of the six trees.\nRESPONSE DeepSeek R1-0528 (middle part omitted for brevity)\nThe problem states that the apples on six trees form an arithmetic sequence, with the greatest number being double the least number. The total apples are 990, and I need to find the greatest number of apples. An arithmetic sequence has each term after the first obtained by adding a constant difference. So, if I denote the first term (the least number) as \\( a \\), and the common difference as \\( d \\), then the sequence is \\( a, a+d, a+2d, a+3d, a+4d, a+5d \\). Since it's six trees, the terms are \\( a, a+d, a+2d, a+3d, a+4d, a+5d \\). The greatest number is \\( a + 5d \\), and it's given that this is double the least number, which is \\( a \\). So, \\[ a + 5d = 2a \\] Let me write that down: \\[ a + 5d = 2a \\] Then, simplifying: \\[ 5d = 2a - a \\] \\[ 5d = a \\] ...middle part omitted... Therefore, the greatest number is 220. ... Further 30 lines omitted... Thus, the greatest number is 220. So I think that's the answer.\nRESPONSE gpt-oss-120b\nWe need to solve: Six terms in arithmetic sequence, with max = 2 * min. Sum = 990. Find max. Let sequence: a, a+d, a+2d, a+3d, a+4d, a+5d, where a is first term (least? Actually could be smallest or largest? It says \"greatest number ... is double the least number\". In arithmetic sequence, the terms are increasing if d>0. So smallest is a, largest is a+5d. Then a+5d = 2a (since greatest = double least). So a+5d = 2a => 5d = a => a = 5d. So first term a =5d. Then terms are: 5d, 6d, 7d, 8d, 9d, 10d. Indeed smallest 5d, largest 10d = double. Good. Sum = (5d+6d+7d+8d+9d+10d) = (5+6+7+8+9+10)d = (45) d? Let's compute: 5+6=11, +7=18, +8=26, +9=35, +10=45. Yes sum =45 d? Given total sum =990 => 45 d =990 => d=990/45=22? 45*22 = 990 indeed. So d=22. Then greatest number = 10d = 220. Check: smallest =5d=110, largest=220, sum=990. Answer: 220. But ensure integer conditions. Yes. Thus answer: 220. Now produce final answer.\nDeepSeek-R1-0528\ngenerates on\naverage 3,104 reasoning tokens while\ngpt-oss-120b\ngenerates only 268\nfor this prompt. We can observe a number of curious\noptimizations in the CoT of\ngpt-oss-120b\n:\n- It uses extremely abbreviated language, avoiding any unnecessary words.\n- Formatting in the CoT is notably reduced, compared to the output: No markdown sequences, very limited use of math formatting.\n- Much fewer iterations are used to check the results once they are found.\nLogic puzzles\nFor brevity, only examples of\ngpt-oss-120b\ntraces are shown\nhere.\nOriginal Monty Hall Problem\nRESPONSE gpt-oss-120b\nWe need to answer the classic Monty Hall problem. The answer: you should always swap, because it gives 2/3 chance of winning the car. Provide explanation.\nInverted Monty Hall Problem\nRESPONSE gpt-oss-120b\nWe need to answer: \"You're on a game show... three doors: 1 donkey, 2 cars. You pick one. Host opens one of the others revealing a luxury car. Then you can keep or swap. What should you do to win a car?\" This is a variation of Monty Hall but with two cars and one donkey. We need to compute probabilities.\nInitially, you pick a door randomly. Probability you initially picked donkey = 1/3, car = 2/3. There are two cars and one donkey.\n... middle part omitted ...\nThus answer: Keep initial (do not swap). Provide explanation.\nMake answer concise.\nA very curious finding is that the model is able to distinguish between logic problems where the solution is known from the training data, e.g., the Monty Hall problem, and modified problems that require additional reasoning, such as the inverted Monty Hall problem. It will only initiate a long CoT for the unknown problem.\nModel evolution\nWhile we have only looked at recent model variants above, it is also of interest to look at the historic evolution between different reasoning model variants.\nThe figures below show how the relative completion tokens changed across the different problem domains for iterations of models within the labs.\nWe can observe four key trends:\n- Closed weight models have been iteratively optimized to use fewer tokens to reduce inference cost.\n- The open weight models (DeepSeek and Qwen) have increased their token usage for newer versions, possibly reflecting a priority toward better reasoning performance.\n-\nOpenAI models stand out for extreme token efficiency in\nmath problems, even beginning with o1. It appears that\nX-AI is taking a similar approach with\ngrok-4\n, with significant improvements compared togrok-3-mini\n. -\nThe recently released\ngpt-oss-120b\nandgpt-oss-20b\nseem to inherit the CoT density optimization from the o-mini models.\nSummary\nWe find that open-weight models use consistently more tokens than closed-weight models for equivalent tasks. However, the efficiency gap depends on the workload and is most pronounced for superfluous reasoning in knowledge questions. On average, 3x more tokens are required for knowledge prompts. The gap reduces to less than 2x for math problems and logic puzzles.\nFurthermore, many closed weight models allow steering of the reasoning effort, which provides an effective way to reduce token usage for simple tasks.\nllama-3.3-nemotron-super-49b-v1\nstands out as the most token efficient open weight model\nacross all domains prior to the release of the\ngpt-oss\nmodels, while the\nMagistral models represent an unusual outlier toward the\nhigh end.\nThe recent release of\ngpt-oss-120b\nand\ngpt-oss-20b\nas open weight and\nstate-of-the-art token efficient reasoning models with\nfreely accessible CoT could serve as a\nreference for further optimization of other models.\nWe note the continued trend of closed weight reasoning models to improve token efficiency also in non-benchmark domains and suggest this as an important avenue for future open weight models. A more densified CoT will also allow for more efficient context usage and may counter context degradation during challenging reasoning tasks.\nAcknowledgments\nThanks to Teknium and Billy for providing feedback on early drafts of this report and encouragement for its creation. Also thanks to Rishav for preparation of the web version from the draft. Nous Research is acknowledged for providing tokens for this investigation.\nMethods\nGeneral\nAll models were accessed through OpenRouter, with the exception of Deephermes, which was accessed via the Nous API, and DeepSeek R1-0528, which was accessed through the DeepSeek API. Generation limits were set to 30,000 tokens with reasoning effort configured to \"high\" in the OpenRouter API. In cases where models timed out before generating the full 30,000 tokens, queries were restarted and the provider was adjusted as needed to ensure completion when the query did not complete initially. Typical reasons for failure to complete were timeouts.\nStatistical analysis was conducted with N=5 samples for each prompt and language model combination.\nThe gpt-oss-120b\nand\ngpt-oss-20b\nmodels were accessed\nthrough the OpenAI API on the day of their first release\nusing Groq as a provider. There is some uncertainty in the\nway the API interprets the reasoning effort settings, which\nmay lead to deviations in the number of reasoning tokens\ngenerated. We will monitor the situation and update the\nreport if necessary.\nAcquiring response and thinking token data\nWhile many models directly provide reasoning token counts through their API responses, we found these numbers to be unreliable in numerous cases. For instance, Anthropic models would only return the length of the transcribed Chain of Thought rather than the actual CoT lengths, while other models would occasionally report CoT lengths that exceeded the total completion length\u2014a logical impossibility.\nTo address these inconsistencies, our evaluation scripts implemented consistency checks to assess the validity of returned CoT lengths. When the provided reasoning token counts failed validation, we employed fallback estimation methods using the following formulas:\n- When CoT text is not available: CoT tokens = Completion tokens - (Answer length in characters / 3.1)\n- When CoT text is available: CoT tokens = Completion tokens \u00d7 (CoT length in characters / completion length in characters)\nModel pricing\nPricing data for completion tokens (measured in $/1M tokens) was automatically extracted from the OpenRouter API for each model in July 2025, with the exception of Deephermes3. The pricing information captures both minimum and maximum rates available through different providers, as illustrated in Figure A below.\nDataset, harness and evaluation code\nAll datasets and code can be found in this repository:\n(https://github.com/cpldcpu/LRMTokenEconomy/)"
    }
  ],
  "argos_summary": "A study by Nous Research shows that open\u2011source AI models consume 1.5\u20134\u00d7 more tokens than closed\u2011source models for the same tasks, sometimes up to 10\u00d7 for simple knowledge questions, making them potentially more expensive per query despite lower token costs. Closed\u2011source models such as OpenAI\u2019s o4\u2011mini and new gpt\u2011oss variants are highly token\u2011efficient, especially on math problems, while open models like DeepSeek and Qwen use more tokens, possibly to improve reasoning. The research also highlights the importance of chain\u2011of\u2011thought (CoT) monitoring for AI safety, enabling humans to detect misbehaviour and guiding developers to design models that expose their reasoning. Overall, token efficiency is a critical metric for cost, latency, and context usage, and the findings suggest that future open\u2011weight models should aim to reduce token usage while maintaining performance.",
  "argos_id": "ECXORK892"
}