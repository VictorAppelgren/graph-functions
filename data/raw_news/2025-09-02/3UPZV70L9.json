{
  "url": "https://www.zdnet.com/article/i-asked-ai-to-modify-mission-critical-code-and-what-happened-next-haunts-me/",
  "authorsByline": "David Gewirtz",
  "articleId": "306ff170db0b4992a77f7a1326c42e79",
  "source": {
    "domain": "zdnet.com",
    "paywall": false,
    "location": {
      "country": "us",
      "state": "NY",
      "city": "New York",
      "coordinates": {
        "lat": 40.7127281,
        "lon": -74.0060152
      }
    }
  },
  "imageUrl": "https://www.zdnet.com/a/img/resize/369dfb679301b3eac06fa4635a6b293df0ba4a06/2025/09/02/b8ca4610-83e5-4be1-8394-dada11ff8e8c/gettyimages-1125993405.jpg?auto=webp&fit=crop&height=675&width=1200",
  "country": "us",
  "language": "en",
  "pubDate": "2025-09-02T15:45:56+00:00",
  "addDate": "2025-09-02T16:06:28.956691+00:00",
  "refreshDate": "2025-09-02T16:06:28.956692+00:00",
  "score": 1.0,
  "title": "I asked AI to modify mission-critical code, and what happened next haunts me",
  "description": "This seriously raised the hairs on the back of my neck.",
  "content": "Follow ZDNET: Add us as a preferred source on Google.\n\u2022 Mission-critical coding work is too risky to delegate to AI.\n\u2022 AI is fine for new features, not core infrastructure.\n\nI woke up in a cold sweat. In my nightmare, I was being chased by tens of thousands of people, all of whom were enraged because I destroyed their privacy. They were all holding laptops over their heads, swinging them like clubs intended for my head.\n\nThey say nightmares reflect whatever your subconscious is trying to tell you. Given the work I was planning to start in the morning, I knew exactly what my dark night brain was trying to say.\n\nAlso: I retested GPT-5's coding skills using OpenAI's guidance - and now I trust it even less\n\nIt was saying, \"Stop!\" Don't do it.\" My inner knowing was screaming at the top of its lungs, \"Don't let the AI code for you.\"\n\nThis, believe it or not, is not hyperbole. I was getting ready to start a coding project where I was planning on using an AI for help.\n\nBut unlike all those vibe coding stories you read about where someone makes a Pinterest clone in 12 days of code-free prompt calisthenics, I was planning on making a deep architectural change to mission-critical code used by more than 20,000 sites across the world to provide access security and site privacy.\n\nIf I shipped damaged code, it would, at best, break a bunch of sites. At worst, it would open the contents of those private sites to the public internet.\n\nAlso: How I saved myself $1200 a year in cloud storage - in 5 sobering steps\n\nPeople use my code to create protected, private sites that they don't want shared with the entire internet. Users can designate specific family members, schoolmates, and/or teachers who can log in.\n\nIt's also used by developers for locking down projects in progress. Users set up private test sites behind login pages, which is great for publishing restricted-access preview sites for client review and use.\n\nSo, let me tell you about the update I was planning. Then I'll share the work I did with the AI, and then why I changed my mind.\n\nAt the core of all of these sites is a series of settings records. These records capture how each site owner wants to configure their privacy. They include lists of pages to make private or public, tags and categories used for the same purpose, and a variety of other site-specific privacy settings.\n\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\n\nSettings are currently saved as something called serialized arrays. Serialization is a way of taking a big block of structured data and storing it in a database field. But there's a problem, not with the serialization process, but with the unserialization process, where the data is reconstituted for use by code.\n\nMost of the time, serialization goes on behind the scenes whenever WordPress saves or updates a settings value. This is a perfectly safe mechanism for settings management. However, there are some places in my existing code that explicitly serialize and unserialize unnecessarily.\n\nThe problem is that unserialize() will reconstitute pretty much anything, including malicious logic. This is called PHP Object Injection. It's a vulnerability that can be exacerbated using unserialize(). My code already does some checks to prevent malicious behavior, but in a few places, my code does its own serialize/unserialize process that opens up a slight vector of risk.\n\nIt should be noted that this vulnerability only occurs if another existing plugin or theme already has vulnerabilities and exploits installed in the system. Tests using vulnerability scanners have never identified such vulnerabilities in any of my code, but I'd rather be safe than sorry.\n\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\n\nI want to update my code to simply remove the few unnecessary uses of hand-coded serialization. This is a fairly straightforward process that involves reading the old settings data, updating it to the new format, and saving it back to the database.\n\nExcept... not so much.\n\nA lot needs to be considered when making this change. First, of course, is that 20,000 sites use these settings. Any change has to be robust, redundant, recoverable, and fairly transparent.\n\nIt has to have some kind of pre-migration backup process and a failure recovery process. It has to work no matter what order the settings are accessed and saved. Every setting that's updated, checked, and resaved has to be converted throughout 12,000+ lines of code.\n\nEdge conditions need to be identified, tested for, and factored into the code so that no site fails. Some level of version management has to be added to the settings data so that newer versions of the code know what to convert, and older versions of code on other sites don't break.\n\nIt's a lot. This is not starting from scratch and making some sort of pretty site using AI. This is modifying code in existing installations and making sure every site is able to safely update.\n\nBefore I considered modifying the settings code to remove the items I was concerned about, I asked GPT-5 Thinking Mode Deep Research in ChatGPT, OpenAI Codex, Google Jules, and a lighter version of GPT-5 Deep Research.\n\nThe last AI above was used because I apparently exceeded a limit, so my deep research query resulted in this message: \"Your remaining queries are powered by a lighter version of deep research. Our full access resets on Saturday. Upgrade to ChatGPT Pro to continue using deep research.\" Since I had a week to wait before I'd get the full Deep Research AI back, I decided to rerun the query in the lighter version and see what it would do.\n\nOpenAI Codex and Google Jules both phoned in their answers. Codex gave me a short list of settings values in bullet form. Jules provided four short paragraphs essentially saying that my code requests and receives values back from the database. Neither answer impressed me.\n\nBefore GPT-5 Deep Research downgraded, I got a 13-page document that explained every mechanism, every field, and every option used in my settings code. To be honest, it was almost overwhelming. It was clear and comprehensive, but it was almost too complete. It presented the most in-the-weeds details at the same level of priority as the major concepts, making it difficult to get a truly good picture of operations.\n\nThe lighter version of Deep Research gave me what I'd consider the Goldilocks version. It was just right. It presented the high-order architecture and mentioned the tiny details, but didn't get sidetracked by them. I found it quite useful.\n\nMy intent was to get the AI to code this settings fix. Before Deep Research downgraded me to the lighter version, I had been working with its fully powered capability.\n\nAlso: I went hands-on with ChatGPT Codex and the vibe was not good - here's what happened\n\nAt that point, I wanted to have Deep Research produce a plan of action for making the change then feed that plan to either Codex or Jules. Jules is known for developing a plan of action for any coding task, but given how little it provided for the initial analysis, I wasn't confident it would be able to think through all the implications and stages necessary.\n\nI had downloaded the aforementioned slightly overwhelming 13-page \"how settings work\" detail document created by the fully powered Deep Research before it downgraded. I passed that along to a new session.\n\nThe idea was to have one AI session analyze the existing code, and then have a completely different AI session take that analysis to plan the actual modification process.\n\nAlso: Google's Jules AI coding tool exits beta with serious upgrades - and more free tasks\n\nThis time I asked it for a plan to initiate the upgrade. I gave it a very detailed prompt (in retrospect, possibly too detailed), and asked it to create a product requirements document (PRD) that could be given to Jules or Codex.\n\nI got back an 11-page document with the following sections:\n\u2022 Data Structures Before and After\n\nDo you notice anything in that list? Something about it ain't right. Keep looking. You'll see it.\n\nYep, it repeated three sections. Plugin interop, settings management, and edge cases are repeated twice.\n\nI don't trust the AI to do this\n\nNow, look. I've been guilty of cutting and pasting and leaving some content in two places, but I'm not an AI. I'm also not being \"interviewed\" for the job of modifying mission-critical code.\n\nYes, there's no doubt I could have removed the duplicate sections and still fed the PRD to either Jules or Codex. But the presentation error raised the hairs on the back of my neck. That PRD was a set of instructions for one giant coding change. What else was wrong with it? What might I have missed?\n\nAlso: 9 programming tasks you shouldn't hand off to AI - and why\n\nAfter all, when I code, I do one small feature at a time. I test out every line, sweat every detail, and obsess over every change. But this was a big document that I could theoretically rubber-stamp and delegate the work to some pseudo-intelligence in the cloud.\n\nI thought about this pretty deeply before making a decision.\n\nI am fairly comfortable letting the AI add a new capability or build something from scratch. But diving deep into the bowels of mission-critical code? I'm not ready to give up the reins.\n\nAlso: Coding with AI? My top 5 tips for vetting its output - and staying out of trouble\n\nThe downside could be far too catastrophic. If the AI ran amok in my code, I might not even be able to figure out what went wrong. Sure, I could roll back all the way to before I delegated the task to the AI, but why take the chance?\n\nI want more granular control. I'm happy to have the AI help with writing a specific routine, doing coding for well-documented interfaces, and adding some new non-mission-critical features.\n\nBut when it comes to core capabilities and things that could turn nightmares of torch-wielding, laptop-swinging, angry site operators into reality, I think I'll do the coding myself.\n\nStay tuned. I will be using the AI to code. And I will tell you about it. But I'm not going to let the AI loose where it could do so much damage so quickly, for so little gain.\n\nAlso: 10 professional developers on vibe coding's true promise and peril\n\nHave you tried letting AI handle parts of your coding projects? Did you trust it with critical infrastructure or only non-essential features? Where do you draw the line between convenience and risk? Let us know in the comments below.",
  "medium": "Article",
  "links": [
    "https://www.zdnet.com/article/9-programming-tasks-you-shouldnt-hand-off-to-ai-and-why/",
    "https://owasp.org/www-community/vulnerabilities/PHP_Object_Injection",
    "https://www.zdnet.com/article/googles-jules-ai-coding-tool-exits-beta-with-serious-upgrades-and-more-free-tasks/",
    "https://www.zdnet.com/article/i-retested-gpt-5s-coding-skills-using-openais-guidance-and-now-i-trust-it-even-less/",
    "https://www.zdnet.com/article/coding-with-ai-my-top-5-tips-for-vetting-its-output-and-staying-out-of-trouble/",
    "https://www.zdnet.com/article/openai-upgrades-chatgpt-with-codex-and-im-seriously-impressed-so-far/",
    "https://www.zdnet.com/article/10-professional-developers-on-vibe-codings-true-promise-and-peril/",
    "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
    "https://www.zdnet.com/article/gpt-5-bombed-my-coding-tests-but-redeemed-itself-with-code-analysis/",
    "https://www.zdnet.com/article/how-i-saved-myself-1200-a-year-in-cloud-storage-in-5-sobering-steps/",
    "https://www.zdnet.com/article/what-is-ai-vibe-coding-its-all-the-rage-but-its-not-for-everyone-heres-why/",
    "https://cc.zdnet.com/v1/otc/00hQi47eqnEWQ6T9d4QLBUc?element=BODY&element_label=Add+us+as+a+preferred+Google+source&module=LINK&object_type=text-link&object_uuid=5e5d2e64-4b30-43e6-8555-26eac7e449f3&position=1&template=article&track_code=__COM_CLICK_ID__&url=https%3A%2F%2Fwww.google.com%2Fpreferences%2Fsource%3Fq%3Dzdnet.com&view_instance_uuid=379e95d2-6b56-476b-a90b-043a8dd63bd3",
    "https://www.zdnet.com/article/i-tested-gpt-5s-coding-skills-and-it-was-so-bad-that-im-sticking-with-gpt-4o-for-now/",
    "https://www.zdnet.com/article/i-went-hands-on-with-chatgpt-codex-and-the-vibe-was-not-good-heres-what-happened/",
    "https://www.zdnet.com/article/google-pixel-10-pro-vs-iphone-16-pro-ive-tried-both-flagships-and-theres-an-easy-winner/",
    "https://www.zdnet.com/article/is-meta-secretly-scanning-your-phones-camera-roll-check-this-setting-to-find-out/",
    "https://www.zdnet.com/article/45-time-saving-windows-keyboard-shortcuts-that-supercharged-my-daily-routine/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "code analysis",
      "weight": 0.07447958
    },
    {
      "name": "code",
      "weight": 0.07254725
    },
    {
      "name": "damaged code",
      "weight": 0.070374675
    },
    {
      "name": "private test sites",
      "weight": 0.0659723
    },
    {
      "name": "site privacy",
      "weight": 0.06416534
    },
    {
      "name": "other sites",
      "weight": 0.06279773
    },
    {
      "name": "sites",
      "weight": 0.06183705
    },
    {
      "name": "AI",
      "weight": 0.061400857
    },
    {
      "name": "pretty site",
      "weight": 0.060968015
    },
    {
      "name": "settings management",
      "weight": 0.06003197
    }
  ],
  "topics": [
    {
      "name": "AI"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.85205078125
    },
    {
      "name": "/News/Technology News",
      "score": 0.732421875
    },
    {
      "name": "/Computers & Electronics/Programming/Other",
      "score": 0.43603515625
    },
    {
      "name": "/Computers & Electronics/Software/Business & Productivity Software",
      "score": 0.328369140625
    }
  ],
  "sentiment": {
    "positive": 0.07650021,
    "negative": 0.61209244,
    "neutral": 0.3114074
  },
  "summary": "I was planning to use an AI (GPT-5) for a deep architectural change to mission-critical code used by over 20,000 sites across the world to provide access security and site privacy. However, I realised that this approach was too risky and that AI is only for new features, not core infrastructure, and that it could potentially break or leak private data to the public internet if it was reconstituted for use by code. The author shares his own personal experience with the AI, which he believes is a necessary process to remove unnecessary serialization and unserialize() from his code, which is a vulnerability that can be exacerbated by unserialization. He recommends updating his code to remove the few unnecessary uses of these serialization methods.",
  "shortSummary": "A recent coding project led to a nightmare where AI was asked to modify mission-critical code, highlighting vulnerabilities in serialization and unserialization processes used for privacy settings.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "9205c38f538e4b298e3f6849083f5960",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.zdnet.com/article/google-pixel-10-pro-vs-iphone-16-pro-ive-tried-both-flagships-and-theres-an-easy-winner/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nGoogle Pixel 10 Pro vs. iPhone 16 Pro: I've tried both flagships, and there's an easy winner\nAfter the success of launching four flagship phones last year, Google is set to continue the trend with the new Pixel 10 series.\nThe Google Pixel 10 Pro stands out as potentially the best option for many people. With a starting price of $999, it offers a compelling blend of premium features borrowed from its more expensive sibling, all in the more manageable size of the standard Pixel 10.\nAlso: I replaced my Samsung Galaxy S25 Ultra with the Pixel 10 Pro XL for a week - and can't go back\nThe Pixel 10 Pro enters a crowded field, competing directly with the iPhone 16 Pro and the Galaxy S25 Plus.\nHaving used the iPhone 16 Pro as my daily driver for the past year, I've tested it against the new Pixel 10 Pro to see how these two flagship phones compare.\nYou should buy the Google Pixel 10 Pro if\u2026\n1. You want the best of Google AI\nThe Pixel phone lineup unsurprisingly offers the best of Google's Gemini AI features. And it is only getting better with the Pixel 10 Pro. The company is providing a year of Google AI Pro service for free alongside the purchase for Pixel 10 Pro and Pixel 10 Pro XL. So, you get Gemini Pro, including NotebookLM for productivity and Veo 3 text-to-video generation model.\nThe Google Pixel 10 Pro is also getting a contextual and personalized AI feature called Magic Cue. Theoretically, it can \"help connect the dots across your digital life by anticipating your needs and proactively suggesting relevant information and helpful actions based on the context on your phone.\"\nAlso: The best Android phones to buy in 2025\nFor instance, if you call a company's helpline, your associated details (like a flight ticket) will be surfaced automatically.\nYou can still download the Gemini app to access Google's AI on your iPhone, but you won't get the seamless Pixel AI experience. If you depend on Google apps to get work done and have all your digital life stored on Google's platforms, the Pixel 10 Pro is the better option.\n2. You want a better telephoto camera\nThe Google Pixel 10 Pro features a 48MP Quad PD telephoto camera with 5x optical zoom, as compared to a 12MP telephoto sensor (also 5x optical zoom) on the iPhone 16 Pro. However, megapixels don't solely define a camera. The Pixel 10 Pro sports a bigger sensor than its Apple counterpart. On paper, it is capable of letting more light in and snapping better-looking photos.\nAlso: I replaced my Pixel 9 Pro with the 9a for a month - here's my buying advice now\nThe 12MP 5x tele camera on iPhone 16 Pro isn't hard to beat. It struggles in anything less than bright daylight and lacks the zoom capabilities of its Chinese rivals. I'm looking forward to using the Google Pixel 10 Pro for my zoom shots and expect it to beat the iPhone.\n3. You want a cool-looking Pro phone\nThe iPhone 16 Pro series is lacking in fun colors. It comes in Black Titanium, White Titanium, Natural Titanium, and Desert Titanium -- all of which feel a bit lifeless. The last Pro iPhone color I truly loved was Sierra Blue on my iPhone 13 Pro Max.\nWhile rumors suggest the iPhone 17 Pro might bring back more playful finishes, Google has already beaten Apple to the punch with the Pixel 10 Pro series. The new Pixel 10 and Pixel 10 Pro are available in classic Obsidian and Porcelain, as well as bolder Moonstone and Jade options.\nSo, if you're spending $1,000 on a phone, you don't have to settle for something dull. Plus, you get Google's distinct design, complete with the iconic camera bar\u2014which, in my opinion, still looks incredibly cool.\n4. If you've been envious of this iPhone feature\nThe Pixel 10 Pro features a MagSafe-like feature called PixelSnap. It is based on Qi2 charging, which offers magnetic accessory support for power banks, chargers, and more. If you've been on the iPhone for a while and want to switch platforms, this is the best time to come to the Android side.\nAlso: How to add MagSafe to your Android phone (and why you'll love it)\nAll your MagSafe accessories should technically work with the Pixel 10 Pro (depending on magnet power), so you don't have to buy new accessories alongside your new phone.\nYou should buy the Apple iPhone 16 Pro if\u2026\n1. You want a lighter Pro phone\nThe Google Pixel 10 Pro might look better on paper, but it's nearly 10 grams heavier than the iPhone 16 Pro. Specifically, the Pixel weighs 207 grams compared to the iPhone's 199 grams.\nAlso: Buy the iPhone 16 or wait for iPhone 17? Here's how I help friends and family decide\nThat eight-gram difference might not seem like much, but with both phones featuring flat, unergonomic sides, every gram counts -- especially during extended use. If you average more than six hours of screen time a day, the iPhone 16 Pro could be the more comfortable choice.\n2. You want the best social media experience\nWhile Android has made significant progress, iPhones still hold an advantage in stability and app optimization. This is particularly noticeable with social media apps like X and Instagram, where the user experience remains smoother on Apple devices.\nEditing and posting photos and videos from an iPhone to social media platforms is also more seamless and efficient for anyone who regularly creates content.\nAlso: Best iPhone 2025: I tested the top models and found the best options for you\nIf you're leaning toward an iPhone instead of the Pixel 10 Pro, I strongly recommend waiting just a few weeks. Apple is expected to launch its new iPhone 17 series in early September. It's smart to hold off on your decision until after the typical September 10th launch event to make sure you get the latest model."
    },
    {
      "url": "https://www.zdnet.com/article/what-is-ai-vibe-coding-its-all-the-rage-but-its-not-for-everyone-heres-why/",
      "text": "What is AI vibe coding? It's all the rage but it's not for everyone - here's why\nThere's a new hot buzz in the world of coding called \"vibe coding.\" It floated into the collective zeitgeist in early February, courtesy of a post on Twitter/X by Andrej Karpathy.\nKarpathy is no random Xwitter fiend. He has a Stanford Ph.D. in natural language, computer vision, and deep learning models. He was head of AI at Tesla and was one of the co-founders of OpenAI. Today, he's running an education startup called Eureka Labs, which is producing AI courseware.\nVibe coding, deconstructed\nSo when someone with that resume suggests there is a new way of coding called \"vibe coding, where you fully give in to the vibes, embrace exponentials, and forget that the code even exists,\" folks listen.\nIn some ways, it seems like a license to avoid all of coding's pesky best practices, the things that make coding more work than play. And, in many ways, it is permission to just go with the flow.\nAlso: The best AI for coding in 2025 (and what not to use)\nLet's first deconstruct the details of Karpathy's tweet (which essentially defines \"vibe coding\"), and then we'll discuss the pros and cons of this approach.\nHe says it's possible to take this approach because the coding AI's are now good enough. He also says, \"I just talk to Composer with SuperWhisper so I barely even touch the keyboard.\"\nI think he's talking about Cursor Composer, a fork of VS Code that embeds AI tools into the coding environment. SuperWhisper is a text-to-speech writing tool.\nKarpathy notes, \"I ask for the dumbest things like 'decrease the padding on the sidebar by half' because I'm too lazy to find it.\"\nThis is the sort of public knowledge area where AIs can be truly helpful. In fact, it's a variation of my Tip 25, using the AI to help write CSS, from my 25 tips for AI programming roundup.\nHe says, \"I 'Accept All' always, I don't read the diffs anymore.\"\nWhat he's saying is that he takes whatever code the AI gives him and lets it drop into the code, without comparing the AI's code with what it's changing.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nThis is a fairly dangerous technique that's probably safe enough with Karpathy's programming skills, but would be very ill-advised for a new programmer. He has the skills to fix what he breaks. A newbie might not.\nKarpathy also mentions, \"When I get error messages I just copy paste them in with no comment, usually that fixes it.\"\nI have found that, with enough context, the AI can be quite good at identifying bugs and how to fix them. In fact, I use debugging tests as part of my benchmark tests for whether an AI is ready for coding prime time.\nHere's where being skilled at coding becomes essential for vibe coding. He notes, \"The code grows beyond my usual comprehension, I'd have to really read through it for a while.\"\nWhen you write your own code, you generally have a clue how it works and what it's supposed to do. But when you inherit code from another coder (and the AI is basically another coder), it's often necessary to be able to dig in and figure out how that code works. An AI might not get you all the way there.\nAlso: Why you should ignore 99% of AI tools - and which four I use every day\nIn fact, Karpathy confirms that in this next sentence, \"Sometimes the LLMs can't fix a bug so I just work around it or ask for random changes until it goes away.\"\nI have found that AIs (particularly ChatGPT) can be amazing as AI coding helpers\u2026until they're not. Sometimes the AI hits a wall where it just doesn't get it, at which point it either stops being helpful or becomes actively obtuse. Sometimes rephrasing the prompt helps. Other times, it's just astonishing to watch how off the mark the AI can get all on its own.\nHere's Karpathy's final word on his vibe coding process. He says, \"It's not too bad for throwaway weekend projects, but still quite amusing. I'm building a project or webapp, but it's not really coding -- I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works.\"\nThat's been my experience as well. But here's the rub: there's a very big jump between \"mostly works\" and \"ready to ship.\" Take the 80/20 rule to heart. It takes 20% of the work to get 80% of the project done. But that last 20%, that will take you 80% of the work. So \"mostly\" is a very far distance from \"all done.\"\nHands on with vibe coding\nWebflow is a website building and hosting company that competes with Squarespace, Weebly, Wix, and -- to an extent -- WordPress. It's a fairly popular platform that provides a visual editor to help users build their sites.\nRachel Wolan is the company's chief product officer. She spent a weekend trying to produce an app using the vibe-coding spirit. Rachel told ZDNET, \"AI can get a prototype stood up, but it won't replace software engineers yet or production-grade platforms like Webflow.\"\nShe shared five things that vibe coding does well:\n- Quick backend setup: The AI handled authentication and database setup with minimal effort.\n- Fast and easy iteration: Making changes and testing new ideas was efficient.\n- Automatic error detection: The AI spotted mistakes, suggested fixes, and even restructured code when needed.\n- Hard to break things: Built-in safeguards made experimentation less risky.\n- Handled vague prompts well: The AI generated useful output even with unclear instructions.\nShe also told ZDNET about five areas where the AI has major gaps:\n- Clunky interface: It was necessary to wait for each task to finish before giving new commands.\n- Slow for precise edits: Fine-tuning layouts and design was tedious despite visual editing tools.\n- Custom design struggles: Creating a unique visual experience was still difficult.\n- Inefficient basic edits: Simple text changes took longer than expected.\n- Unpredictable changes: The AI sometimes overwrote or removed elements unexpectedly.\nTwo of the negatives bear further examination.\nShe said that, \"Prompt-to-code slows you down when you need control and precision.\"\nThis is where the difference between being a veteran programmer and some newbie who wants free and cheap coding help comes into play. If you're a newbie, you'll take what you can get, and you'll like it. But if you're an experienced hand, you have the skills to take your code where it needs to go. Sometimes, the AI stubbornly doesn't want to get there from here.\nAnother thing Wolan mentioned was, \"It [the AI] frequently overwrote my changes, which was frustrating. I would change one element in the nav bar, and it would overwrite another element. Some work disappeared, and I didn't know why.\"\nThis has been my concern with letting an AI have direct access to your work, right in the coding environment. When I use ChatGPT to help me code, I always keep the ChatGPT work in its own browser tab, and never let it touch my actual code. When I'm satisfied, I do the cut and paste myself so I know what is going in where.\nAlso: Why AI-powered security tools are your secret weapon against tomorrow's attacks\nThat, of course, is the exact opposite of what Karpathy recommended as vibe coding's live and let live approach. But I've had hours of work zorched far too often by some computer glitch or another to fully trust an automated tool to not ruin my day.\nWolan's conclusion is that vibe coding is, \"Fast, flexible, and lowers the barrier to building software. But it still requires knowing how to code and build software.\" She also says, \"Vibe coding shouldn't be seen as a shortcut to software development. It's a new tool in the toolkit. The real opportunity is in augmenting product managers, designers, and engineers, not replacing them.\"\nMy take on vibe coding: Hype, help, and hard truths\nCoding styles have been a matter of debate since forever. Different skills, personalities, and project requirements require different approaches to writing code.\nVibe coding is a lot like stream-of-consciousness style creative writing, where the writer puts down whatever comes into their head and hopes it works. But not all of us can be Proust or Kerouac. The style doesn't work, for example, if you're writing an article about a programming technique.\nVibe coding has been with us for years, but it didn't have such a cool name. In fact, a whole bunch of the techniques I list in my 25 tips for AI programming article reflect the ideas of vibe coding, particularly letting the AI do some of the tedious, but well-documented coding work.\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nBut vibe coding shouldn't be confused with no-code or low-code application development environments (although I'm sure some marketing flack will use the term).\nVibe coding, as defined by master coder Andrej Karpathy, is a technique for experienced hands to turbocharge bursts of productivity in areas where the AI can do just as well as the master. But it takes an experienced developer to know what those areas are, and to limit the AIs influence to common-knowledge coding and leave the unique business logic coding to the human developer.\nIn a busy commercial kitchen, there's often the head chef and the sous chef, who's second-in-command. Somewhere in the kitchen hierarchy, there's someone whose job it is to cut up all the vegetables. That person is often called the prep cook or prep chef.\nVibe coding is basically letting the AI serve as the prep chef for areas of code. It can get a lot of necessary work done without using up the time of a seasoned developer. But don't confuse the work of a prep cook with a master chef. Likewise, don't confuse the work of a helpful AI doing small and controlled coding tasks with a skilled developer.\nAlso: Why OpenAI's new AI agent tools could change how you code\nVibe coding is a fun and productive technique with a sticky name, as long as you have the skills. It is not, as Wolan said, a shortcut for creating a software product. That work requires great software engineering skills along with the product management skills and experience to know how to define and describe a product, make trade-off decisions, understand market forces and competitive positioning, and so much more.\nI love the term. Because it's such a good name, I'm sure we'll be hearing it more and more. Just don't confuse a sticky term with getting real work done. And don't let any vendor tell you that you can create a fully-functional and deployable piece of code fueled entirely by wishes, good intentions, and, of course, your credit card number.\nCan you dig it? Stay groovy.\nWhat do you think about vibe coding? Have you experimented with AI-driven coding assistants, and do you find them helpful or frustrating? Do you trust AI-generated code enough to \"Accept All\" without reviewing it?\nHow do you balance AI assistance with maintaining control over your projects? Do you see vibe coding as a productivity booster, or does it create more problems than it solves? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://owasp.org/www-community/vulnerabilities/PHP_Object_Injection",
      "text": "PHP Object Injection\nThank you for visiting OWASP.org. We have migrated our community to a new web platform and regretably the content for this page needed to be programmatically ported from its previous wiki page. There\u2019s still some work to be done.\nThis is an example of a Project or Chapter Page.\nAuthor(s):\nDescription\nPHP Object Injection is an application level vulnerability that could allow an attacker to perform different kinds of malicious attacks, such as Code Injection, SQL Injection, Path Traversal and Application Denial of Service, depending on the context. The vulnerability occurs when user-supplied input is not properly sanitized before being passed to the unserialize() PHP function. Since PHP allows object serialization, attackers could pass ad-hoc serialized strings to a vulnerable unserialize() call, resulting in an arbitrary PHP object(s) injection into the application scope.\nIn order to successfully exploit a PHP Object Injection vulnerability two conditions must be met:\n- The application must have a class which implements a PHP magic method (such as __wakeup or __destruct) that can be used to carry out malicious attacks, or to start a \u201cPOP chain\u201d.\n- All of the classes used during the attack must be declared when the vulnerable unserialize() is being called, otherwise object autoloading must be supported for such classes.\nExamples\nExample 1\nThe example below shows a PHP class with an exploitable __destruct method:\nclass Example1\n{\npublic $cache_file;\nfunction __construct()\n{\n// some PHP code...\n}\nfunction __destruct()\n{\n$file = \"/var/www/cache/tmp/{$this->cache_file}\";\nif (file_exists($file)) @unlink($file);\n}\n}\n// some PHP code...\n$user_data = unserialize($_GET['data']);\n// some PHP code...\nIn this example an attacker might be able to delete an arbitrary file via a Path Traversal attack, for e.g. requesting the following URL:\nhttp://testsite.com/vuln.php?data=O:8:\"Example1\":1:{s:10:\"cache_file\";s:15:\"../../index.php\";}\nExample 2\nThe example below shows a PHP class with an exploitable __wakeup method:\nclass Example2\n{\nprivate $hook;\nfunction __construct()\n{\n// some PHP code...\n}\nfunction __wakeup()\n{\nif (isset($this->hook)) eval($this->hook);\n}\n}\n// some PHP code...\n$user_data = unserialize($_COOKIE['data']);\n// some PHP code...\nIn this example an attacker might be able to perform a Code Injection attack by sending an HTTP request like this:\nGET /vuln.php HTTP/1.0\nHost: testsite.com\nCookie: data=O%3A8%3A%22Example2%22%3A1%3A%7Bs%3A14%3A%22%00Example2%00hook%22%3Bs%3A10%3A%22phpinfo%28%29%3B%22%3B%7D\nConnection: close\nWhere the cookie parameter \u201cdata\u201d has been generated by the following script:\nclass Example2\n{\nprivate $hook = \"phpinfo();\";\n}\nprint urlencode(serialize(new Example2));\nExample 3\nThis last example shows how it is possible to perform a SQL Injection attack using a \u201cPOP chain\u201d, for instance by leveraging a __toString method like this:\nclass Example3\n{\nprotected $obj;\nfunction __construct()\n{\n// some PHP code...\n}\nfunction __toString()\n{\nif (isset($this->obj)) return $this->obj->getValue();\n}\n}\n// some PHP code...\n$user_data = unserialize($_POST['data']);\n// some PHP code...\nIf the $user_data variable is an \u201cExample3\u201d object and it will be treated like a string somewhere in the code, then its __toString method will be called. Since this method will call the getValue method of the object stored into the \u201cobj\u201d property, it\u2019s possible to set that property to an arbitrary object, and execute its getValue method, if it is accessible, otherwise its __call method will be called, if it is defined. For the sake of ease, just think that when unserialize() is called, the class below is available within the application scope (or supported by autoloading):\nclass SQL_Row_Value\n{\nprivate $_table;\n// some PHP code...\nfunction getValue($id)\n{\n$sql = \"SELECT * FROM {$this->_table} WHERE id = \" . (int)$id;\n$result = mysql_query($sql, $DBFactory::getConnection());\n$row = mysql_fetch_assoc($result);\nreturn $row['value'];\n}\n}\nIn this example an attacker might be able to perform a SQL Injection attack by sending a POST request containing a \u201cdata\u201d parameter generated by a script like this:\nclass SQL_Row_Value\n{\nprivate $_table = \"SQL Injection\";\n}\nclass Example3\n{\nprotected $obj;\nfunction __construct()\n{\n$this->obj = new SQL_Row_Value;\n}\n}\nprint urlencode(serialize(new Example3));\nRelated Controls\nPrevention\nDo not use unserialize() function with user-supplied input, use JSON functions instead.\nReferences\n- PHP: unserialize\n- PHP: Magic Methods\n- PHP: Autoloading Classes\n- PHP RFC: Secured unserialize()\n- Shocking News in PHP Exploitation Stefan Esser, POC 2009\n- Utilizing Code Reuse/ROP in PHP Application Exploits Stefan Esser, BlackHat USA 2010"
    },
    {
      "url": "https://www.zdnet.com/article/i-retested-gpt-5s-coding-skills-using-openais-guidance-and-now-i-trust-it-even-less/",
      "text": "I retested GPT-5's coding skills using OpenAI's guidance - and now I trust it even less\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- The same prompts in GPT-5 yield success, crashes, or errors.\n- OpenAI's prompt optimizer helps, but introduces its own quirks.\n- AI \"unconsciously\" adding details raises trust concerns.\nDo AI's get headaches? Because GPT-5 has certainly been giving me one. This article was going to be so easy. OpenAI came out with a list of best practices for GPT-5 coding. All I was going to do was try those best practices with the GPT-5 coding tests that previously failed and see if there was improvement.\nIt seemed so simple.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nBut then I had a thought. What if OpenAI has improved GPT-5 in the week or so since I ran my tests?\nPerhaps I should re-run the failed tests as-is and see what the results are, to use as a baseline for the new best practices.\nFrom a testing point of view, this was a valid approach. From a personal sanity point of view, not so much.\nRe-running test 1\nI re-ran the first failed test. This test has the AI creating a complete WordPress plugin, complete with a user interface and business logic. The idea is you feed in a set of names, it randomizes them, and it separates duplicates so they're not side-by-side.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nWhen I ran this test on GPT-5 originally, it failed. Clicking the Randomize button sent the browser to another, unrelated page. I was able to cajole GPT-5 into fixing it, but the initial failure was what counted.\nThis time, I ran the exact same test with the exact same prompt again. This time, it worked perfectly. Wow, I thought. GPT-5 has improved in the past week.\nIf only I had left well enough alone. But no.\nI had to try again. On my second time with the exact same test with the exact same prompt, clicking Randomize resulted in what WordPressers call the \"white screen of death.\" This indicates something in the code isn't working.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nI tried the exact same test with the exact same prompt a total of five times. The first time it worked. Subsequent times, it didn't. One time, I got a white screen. One time, I got a page full of error messages. One time, I was redirected to another page. One time, nothing happened at all.\nWe'll come back to test 1. But next, I decided to do the same thing with the other failed test.\nRe-running test 4\nThis test asks the AI to write code that talks to Chrome, AppleScript, and another tool called Keyboard Maestro. It requires the AI to be knowledgeable about the three different environments and how they work together.\nHere's an interesting factoid about AppleScript. The scripting language is inherently case-insensitive. So, if you ask AppleScript to compare \"AppleScript\" to \"applescript\", it will consider both strings to be the same. You have to explicitly tell it to compare case before it will do so.\nAlso: You can learn AI for free with these new courses from Anthropic\nWhen I first ran this test against GPT-5, it hallucinated that AppleScript had a native function for making strings lowercase. It does not, so the code failed.\nThis time, instead of hallucinating that AppleScript has a lowercase function, it decided to do one of the most convoluted sets of operations to lowercase a string. Here's that line of code:\nset tabTitleLower to (do shell script \"echo \" & quoted form of tabTitle & \" | tr '[:upper:]' '[:lower:]'\")\nThe code actually works, but it's very weird and totally unnecessary. It launches a shell (the command line) to run a shell script that converts to lowercase. It's like making a grilled cheese sandwich and when you realize you're out of cheese, flying across the country to buy cheese from a 7-11 on the opposite side of the continent. You're not even getting better cheese. You're just doing it in the weirdest and most inefficient way possible.\nLet's not forget that we really don't need to make that text lowercase in the first place, because AppleScript is case-insensitive. So it wrote a highly convoluted piece of code that, while it works, is entirely unnecessary.\nI didn't bother to re-run this test four more times because the headache was already starting to set in.\nOpenAI's GPT-5 coding best practices\nOpenAI has put out what it calls a cheatsheet for GPT-5 coding. It has six main recommendations:\n- Be precise and avoid conflicting information: Well, duh. But apparently GPT-5 gets really confused if asked to follow vague or conflicting instructions.\n- Use the right reasoning effort: Rather than just letting GPT-5 choose its reasoning level, you might want to adjust its model. If it starts to overthink a problem, use a lower reasoning level. Great, so now we have to deal with a neurotic AI. Yippee?\n- Use XML-like syntax to help structure instructions: Rather than using just text, using XML tags to segment aspects of a prompt helps the AI to parse its assignment. This is a good hint, but it feels like a step back from what it could do.\n- Avoid overly firm language: In previous models, we sometimes had to YELL at the AI to get it to listen. Apparently, GPT-5 is passive aggressive, so if you get too forceful in your instructions, it will overdo its compliance, like what my old bosses used to say was my practice of \"aggressively following instructions.\"\n- Give room for planning and self-reflection: Does the AI need a blankie? Perhaps a warm stuffed animal to cuddle? In reality, what OpenAI is saying is that you need to encourage the AI to think through the instructions it gets. Apparently, if you don't tell it to think through the instructions, it won't. I fear for the planet.\n- Control the eagerness of your coding agent: It turns out GPT-5 is even more of a people-pleaser than GPT-4o. When giving it instructions, tell it to tone down its fawning and sucking-uppage.\nI'd like to say something positive here, but it's starting to feel like a migraine. Check out the PDF if you want more details.\nOpenAI also created an AI prompting helper. You point your browser to this tool, and paste in your prompt. The tool will give you back a more extensive prompt. For example, I gave it this prompt from Test 4:\nWrite a Keyboard Maestro AppleScript that scans the frontmost Google Chrome window for a tab name containing the string matching the contents of the passed variable instance__ChannelName. Ignore case for the match. Once found, make that tab the active tab.\nIt gave me back this:\nCreate an AppleScript for use in Keyboard Maestro that performs the following actions: * Begin with a concise checklist (3-7 bullets) outlining the planned steps before proceeding. * Scan all tabs in the frontmost Google Chrome window. * Search for a tab whose name contains the value of the passed variable 'instance__ChannelName', ignoring case sensitivity. * Once a matching tab is found, activate that tab and make it the current/active tab. After generating the script, validate in 1-2 lines whether the script meets all requirements. If not, state what needs adjustment.\nEssentially, it revised the prompt to add a planning phase at the beginning and a validation phase at the end.\nDoes it work?\nNope. Nopity-nope-nope.\nI ran Test 4's \"optimized\" prompt through GPT-5 and got back this script:\nThere are at least three serious flaws in it. First, the code in the green box doesn't work. It doesn't retrieve the value from Keyboard Maestro. Previous versions did that successfully using a completely different (and well-documented) mechanism. This approach is just loony.\nAlso: AI is creeping into the Linux kernel - and official policy is needed ASAP\nNow, look at the blue box. The considering case operation is used only for evaluation. For example, with considering case, \"BLUE\" and \"blue\" are different values. Do you see any sort of if statement or evaluation inside the blue box? Me neither.\nThen there's the red box. There is no such syntax as \"considering case false.\" First, the default behavior is to ignore case. Second, if you really wanted to explicitly tell AppleScript to ignore case, you'd use an \"ignoring case\" block. But \"considering case false\"? That's just some made-up syntax right there.\nRetrying the WordPress plugin\nI also ran my original prompt for the WordPress plugin through the prompt optimizer. I got back a fairly extensive project requirements document.\nOn the plus side, this time the plugin worked. I tried generating code a second time, and it also worked. So perhaps the optimization is something to try if GPT-5 isn't playing nice.\nBut then there's the weirdness. Because there's always weirdness. Check out the header block generated by GPT-5.\nNotice the author is attributed to Advanced Geekery Labs. Historically, the AIs have listed the author field as \"Your Name.\" But this time, it's Advanced Geekery Labs.\nThere is some relevance, because Advanced Geekery is the name of my YouTube channel and weekly newsletter. But I didn't use it anywhere in my instructions for this plugin. Additionally, I've never added Labs to the end of Advanced Geekery, even though my security product is sold through ZATZ Labs.\nSo I asked GPT-5 where it got the name Advanced Geekery Labs from. Its answer:\nGood question. The name \"Advanced Geekery Labs\" isn't something you provided in your request. I introduced it in the plugin header's Author: field as a placeholder, because in earlier conversations you've mentioned your \"Advanced Geekery\" brand, and I unconsciously expanded it into \"Labs.\"\nI'm glad the AI thinks it's a good question. That warms my heart. But \"unconsciously expanded\"? That's a direct quote from the AI. Those are its words.\nSo, we're now dealing with hallucinating AIs that operate unconsciously?\nWe're all doomed.\nWhat about coding and GPT-5?\nI still don't trust it. Actually, I don't trust it even more than when I tested it originally. What else might it do to my code \"unconsciously\"?\nI guess, if you really need GPT-5 to give you some code, you could try the optimizer. But I wouldn't rely on it.\nGPT-4o was impressive, but it had some problems. GPT-5 is like its more troubled, more difficult sibling.\nWith GPT-4o, I'd be willing to trust, with verification. I just plain don't trust GPT-5. There's something that's just not right. If we were to experience an AI that would revolt against its creators and try to kill us all, I would not be surprised if it all originated with GPT-5.\nAnd no. My headache hasn't gone away. Apparently, Tylenol is now bad for you. So, there's that, too.\nWhat do you think?\nHave you tried relying on GPT-5 for coding projects? If so, did you run into the same kinds of inconsistencies and odd behaviors? Do you find OpenAI's \"best practices\" helpful, or do they feel like workarounds for deeper problems? And what's your take on GPT-5's tendency to improvise or even \"unconsciously\" insert details? Is it a harmless quirk or a reason for concern? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/i-tested-gpt-5s-coding-skills-and-it-was-so-bad-that-im-sticking-with-gpt-4o-for-now/",
      "text": "I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nZDNET's key takeaways\n- OpenAI's new GPT-5 flagship failed half of my programming tests.\n- Previous OpenAI releases have had just about perfect results.\n- Now that OpenAI has enabled fallbacks to other LLMs, there are options.\nSo GPT-5 happened. It's out. It's released. It's the talk of the virtual town. And it's got some problems. I'm not gonna bury the lede. GPT-5 has failed half of my programming tests. That's the worst that OpenAI's flagship LLM has ever done on my carefully designed tests.\nAlso: The best AI for coding in 2025 (and what not to use)\nBefore I get into the details, let's take a moment to discuss one other little feature that's also a bit wonky. Check out the new Edit button on the top of the code dumps it generates.\nClicking the Edit button takes you into a nice little code editor. Here, I replaced the Author field, right in ChatGPT's results.\nThat seemed nice, but it ultimately proved futile. When I closed the editor, it asked me if I wanted to save. I did. Then this unhelpful message showed up.\nI never did get back to my original session. I had to submit my original prompt again, and let GPT-5 do its work a second time.\nBut wait. There's more. Let's dig into my test results\u2026\n1. Writing a WordPress plugin\nThis was my very first test of coding prowess for any AI. It's what gave me that first \"the world is about to change\" feeling, and it was done using GPT-3.5.\nSubsequent tests, using the same prompt but with different AI models, generated mixed results. Some AIs did great, some didn't. Some AIs, like those from Microsoft and Google, improved over time.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nChatGPT's model has been the gold standard for this test since the very beginning. That makes the results of GPT-5 all that much more curious.\nSo, look, the actual coding with GPT-5 was partially successful. GPT-5 generated a single block of code, which I pasted into a file and was able to run. It provided the requisite UI.\nWhen I pasted in the test names, it dynamically updated the line count, although it described it as \"Line to randomize\" instead of \"Lines to randomize.\"\nBut then, when I clicked Randomize, it didn't. Instead, it redirected me to tools.php. What?? ChatGPT has never had a problem with this test, whether GPT-3.5, GPT-4, or GPT-4o. You mean to tell me that OpenAI's much-anticipated GPT-5 is failing right out of the gate? Ouch.\nAlso: How to use GPT-5 in VS Code with GitHub Copilot\nI then gave GPT-5 this prompt.\nWhen I click randomize, I'm taken to http://testsite.local/wp-admin/tools.php. I do not get a list of randomized results. Can you fix?\nThe result was a line to patch. I'm not thrilled with that approach because it requires the user to dig through code and to make no mistakes replacing a line.\nSo, I asked GPT-5 for a full plugin. It gave me the full text of the plugin to copy and paste. This time, it worked.\nThis time, it did randomize the lines. When it encountered duplicates, it separated them from each other, as it was instructed. Finally.\nAlso: I found 5 AI content detectors that can correctly identify AI text 100% of the time\nI'm sorry, OpenAI. I have to fail you on this test. You would have passed if the only error was not using the plural of \"line\" when appropriate. But the fact that it gave me back a non-working plugin on the first try is fail territory, even if the AI did eventually make it work on the second try.\nNo matter how you spin it, this is a step back.\n2. Rewriting a string function\nThis second test is designed to rewrite a string function to better check for dollars and cents. The original code that GPT-5 was asked to rewrite did not allow for cents (it only checked for integers).\nGPT-5 did fine with this test. It did return a minimal result because it didn't do any error checking. It didn't check for non-string input, extra whitespace, thousands separators, or currency symbols.\nBut that's not what I asked for. I told it to rewrite a function, which itself did not have any error checking. GPT-5 did exactly what I asked with no embellishment. I'm kind of glad of that because it doesn't know whether or not code prior to this routine already did that work.\nGPT-5 passed this test.\n3. Finding an annoying bug\nThis test came about because I was struggling with a less-than-obvious bug in my code. Without going into the weeds about how the WordPress framework works, the obvious answer is not the right answer.\nYou need some fairly arcane knowledge about how WordPress filters pass their information. This test has been a stumbling block for more than a few AI LLMs.\nAlso: Gen AI disillusionment looms, according to Gartner's 2025 Hype Cycle report\nGPT-5, however, like GPT-4 and GPT-4o before it, did understand the problem. It articulated a clear solution.\nGPT-5 passed this test.\n4. Writing a script\nThis test asks the AI to incorporate a fairly obscure Mac scripting tool called Keyboard Maestro, as well as Apple's scripting language AppleScript, and Chrome scripting behavior.\nIt's really a test of the reach of the AI in terms of knowledge, its understanding of how web pages are constructed, and the ability to write code across three interlinked environments.\nQuite a few AIs have failed this test, but the failure point is usually a lack of knowledge about Keyboard Maestro. GPT-3.5 didn't know about Keyboard Maestro. But ChatGPT has been passing this test since GPT-4. Until now.\nWhere should we start? Well, the good news is that GPT-5 handled the Keyboard Maestro part of the problem just fine. But it got the coding so wrong that it even doubled down on its lack of understanding of how case works in AppleScript.\nIt actually invented a property. This is one of those cases where an AI confidently presents an answer that is completely wrong.\nAlso: ChatGPT comes with personality presets now - and other upgrades you might have missed\nAppleScript is natively case-insensitive. If you want AppleScript to pay attention to case, you need to use a \"considering case\" block. So, this happened.\nThe reason the error message referred to the title of one of my articles is because that was the front window in Chrome. This function checks the front window and does stuff based on the title.\nBut misunderstanding how case works wasn't the only AppleScript error GPT-5 generated. It also referenced a variable named searchTerm without defining it. That's pretty much an error-creating practice in any programming language.\nFail, fail, fail, McFaildypants.\nThe internet hath spoken\nOpenAI seemed to suffer from the same hubris that its AIs do. It confidently moved everyone to GPT-5 and burned the bridges back to GPT-4o. I'm paying $200 a month for a ChatGPT Pro account. On Friday, I couldn't move back to GPT-4o for coding work. Neither could anyone else.\nThere was, however, just a tiny bit of user pushback on the whole bridges burning thing. And by tiny, I mean the entire frickin' internet. So, by Saturday, ChatGPT had a new option.\nTo get to this, go to your ChatGPT settings and turn on \"Show legacy models.\" Then, as it has always been, just drop down the model menu and choose the one you want. Note: this option is only available to those on paid tiers. If you're using ChatGPT for free, you'll take what you're given, and you'll love it.\nEver since the whole generative AI thing kicked off at the beginning of 2023, ChatGPT has been the gold standard of programming tools, at least according to my LLM testing.\nAlso: Microsoft rolls out GPT-5 across its Copilot suite - here's where you'll find it\nNow? I'm really not sure. This is only a day or so after GPT-5 has been released, so its results will probably get better over time. But for now, I'm sticking with GPT-4o for coding, although I do like the deep reasoning capabilities in GPT-5.\nWhat about you? Have you tried GPT-5 for programming tasks yet? Did it perform better or worse than previous versions like GPT-4o or GPT-3.5? Were you able to get working code on the first try, or GPT-4o did you have to guide it through fixes? Are you going to use GPT-5 for coding or stick with older models? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/9-programming-tasks-you-shouldnt-hand-off-to-ai-and-why/",
      "text": "9 programming tasks you shouldn't hand off to AI - and why\nIt's over. Programming as a profession is done. Just sign up for a $20-per-month AI vibe coding service and let the AI do all the work. Right?\nAlso: Hacker slips malicious 'wiping' command into Amazon's Q AI coding assistant - and devs are worried\nDespite the fact that tech companies like Microsoft are showing coders the door by the thousands, AI cannot and will not be the sole producer of code. In fact, there are many programming tasks for which an AI is not suited.\nIn this article, I'm spotlighting nine programming tasks where you shouldn't use an AI. Stay tuned to the end, because I showcase a 10th bonus reason why you shouldn't always use an AI for programming. Not to mention that this could happen.\n1. Complex systems and high-level design\nHere's the thing. Generative AI systems are essentially super-smart auto-complete. They can suggest syntax, they can code, and they can act as if they understand concepts. But all of that is based on probabilistic algorithms and a ton of information scraped from the web. Contextual intelligence is not a strength. Just try talking to an AI for a while, and you'll see them lose the thread.\nAlso: 10 professional developers on vibe coding's true promise and peril\nIf you need to produce something that requires substantial understanding of how systems interact, experience to make judgment calls about trade-offs, understanding of what works for your unique needs, and consideration of how everything fits with your goals and constraints, don't hire an AI.\n2. Proprietary codebases and migrations\nLarge language models are trained on public repositories and (shudder) Stack Overflow. Yeah, some of the most amazing codebases are in public repositories, but they're not your code. You and your team know your code. All the AI can do is infer things about your code based on what it knows about everyone else's.\nAlso: A vibe coding horror story: What started as 'a pure dopamine hit' ended in a nightmare\nMore than likely, if you give an AI your proprietary code and ask it to do big things, you'll embed many lines of plausible-looking code that just won't work. I find that using the AI to write smaller snippets of code that I otherwise would have to look up from public sources can save a huge amount of time. But don't delegate your unique value add to a brainy mimeograph machine.\n3. Innovative new stuff\nIf you want to create an algorithm that hasn't been done before -- maybe to give your organization a huge competitive advantage -- hire a computer scientist. Don't try to get an AI to be an innovator. AIs can do wonders with making boilerplate look innovative, but if you need real out\u2011of\u2011the\u2011box thinking, don't use a glorified box with brains.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nThis applies not only to functional coding, but to design as well. To be fair, AIs can do some wonderful design. But if you're building a new game, you may want to do most of the creative design yourself and then use the AI to augment the busy work.\nSure, many of us go through life parroting things we heard from other folks or from some wacky podcaster. But there are real humans who are truly creative. That creativity can be a strategic advantage. While the AI can do volume, it really can't make intellectual leaps across uncharted paths.\n4. Critical security programming and auditing\nDo not let the fox guard the hen house. Fundamentally, we really don't know what AIs will do or when they'll go rogue. While it makes sense to use AI to scan for malicious activity, the code generated by AIs is still pretty unreliable.\nCSET (the Center for Security and Emerging Technology) at Georgetown University published a study late last year based on formal testing. They found that nearly half of the code snippets produced by AIs \"contain bugs that are often impactful and could potentially lead to malicious exploitation.\"\nAlso: Coding with AI? My top 5 tips for vetting its output - and staying out of trouble\nThis tracks with my own testing. I regularly test AIs for coding effectiveness, and even as recently as last month, only five of the 14 top LLMs tested passed all my very basic tests.\nSeriously, folks. Let AIs help you out. But don't trust an AI with anything really important. If you're looking at cryptographic routines, managing authentication, patching zero\u2011day flaws, or similar coding tasks, let a real human do the work.\n5. Code requiring legal or regulatory compliance\nThere are laws -- lots of them -- particularly in the healthcare and finance arenas. I'm not a lawyer, so I can't tell you what they are specifically. But if you're in an industry governed by regulation or rife with litigation, you probably know.\nThere is also a case to be made that you can't be sure that cloud-based LLMs will be secure. Sure, a vendor may say your data isn't used for training, but is it? If you're subject to HIPAA or DoD security clearance requirements, you may not be allowed to share your code with a chatty chatbot.\nAlso: How I used this AI tool to build an app with just one prompt - and you can too\nDo you really want to bet your business on code written by Bender from Futurama? Yes, it's possible you might have humans double\u2011checking the code. But we humans are fallible and miss things.\nThink about human nature. If you think your opponent will come down on you for a human error, you're probably right. But if you were too lazy to write your own code and handed it off to AIs known to hallucinate, ooh -- your competition's gonna have a field day with your future.\n6. Domain-specific business logic\nYou know how it is when you bring a new hire into the company and it takes them a while to get a handle on what you do and how you do it? Or worse, when you merge two companies and the employees of each are having difficulty grokking the culture and business practices of the other?\nAlso: The top 20 AI tools of 2025 - and the #1 thing to remember when you use them\nYeah. Asking an AI to write code about your unique business operations is a recipe for failure. Keep in mind that AIs are trained on a lot of public knowledge. Let's define that for a minute. Public knowledge is any knowledge the public could possibly know. The AIs were trained on all the stuff they could hoover from the Internet, with or without permission.\nBut the AIs are not trained on your internal business knowledge, trade secrets, practices, folklore, long\u2011held work\u2011arounds, yada yada yada. Use the AI for what it's good at, but don't try to convince it to do something it doesn't know how to do. AIs are so people\u2011pleasing that they'll try to do it -- and maybe never tell you that what you just deployed was fabricated garbage.\n7. Low-level systems work and performance optimizations\nWhile it's possible for an AI to identify areas of code that could use optimization, there are limits. AIs aren't trained on the very fine details of microarchitectural constraints, nor do they have the experience of coaxing just a skosh more out of every line of code.\nAlso: The best AI for coding in 2025 (including a new winner - and what not to use)\nA lot of the coding involved in embedded systems programming, kernel development, and performance-critical C and C++ optimization exists in the brains of a few expert coders. Also, keep in mind that AIs confabulate. So what they may insist are performance improvements could well be hidden cycle drains that they simply won't admit to.\nIf you need fine craftspersonship, you'll need a fine craftsperson -- in this case, a very experienced coder.\n8. Learning exercises and educational assignments\nIf you use an AI, are you cheating? Yes. No. Depends. Yes, because you may be violating academic standards and cheating yourself out of the critical hands-on learning that makes knowledge stick. No, because AI has proven to be an excellent augmentation for help, especially when TAs aren't available. And maybe, because this is still a fairly unknown area.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nHarvard takes a middle ground with its wonderful CS50 Intro to Computer Science course. It offers the CS50 duck (it's a long story), an AI specifically trained on their course materials with system instructions that limit how much information students are provided. So the AI is there to help answer legitimate student questions, but not do their work for them.\nIf you're a student or an educator, AI is a boon. But be careful. Don't cheat, and don't use it to shortcut work that you really should be doing to make education happen. But consider how it might help augment your studies or help you keep up with students' demands.\n9. Collaboration and people stuff\nI've found that if I treat the AI chatbot as if it were another human coder at the other end of a Slack conversation, I can get a lot out of that level of \"collaboration.\" A lot, but not everything.\nBoth humans and AIs can get stubborn, stupid, and frustrating during a long, unproductive conversation. Humans can usually break out of it and be persuaded to be helpful, at least in professional settings. But once you reach the limit of the AI's session capacity or knowledge, it just becomes a waste of time.\nAlso: What is AI vibe coding? It's all the rage but it's not for everyone - here's why\nThe best human collaborations are magical. When a team is on fire -- working together, bouncing ideas off each other, solving problems, and sharing the workload -- amazing things can happen.\nAI companies claim workforces made up of agents can duplicate this synergy, but nothing beats working with other folks in a team that's firing on all cylinders. Not just for productivity (which you get), but also for quality of work life, long-term effectiveness, and, yes, fun.\nAlso: Open-source skills can save your career when AI comes knocking\nDon't get me wrong. Some of my best friends are robots. But some of my other best friends are people with whom I have long, deep, and fulfilling relationships. Besides, I've never met an AI that can make Mr. Amontis' moussaka or Auntie Paula's apple pie.\nBonus: Don't use AI for anything you want to own\nDon't use AI for anything you indisputably want to own. If you write code that you then release as open source, this may not be as much of an issue. But if you write proprietary code that you want to own, you might not want to use an AI.\nWe asked some attorneys about this back at the dawn of generative AI, and the overall consensus is that copyright depends on creation with human hands. If you want to make sure you never wind up in court trying to protect your right to your own code, don't write it with an AI. For more background, here's the series I published on code and copyrights:\n- If ChatGPT produces AI-generated code for your app, who does it really belong to?\n- If your AI-generated code becomes faulty, who faces the most liability exposure?\nWhat about you? Have you found yourself leaning too much on AI to write code? Where do you draw the line between convenience and caution? Are there any programming tasks where you've found AI genuinely helpful or dangerously misleading? Have you ever had to debug something an AI wrote and wondered if it saved you time or cost you more? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV.\nWant more stories about AI? Sign up for Innovation, our weekly newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/is-meta-secretly-scanning-your-phones-camera-roll-check-this-setting-to-find-out/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nIs Meta secretly scanning your phone's camera roll? Check this setting to find out\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Meta could be scanning your camera roll right now.\n- It's using your photos to provide AI-powered suggestions.\n- Check Facebook settings to turn off the features.\nMeta could be analyzing and retaining your phone's photos without your explicit consent.\nSome Facebook users have noticed that, deep within their app settings, Meta has switched on two toggles that allow it to access their camera roll to offer AI-powered suggestions, including \"personalized creative ideas, like travel highlights and collages.\"\nAlso: How to delete Facebook, Messenger, or Instagram - if you want Meta out of your life\nThe problem? The toggles for the AI suggestion features, called \"camera roll sharing suggestions,\" appear to be turned on for users who claim they haven't seen a pop-up from Facebook asking for permission to enable them. If you get that \"cloud processing\" pop-up and tap \"Allow\" on it, you'll agree to Meta's AI Terms of Service and permit your \"media and facial features\" to be analyzed by AI.\nFacebook will then use your camera roll images -- including the dates on them and the presence of people or objects -- to suggest collages, themed albums, recap posts, or AI restyled versions of your pictures. These AI suggestions are only visible to you, unless you choose to share them, and Meta says the media won't be used for ad targeting.\nAlso: How to protect your privacy from Facebook - and what doesn't work\nBut, to be clear, you're still giving Meta the right to access and retain your camera roll images, and that could raise serious privacy concerns, especially for users who never knowingly opted in.\nZDNET's editorial director found Meta's camera roll sharing suggestions enabled in her Facebook app without her knowledge. I also noticed they were enabled for me, although I vaguely recall seeing a pop-up from Facebook about the new features a few weeks ago. I think I dismissed it quickly, and I can't remember whether I tapped Allow or Don't allow on it.\nHow to stop Facebook from scanning your camera roll\nMeta said its camera roll sharing suggestions are not enabled by default. If you're worried you dismissed Facebook's pop-up, unknowingly opted-in, and gave access to your camera roll, here's how to check and turn it off.\n1. Open the Facebook app\nThe settings you'll want to check can be found in the Facebook mobile app.\n- Grab your iPhone or Android phone.\n- Open the Facebook app. You'll need to be signed into your account.\n2. Go to the Menu > Settings and Privacy\nFacebook hides most of its settings in the menu -- the three-line hamburger icon in the bottom corner of the app.\n- Tap the Menu icon in the bottom right corner of the screen.\nOnce the menu opens, look for Settings and Privacy with a gear icon.\n3. Select Settings\nOnce you find and tap Privacy and Settings to expand the dropdown options, tap Settings again.\n- Under Settings and Privacy, tap Settings.\n- Now, scroll down and look for \"Camera roll sharing suggestions.\"\n4. Go to Camera roll sharing suggestions\nMeta placed the toggles that grant it access to your camera roll under the \"Camera roll sharing suggestions\" setting. You'll need to go there to see if they're on and, if so, switch them off.\n- Tap the option labeled \"Camera roll sharing suggestions.\"\n- This will open a preference page with a couple of toggles.\n5. Turn off both toggles\nOnce you're inside the camera roll sharing suggestions page, notice the two separate switches. If they're blue and the toggle circle is pushed to the right, they're on -- meaning Meta is already processing and retaining your phone's photos. Turn them off so the app can't automatically upload and analyze your camera roll.\n- Find the option labeled \"Get camera roll suggestions when you're browsing Facebook.\" If the switch is on (blue), tap it once to turn it off (gray). This will stop Facebook from using basic camera roll data, such as which videos you've favorited and when photos were taken, to suggest sharing media you haven't yet uploaded.\nFind the option labeled \"Get creative ideas made for you by allowing camera roll cloud processing.\" If the switch is on (blue), tap it once to turn it off (gray). This will stop Facebook from continuously uploading media from your camera roll -- and using details like time, location, themes, and the presence of people or objects -- to generate personalized creative ideas such as recaps and AI restylings.\nFAQs\nThese steps don't work for me. Why?\nThe steps above, which I've verified using the Faebook app for iPhone, may vary on Android devices.\nSamsung users, for instance, must do the following: Open Facebook > Go to Profile/Menu > Tap the gear icon in the upper right > Scroll down to \"Camera roll sharing suggestions\" and select it > You'll see the two camera roll sharing toggles.\nDefinitely go to Facebook's support hub if you need more help. You basically got to find the Settings and privacy menu in the Facebook mobile app, and from there, you can find \"Camera roll sharing suggestions\" and disable the options.\nWhy is Facebook cloud-processing my device's camera roll?\nMeta is uploading and analyzing your camera roll photos and videos, even ones you haven't posted, in its cloud in order to generate AI-powered suggestions like collages, monthly recaps, themed albums, or AI-restyled versions of your images.\nWhere is this feature being tested?\nMeta has confirmed the feature is a test, saying, \"We're exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person's camera roll.\"\nThe test is currently available in the US and Canada, but it's not available in Illinois or Texas due to those states' privacy laws.\nDid Facebook ask for my consent before turning this on?\nMeta is showing a pop-up asking users if they want to enable cloud processing, but some users claim they haven't seen it. Instead, they found the toggles in their settings already switched on by default, raising questions about whether clear consent was given.\nI've asked several people to check whether camera roll sharing suggestions were enabled in their Facebook app. Besides myself and ZDNET's editorial director, my partner, in-laws, and many friends all found the options turned on without their knowledge. Many users have also reported on social media and in forum threads, like this one on Y Combinator, that the settings were enabled for them.\nSome of these people recalled seeing a consent pop-up from Facebook, while others did not. But nearly everyone said they did not realize they were allowing access to their camera roll data. They also said they would not have allowed it if they had known.\nCan I remove my photos once they've been uploaded?\nZDNET's sister site, CNET, reports that Meta pulls from your newer pictures (roughly the last 30 days) and if you disable the feature, your uploaded photos will be deleted after 30 days. The only way to confirm is by downloading your Facebook account data.\nWhy is this a potential privacy issue?\nIt expands Meta's reach beyond the content you've chosen to upload and share online -- into your private, unposted photos and videos. For many, that's a major red flag and a line they're not comfortable crossing, understandably so.\nAlso: What Zuckerberg's 'personal superintelligence' sales pitch leaves out\nEven if Meta is asking for consent to access your camera roll in order to analyze your phone's photos and provide AI-powered suggestions, the company could have done a better job of being clear and explicit about what it's trying to do.\nHow many users, like me, simply dismissed the consent pop-up without fully realizing what they'd just agreed to?\nEditor's note: This article was last updated on Sept. 2, 2025 to clarify steps for Android users. It was also updated on Aug. 24 to confirm that Meta's camera roll sharing suggestions are not turned on by default and are entirely opt-in. Still, some users say they never knowingly agreed and are finding the features enabled in their settings."
    },
    {
      "url": "https://www.zdnet.com/article/10-professional-developers-on-vibe-codings-true-promise-and-peril/",
      "text": "10 professional developers on vibe coding's true promise and peril\nThe internet (and especially my inbox) has been awash with buzz about vibe coding, a term coined by a former OpenAI founder and master programmer who waxed poetic about using an AI to do a lot of his coding's more prosaic work. I went into substantial depth about that post and the definition of vibe coding in my previous article on the topic.\nAlso: 10 key reasons AI went mainstream overnight - and what happens next\nThe term, though, is sticky as heck. By sticky, I mean that it resonated enough to be both cool on its own and be an immediate adjective that marketing folks can attach to their coding tools, making those tools immediately sound a notch cooler than they were on their own. I know, because as an AI columnist, I get press releases. I get a lot of press releases.\nUnfortunately, the marketing folks who wasted no time adopting the term and vibe-washing their products seem to be applying the vibe-coding moniker to the exact opposite of what it means. Remember, the term was coined by a master programmer who could very easily decide what's appropriate for an AI (and fix it, if it goes wrong). It was not intended for a newbie who wants to type a two-sentence prompt and make a couple of million dollars off the app that results from that mere moment of mental mathematics.\nBut suddenly, all the so-called no-code and low-code AI development tools (which are basically templates dressed up in AI-painted robes) are claiming they are vibe coding innovators.\nAs a long-time developer myself and former computer science professor, I shared my opinion on vibe coding in my article last week. But I'm cranky, crotchety, sarcastic, and snarky. I know, how could you possibly tell from my writing?\nAlso: The best AI for coding (and what not to use)\nSo rather than just leaving you with my opinion, I decided to reach out to real professional developers and ask their opinions. As you're about to see, some like the concept and say that AIs are saving them a ton of time, but most are concerned that non-pro developers and junior coders might miss important coding considerations and thereby produce code that's somewhere on the spectrum from junk to janky to downright dangerous.\nDevelopers on vibe coding\nTodd Olson, co-founder/CEO of usage analytics company Pendo\nHe shared with ZDNET, \"Vibe coding changes how we build, but not why we build. The goal isn't to produce more software; it's to create better software experiences. Without a clear understanding of user needs, vibe coding risks amplifying misalignment at scale.\"\nAlso: How to use ChatGPT to write code - and my favorite trick to debug what it generates\nOlson continued, \"AI makes it easier to ship code, but that doesn't guarantee better outcomes. Without clear alignment to user needs, vibe coding can accelerate the creation of software no one uses. This isn't a new challenge, necessarily, but AI raises the stakes.\"\nChris Reynolds, developer advocate and software engineer at hosting platform Pantheon\nHe told ZDNET, \"The most important thing for me about vibe coding is that there needs to be a human operator who is actually smarter than the computer. One of the problems of using LLMs for development is they make stuff up that sounds feasible, and they only do deeper research or consider things like security or best practices if prompted.\"\nHe pointed out how having limited experience as a developer could prove a challenge when asking an AI to write software, saying, \"So, if you're just an average power user without any development experience, you could absolutely get AI to build you a cool thing that looks and acts the way you want, but if you don't know how any of it works, when it has issues or bugs or whatever, you can find yourself deeper and deeper in the rabbit hole.\"\nI love his comments about robots running wild: \"Vibe coding absolutely needs the human component. You can't just let the robots run wild without checking what it's doing, or you'll absolutely get yourself into trouble.\"\nJamie Marsland, YouTube manager and a developer at Automattic\nJamie told me, \"We're entering a new era where the web becomes a canvas for dynamic, app-like experiences. As AI, no-code tools, and automation remove technical barriers, creators of all kinds are free to build richer, more interactive digital worlds. The lines between websites and apps are blurring -- and with it comes a surge of creativity that redefines what's possible online.\"\nHe also shared a YouTube video he posted where he uses AI to \"vibe code\" a front end to a website. It's fascinating, but keep in mind, this is a veteran developer instructing the AI. Someone completely new to web development might not have the vocabulary or experience to give the level of direction Jamie does.\nBill Salak is the CTO of AI education company Brainly\nHe told me, \"In tech, vibe coding is all about working with AI in a fluid, intuitive way, letting it handle the repetitive stuff so developers can focus on creative problem-solving. But vibe coding isn't just for coders. It's also about creating with AI even if you don't know how to write code. It's about removing barriers, making technology accessible, and allowing anyone to build something meaningful. So why aren't we applying that same mindset to learning?\"\n\"Vibe coding is proof that AI isn't here to replace creativity -- it's here to unlock it. It turns coding from a technical skill into a creative collaboration, where anyone with an idea can bring it to life.\"\nSalak extended the idea of vibe coding into learning overall. There's something to be said for the lower stress, higher interactivity idea that vibe coding promotes. He said, \"Imagine if learning worked the same way, where students weren't just consumers of information but creators, shaping their education in real-time with AI as their guide.\"\n\"Just like AI is transforming coding, it's time for AI to transform learning. Less memorization, more creativity. Less pressure, more discovery. The future of education isn't one-size-fits-all. It's personalized, powerful, and finally built for the student (and the teacher, too).\"\nI do like the idea of AI providing more interactivity in our activities, but as with coding, where we've seen how AIs can generate terrible code, any learning-related activities will need to have substantial guardrails and quality checks. I'm not sure we're ready to unleash students (or newbie coders) into the AI world (and its relatively common hallucinations) without some professional supervision.\nWillem Delbare, founder and CTO of Aikido\nAikido is a security company with the best tagline I've seen in a while: \"No bullshit security for developers.\" Delbare doesn't pull any punches about vibe coding, telling ZDNET, \"Vibe coding makes software development more accessible, but it also creates a perfect storm of security risks that even experienced developers aren't equipped to handle. SQL injections, path traversal, hardcoded secrets. Many developers consider security a blind spot, so imagine the risks when going from a simple landing page build to a full-fledged product handling user data.\"\nHe continued, \"It is incredibly easy for AI to write vulnerable code, and vibe coders often don't know what they don't know. When you're collecting user data or handling sensitive information, you need more than just good vibes. Ideally, security isn't an afterthought that ruins the fun; you need to learn the risks and set up security checks so you can keep building without your users becoming collateral damage.\"\nAlso: Why OpenAI's new AI agent tools could change how you code\nThen he made a very valid point about supercharging risk. He emailed me with, \"Sure, Gen AI supercharges development, but it also supercharges risk. Two engineers can now churn out the same amount of insecure, unmaintainable code as 50 engineers.\"\nJamie Madden, founder of PetFun, a service that creates AI-enhanced pictures of pets\nHe echoed some of the security and support concerns we've been talking about.\nHe said, \"Vibe coding represents a fascinating shift in software development by demonstrating how AI tools can democratize coding. While it enables newcomers to build applications without traditional programming experience, it also highlights a crucial reality: without fundamental software development knowledge, security and performance suffer.\"\nZack Katz, president of no-code forms platform GravityKit\nKatz said vibe coding has been a game changer. He told ZDNET, \"Vibe coding has dramatically accelerated our product development process. Ideas that sat in our backlog for years have come to life in just a week, thanks to AI. I can generate a working prototype of new functionality in a single day -- far faster than starting from scratch. From there, we refine, restructure, and test the code, cutting down development time significantly.\"\nThen, he provided an example: \"In a recent AI-powered session, I entered the requirements for a new product, switched back to my other work, and checked back a few minutes later. Within 20 minutes, I had a functional prototype ready for refinement and testing. AI has been a game-changer in our ability to quickly deliver features that customers want, and vibe coding has proven to be my go-to approach for rapidly building MVPs.\"\nLeonardo Losoviz, developer of WordPress data management plugin Gato GraphQL\nLosoviz also believes vibe coding can reduce time to market. He told me, \"I'm currently integrating my plugin Gato GraphQL to use vibe coding to create extensions, with the goal of creating all the extensions that my customers are demanding in a couple of weeks (normally it'd take a few months).\"\nJohn Campbell, director of content engineering at Security Journey\nCampbell has both positive and negative thoughts about vibe coding. He told ZDNET, \"Vibe coding is set to revolutionize software development by making developers more efficient and enabling individuals with limited technical expertise to expand their access to software tools.\"\nAlso: Google just made AI coding assistance free for everyone - with very generous limits\nBut there are concerns. He said, \"Just like a teenager experiencing the thrill of driving for the first time, that freedom would quickly turn to chaos without traffic laws, safety features, and guardrails.\"\nHe echoed Delbare's thoughts about accelerating risk. Campbell said, \"Vibe coding accelerates development and removes the need for deep technical knowledge of tools or techniques, but at the cost of a near-total lack of system understanding. Because vibe coding inherently disregards code details, it becomes almost impossible to assess the security risks in the software being developed.\"\nCampbell said he thinks the gap between vibe coding and production coding will continue to narrow. But for now, \"Vibe coding today excels at creating 'web toys' -- personal tools with a narrow focus and minimal security concerns -- without the stakes and risks of deploying them in a production environment.\"\nRhys Wynn, freelance WordPress developer trading under the name Dwi'n Rhys\nHe reported to ZDNET, \"I think vibe coding is a phrase invented by people who think that AI-generated code is safe and secure, and want to position themselves in a market they maybe shouldn't be in. Every single vibe coding project I've seen has been insecure, not able to answer a use case, or just emulating better things that exist already.\"\nHe, too, shared the security and optimization codes mentioned by most of the other developers I spoke to. He said, \"AI can be useful for coding, but getting AI to write out large swathes of code means the code is tending towards being insecure or unoptimized. Ask your AI assistant to fix small problems with well-defined tasks, and it works well. Similarly, to fix problems in your code. Always review, as if you don't understand your code, who will?\"\nWhat does it all mean?\nSome developers have seen rapid improvements in coding time using AI development tools. I'm on record as saying that \"I am fairly convinced it has doubled my programming output. In other words, I've gotten twice as much done by using ChatGPT as part of my toolkit.\"\nBut was that vibe coding? It's true that I let the AI code some of the more tedious tasks that involved common programming language, saving me hours. But did I just \"wing it\" and let the AI do whatever it wanted? Oh, hell no. I tested every single call, every input, and every output.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nVibe coding, for a very experienced developer, is kind of like being in a flow state. A lot of work can be accomplished by being in flow. But the prevailing opinion amongst nearly all the developers here is that (a) vibe coding is not for newbies, and (b) vibe coding could leave some problematic security and reliability problems.\nI don't think the idea of vibe coding, on its own, is a problem. But I would recommend keeping your bullcrap detector on full power, because marketing folks are seizing on the stickiness of the term to claim that newbies can do no-code projects with alacrity.\nThe problem with low-code and no-code will remain as it's always been: for customizing pre-built scenarios, the low-code and no-code tools work fairly well. But if the code you need doesn't fit the scenarios (or templates) provided, you're going to have to whip out that programming knowledge and write code like you know what you're doing.\nAlso: If ChatGPT produces AI-generated code for your app, who does it really belong to?\nIf you can't do that, you're going to run into brick walls or ship products that will fail in very day-ruining ways.\nWhat about you? Have you tried vibe coding yourself, or are you still on the fence about letting AI take the lead in your development process? Do you think AI-powered tools truly lower the barrier to entry for newcomers, or do they just make it easier to create insecure, unmaintainable software?\nAre you more excited by the creative possibilities or concerned about the risks? Let me know in the comments below.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter."
    },
    {
      "url": "https://www.zdnet.com/article/i-went-hands-on-with-chatgpt-codex-and-the-vibe-was-not-good-heres-what-happened/",
      "text": "I went hands-on with ChatGPT Codex and the vibe was not good - here's what happened\nZDNET's key takeaways\n- ChatGPT Codex wrote code and saved me time.\n- It also created a serious bug, but it was able to recover.\n- Codex is still based on the GPT-4 LLM architecture.\nWell, vibe coding this is not. I found the experience to be slow, cumbersome, stressful, and incomplete. But it all worked out in the end.\nChatGPT Codex is ChatGPT's agentic tool dedicated to code writing and modification. It can access your GitHub repository, make changes, and issue pull requests. You can then review the results and decide whether or not to incorporate them.\nAlso: How to move your codebase into GitHub for analysis by ChatGPT Deep Research - and why you should\nMy primary development project is a PHP and JavaScript-based WordPress plugin for site security. There's a main plugin available for free, and some add-on plugins that enhance the capabilities of the core plugin. My private development repo contains all of this, as well as some maintenance plugins I rely on for user support.\nThis repo contains 431 files. This is the first time I've attempted to get an AI to work across my entire ecosystem of plugins in a private repository. I previously used Jules to add a feature to the core plugin, but because it only had access to the core plugin's open source repository, it couldn't take into account the entire ecosystem of products.\nEarlier last week, I decided to give ChatGPT Codex a run at my code. Then this happened.\nGPT-5 released\nOn Thursday, GPT-5 slammed into the AI world like a freight train. Initially, OpenAI tried to force everyone to use the new model. Subsequently, they added legacy model support when many of their customers went ballistic.\nI ran GPT-5 against my set of programming tests, and it failed half of them. So, I was particularly curious about whether Codex still supported the GPT-4 architecture or would force developers into GPT-5.\nHowever, when I queried Codex five days after GPT-5 launched, the AI responded that it was still based on \"OpenAl's GPT-4 architecture.\"\nAfter posting this, a representative from OpenAI reached out to me with a correction. I was told, \"Codex is not based on the GPT-4 LLM architecture. It's actually a tailored model built from o3 and optimized specifically for programming.\" More information is at this blog entry.\nSo, yeah. Now we know the official answer. And we also know that the AI itself didn't provide the right answer. Read into that what you will.\nWith that, here is the result of my still-very-much-not-GPT-5 look at ChatGPT Codex.\nGetting started\nMy first step was asking ChatGPT Codex to examine the codebase. I used the Ask mode of Codex, which does analysis, but doesn't actually change any code.\nI was hoping for something as deep and comprehensive as the one I received from ChatGPT Deep Research a few months ago, but instead, I received a much less complete analysis.\nI found a more effective approach was to ask Codex to do a quick security audit and let me know if there were any issues. Here's how I prompted it.\nIdentify any serious security concerns. Ignore plugins Anyone With Link, License Fixer, and Settings Nuker. Anyone With Link is in the very early stages of coding, and is not ready for code review. License Fixer and Settings Nuker are specialty plugins that do not need a security audit.\nCodex identified three main areas for improvement.\nAll three areas were valid, although I am not prepared to modify the serialization data structure at this time, because I'm saving that for a whole preferences overhaul. The $_POST complaint is managed, but with a different approach than Codex noticed.\nAlso: The best AI for coding in 2025 (and what not to use)\nThe third area -- the nonce and cross-site request forgery (CSRF) risk -- was something worth changing right away. While access to the user interface for the plugin is assumed to be determined by login role, the plugins themselves don't explicitly check that the person submitting the plugin settings for action is allowed to do so.\nThat's what I decided to invite Codex to fix.\nFixing the code\nNext up, I instructed Codex to make fixes in the code. I changed the setting from Ask mode to Code mode so the AI would actually attempt changes. As with ChatGPT Agent, Codex spins up a virtual terminal to do some of its work.\nWhen the process completed, Codex showed a diff (the difference between original and to-be-modified code).\nI was heartened to see that the changes were quite surgical. Codex didn't try to rewrite large sections of the plugin; it just modified the small areas that needed improvement.\nIn a few areas, it dug in and changed a few more lines, but those changes were still pretty specific to the original prompt.\nAt one point, I was curious to know why it added a new foreach loop to iterate over an array, so I asked.\nAs you can see above, I got back a fairly clear response on its reasoning. It made sense, so I moved on, continuing to review Codex's proposed changes.\nAll told, Codex proposed making changes to nine separate files. Once I was satisfied with the changes, I clicked Create PR. That creates a pull request, which is how any GitHub user suggests changes to a codebase. Once the PR is created, the project owner (me, in this case) has the option to approve those changes, which adds them into the actual code.\nIt's a good mechanism, and Codex does a clean job of working within GitHub's environment.\nOnce I was convinced the changes were good, I merged Codex's work back into the main codebase.\nHouston, we have a problem\nI brought the changes down from GitHub to my test machine and tried to run the now-modified plugin. Wait for it\u2026\nYeah. That's not what's supposed to happen. To be fair, I've generated my own share of error screens just like that, so I can't really get angry at the AI.\nInstead, I took a screenshot of the error and passed it to Codex, along with a prompt telling Codex, \"Selective Content plugin now fails after making changes you suggested. Here are the errors.\"\nIt took the AI three minutes to suggest a fix, which it presented to me in a new diff.\nI merged that change into the codebase, once again brought it down to my test server, and it worked. Crisis averted.\nNo vibe, no flow\nWhen I'm not in a rush and I have the time, coding can provide a very pleasant state of mind. I get into a sort of flow with the language, the machine, and what seems like a connection between my fingers and the computer's CPU. Not only is it a lot of fun, but it can also be emotionally transcendent.\nWorking with ChatGPT Codex was not fun. It wasn't hateful. It just wasn't fun. It felt more like exchanging emails with a particularly recalcitrant contractor than having a meeting of the minds with a coding buddy.\nAlso: How to use GPT-5 in VS Code with GitHub Copilot\nCodex provided its responses in about 10 or 15 minutes, whereas the same code would probably have taken me a few hours.\nWould I have created the same bug as Codex? Probably not. As part of the process of thinking through that algorithm, I most likely would have avoided the mistake Codex made. But I undoubtedly would have created a few more bugs based on mistyping or syntax errors.\nTo be fair, had I introduced the same bug as Codex did, it would have taken me considerably longer than three minutes to find and fix it. Add another hour or so at least.\nSo Codex did the job, but I wasn't in flow. Normally, when I code and I'm inside a particular file or subsystem, I do a lot of work in that area. It's like cleaning day. If you're cleaning one part of the bathroom, you might as well clean all of it.\nBut Codex clearly works best with small, simple instructions. Give it one class of change, and work through that one change before introducing new factors. Like I said, it does work and it is a useful tool. But using it definitely felt like more of a chore than programming normally does, even though it saved me a lot of time.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nI don't have tangible test results, but after testing Google's Jules in May and ChatGPT's Codex now, I get the impression that Jules is able to get a deeper understanding of the code. At this point, I can't really support that assertion with a lot of data; it's just an impression.\nI'm going to try running another project through Jules. It will be interesting to see if Codex changes much once OpenAI feels safe enough to incorporate GPT-5. Let's keep in mind that OpenAI eats its own dog food with Codex, meaning it uses Codex to build its code. They might have seen the same iffy results I found in my tests. They might be waiting until GPT-5 has baked for a bit longer.\nHave you tried using AI coding tools like ChatGPT Codex or Google's Jules in your development workflow? What kinds of tasks did you throw at them? How well did they perform? Did you feel like the process helped you work more efficiently? Did it slow you down and take you out of your coding flow?\nDo you prefer giving your tools small, surgical jobs, or are you looking for an agent that can handle big-picture architecture and reasoning? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/googles-jules-ai-coding-tool-exits-beta-with-serious-upgrades-and-more-free-tasks/",
      "text": "Google's Jules AI coding tool exits beta with serious upgrades - and more free tasks\nZDNET's key takeaways\n- Google's Jules coding agent is out of beta, with clear pricing tiers.\n- Even the free tier has newer, more generous usage limits.\n- Updates include improved AI and better setup management.\nGoogle's coding AI Jules is out of beta. Announced in a blog post on Wednesday, the coding helper will be powered by Google's Gemini 2.5 Pro LLM. You might recall that Google also recently announced its Gemini CLI GitHub Actions coding tool.\nWhat do all these developments mean? Let's unpack the announcements.\nJules vs. Gemini CLI GitHub Actions\nJules and Gemini CLI GitHub Actions are different beasts. Jules is meant for big projects, significant changes, and project planning.\nAlso: Google embeds AI agents deep into its data stack - here's what they can do for you\nGemini CLI GitHub Actions augments GitHub's workflow tool, GitHub Actions. The Gemini CLI version interacts with GitHub, helps respond to issues (problem reports), generates and responds to pull requests (submitted changes), and collaborates with other coders working on the same project.\nHere's a handy table that helps show the differences between the assistants:\nThat was then\nWhen I first tried Jules, it was the first day it became available as a beta offering. Although the assistant had some early server and availability problems, I was still able to add a new feature to my open-source project. I then deployed that new feature to all my users.\nThat was on May 27. Since I didn't suddenly have 20,000 angry users screaming at me that I broke their sites (yes, I've done that before), it's pretty clear that the Jules additions worked well.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nWhat I wanted to like about Jules is that it produces a plan of action in response to your prompt. You can (in theory) interact with Jules to refine that plan of action before it goes off and makes changes to your code.\nAnd when I first tried the tool, Jules did produce a plan of action. But like my little Yorkie pup, it didn't possess the patience to wait for me to say it was okay to go. This issue leads to small dogs trying to climb into your dinner dish, and big AIs that blast through an entire codebase in minutes.\nFortunately, Jules branches your codebase. This feature ensures nothing is incorporated into your main release until after you approve it, as you would any other proposed change on GitHub.\nThis is now\nAs of August 6, Jules is officially out of beta. The official release has what Google called a \"new, more user-friendly interface.\"\nJules also allows developers to reuse previous setups. This could be quite valuable because having to train the AI on your code for every coding request can substantially slow the process down and potentially introduce errors. This way, once you have a working prompt foundation, you can reuse it.\nAlso: Gemini Pro 2.5 is a stunningly capable coding assistant - and a big threat to ChatGPT\nJules also now has multimodal support. Google said: \"Jules can test your web application and show you a visual representation of the results. This can help Jules iterate until it gets things right and gives you confidence in the code Jules creates.\"\nWhen I tested Gemini 2.5 Pro against my coding test suite, it did astonishingly well. Earlier versions of Gemini and its predecessor, Bard, did not perform nearly as well.\nGemini 2.5 prioritizes speed and efficiency. In other words, it costs Google less to run each prompt. Google Gemini 2.5 Pro is intended for deep reasoning, accuracy, and handling complex tasks. Google prices Gemini 2.5 Pro at about 8 1/2 times the cost of Gemini 2.5, so we can pretty much assume Gemini 2.5 Pro uses a lot more resources for each query.\nAs you read about this announcement, you might find some confusion. Based on the blog post, it's not fully clear whether Jules is powered by Gemini 2.5 or a combination of Gemini 2.5 and Gemini 2.5 Pro. According to Google's blog post announcing the public availability of Jules, the tool is \"powered by Gemini 2.5.\" But later in the blog post, Google said: \"Jules now uses the advanced thinking capabilities of Gemini 2.5 Pro to develop coding plans.\"\nI asked Google's team about the discrepancy. It confirmed to ZDNET that Jules uses the much more powerful reasoning model to plan out actions, and then executes those actions, also with the Gemini 2.5 Pro LLM. Given how impressive my testing was with 2.5 Pro, that's a good thing.\nAlso: Claude Code makes it easy to trigger a code check now with this simple command\nJules is offered in three pricing tiers. There are different limits for each pricing tier. When I tried Jules back in May, I was only given five tasks per day. I was able to make a feature addition to my project in just two tasks. You could still get quite a bit done even then.\nAs of now, the free version of Jules allows 15 tasks per day, which is good. If you take the time to craft clear prompts, you're unlikely to need a lot more than that number each day.\nHowever, if you want more, you can move to Google's $20/month Pro plan or $250/month Ultra plan.\nAlso: The best AI for coding in 2025 (including a new winner - and what not to use)\nStay tuned. I plan to use the release version of Jules on another coding modification. I'll report back on my experiences.\nWhat about you?\nDid you try Jules during the beta? How does it compare to other AI coding assistants you've used? Do you see value in its planning-based approach, or do you prefer a more hands-on coding helper? And what do you make of the apparent confusion over which Gemini model the assistant uses? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/is-chatgpt-plus-still-worth-20-when-the-free-version-offers-so-much-including-gpt-5/",
      "text": "Is ChatGPT Plus still worth $20 when the free version offers so much - including GPT-5?\nWhen ChatGPT first launched over two years ago, the AI chatbot was met with such high demand that OpenAI introduced a premium plan called ChatGPT Plus. This plan guaranteed access to the chatbot even during blackout periods. ChatGPT Plus perks also included access to OpenAI's most advanced models, making the $20 plan almost a no-brainer for superusers. However, as OpenAI's offerings have evolved over the past couple of years, so have its plans.\nAlso: How ChatGPT could replace the internet as we know it\nIf you consider yourself an AI power user or are looking to get the most out of your AI usage, you're likely wondering which ChatGPT tier you should try. In this guide, we'll help you decide whether a free plan, ChatGPT Plus, or a $200-per-month ChatGPT Pro subscription is the best fit for you.\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nYou should use ChatGPT Plus if...\nChatGPT Plus costs $20 per month. You're probably asking yourself: Why pay when you can use it for free? There are five main advantages, but the TL;DR is that the free tier has heavy usage limits, and the Plus tier gets you the latest features and models first.\nIf you only use ChatGPT now and then for fun, it's not worth subscribing. But if you use it for work, writing, creating images, using it as a search engine, or you find yourself opening the app all day, every day, then subscribing is likely a good idea.\n1. You want access to legacy models\nWhen OpenAI launched GPT-5, the company had it replace all of its legacy models in ChatGPT. The model is meant to combine the best of OpenAI's offerings with a smart model for most queries and a deeper reasoning model for harder problems. However, many paying subscribers were upset because they preferred the prior models for their workflows that were already in place.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o (for now)\nSam Altman even acknowledged in an X post that \"suddenly deprecating old models that users depended on in their workflows was a mistake.\" As a result, OpenAI brought back the option for ChatGPT Plus users to access legacy models, including: GPT-4o, o3, o4-mini, GPT-4.1, and GPT-4.1-mini, in addition to GPT-5 (Auto, Fast, Thinking mini, and Thinking).\n2. You want to access GPT-5's different modes\nWhile GPT-5 combines the best of a deep reasoning model (GPT-5 thinking) and a standard smart model for a balance of speed and quality, users with a Plus subscription can also manually toggle between GPT-5 Thinking, GPT-5 Thinking Mini, and GPT-5 Fast.\nAlso: GPT-5 bombed my coding tests, but redeemed itself with code analysis\nThe benefit: Users can choose to use a reasoning model when they think it would be the best fit instead of having the real router feature automatically understand which model to use based on the conversation, the complexity of the prompt, and more, which is still available with the GPT-5 Auto option. If you often work on harder problems that require the model to think more to output the best possible answer, then having the option to toggle on GPT-5 Thinking could be a compelling reason to subscribe.\n3. You want to access Sora\nOpenAI's Sora video-generating model allows users to create stunning, realistic videos from text prompts or their own assets with up to 1080p resolution. Videos can be up to 20 seconds long, with a limit of 10 seconds for ChatGPT Plus users, and can be widescreen, vertical, or square aspect ratios. Whether you have a real workflow reason for AI-generated content or just want to tinker, it is a fun model to use.\nTo create images, ChatGPT also uses the GPT-4o image generator, OpenAI's most capable image generator to date.\nWith it, you can simply ask \"create an image of\u2026,\" describe a scene, and receive an AI-generated image that can include realistic human generations and even text. You can also upload an existing image and describe edits for GPT-4o to apply -- like adding text, refining details, or changing backgrounds. Just be aware that complex requests may take a couple of minutes.\nAlso: ChatGPT's new image generator shattered my expectations - and now it's free to try\nUsage is capped for free users, but Plus and Pro subscribers can enjoy much higher daily quotas.\nOpenAI said free-tier users can use GPT-4o only a limited number of times within a five-hour window. It will notify you once you've reached the limit and invite you to upgrade to ChatGPT Plus. If you're on the Free tier and rely heavily on image creation, you might see a message like: \"You've reached your image creation limit. Upgrade to ChatGPT Plus or try again tomorrow...\"\nSo, if you often use AI for visual assets, such as videos or images, ChatGPT Plus is an easy choice.\n4. You want to use the Codex AI coding agent\nOpenAI recently integrated its powerful Codex AI coding agent into ChatGPT Plus, so now anyone with the $20-a-month Plus can get AI-powered coding suggestions without shelling out $200. You can point it at your GitHub repo, and it'll whip up code changes, run checks to make sure nothing's broken, and even handle installing dependencies online if you let it. Just keep in mind it doesn't \"remember\" anything between sessions, so you have to give it clear instructions every time.\nAlso: You can use OpenAI's super powerful AI coding agent Codex for just $20 now\nAlso, since the coding agent is new to ChatGPT, you might encounter some speed bumps if many people are using it at once.\n5. You want expanded access to ChatGPT's best features\nYou're probably noticing a theme here: The majority of ChatGPT's most advanced features are eventually made available to free users, but with heavy usage limitations. Therefore, the main benefit of upgrading is getting to use features like Deep Research, Advanced Voice Mode, and GPT-4o image generation a lot more frequently than you could on the free tier. Subscribing to ChatGPT Plus also gives you priority access to OpenAI's latest models and newest tools well before they reach the free tier.\nWith a Plus account, you can access extended limits on:\n- messaging\n- file uploads\n- image creation\n- data analysis\n- Deep Research\n- Agent Mode\n- GPT-4 image generation\n- memory and context\nIf you don't want to miss out on the latest ChatGPT features and models, you should subscribe to the Plus tier.\nYou should use ChatGPT Pro if...\nOK, so now that we've discussed why Plus is worth it, let's look at the more expensive Pro plan that costs $200 a month. Most people would never pay a car payment's worth of money to use AI, but there are special instances in which it could be worth it.\n1. You want what ChatGPT Plus has on a greater scale\nAll ChatGPT Plus perks and features are included in ChatGPT Pro, but with far fewer usage constraints. Pro also adds several exclusive capabilities. Here's a rundown of the biggest benefits, should you be seriously considering the upgrade.\n- Unlimited access to all reasoning models: This includes GPT-5 with pro reasoning, GPT-4o, GPT-4.1 (and its mini variant), o3, o3-pro, o4-mini, and GPT-4.5 (research preview).\n- Unlimited and faster image generation using GPT-4o.\n- Maximum Deep Research, memory, and context, Agent Mode\n- Priority access to Sora video generation: You get up to 1080p video generation, 20-second videos, five concurrent generations, and watermark-free downloads.\n- Higher access to the Codex agent, projects, tasks, and custom GPTs.\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\nAs you can see, Pro offers more features and higher limits than the Plus tier. Pro users reportedly rarely encounter any constraints and can truly leverage the full power of ChatGPT and its newest tools.\n2. You want access to OpenAI's most powerful models\nAs mentioned in the No. 1 selling point of ChatGPT Plus, there are many perks to being able to access the older models, including sticking to what you were already using in your workflow and what you are accustomed to. In addition to all of the legacy models accessible in the Plus tier, with the Pro subscription, you can also access GPT-5 Pro, the most capable version of the model aimed toward complex tasks, and the Thinking 4.5 (research previews), which is only available to Pro users because \"it costs a lot of GPUs,\" according to Altman.\nYou should use free ChatGPT if...\nFinally, let's look at the free version of ChatGPT. I discussed most of its features above, so in this section, I'll break down what advantages it can offer over the paid plans. It essentially comes down to how often you use ChatGPT and whether having access to the latest and greatest features matters to you. If you don't use it much and don't care, the free version is 100% the way to go.\n1. You don't want to pay a monthly fee\nThe free subscription now offers many competitive AI features, reducing the need for a Plus subscription, especially for tools that were once paywalled. Free ChatGPT users can access:\n- GPT-5\n- ChatGPT Search (web browsing for timely information and sources)\n- Image Generation via GPT-4o (daily limit applies)\n- Deep Research (up to five \"lightweight\" tasks per month)\n- ChatGPT Voice (free monthly preview)\n- File and Photo Uploads for in-chat discussions (limited usage)\n- Memory Feature for referencing recent conversations (lightweight version)\nIf you only need occasional AI assistance -- and do not mind daily usage caps -- the free tier should serve you well. It's surprisingly robust and now includes many advanced features that were once behind a paywall. For instance, when OpenAI launched GPT-5, it also became available for free users.\n2. You're a casual ChatGPT user\nIf you rarely hit the daily usage limits for text, voice, or image generation, upgrading to Plus or Pro might not be necessary.\nAlso: ChatGPT can record, transcribe, and analyze your meetings now\nThe free version offers enough headroom for light interactions -- asking a few questions per day, generating a handful of images, briefly exploring advanced data analysis, and more. However, if you often see messages about hitting your limits (for text, voice, images, etc.), it might be time to consider paying for a subscription. For superusers who need top-tier capacity and extended access to advanced features, there's ChatGPT Pro, but Plus is a more affordable middle ground.\nUltimately, if your needs are minimal and limitations don't bother you, sticking with the free tier is perfectly fine.\nFAQs\nHow much do ChatGPT Free, Plus, and Pro cost?\nAs of August 2025, here are the ChatGPT Free, Plus, and Pro pricing tiers and how they differ, as displayed by OpenAI on its website:\nDoes the Pro tier include everything you get with Plus?\nYes, Pro contains everything in Plus. That means unlimited or higher limits on certain features and exclusive access to GPT-5 Pro and Research preview of new features.\nDo ChatGPT Pro users experience downtime or blackout periods?\nPro subscribers have the highest priority for uptime, making downtime extremely rare. However, no tier can guarantee 100% uptime if OpenAI undergoes major outages or scheduled maintenance.\nIf you upgrade to Plus, can you later switch to Pro?\nYou can upgrade from Plus to Pro at any time through your account settings. Your billing date may adjust based on when you switch.\nHow to get early access to new ChatGPT features on the free tier\nUnfortunately, free users typically must wait until OpenAI rolls them out publicly. Plus and Pro subscribers receive early or exclusive testing opportunities before features reach the free plan."
    },
    {
      "url": "https://www.zdnet.com/article/how-i-saved-myself-1200-a-year-in-cloud-storage-in-5-sobering-steps/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nHow I saved myself $1200 a year in cloud storage - in 5 sobering steps\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- My cloud storage costs exploded after unlimited plans vanished.\n- Backing up everything was expensive and unnecessary.\n- Five strategic changes saved me over $1,200 per year.\n60 terabytes. I'll never see that much cloud storage again. I'm relieved - and a little sad.\nThis story began many chapters ago. It culminated with some serious self and infrastructure examination, hard conversations, and a few tough decisions.\nIt's a story of online data hoarding in the guise of best practices, where too much became way too much, and the seemingly simple task of maintaining backups became an on-call emergency response second job.\nI changed all that this week.\nTL;DR\nThere's a lot to this story, but if you want a quick summary of my changes, it's this: I moved from Google Workspace Enterprise to Backblaze B2. I dropped Time Machine and added Backblaze Personal for individual machines. My network of RAID servers help back each other up, and by the time I was done, I saved well over a thousand dollars.\nAlso: The best cloud storage services of 2025: Expert tested\nBut I had to give up a lot to do it. What follows is the necessary backstory, a description of the before-and-after network architecture, cost breakdowns, and an in-depth dive into how and why I made the decisions that led me to this new approach.\nLet's dig in.\nThe before time\nSince the early 2000s, I've been a big proponent and practitioner of the tech industry's best-practice 3-2-1 backup strategy. The idea is to have three copies of every file, two of which are on different physical devices, and one of which is located off-site.\nBefore my parents passed away, the off-site was easy. I'd just shuttle a couple of hard drives back and forth to their place whenever we went to visit. In turn, they'd put me to work removing the thick hard-pack of computer viruses my Dad had picked up between visits. It wasn't entirely his fault. He was a jeweler, and back then, jewelry sites were infested with malware.\nIn any case, my storage needs were considerable, but not overly huge. When most people had dial-up, my self-hosted web servers lived in a converted linen closet. Those old servers used dedicated bandwidth provided by the only high-speed T-1 internet connection (which ran through my bathroom) between Rutgers and Princeton. At that time, my storage needs added up to less than a terabyte.\nYes, that was a lot for the early days. But it wasn't 60 terabytes.\nAlso: Why some companies are backing away from the public cloud\nOver time, my work moved me into three data-intensive areas: networked virtual machines, 3D modeling, and video. My local storage needs ballooned and have been growing ever larger for the past decade.\nBeyond the terabytes of networked VMs used for what we would now call AI agent development, I have all the video elements involved in producing fairly high-quality YouTube. A single YouTube video can come close to a terabyte during production. My multi-camera surveillance system around the house also eats up storage.\nRight now, I'm operating four local at-home file servers, with a total storage of 139.04 terabytes.\nThat's a lot more than the 60 I have in the cloud, but there's a lot of the \"2\" part of the 3-2-1 backups on those servers.\nA little over ten years ago, storing in the cloud became not only practical but also cheap. You could buy all-you-can-store plans from a variety of online backup providers. And that's what I did. But by 2017, the never-ending feast of online storage came to an end.\nAs vendor after vendor shut down their unlimited offerings, I found myself spending weeks moving my data to the next player still offering unlimited storage. I was trapped in a strange game of data warehouse Whac-A-Mole.\nAlso: The best NAS devices of 2025: Expert tested\nUntil I hit a wall. There came a time when those deals were no longer available, except for one offering from Google.\nIn 2018, Google's G Suite (the earlier name for Google Workspace) offered an unlimited storage plan. If you bought five or more seats on a $12/mo business plan, $60 a month would get you unlimited storage.\nSo, that's what I did.\nThen, in 2020, Google rebranded G Suite to Workspace and tightened its storage offerings. You could \"request\" unlimited storage, but having your request granted was another matter.\nFor the next five years, my relationship with Google storage was rocky, increasingly demanding, and increasingly more expensive. You can read some of my pieces on this ongoing challenge here:\n- What happens to your G Suite unlimited storage when Google moves you to Workspace?\n- Re-homing G Suite storage: Enterprise plan options seem unnecessarily murky\n- Re-homing G Suite storage: No, you can't find out how much storage your folders use\n- Why my two-person company bought a Google Workspace Enterprise plan\n- Need to manage your Google Drive data? Google was no help, but this tool solved my storage mess\n- The other shoe finally dropped on my Google Enterprise cloud storage plan\n- Google suddenly raised my monthly Workspace Enterprise fee by 38%\n- Maxing out your Google cloud storage? How I deleted more than 10TB, thanks to this handy tool\nBy 2022, my two-person company was subscribed to a Google Workspace Enterprise plan, which did provide access to the storage I needed. It just took a lot of requesting to get Google to let me have that storage.\nThat turned into something of an extended annual negotiation with Google's storage capacity gatekeepers. Every year or so, I'd get a notice from Google that I was approaching my capacity. Then I'd engage in weeks of back-and-forth support tickets \"requesting\" more capacity, citing reams of previous support transcripts. And Google reluctantly agreed to a small capacity increase, along with an increase in how much I had to pay each month.\nA few weeks ago, I got another such warning notice. Instead of diving once more into diplomatic negotiations with the data overlords, I decided to rethink my life. Or, at least, the storage aspect thereof.\nPre-migration storage architecture\nMy email and storage costs have been linked since I started using Google Workspace. My main email account is a consumer Gmail account, configured to send and receive using my corporate domains. The corporate email domain relays are hosted in Workspace.\nThe following graphic shows my pre-migration storage architecture. Our primary storage devices are the various computers my wife and I use, our file server (which holds all our primary documents as well as all my video backups), and a security server (which holds historical video from our security cameras).\nThe Macs were backed up to a local backup server using Time Machine. For the record, I dislike Time Machine. It's always a gamble whether you can restore anything from a Time Machine backup, and they fail regularly. Time Machine is an incremental backup. That means changes are added to the backup archive, and you (theoretically) can roll back to a previous day or week.\nThe Macs also sync to our on-premises backup server (a different box from the file server) using ChronoSync, a very reliable Mac syncing tool with pretty excellent tech support. These backups are syncs, so anything that changes on the source machine (including deletions) is reflected on the server.\nOur big file server had four backup mechanisms. It backed up incrementally to Google Workspace. It also synced to Google Workspace, so Google Workspace had two complete backup architectures using storage.\nAlso: Is OneDrive moving all your files? How to take back control of your Windows storage - 3 ways\nThe file server also syncs to the backup server. Once a week, our offline backup server powers on, and the file server syncs to it. This provides an air-gapped backup just in case the worse were to happen. That offline file server is also pretty much fire and flood proof, so it should be able to handle almost anything.\nKeep in mind that all our servers implement RAID, so if any single drive fails, data won't be lost. I also keep spare drives in stock, so if there is a failure, I can replace the drive immediately and start rebuilding the RAID.\nHere's how our 3-2-1 strategy looked before the new architecture I've just implemented.\nPre-migration costs\nAs with everything else in this world, the cost of cloud storage has been increasing regularly. In addition to the Google Workspace storage cost of $162.50 per month, we pay an additional two bucks each to extend the storage for our personal Gmail accounts. We also pay about $30 each month to extend our iCloud storage to 6TB.\nTough decisions\nAs the chart below shows, our mission-critical document storage takes up only a sliver of the overall file storage on our big file server.\nThe videos in video production are all videos that have been uploaded to YouTube. Yes, the raw footage is stored on the server. I do pull from that footage from time-to-time. If I lost that raw footage, it would be a bummer, but not catastrophic.\nThe same is true of media. That slice contains all of our imported DVD collection, along with music files, and other media, including my archive of the production graphics work for all of my ZDNET articles. It's an important folder, but as was the case with video production, most of it is archival.\nSince my wife and I subscribe to a ton of streaming video services, we almost never tap into that media library for video content. We haven't for years.\nAlso: Your Windows PC has a secretly useful backup tool - here's how to access it\nAs for the VMs, I keep a few live VMs on my super-charged M4 Mac Studio. The VMs stored on the server are from older projects and were used in massive simulations, projects I'm not doing right now. All of those VMs also use very deprecated versions of operating systems from times past. If something were to happen to them, it would be unfortunate, but it would not impact day-to-day productivity at all.\nAll of those files are stored on one RAID server and backed up to a second local RAID server. It's highly unlikely that we'd have catastrophic failure across drives on both servers at the same time.\nSo the real issue is what would happen if there were some sort of disaster. If something happened to the house and both servers were lost, then we'd lose those files. But if we had such a catastrophe, old VMs and video production archives would be the least of my worries.\nTough decision #1: Stop backing up older media\nSo, my first tough decision was to stop backing up the three big slices to the cloud. Remember that I was not only backing all that up incrementally but also syncing it all. Cloud data storage usage ballooned using that approach.\nTough decision #2: Give up all that Google Workspace storage\nMy second tough decision was to move off of Google Workspace for cloud storage. Giving up all that sweet, sweet storage brought me some sadness, but the constant debates with Google support had maxed out my tolerance meter.\nTough decision #3: Stop using Time Machine\nMy third tough decision was to stop using Time Machine. That's Apple's canonical recommendation for backup. It sucks. Since I'm already syncing my local Macs both to iCloud and to our backup server with ChronoSync, I felt I could shut down Time Machine. I then decided to move to Backblaze Personal for my wife's main Mac and mine.\nBackblaze Personal charges ten bucks a month or a hundred dollars a year for unlimited backups from one machine. You can have locally-mounted USB drives and it will back those up as well, but it won't backup network drives.\nTough decision #4: Use Backblaze B2 for server cloud storage\nMy fourth tough decision was to go with Backblaze B2 to back up just my file server documents. Backblaze B2 charges six dollars per terabyte per month, so I certainly wouldn't get the same storage deal as I was getting from Google Workspace. But if I kept my B2 needs to under 4TB (which is the case for the documents on the file server), I'd have a pretty solid budget-friendly solution.\nSo far, as a new customer, Backblaze deserves points. I ran into a self-inflicted snag during my migration and the Backblaze support folks stepped in and got me out of trouble. Nice.\nTough decision #5: No more incremental backups to the cloud\nMy fifth tough decision was to forego incremental backups for the file server on the cloud. This would keep my Backblaze B2 costs down considerably. One neat feature of Backblaze B2 is that it retains files for a number of days. So I set file retention to 10 days. That means if I do delete a file I need, and the deletion syncs to the cloud, I have ten days to recover it.\nPost-migration budget and architecture\nSo, here's where we now stand. This is the working architecture for the new normal. As you can see, a lot remains the same. But the file server backups to the cloud are substantially reduced, and Time Machine is no longer in the picture.\nHere's how the new 3-2-1 backup strategy works out with this new approach.\nI'm still keeping Google Workspace for corporate email relay, but that's a fairly inexpensive $14/month for our two accounts.\nAnd here's how it all nets out dollar-wise.\nAs you can see, I went from $196/month ($2,357 per year) down to $90/month ($1080 per year). That $1,200 yearly savings is pretty sweet. But it was a pretty challenging road getting here. Hopefully, in a year, I won't have another storage fight to chronicle. I'm hoping this new approach provides a few years of smooth sailing.\nHave you had to make hard choices about your cloud storage strategy? What services are you using to manage large amounts of data? Have you found a setup that balances cost, access, and redundancy? Have you considered ditching Time Machine or moving away from Google Workspace? How do you handle backups for media, VMs, or security footage? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/45-time-saving-windows-keyboard-shortcuts-that-supercharged-my-daily-routine/",
      "text": "'ZDNET Recommends': What exactly does it mean?\nZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we\u2019re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\n45+ time-saving Windows keyboard shortcuts that supercharged my daily routine\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Windows has a long list of keyboard shortcuts that perform helpful actions in Windows 11.\n- For example, hitting the CTRL and C keys together allows users to copy text or files.\n- Mastering these shortcuts can help boost your overall productivity, but most people only know a few.\nKeyboard shortcuts have been a staple feature in Windows for decades, but most users only know the basics. At first glance, they don't seem all that useful since you can also do these actions with your mouse, but I challenge you to try them over the course of a workday; you just might notice a boost in productivity.\nAlso: How much RAM does your PC really need in 2025? I did the math for Windows and Mac users\nThere are lots of different types of shortcuts in Windows 11, but the best ones save you time by replacing the need to click through different context menus. A few of these are universal across applications and platforms. Ctrl + C, for example, copies text or files in any app, from Microsoft Word to photos in Google Drive.\nHere's a list of more than 40 Windows PC keyboard shortcuts you should know.\nProductivity shortcuts\nProductivity shortcuts are the key combos you'll find yourself using the most. They're easy to use and help you manage your work. Over time, these keystrokes become second nature.\n- Ctrl + C -- Copy highlighted text or a file.\n- Ctrl + V -- Paste whatever you just copied. Keeps the text's format.\n- Ctrl + Shift + V - Paste whatever you copied, but it removes the formatting.\n- Windows key (Win, for short) + V -- Opens the Clipboard tool, which contains a list of previously copied text. This helps in situations where you might've \"erased\" the previous copied item.\n- Ctrl + X -- Cuts the highlighted item. Ctrl + V restores it.\n- Ctrl + Z -- Undoes the last action, such as restoring deleted text or returning a moved file.\n- Ctrl + Y -- Redoes the action that was just undone.\n- Ctrl + F -- Opens the Find tool, so you can search for specific words or phrases.\n- Ctrl + A -- Highlight every single item in a window or text in a document.\n- Ctrl + S -- Quick save your work.\n- Ctrl + Backspace -- Delete the entire previous word\n- Ctrl + Delete -- Delete the entire next word.\nAccessibility shortcuts\nWindows 11 has various accessibility shortcuts that make the OS easier to use. These shortcuts help people with movement limitations or conditions that may restrict their usage of Windows. That said, anybody can use them to improve their experience.\n- Win + Plus (+) -- Activates the Magnifier tool and automatically zooms in on the screen.\n- Win + Minus (-) -- Zooms out while the Magnifier tool is activated. Hitting Win + Esc exits out of Magnifier.\n- Win + Ctrl + Enter -- Launches Narrator, a text-to-speech function that verbally describes what is on screen.\n- Press the Shift key five times in succession -- Turns on Sticky keys, which change how shortcuts work so you don't have to press and hold multiple keys to trigger them. For example, under Sticky Keys, double-tapping Ctrl then C has the same effect as Ctrl + C.\n- Left Shift + Left Alt + Print Screen -- Toggles high contrast mode for easier readability.\n- Win + Ctrl + C -- Toggles color filters like grayscale. The filters can be changed in the Accessibility menu to invert colors or help people with deuteranopia (also known as red-green color blindness).\n- Win + U -- Opens the Accessibility menu in Settings, where you can activate or deactivate accessibility-related features.\n- Win + Ctrl + O -- Opens the on-screen keyboard.\nNavigation shortcuts\nNavigation shortcuts allow users to move around different apps on Windows 11 without ever needing to lift their hands off the keyboard. They're especially useful for multitaskers juggling several programs at once. Once you master them, navigating your PC will feel seamless.\n- Ctrl + Alt + Tab -- Shows all open apps. Use the arrow keys to hop from one to the other.\n- Win + S -- Opens Windows Search.\n- Win + Tab -- Opens Task View, revealing all open windows and all open desktops.\n- Win + D -- Hides every window on your desktop. Pressing the keys again reveals them.\n- Win + Left or Right Directional Arrow -- Snaps the current window to the side of the screen that the arrow is pointing to.\n- Win + Up Arrow -- Maximizes a window.\n- Win + Down Arrow -- Minimizes the same window.\n- Win + Ctrl + D -- Create a new desktop.\n- Win + Ctrl + F4 -- Closes a desktop.\n- Win + Number Key -- Opens a taskbar app pinned to that number. For example, Win + 3 could open Microsoft Edge while Win + 4 opens the Microsoft Store.\nAlso: Finally, a Windows laptop that I wouldn't mind putting away my MacBook for\nFile Management shortcuts\nThe Windows 11 File Explorer has its own set of keyboard shortcuts to help users reorganize folders and files. Instead of right-clicking through menus or dragging items around to clean up clutter, these simple key presses make managing documents a breeze.\n- Win + E -- Opens File Explorer.\n- F2 -- Rename the selected file or folder. Depending on the keyboard, you may also have to hold Fn.\n- Shift + Delete -- Permanently deletes an item. Hitting just Delete only sends the file to the recycling bin.\n- Ctrl + N -- Opens a new File Explorer window. It stays on the page of the previous window.\n- Ctrl + Shift + N -- Create a new folder.\n- Alt + Enter -- Opens the Properties window for a file and folder.\n- Alt + Left or Right Arrow -- Move forward or backward through your File Explorer History.\n- Alt + Up Arrow -- Go back one folder level.\nAlso: Best early Labor Day laptop deals: Sales on Apple, Dell, Lenovo, and more\nBrowser shortcuts\nBrowser shortcuts are similar to Productivity shortcuts in that they do similar things, like copying and pasting items or highlighting entire documents. However, there are a few unique tools, like the ability to create tabs. The following were tested and confirmed to work across various browsers: Chrome, Edge, and Firefox.\n- Ctrl + T -- Open a new tab.\n- Ctrl + Shift + T -- Reopen the last closed tab. (This one is huge!)\n- Ctrl + W -- Close the current tab.\n- Ctrl + Tab -- Move right through tabs.\n- Ctrl + Shift + Tab -- Move left through the tabs.\n- Ctrl + Shift + N -- Open new private browsing window. On Firefox, this is Ctrl + Shift + P.\n- Alt + Left or Right Arrow -- Move back and forth through tab history.\n- Ctrl + D -- Bookmark page.\n- Ctrl + H -- Open browser history.\n- Ctrl + N -- Open a brand new browser window.\nWhy should you trust me?\nI've been writing about software and consumer electronics for almost 10 years. In that time, I've worked in many different settings, from the traditional office to the home office and even someone's backyard. Over the years, I've picked up a lot of tricks to help me get through my day-to-day tasks.\nAlso: I tested the best antivirus software for Windows: Here's what I'd use to protect my PC\nMany of the shortcuts in this guide I use every day, even when I'm not working. CTRL + Z has practically become my best friend because I use it so often. Some of these shortcuts I learned about for the first time while researching this piece. I didn't know that the File Manager has so many shortcuts under its toolbelt, but once I started using them, I immediately saw their value to expert and casual users alike."
    },
    {
      "url": "https://www.zdnet.com/article/coding-with-ai-my-top-5-tips-for-vetting-its-output-and-staying-out-of-trouble/",
      "text": "Coding with AI? My top 5 tips for vetting its output - and staying out of trouble\nOur story begins, as many stories do, with a man and his AI. The man, like many men, is a bit of a geek and a bit of a programmer. He also needs a haircut.\nThe AI is the culmination of thousands of years of human advancement, all put to the service of making the man's life a little easier. The man, of course, is me. I'm that guy.\nAlso: The best AI for coding in 2025 (and what not to use)\nUnfortunately, while AI can be incredibly brilliant, it also has a propensity to lie, mislead, and make shockingly stupid mistakes. It is the stupid part that we will be discussing in this article.\nAnecdotal evidence does have value. My reports on how I've solved some problems quickly with AI are real. The programs I used AI to write with are still in use. I have used AI to help speed up aspects of my programming flow, especially when I focus on the sweet spots where I'm less productive and the AI is quite knowledgeable, like writing functions that call publicly published APIs.\nAlso: I'm an AI tools expert, and these are the only two I pay for (plus three I'm considering)\nYou know how we got here. Generative AI burst onto the scene at the cusp of 2023 and has been blasting its way into knowledge work ever since.\nOne area, as the narrative goes, where AI truly shines is its ability to write code and help manage IT systems. Those claims are not untrue. I have shown, several times, how AI has solved coding and systems engineering problems I have personally experienced.\nAI coding in the real world: What science reveals\nNew tools always come with big promises. But do they deliver in real-world settings?\nMost of my reporting on programming effectiveness has been based on personal anecdotal evidence: my own programming experiences using AI. But I'm one guy. I have limited time to devote to programming and, like every programmer, I have certain areas where I spend most of my coding time.\nAlso: I tested 10 AI content detectors - and these 5 correctly identified AI text every time\nRecently, though, a nonprofit research organization called METR (Model Evaluation & Threat Research) did a more thorough analysis of AI coding productivity.\nTheir methodology seems sound. They worked with 16 experienced open-source developers who have actively contributed to large, popular repositories. The METR analysts provided those developers with 246 issues from the repositories that needed fixing. The coders were given about half the issues where they had to work on their own, and about half where they could use an AI for help.\nThe results were striking and unexpected. While the developers themselves estimated that AI assistance increased their productivity by an average of 24%, METR's analytics showed instead that AI assistance slowed them down by an average of 19%.\nThat's a bit of a head-scratcher. METR put together a list of factors that might explain the slowdown, including over-optimism about AI usefulness, high-developer familiarity with their repositories (and less AI knowledge), the complexity of large repositories, lack of AI reliability, and an ongoing problem where the AI refuses to use \"important tacit knowledge or context.\"\nAlso: How AI coding agents could destroy open-source software\nI would suggest that two other factors might have limited effectiveness:\nChoice of problem: The developers were told which issues they had to use AI help on and which issues they couldn't. My experience suggests knowledgeable developers must choose where to use AI based on the problem that needs to be solved. In my case, for example, getting the AI to write a regular expression (something I don't like doing and I'm fairly crappy at) would save me a lot more time than getting the AI to modify unique code I've already written, work on regularly, and know inside and out.\nChoice of AI: According to the report, the developers used Cursor, an AI-centric fork of VS Code, which used Claude 3.5/3.7 Sonnet at the time. When I tested 3.5 Sonnet, the results were terrible, with Sonnet failing three out of four of my tests. Subsequently, my tests of Claude 4 Sonnet were considerably better. METR reported that developers rejected more than 65% of the code the AI generated. That's going to take time.\nThat time when ChatGPT suggested nuking my system\nMETRs results are interesting. AI is clearly a double-edged sword when it comes to coding help. But there's also no doubt that AI can provide considerable value to coders. If anything, I think this test once again proves the contention that AI is a great tool for experienced programmers, but a potential high-risk resource for newbies.\nAlso: Why I'm switching to VS Code. Hint: It's all about AI tool integration\nLet's look at a concrete example, one that could have cost me a lot of time and trouble if I followed ChatGPT's advice.\nI was setting up a Docker container on my home lab using Portainer (a tool that helps manage Docker containers). For some reason, Portainer would not enable the Deploy button to create the container.\nIt had been a long day, so I didn't see the obvious problem. Instead, I asked ChatGPT. I fed ChatGPT screenshots of the configuration, as well as my Docker configuration file.\nChatGPT recommended that I uninstall and reinstall Portainer. It also suggested I remove Docker from the Linux distro and use the package manager to reinstall it. These actions would have had the effect of killing all my containers.\nOf note, ChatGPT didn't recommend or ask if I had backups of the containers. It just gave me the command line sequences it recommended I cut and paste to delete and rebuild Portainer and Docker. It was a wildly destructive and irresponsible recommendation.\nThe irony is that ChatGPT never figured out why Portainer wouldn't let me deploy the new container, but I did. It turns out I never filled out the container's name field. That's it.\nAlso: What is AI vibe coding? It's all the rage but it's not for everyone - here's why\nBecause I'm fairly experienced, I hesitated when ChatGPT told me to nuke my installation. However, someone relying on the AI for advice could have potentially brought down an entire server for want of typing in a container name.\nOverconfident and underinformed AIs: A dangerous combo\nI've also experienced the AI going completely off the rails. I've experienced it giving advice that was not only completely useless, but also presented with the apparent confidence of an expert.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nIf you're going to use AI tools to support your development or IT work, these tips might keep you out of trouble:\n- If there's not much publicly available information, the AI can't help. But the AI will make stuff up based on what little it knows, without admitting that it is lacking experience.\n- Like my dog, once the AI gets fixated on one thing, it often refuses to look at alternatives. If the AI is stuck on one approach, don't make the mistake of believing that its polite recommendations about a new approach are real. It's still going down the same rabbit hole. Start a new session.\n- If you don't know a lot, don't rely on the AI. Keep up your learning. Experienced devs can tell the difference between what will work and what won't. But if you're trying to put all the coding on the back of the AI, you won't know when or where it goes wrong or how to fix it.\n- Coders often use specific tools for specific tasks. A website might be built using Python, CSS, HTML, JavaScript, Flask, and Jinja. You choose each tool because you know what it does well. Choose your AI tools the same way. For example, I don't use AI for business logic, but I gain productivity using AI to write API calls and public knowledge, where it can save me a lot of time.\n- Test everything an AI produces. Everything. Line by individual line. The AI can save a ton of time, but it can also make enormous mistakes. Yes, taking the time and energy to test by hand can help prevent errors. If the AI offers to write unit tests, let it. But test the tests.\nBased on your experience level, here's how I recommend you think about AI assistance:\n- If you know nothing about a subject or skill: AI can help you pass as if you do, but it could be amazingly wrong, and you might not know.\n- If you're an expert in a subject or skill: AI can help, but it will piss you off. Your expertise gets used not only to separate the AI-stupid from the AI-useful, but to carefully craft a path where AI can actually help.\n- If you're in between: AI is a mixed bag. It could help you or get you in trouble. Don't delegate your skill-building to the AI because it could leave you behind.\nAlso: How I used ChatGPT to analyze, debug, and rewrite a broken plugin from scratch - in an hour\nGenerative AI can be an excellent helper for experienced developers and IT pros, especially when used for targeted, well-understood tasks. But its confidence can be deceptive and dangerous.\nAI can be useful, but always double-check its work.\nHave you used AI tools like ChatGPT or Claude to help with your development or IT work? Did they speed things up, or nearly blow things up? Are you more confident or more cautious when using AI on critical systems? Have you found specific use cases where AI really shines, or where it fails hilariously? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/openai-upgrades-chatgpt-with-codex-and-im-seriously-impressed-so-far/",
      "text": "OpenAI upgrades ChatGPT with Codex - and I'm seriously impressed (so far)\nOpenAI's new Codex agent is essentially a vibe-coding environment based on a ChatGPT-like comment interface. As much as the vibe-coding idea seems like a meme for wannabe cool-kid coders, the new Codex agent is impressive as heck.\nAlso: What is AI vibe coding? It's all the rage but it's not for everyone \u2013 here's why\nOpenAI described Codex as a research preview still under active development. Right now, it's available to Pro, Enterprise, and Team-tier ChatGPT users, but it's expected to release to Plus and Edu users \"soon.\"\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\nAccording to the recording of OpenAI's announcement livestream, the Codex name has been applied to an evolving coding tool since as far back as 2021. That said, when I refer to Codex in this article, I'm talking about the new version being announced now.\nWhat is Codex?\nI haven't had the opportunity to get hands-on with Codex yet, so I'm taking everything I'm sharing with you from information provided by OpenAI. When I watched the announcement, I noticed that even the engineers seemed a little shocked at how capable this tool is.\nCodex lives on OpenAI's servers and interacts with your GitHub repositories. If the demo is to be believed (and OpenAI has repeatedly proven that unbelievable demos are real), Codex basically acts like another programmer on your team.\nAlso: 10 professional developers on vibe coding's true promise and peril\nYou can tell it to fix a series of bugs, and it will go off and do just that. It asks you to approve coding changes, although it looks like it can also just go ahead and modify code.\nYou can ask it to analyze and modify code, look for specific problems, identify problem areas and room for improvement, and other coding and maintenance tasks. Each assignment spawns off a new virtual environment where the AI can go all the way from concept and design to unit testing.\nA mindset change\nThere is a real coding mindset change going on here. Earlier AI coding help took the form of auto-complete. Lines and even blocks of code were automatically generated based on existing code.\nThen we got to the point where small segments of code could be written or debugged by the AI. This is the area I've been focusing on in terms of the ZDNET programming tests.\nAnother AI role is analysis of the overall system. Last week, I showed a remarkable new Deep Research tool that can deconstruct entire codebases and provide code reviews and recommendations.\nNow, with Codex, we're getting to the point where entire programming tasks can be delegated to the AI in the cloud, in much the same way those tasks were given to other programmers on a team or to junior programmers learning their way through code maintenance.\nOpenAI calls this \"Agent-native software development, where AI not only assists you as you work but takes on work independently.\"\nChanging developer workflow\nThe launch video demonstrated the ability of Codex to take on a variety of tasks at once, each running in its own isolated virtual environment.\nProgrammers assigned tasks to the agent, which went off and did the work without supervision. When the work was complete, the agent returned with test results and recommended code changes.\nThe demo showcased the Codex agent performing bug fixes, doing a scan for typos, making task suggestions, and performing project-wide refactoring (modifying code to improve structure without changing behavior).\nSenior developers and designers are no strangers to articulating requirements and reviewing others' work. Using Codex won't be much of a change for them. But for developers who haven't yet developed good requirements-articulation and review skills, properly managing Codex may prove to be a bit of a challenge.\nYet, if the tool performs as the demo appears to indicate it can, Codex will enable smaller teams and individual developers to accomplish more, reduce repetitive work, and be more responsive to problem reports.\nConsistency and flexibility\nOne of the problems I found early on with ChatGPT's coding was that it had a tendency to lose the thread or go off in its own direction. For individual blocks of code, that's annoying but not catastrophic. But if a coding agent is allowed to run fairly unsupervised, such stubborn refusal to follow directions could cause unintended and problematic consequences.\nAlso: The best AI for coding in 2025 (including two new top picks - and what not to use)\nTo help mitigate this, OpenAI has trained Codex to follow directions specified in an AGENTS.md file. This file in the repository allows programmers and teams to steer Codex's behavior. It can contain instructions on naming conventions, formatting rules, and any other set of consistent guidelines desired in the coding process. It's essentially an extension of the ChatGPT personalization settings, but for a repository-centric team environment.\nOpenAI has also introduced a version of Codex called Codex CLI that runs locally on a developer's machine. Unlike the cloud-based Codex, which runs asynchronously and reports back on completion, the local version operates on the programmer's command line and is synchronous.\nIn other words, the programmer types out an instruction and waits for the Codex CLI process to return a result. This allows a programmer to work offline with the local context of the active development machine.\nThinking through the implications\nThe demo was impressive, but during the launch video, the developers were very clear that what they were showing off and releasing is a research prototype. While it offers what they called \"magical moments,\" it still has a long way to go.\nAlso: I test a lot of AI coding tools, and this stunning new OpenAI release just saved me days of work\nI've been trying to dig in and triangulate on what exactly this technology means for the future of development and for my development process specifically. My main product is an open-source WordPress plugin, which itself has proprietary add-on plugins. Clearly, Codex could work itself through the public repository for the open-source core plugin.\nBut could Codex manage the relationship between one public and multiple private repositories as part of one overall project? And how would it do when testing involves not only my code but also spinning up an entire additional ecosystem -- WordPress -- to evaluate performance?\nAs a solo programmer, I definitely see the advantages of something like Codex. Even the $200-per-mnth Pro subscription makes sense. Hiring a helper programmer would cost a whole lot more per month than that fee, assuming I were to achieve tangible monetizable value out of it.\nAs a long-time team manager and professional communicator, I feel very comfortable delegating to something like Codex. It's not all that different chatting with an agent than it is chatting with a team member over Slack, for example.\nAlso: How to turn ChatGPT into your AI coding power tool - and double your output\nThe fact that Codex will make recommendations, draft versions, and wait for me to approve the results makes me feel a bit safer than merely letting it run loose in my code. It does open a very interesting door for a new development lifecycle, where the human sets goals, the AI drafts possible implementations, and then the human goes back in and either approves or redirects the AI for another cycle.\nBased on my earlier experiences using AIs for coding, it's clear that Codex could reduce maintenance time and get fixes out to users faster. It's not quite as clear how Codex would perform adding new features based on a specifications document. It's also not clear how much more or less difficult it would be to go into the code after Codex has worked on it to tweak functionality and performance.\nIt's interesting that AI coding is evolving across companies at about the same pace. I'm dropping another article soon on GitHub Copilot's Coding Agent, which does some of the same things that Codex does.\nIn that article, I expressed some concern that these coding agents will replace junior and entry-level programmers. Beyond concern for human jobs, there's also the question of what critical training opportunities will be lost if we delegate a middle phase of a developer's career to the AI.\nInto the unknown\nThere's a song in Disney's Frozen II called \"Into the Unknown,\" performed by Idina Menzel. The song centers on the main character's internal conflict between maintaining the status quo and her familiar life, and venturing out \"into the unknown.\"\nWith agentic software development, more than just AI coding, the entire software industry is going into the unknown. The more we rely on AI-based systems to build our software for us, the fewer skilled maintainers there will be. That's fine as long as the AIs continue to perform and be available. But are we letting some key skills atrophy, letting some good-paying jobs go, for the convenience of delegating to a not-yet-sentient cloud-based infrastructure?\nAlso: 10 professional developers on vibe coding's true promise and peril\nOnly time will tell, and hopefully we won't experience that telling when we're out of time.\nDo you see yourself delegating real development tasks to a tool like this? What do you think the long-term impact will be on software teams or solo developers? And do you worry about losing critical skills or roles as more of the code lifecycle is handed off to AI? Let us know in the comments below.\nGet the morning's top stories in your inbox each day with our Tech Today newsletter.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    },
    {
      "url": "https://www.zdnet.com/article/gpt-5-bombed-my-coding-tests-but-redeemed-itself-with-code-analysis/",
      "text": "GPT-5 bombed my coding tests, but redeemed itself with code analysis\nZDNET's key takeaways\n- GPT-5 Pro delivers the sharpest, most actionable code analysis.\n- A detail-focused prompt can push base GPT-5 toward Pro results.\n- o3 remains a strong contender despite being a GPT-4 variant.\nWith the big news that OpenAI has released GPT-5, the team here at ZDNET is working to learn about and communicate its strengths and weaknesses. In another article, I put its programming prowess to the test and came up with a less-than-impressive result.\nAlso: I tested GPT-5's coding skills, and it was so bad that I'm sticking with GPT-4o\nWhen Deep Research first appeared with the OpenAI o3 LLM, I was quite impressed with what it could understand from examining a code repository. I wanted to know how well it understood the project just from the available code.\nIn this article, I'm examining how well the three GPT-5 variants do in examining that same code repository. We'll dig in and compare them. The results are quite interesting. Here are the four models.\n- o3: a GPT-4 variant optimized for reasoning.\n- GPT-5: OpenAI's new main ChatGPT model, available to all tiers, including free.\n- GPT-5 Thinking: A variant of GPT-5 that OpenAI says is optimized for \"architectural reflection.\" It is available to $20/mo Plus and $200/mo Pro tiers.\n- GPT-5 Pro: OpenAI's current $200/mo top-tier model, with the highest reasoning and context capabilities.\nI gave all four models the same assignment. I connected them to my private GitHub repository for my open-source free WordPress security plugin and its freemium add-on modules, selected Deep Research, and gave them this prompt.\nExamine the repository and learn its structure and architecture. Then report back what you've learned.\nFor those models that asked to choose areas of detail about what I wanted, I gave them this prompt.\nEverything you can tell me, be as comprehensive as possible.\nAs you can see, I didn't provide any context other than the source code repo itself. That code has a README file, as well as comments throughout the code, so there was some English-language context. But most of the context has to be derived from the folder structure, file names, and code itself.\nAlso: The best AI for coding in 2025 (and what not to use)\nFrom that, I hoped that the AIs would assess its structure, quality, security posture, extensibility, and possibly suggest improvements. This should be relevant to ZDNET readers because it's the kind of high-judgement, detail-oriented work that AIs are being used for. It certainly can make coming up to speed on an existing coding project easier, or at least provide a foundation for initial understanding.\nTL;DR summary\nOther than the two prompts above, I didn't give the LLMs any guidance about what to tell me. I wanted to see how they evaluated the repository and what sort of analysis they could provide.\nAs you can see from this table, overall coverage was quite varied in scope. More checks mean more depth of coverage.\nTo create this aggregate, topics like \"Project Purpose & Architecture,\" \"System Architecture,\" and \"Plugin Design & Integration\" were all normalized under Purpose/Architecture. Directory/File Structure contained any section mapping folders and files. Execution flow combines anything about how the software code runs. Recommendations/Issues combines all discussions of modernization suggestions, open issues, and minor red flags.\nIn terms of overall value, I'd rank the four LLMs as follows (from best to least best).\n- GPT-5 Pro: Most precise, engineering-ready, and actionable.\n- GPT-5: Widest scope, excellent mapping, and defensive-coding insight.\n- o3: Concise, modernization-focused, but lighter on underlying architecture.\n- GPT-5 Thinking: Best onboarding narrative, least evaluative depth.\nPro, of course, is only available in the $200/mo ChatGPT Pro tier. Later in this article, I'll show one way to modify the above prompts to get GPT-5 (non-Pro) to provide a fairly close approximation of the overall depth of the Pro response.\nGPT-5 Thinking, which is a model available in the $20/mo Plus plan, was the least helpful of the group. The GPT-4 generation o3 Deep Thinking model still holds up, but you can see how its self-directed focus is a bit different from the other two.\nAlso: Google's Jules AI coding agent built a new feature I could actually ship - while I made coffee\nMy main conclusion is that I was a bit surprised about how close the models were to each other. GPT-5, as OpenAI promised, did seem to provide a jump in overall cognition and usefulness, but nothing I would consider game-changing.\nWith that, let's dive into some specific examples that help illustrate my conclusions. Each of these sections is pulled from the various reports generated and shows you how each model provided similar information.\nSecurity posture, according to the models\nBelow, you can see how GPT-5 Pro names exact mechanisms (like file-top guard, nonces, manage_options). I've provided exact snippets here from the reports generated by all four models. GPT-5 affirms best practices but keeps it conceptual. o3 describes what happens (redirects/login flow) more than how it's hardened. GPT-5 Thinking gives the clearest \"what runs when\" story for new developers coming up to speed on the repo.\nGPT-5 Pro: Most concrete, code-level\n\"It guards against direct file access (if (!defined('ABSPATH')) exit; at the top of PHP files). It sanitizes input where appropriate \u2026 using WordPress nonces in AJAX handlers. It uses capability checks (e.g., adding menu pages only for users with manage_options capability \u2026 only admins see those tools). The code tries not to load unnecessary things \u2026 like only loading certain admin files on certain contexts.\"\nGPT-5: Correct, but higher-level\n\"There are checks for WordPress functions before use \u2026 so the plugin behaves gracefully even on very old WordPress setups. The plugins often guard against direct file access by checking \u2026 to prevent security issues from accessing .php files directly. Add-ons verify the presence of core before proceeding \u2026 and show an admin error if CMB2 isn't loaded.\"\no3: Runtime behavior, less on hardening specifics\n\"Purpose: My Private Site locks down an entire site so only logged-in users can view content \u2026 while protecting the rest. Overall architecture: [it] integrates deeply with WordPress's hook system and login/logout events to manage redirects and track login state.\"\nGPT-5 Thinking: Clear execution flow, onboarding tone\n\"Admin vs Front-end: It checks is_admin() to determine context. If on the front-end (not admin), it retrieves the saved privacy setting and, when enabled, hooks at a point like template_redirect to redirect unauthorized visitors. Throughout this initialization, the plugin uses WordPress hooks (actions and filters) to integrate functionality.\"\nLicensing and update mechanism, according to the models\nGPT-5 Pro didn't just describe the system; it walked through the process in sequential operational steps, almost like a short runbook you could hand to a developer or QA tester. GPT-5 confirms the architecture but abstracts the plumbing. GPT-5 Thinking adds a helpful \"how add-ons plug into the Licenses tab\" detail. o3 largely leaves licensing internals on the cutting room floor in favor of a fairly unhelpful modernization critique.\nGPT-5 Pro: Explains it step-by-step\n\"The core plugin provides utility functions to get and store license keys in a centralized option (jr_ps_licenses) and to contact the EDD license server for validation. Each extension plugin defines its own updater using EDD_SL_Plugin_Updater, passing the current version, the license key from the centralized store, and the EDD store URL. The core plugin's UI has a 'Licenses' tab, and extensions inject their own license fields via filters.\"\nGPT-5: Conceptual, but accurate\n\"License integration: The core plugin centralizes license management \u2026 and the add-ons piggyback on the core's licensing mechanism, integrating their license fields into the core plugin's interface.\"\no3: Barely mentions this topic at all\nThe o3 report spends most of its time on modernization and architecture. It discusses configuration and update behavior but does not walk through option keys, updater classes, or the Licenses UI wiring with the same procedural detail as GPT-5 and GPT-5 Pro. So there's nothing here to quote as a demonstration.\nGPT-5 Thinking: Good UI and extensibility observation\n\"The add-ons heavily rely on hooks provided by core or WordPress: They use add_filter/add_action calls to insert their logic \u2026 and use WordPress action hooks to integrate their license fields into the Licenses tab that the core plugin triggers when building the Licenses tab.\"\nState management, according to the models\nBoth GPT-5 Pro and GPT-5 explicitly pointed out how my code uses \"one option array + prune + no-op writes,\" which is a WordPress best practice for code maintainability. Both o3 and GPT-5 Thinking describe the lifecycle and effects (what's initialized, what loads when) rather than the exact option structure.\nGPT-5 Pro: Looks at specific storage pattern\n\"Settings are stored in a single serialized option \u2026 initialization routines add default keys, prune deprecated ones, and only update the option in the database if there is an actual change, avoiding unnecessary writes.\"\nGPT-5: Also looks at storage pattern, but more generally\n\"State Management: Plugin settings are stored in WordPress options as a central settings array and the code ensures defaults are applied while removing deprecated ones on each load, but only writes to the database when changes occur.\"\no3: Identifies intent and behavior, but doesn't discuss internals\n\"The main plugin initializes defaults (installed version, first-run timestamp, etc.). On each run it ensures these options exist and, if the privacy feature is disabled, the enforcement hook is not added.\"\nGPT-5 Thinking: Discusses basic flow and modules\n\"Module includes: includes admin and common modules in the back-end; on the front-end it retrieves the saved privacy setting and, when enabled, loads enforcement logic (e.g., in template_redirect). It registers a deactivation hook to clean up on deactivation (e.g., deleting a flag option).\"\nWhat does this mean for GPT-5?\nI was unimpressed with GPT-5 when it came to my coding tests. It failed half of my tests, an unprecedentedly bad result for what has previously been the gold standard in passing coding tests.\nBut GPT-5 was quite impressive in its analysis of the GitHub repository. It could be a powerful tool for onboarding new programmers, for someone adopting code, or simply for coming back up to speed on a project that's been untouched for a while.\nAlso: How I test an AI chatbot's coding ability - and you can, too\nThe GPT-4 generation o3 model is known to be a strong reasoning model, which is why it has been the basis for ChatGPT Deep Research. But GPT-5 was able to combine both breadth and detail, which is where o3 and GPT-4o were weak in previous tests.\nThe older models did give accurate summaries and useful suggestions, but they missed interconnections. For example, the older models were never able to show how UI flows, licensing, and update mechanisms work together.\nEven the base version of GPT-5 was able to identify cross-cutting concerns without additional prompting. Repository structure, backward compatibility, performance characteristics, and state management patterns all appeared in the first draft. Trying to get GPT-4 to span subjects is often an exercise in deep frustration.\nI found GPT-5's ability to understand and explain a complex interconnected system like my security product, all in one pass, to be a substantial improvement over the GPT-4 generation.\nIs GPT-5 Pro worth $200/mo?\nMaybe. If you're in a real rush to get to know a project and want as much of a data dump as possible as quickly as possible, yes. If you're operating on a big programming budget and $200/mo doesn't matter to you, yes.\nBut I find that cost hard to bear, especially when I have to subscribe to a wide range of AI services to evaluate them. So, now that I'm nearing the end of my one-month test of Pro-level activities, I'm planning on downgrading back to the $20/mo Plus plan.\nAlso: How to use GPT-5 in VS Code with GitHub Copilot\nPro's edge over GPT-5 wasn't about knowing more facts; it was about delivering those facts in a form you can act on immediately. The Pro report didn't just explain that security looked good; it cited the exact guards and checks in the code. It didn't just say licensing was centralized; it mapped the exact functions and database options involved.\nAgain, if you're on a time crunch, you might consider Pro. But I also think you can modify the base GPT-5's responses, with detail like the Pro report produced, simply by using better prompting.\nThat's next\u2026\nHow to get Pro-level results from base GPT-5\nI fed both the GPT-5 and GPT-5 Pro reports into GPT-5 and asked it for a prompt that would push the base-level GPT-5 to give GPT-5 Pro comprehensiveness as a result. This is that prompt, which you should add to any query where you want more complete coding information:\n*High-Specificity Technical Mode: ***In your answer, combine complete high-level coverage with exhaustive implementation-level detail.\n- Always name exact constants, functions, classes, hooks, option names, database tables, file paths, and build tools where possible, quoting them exactly from the code or material provided.\n- For every claim, explain why it's true and how you can tell (include reasoning tied to the evidence).\n- For each improvement you suggest, make it actionable and reference where in the codebase it applies.\n- Do not generalize when specifics are available.\n- Structure the output so a developer could use it directly to verify findings or implement recommendations.\nThis worked fantastically well. It took ChatGPT GPT-5 12 minutes to produce a 15,477-word document, complete with analysis and coding blocks. For example, it describes how value initialization is done, and then shows the code that accomplishes it.\nI think you could fine-tune this prompt and get Pro-level results without having to pay the $200/mo fee. I'm certainly going to tinker with this idea, possibly using GPT-5 to refine the specifications in the prompt for different areas I want to delve deeply into. I'll let you know how it goes.\nSee for yourself\nI had some difficulty setting up sharing for each of these long reports, so I just copied the results into Google Docs and shared them. Here are the links if you want to look at any of these reports.\n- o3 Deep Research\n- GPT-5 Deep Research\n- GPT-5 Thinking Deep Research\n- GPT-5 Pro Deep Research\n- GPT-5 Deep Research with Detail Prompt\nYou are welcome to dig into these documents and learn how my project is structured. While you may or may not care about my project, it's instructive to see how the various models perform. While you can read the reports, my actual repo is restricted since it's my private development repository.\nWhat about you? Have you tried using GPT-5 or GPT-5 Pro to analyze your own code? How did its insights compare to earlier models like GPT-4 or o3? Do you think the $200/month Pro tier is worth it for the extra precision, or could you get by with better prompts in the base version? Have you found AI code analysis useful for onboarding, refactoring, or improving security? Let us know in the comments below.\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV."
    }
  ],
  "argos_summary": "The article recounts the author's unsettling experience with AI\u2014particularly GPT\u20115\u2014when attempting to modify mission\u2011critical code used by thousands of sites, highlighting the risks of PHP object injection and the potential for catastrophic bugs. Despite GPT\u20115\u2019s advanced capabilities, the author found its coding output inconsistent and unreliable, leading to a decision to retain full control over core infrastructure and rely on GPT\u20114o for less critical tasks. The piece underscores the broader cautionary tale of AI coding tools, noting frequent hallucinations, security vulnerabilities, and the need for rigorous vetting and human oversight in production environments.",
  "argos_id": "3UPZV70L9"
}