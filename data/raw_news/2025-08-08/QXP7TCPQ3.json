{
  "url": "https://www.forbes.com/sites/janicegassam/2025/08/08/your-ai-hiring-tool-might-be-racist-here-are-three-ways-to-address-ai-bias-and-make-hiring-more-fair/",
  "authorsByline": "Ph.D., Janice Gassam Asare",
  "articleId": "0a5b3a87c62e44b6b6be0f84368b5482",
  "source": {
    "domain": "forbes.com",
    "paywall": true,
    "location": {
      "country": "us",
      "state": "NJ",
      "county": "Hudson County",
      "city": "Jersey City",
      "coordinates": {
        "lat": 40.7215682,
        "lon": -74.047455
      }
    }
  },
  "imageUrl": "https://imageio.forbes.com/specials-images/imageserve/6896490c7c31c57082aa661c/0x0.jpg?format=jpg&crop=2318,1302,x0,y0,safe&height=900&width=1600&fit=bounds",
  "country": "us",
  "language": "en",
  "pubDate": "2025-08-08T19:07:35+00:00",
  "addDate": "2025-08-08T19:12:21.320131+00:00",
  "refreshDate": "2025-08-08T19:12:21.320132+00:00",
  "score": 1.0,
  "title": "Your AI Hiring Tool Might Be Racist. Here Are Three Ways To Address AI Bias And Make Hiring More Fair",
  "description": "AI hiring tools can speed up recruiting but they can also scale discrimination. Here are three steps to address AI bias and protect job candidates.",
  "content": "Many employers use AI tools to help them assess and manage job applicants; one Resume Builder survey indicated that by the end of 2025, 68% of companies will be using AI for hiring. But many of these AI tools are laden with bias, leading to candidates from underrepresented racial groups being filtered out. One 2024 study by Kyra Wilson and Aylin Caliskan revealed that the Massive Text Embedding (MTE) models used by many resume screening tools were biased. The MTE models the researchers analyzed significantly favored white-associated names (in 85.1% of the cases); further analysis also determined that Black males were disadvantaged in 100% of the cases.\n\nA 2024 Bloomberg analysis revealed racial bias in OpenAI\u2019s ChatGPT 3.5. According to a Business Insider report, researchers used ChatGPT to screen resumes for jobs, with their analysis indicating that the AI tool over-selected Asian women job applicants and under-selected Black men job applicants. In a separate Australian study, researcher Natalie Sheard demonstrated that the AI tools used during the hiring process penalized non-native English speakers with accents from other countries more than English-language speakers in the U.S. The Australian human resource professionals interviewed indicated that curriculum vitae analysis systems and video interview systems were common AI tools used to evaluate job candidates. Many companies use facial analysis and recognition software to make workplace decisions, but these tools can exacerbate existing racial disparities. \n\n\n\n There are a few different reasons why bias creeps into the AI tools used for hiring. The first is faulty training data. According to IBM, machine learning that is trained on samples that are homogenous and lack diversity will reproduce bias. In the aforementioned Australian study, the researcher noted another way bias can creep into these systems: an algorithmic model may prioritize characteristics and traits that are typically associated with dominant cultural norms, disadvantaging non-white job candidates and those with less traditional backgrounds and characteristics. IBM argues that when humans are programming AI models, our biases can seep into these models and show up in ways like exclusion bias; AI models also learn and replicate societal biases, which can show up as stereotyping bias.\n\nAI hiring tools that end up causing bias can be a costly mistake for employers. Here\u2019s how companies can address biases in AI hiring tools:\n\n\n\n 1. Require audits of AI tools. Under New York City\u2019s Automated Employment Decision Tools Law, which went into effect in July 2023, employers are prohibited from using \u201cautomated employment decision tools\u201d to make hiring and promotion decisions unless an independent auditor reviews the tool before usage. Under the law, job candidates are also required to receive notice that the tool is being used. Organizations should follow similar guidelines and consider implementing AI tools to make hiring decisions only after an outside review from an auditor.\n\n2. Assess AI tool over time. After making the decision to implement AI hiring tools into the workplace, these tools should be continuously monitored over time. A 2025 article from Peoplebox.ai suggests that in order to get the most out of your AI tool, consistent checks are imperative. Assess important metrics to ensure AI tools are addressing the issues they were adopted for. Some questions the article suggests considering: \u201cHas AI helped you hire faster? Are your new hires performing better? Is the candidate\u2019s experience improving?\u201d Also evaluate whether job candidates from certain backgrounds are being filtered out by the AI tool and manually assess these candidates to ensure fairness in the process.\n\n3. Don\u2019t rely solely on AI tools. There should be some oversight when deploying AI hiring tools and companies shouldn\u2019t rely on these tools to make the final hiring decisions. Employees that will be engaging with these tools should receive adequate and thorough training on ethical use and deployment of AI tools. Trust (to an extent) but verify, when it comes to workplace AI tools. It\u2019s important to remember that AI cannot replace humans in the hiring process.\n\nWe must remember that no workplace tool will ever be perfect. AI is not a panacea for recruitment and hiring issues and should be implemented with care, concern and consideration. AI is revolutionizing our workplaces in a plethora of ways and in the years to come, AI in our workplaces will be inevitable. While the possibilities of these tools seem endless, we must also be mindful of their vulnerabilities and limitations.",
  "medium": "Article",
  "links": [
    "https://www.peoplebox.ai/blog/are-ai-hiring-tools-overpromising/",
    "https://www.forbes.com/sites/janicegassam/2025/08/06/your-face-could-cost-you-the-job-the-dangerous-rise-of-facial-recognition-at-work/",
    "https://www.ibm.com/think/topics/ai-bias#:~:text=Algorithm%20bias:%20Misinformation%20can%20result,identify%20new%20patterns%20or%20trends.",
    "https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcwOTg1NjE0OCwiZXhwIjoxNzEwNDYwOTQ4LCJhcnRpY2xlSWQiOiJTQTA1Q1FUMEFGQjQwMCIsImJjb25uZWN0SWQiOiI2NDU1MEM3NkRFMkU0QkM1OEI0OTI5QjBDQkIzRDlCRCJ9.MdkSGC3HMwwUYtltWq6WxWg3vULNeCTJcjacB-DNi8k&leadSource=uverify%20wall",
    "https://www.resumebuilder.com/7-in-10-companies-will-use-ai-in-the-hiring-process-in-2025-despite-most-saying-its-biased/",
    "https://arxiv.org/abs/2407.20371",
    "https://www.businessinsider.com/chatgpt-racial-bias-job-hiring-report-2024-3",
    "https://onlinelibrary.wiley.com/doi/epdf/10.1111/jols.12535",
    "https://www.americanbar.org/groups/business_law/resources/business-law-today/2024-april/navigating-ai-employment-bias-maze/"
  ],
  "labels": [],
  "claim": "",
  "verdict": "",
  "keywords": [
    {
      "name": "AI hiring tools",
      "weight": 0.1303537
    },
    {
      "name": "AI tool",
      "weight": 0.1249196
    },
    {
      "name": "AI tools",
      "weight": 0.1249196
    },
    {
      "name": "workplace AI tools",
      "weight": 0.11680275
    },
    {
      "name": "common AI tools",
      "weight": 0.11081534
    },
    {
      "name": "AI Bias",
      "weight": 0.09644267
    },
    {
      "name": "automated employment decision tools",
      "weight": 0.095801316
    },
    {
      "name": "many resume screening tools",
      "weight": 0.09440465
    },
    {
      "name": "AI models",
      "weight": 0.09375781
    },
    {
      "name": "AI",
      "weight": 0.082656935
    }
  ],
  "topics": [
    {
      "name": "Social Issues"
    },
    {
      "name": "AI"
    },
    {
      "name": "Racial Injustice"
    },
    {
      "name": "Artificial Intelligence"
    }
  ],
  "categories": [
    {
      "name": "Tech"
    }
  ],
  "taxonomies": [
    {
      "name": "/Science/Computer Science/Machine Learning & Artificial Intelligence",
      "score": 0.75537109375
    },
    {
      "name": "/Jobs & Education/Jobs/Career Resources & Planning",
      "score": 0.62841796875
    },
    {
      "name": "/News/Technology News",
      "score": 0.5498046875
    },
    {
      "name": "/Business & Industrial/Business Operations/Human Resources",
      "score": 0.403076171875
    },
    {
      "name": "/People & Society/Social Issues & Advocacy/Other",
      "score": 0.3369140625
    },
    {
      "name": "/People & Society/Social Issues & Advocacy/Work & Labor Issues",
      "score": 0.32373046875
    },
    {
      "name": "/People & Society/Other",
      "score": 0.317138671875
    }
  ],
  "sentiment": {
    "positive": 0.08355713,
    "negative": 0.6020508,
    "neutral": 0.31445312
  },
  "summary": "Many employers use AI tools to manage job applicants and manage job applications, but many are often laden with bias, leading to candidates from underrepresented racial groups being filtered out. One 2024 study revealed that the Massive Text Embedding (MTE) models used by many resume screening tools were biased, favoring white-associated names in 85.1% of cases and 100% disadvantaged Black males in 100% cases. Another study found that AI tools penalized non-native English speakers with accents from other countries more than English-language speakers in the U.S. New York City\u2019s Automated Employment Decision Tools Law prohibits employers from using \u201cautomated employment decision tools\u201d to make hiring and promotion decisions unless an independent auditor reviews the tool before usage. Companies should require audits of AI tools and monitor these tools over time. It is also recommended that employees be trained on ethical use and deployment of these tools.",
  "shortSummary": "Despite widespread adoption of AI tools for hiring, many AI tools are laden with bias, requiring consistent checks to ensure fairness and ensure fair hiring practices.",
  "translation": "",
  "translatedTitle": "",
  "translatedDescription": "",
  "translatedSummary": "",
  "reprint": false,
  "reprintGroupId": "0523b89c90d748a489c1e81973739c21",
  "places": [],
  "scraped_sources": [
    {
      "url": "https://www.forbes.com/sites/janicegassam/2025/08/06/your-face-could-cost-you-the-job-the-dangerous-rise-of-facial-recognition-at-work/",
      "text": "The Covid-19 pandemic ushered in a new era of remote work. Many employers desperate to track and monitor employees working away from the office implemented different technology tools to surveille employees. According to one 2025 report, more than half of fortune 100 employees were required to return back to the office fulltime. With many back-to-office mandates in place, remnants of surveillance culture have remained.\nMany companies are using facial recognition software to manage employees. A recent survey by ExpressVPN indicated that 74% of U.S. employers are using online monitoring and surveillance tools with 67% using biometric tracking like facial recognition and fingerprint scans. Employers use facial recognition software in a number of different ways: to track employee attendance, to identify employees, to interview and screen job candidates, to reduce the number of employee touchpoints, and to track employees (this is common for delivery and gig workers). What are the vulnerabilities and limitations of using facial analysis and recognition software in the workplace and how does it reinforce biases?\nThere have been several different cases where facial analysis and recognition software has caused harm, reinforcing biases in the workplace. In 2025, a complaint was filed with the Colorado Civil Rights Division and the Equal Employment Opportunity Commission (EEOC) against the software company Intuit and the human resources assessment software vendor HireVue. The complaint alleges that the AI used by HireVue resulted in an Indigenous and Deaf woman being denied a promotion based on her race and disability. In a separate case, a makeup artist at a leading brand claimed to have been fired in 2020 because of a video interview through HireVue, where the facial analysis software marked her poorly for her body language during the interview.\nIn 2024, an Uber Eats driver won a case where he alleged that that company fired him because of racist facial recognition software. The former employee claimed that he was fired after the company\u2019s verification checks, which use facial recognition software, failed to recognize his face. Scholar and writer Dr. Joy Buolamwini has focused much of her research on the flaws with facial recognition technology discussing in her book Unmasking AI as well as the documentary Coded Bias how the technology is less accurate at identifying darker skin tones.\nThere is a wealth of evidence that indicates that facial analysis and recognition technology disproportionately impacts marginalized communities. This technology frequently misidentifies Black people leading to wrongful arrests. One 2025 study indicated that facial recognition tools had higher error rates for adults with Down syndrome. Researchers also note that facial recognition tools are less accurate for transgender individuals and these tools struggle to identify non-binary folks.\nIntegrating facial analysis and recognition tools into the workplace can have deleterious effects on employees. A 2023 assessment of feedback shared with the White House Office of Science and Technology Policy indicated that digital surveillance in the workplace creates a sense of distrust among employees, making them feel constantly monitored and leading to a decline in productivity and morale. Workers also noted that digital surveillance could limit unionizing, deterring this type of behavior in the workplace. There were also employee concerns about data privacy and how the data collected would be used.\nEmployers should think twice about implementing facial analysis and recognition software in the workplace. Not only is the type of technology prone to bias, but it can also erode employee trust and morale. If organizations have this type of technology in place already, they should request more information from the vendor about audits and what accountability measures are in place to ensure accuracy and mitigate bias. Employees should know their rights and there must be transparency around how data is collected, stored, and used. We must deeply consider the future we are creating when our face holds the key to whether we praised or punished."
    },
    {
      "url": "https://arxiv.org/abs/2407.20371",
      "text": "Computer Science > Computers and Society\n[Submitted on 29 Jul 2024 (v1), last revised 20 Aug 2024 (this version, v2)]\nTitle:Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval\nView PDF HTML (experimental)Abstract:Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1\\% of cases and female-associated names in only 11.1\\% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100\\% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have implications for widely used AI tools that are automating employment, fairness, and tech policy.\nSubmission history\nFrom: Kyra Wilson [view email][v1] Mon, 29 Jul 2024 18:42:39 UTC (5,646 KB)\n[v2] Tue, 20 Aug 2024 21:49:26 UTC (5,642 KB)\nCurrent browse context:\ncs.CY\nReferences & Citations\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs."
    },
    {
      "url": "https://www.resumebuilder.com/7-in-10-companies-will-use-ai-in-the-hiring-process-in-2025-despite-most-saying-its-biased/",
      "text": "To gain an understanding of how many companies are utilizing artificial intelligence (AI) in hiring, Resume Builder surveyed 948 business leaders in October 2024.\nKey findings:\n- Half of companies currently use AI in the hiring process, and 68% will by the end of 2025\n- A number of companies currently have AI conduct entire interviews\n- Nearly all companies believe AI produces biased recommendations\n- Majority of companies let AI reject candidates without human oversight\n7 in 10 Companies Will Use AI in Hiring in 2025\nCurrently, more than half (51%) of companies are leveraging AI technology in their hiring processes, and this number is set to rise. By the end of 2025, 68% of companies will be using AI to acquire new talent. Larger companies are more likely to adopt these technologies.\nHow companies currently use AI in hiring\nToday, 82% of companies use AI to review resumes, while 40% employ AI chatbots to communicate with candidates. About 23% use AI to conduct interviews, and 64% apply AI to review candidate assessments. Additionally, 28% of companies use AI for onboarding new hires, and 42% scan social media or personal websites as part of the hiring process. Only 0.2% of companies report not using AI in their hiring practices.\nHow companies plan to use AI in hiring\nIn 2025, most companies plan to use AI for reviewing resumes (83%) and conducting candidate assessments (69%). AI will also play a role in analyzing a candidate\u2019s digital footprint, with nearly half (47%) of companies planning to scan social media profiles or personal websites as part of the evaluation process. Furthermore, 39% will implement AI-powered chatbots to communicate with candidates throughout the hiring journey.\nAI\u2019s influence will extend beyond initial screenings, with 36% of companies intending to integrate AI into onboarding processes. A smaller, yet notable percentage (19%) are even looking to AI for conducting interviews.\n\u201cAs organizations face an overwhelming influx of resumes, especially with the expansion of remote and hybrid work models, more companies are leveraging AI in the hiring process,\u201d says Resume Builder\u2019s Chief Career Advisor Stacie Haller. \u201cThese models have broadened the talent pool, allowing candidates from various geographic locations to apply, resulting in an exponential increase in applications for every open position.\u201d\n\u201cGiven the competition for top talent, companies understand that the longer the hiring process takes, the higher the risk of losing the best candidates. AI helps by automating tasks, allowing organizations to speed up their decision-making and reduce the manual workload of hiring managers.\n\u201cOrganizations adopt AI to varying degrees. While some use it primarily for resume screening, others incorporate AI more extensively, using it in everything from chatbots that handle candidate queries to sophisticated algorithms that predict a candidate\u2019s success based on historical data.\u201d\n1 in 4 companies using AI for interviews rely on it for the entire process\nCompanies that currently use AI in interviews do so across a range of tasks. A total of 81 % of companies use AI to ask interview questions, 65% employ it to analyze candidates\u2019 language, and 60% use it to transcribe interviews and assess tone, language, or body language. Additionally, 52% utilize AI to collect data through facial recognition.\nBy 2025, 76% of companies plan to use AI for asking interview questions, 63% will collect facial recognition data, and 62% will analyze candidates\u2019 language. Additionally, 60% will use AI to transcribe interviews, while 59% will assess tone, language, or body language.\nCurrently, 24% of companies have AI conduct the entire interview process, and this figure is expected to rise to 29% by 2025.\n\u201cCandidates must recognize the increasing role AI plays in the hiring process. It\u2019s critical that applicants tailor their resumes for each specific job, highlighting the key skills and qualifications that are mentioned in the job description,\u201d says Haller.\n\u201cIn addition to resume customization, candidates should be well-prepared for interviews. It\u2019s essential to practice key elements of a successful video interview, such as maintaining eye contact by looking directly into the camera, dressing professionally, and ensuring that the background is appropriate and distraction-free.\n\u201cIn this increasingly automated landscape, candidates who adapt their approach by preparing for AI-driven screenings and interviews will have a competitive advantage in their job search.\u201d\n7 in 10 Let AI Reject Candidates Without Human Oversight\nCurrently, 21% of companies automatically reject candidates at all stages of the hiring process without human review, while 50% use AI only for rejections at the initial resume screening stage. However, 29% maintain human oversight for all rejection decisions.\nLooking ahead to 2025, slightly fewer companies (16%) plan to allow AI to reject candidates throughout the entire hiring process, while 49% will limit AI\u2019s ability to reject candidates to initial resume reviews. Meanwhile, more (35%) intend to maintain human oversight for all rejection decisions.\nNearly All Companies Say AI Produces Biased Recommendations\nWhen it comes to bias in AI hiring tools, 9% of companies using the technology report it always produces biased recommendations, while 24% say it often does. Another 34% say bias occurs sometimes, 30% believe it happens rarely, and only 4% say AI never produces bias.\nRegarding the specific types of bias AI introduces, 47% of companies believe it leads to age bias, 44% cite socioeconomic bias, 30% mention gender bias, and 26% point to racial or ethnic bias.\nFor companies planning to use AI in hiring by 2025, bias is just one of several concerns. About 56% of companies are worried that AI could screen out qualified candidates, and 48% are concerned about the lack of human oversight. Additionally, 46% fear that AI may introduce bias based on factors such as age, gender, or race. Other concerns include a reduction in the candidate experience (21%), though 9% of companies report having no concerns about using AI in hiring.\n\u201cIt\u2019s important to note that while AI can be a powerful tool, it\u2019s not without its challenges. If not properly calibrated, AI systems can introduce biases based on past hiring patterns or data that reflect historical inequalities. Therefore, companies must strike a balance between speed, efficiency, and fairness to ensure that AI enhances, rather than hinders, their ability to find the best talent,\u201d says Haller.\nMethodology\nThis survey, launched on October 9, 2024, was commissioned by ResumeBuilder.com and conducted online by the polling platform Pollfish. Overall, 948 business leaders completed the survey.\nTo qualify for the survey, all participants had to be over 25, have a household income of at least $75,000, have a bachelor\u2019s degree or a higher level of education, have a manager-level role or higher, and work at a company with more than 21 employees.\nRespondents also had to indicate their level of knowledge about hiring practices at their company via a screening question.\nFor all media inquiries, contact [email protected]."
    },
    {
      "url": "https://www.businessinsider.com/chatgpt-racial-bias-job-hiring-report-2024-3",
      "text": "- OpenAI's ChatGPT showed racial bias when screening resumes for jobs, a Bloomberg investigation found.\n- The AI tool over-selected Asian women candidates and under-select Black men, Bloomberg reported.\n- AI has been touted as a way to reduce bias in hiring.\nIf you're looking for a job, beware: The AI tool screening your resume might have a racial bias similar to some human recruiters.\nOpenAI's ChatGPT has become a popular tool for recruiters as hiring ramps up at the start of 2024, Bloomberg reported on Friday. But the news outlet's investigation found that the tool seemed to prefer names generally associated with Asian applicants more than those associated with Black job seekers.\nBloomberg created resumes for fictitious candidates, then asked ChatGPT 3.5 to rank them for multiple different job listings, from \"financial engineer\" to \"HR specialist.\"\nAfter it ran the experiment 1,000 times, Bloomberg found that resumes using names associated with Asian women were ranked as the top candidate 17.2% of the time \u2014 the highest percentage out of the eight race- and gender-based groups that the investigation tested.\nThe same could be said of Black men just 7.6% of the time, according to the results.\n\"If GPT treated all the resumes equally, each of the eight demographic groups would be ranked as the top candidate one-eighth (12.5%) of the time,\" Bloomberg reported.\nOpenAI did not immediately respond to Business Insider's request for comment on the report.\nThe company told Bloomberg that companies may adjust ChatGPT when using it to screen resumes. Recruiters can add their own safeguards against bias, the company said, like stripping names out of the screening process.\nHiring processes usually involve more than screening names and resumes, the report says. But applicants of certain genders or races \"would be adversely impacted\" if a company used only an AI tool, the report says, referencing the federal government's standard for hiring discrimination.\nBI previously reported how some have hoped that AI could reduce bias in hiring \u2014 though they've cautioned that the technology should be just one tool that human recruiters use to fill positions.\nBut AI's bias is a big challenge for the technology \u2014 potentially even larger than the risk that it will become sentient.\nFor example, AI-generated faces of white people were more convincing than similar images of people of color, researchers found last year.\nOne explanation: AI models are trained more on white faces than those of other racial groups.\nJoy Buolamwini, a computer scientist and founder of The Algorithmic Justice League, found that AI can hurt and discriminate against marginalized communities, including through facial recognition."
    },
    {
      "url": "https://www.peoplebox.ai/blog/are-ai-hiring-tools-overpromising/",
      "text": "You\u2019ve invested in an AI hiring tool, hoping it would save time, reduce bias, and help you find the perfect candidate. But while 42% of the global companies now rely on AI for the hiring processes, the results are often mixed. Nearly half of U.S. job seekers, on average, 49%, believe AI is more biased than human recruiters, and many companies are still struggling to see clear results.\nWhat is the primary root cause here? As companies increasingly rely on AI-driven hiring platforms, highly qualified candidates are often overlooked\u2014filtered out because they don\u2019t match specific keywords or standard patterns. Six/seven months into using AI, you might still find yourself overwhelmed with resumes, frustrated applicants, and a hiring process that feels just as time-consuming as before.\nThe truth is that AI can be an incredibly powerful tool, but it\u2019s not perfect. It works best when paired with human insight and used with a clear understanding of what it can\u2014and can\u2019t\u2014do.\nIn this blog, we\u2019ll explore:\n- What AI tools can help you achieve.\n- Where they fall short.\n- How to combine AI with human expertise for better, smarter hiring results.\nLet\u2019s Start with the Strengths: What AI Does Best in Hiring\nAI is an effective tool that is great at enhancing talent acquisition by automating repetitive tasks and improving efficiency. Such as:\n1. Automating Repetitive Tasks\nA big part of the talent acquisition process involves repetitive, boring tasks that take up a lot of time. Going through piles of resumes, scheduling interviews, and updating systems can make the process feel like a never-ending slog. AI handles these tasks quickly and efficiently, freeing recruiters to focus on more important things.\n- Sorting resumes: On average, recruiters spend 23 hours screening resumes for just one hire. What\u2019s worse? 75% to 88% of those resumes aren\u2019t even qualified. AI tools like Peoplebox.ai use algorithms to scan thousands of resumes in seconds, searching for job-specific keywords, required skills, and relevant experience. It compares resumes to predefined criteria, instantly ranking the best matches so recruiters only see the top candidates.\n- Updating ATS data: Resumes in your ATS (Applicant Tracking System) are often outdated or incomplete. AI updates this information by pulling data from public sources, making sure no reliable candidate is left behind.\n- Keeping resumes organized: AI automatically tags resumes in your ATS with labels for skills, experience, or job history so they\u2019re easy to find later. It also flags potential matches for other current or future openings.\n- Scheduling interviews: Coordinating calendars is a headache. AI tools check recruiter and candidate availability, sync schedules, and send reminders to both parties. This eliminates the back-and-forth of emails and ensures the process moves quickly.\nBy automating these tasks, AI helps companies fill roles faster and reduce delays. Open positions hurt productivity and add stress to teams, so hiring faster keeps things running smoothly. Best of all, recruiters can spend less time on admin work and more time connecting with the right deserving candidates.\n2. Making Hiring Decisions Fairer\nIt\u2019s hard to stay completely unbiased when reviewing candidates. Sometimes, a name, school, or background can unconsciously affect decisions\u2014even if you don\u2019t mean for it to. AI helps remove this bias by focusing only on the data that really matters. In fact, according to an article by Ashley Whillans and Jeff Polzer \u2013 Applied: Using Behavioral Science to Debias Hiring shows that 50-75% of hiring decisions are influenced by unconscious bias, which means relying on AI creates consistency and reduces the risk of overlooking great talent.\n- AI tools evaluate candidates based on skills, experience, and qualifications, ignoring personal details like names, genders, or photos.\n- Machine learning and predictive analytics train these tools to apply the same evaluation criteria to every candidate, ensuring consistency.\n- Advanced tools anonymize resumes by hiding details like names and photos, so decisions are based only on relevant information.\n- AI identifies patterns from past successful hires and compares them to current candidates to predict who is most likely to succeed.\n3. Keeping Qualified Candidates Engaged\nMore than 75% of job applicants prefer a faster, streamlined process and are less likely to recommend a company if it takes weeks to hear back. In fact, a study by Clutch says that 35% of candidates say it\u2019s unreasonable for companies to ghost applicants, which can damage your reputation. Even if you don\u2019t have the resources to respond to every candidate personally, automated updates can help prevent this and keep communication flowing.\nAI in the recruitment process makes this process seamless by:\n- Sending updates automatically: AI tools send instant updates about application status, interview schedules, or next steps. For example, when a recruiter moves a candidate to the next stage in the ATS, AI automatically notifies the candidate. This keeps job applicants in the loop and ensures they\u2019re not left wondering about their progress.\n- Answering questions instantly: Candidates often have common questions, like, \u201cWhat happens next?\u201d or \u201cWhen will I hear back?\u201d AI-powered chatbots can answer these questions in real-time, even outside of business hours. This helps job applicants feel supported and reduces the frustration of waiting for replies.\n- Making communication feel personal: Even though updates are automated, AI customizes them with the candidate\u2019s name and other details, ensuring the tone feels professional and aligned with your company\u2019s brand.\nDespite AI\u2019s benefits in hiring, there are instances where it falls short and human intervention becomes the need of an hour. Let\u2019s study it in the upcoming section.\nWhere AI Falls Short (And Why Humans Are Still Essential)\nWhile AI brings incredible efficiency and consistency to hiring, it\u2019s not without its flaws. Like any tool, AI works best when its limitations are understood and paired with human judgment. For job applicants, these limitations can impact their experience, making human oversight crucial to ensure a positive and engaging recruitment process. Here\u2019s a closer look at where AI in recruitment falls short and why human intervention is still important.\n1. AI Can\u2019t Fully Eliminate Bias\nAI is often used to reduce bias in hiring, but it\u2019s not a guaranteed fix. The reality is that AI learns from historical hiring data. If that data carries bias (e.g., past hiring preferences for a specific gender, ethnicity, or background), the AI will replicate those patterns.\nThis is why some AI tools have unintentionally ranked male candidates higher for leadership roles or dismissed candidates from underrepresented groups because they didn\u2019t match the historical profile of successful hires.\nAI may also prioritize resumes with frequent mentions of standard keywords, which can unintentionally favor applicants who know how to \u201cgame\u201d the system rather than those with the most potential. This leaves room for bias to creep in, even when AI is meant to eliminate it.\nHow humans make a difference:\nRecruiters can actively monitor AI-generated results to spot patterns of bias and ensure decisions align with the company\u2019s diversity goals. They can also conduct manual resume reviews for flagged cases, ensuring qualified candidates from non-traditional backgrounds or underrepresented groups aren\u2019t overlooked.\nRegular audits of AI decisions and retraining the system with diverse datasets\u2014including data from non-linear career paths, candidates with career breaks, or applicants from non-traditional industries\u2014are essential steps that only humans can lead effectively.\nFor example, if an AI tool consistently filters out candidates from certain demographics, recruiters can intervene to review those profiles manually and identify potential talent that the AI missed. This ensures fairness and prevents talented candidates from slipping through the cracks.\n2. Struggling with Creativity and Nuance\nAI tools are designed to work with structured, predictable data, but not all candidates or roles fit into neat boxes. For instance, candidates with unique career paths like someone transitioning from a different industry or someone with freelance experience\u2014may not score highly in AI rankings because their resumes don\u2019t match conventional patterns.\nSimilarly, resumes with creative or non-traditional formatting can confuse AI systems, leading to missed opportunities.\nAI also struggles to evaluate soft skills, like communication, leadership potential, or cultural fit, which are often critical for roles involving collaboration or creativity. While AI can predict success based on historical data, it may overlook candidates with high potential who don\u2019t match the exact profile of past hires.\nHow human intervention make a difference:\nRecruiters bring intuition and experience to the table, allowing them to recognize potential beyond what\u2019s written on paper.\nFor example:\n- They can spot transferable skills in candidates transitioning from other industries.\n- They can evaluate soft skills and cultural fit during interviews, which AI cannot measure.\n- They can assess nuanced qualities, like a candidate\u2019s creativity, problem-solving style, or ability to adapt to new challenges, by asking tailored questions during interviews.\nIn short, humans can look beyond the numbers and rankings to identify the hidden gems AI might miss. This is especially important for senior or leadership roles, where qualities like emotional intelligence, strategic thinking, and the ability to inspire others often matter more than technical qualifications.\n3. Losing the Human Touch in Recruitment\nOver-automation can make the hiring process feel robotic and impersonal, especially for job applicants. While AI excels at automating tasks like resume screening and scheduling, it can\u2019t replicate the empathy, understanding, and personal connection that humans provide.\nFor example, job applicants who only interact with chatbots and automated emails might feel disconnected or undervalued, especially in roles where they expect personalized communication. High-priority candidates, such as those with niche skills or leadership experience, may even lose interest in the role if they feel the company isn\u2019t invested in their experience.\nAI in recruitment also struggles with providing meaningful feedback to rejected candidates. Automated rejection emails often feel cold and generic, leaving candidates with a negative impression of the company.\nSo, how can human recruiters be of help here?\nThey help:\n- Build trust and relationships: Recruiters can connect personally with candidates through calls or face-to-face interactions, answering unique questions and addressing concerns that a chatbot simply can\u2019t handle.\n- Provide thoughtful feedback: Instead of sending generic rejection emails, recruiters can offer personalized feedback to help candidates improve for future opportunities. This leaves candidates with a positive impression of the company, even if they weren\u2019t selected.\n- Tailor the experience for top candidates: For high-value candidates, recruiters can customize the hiring process to make them feel prioritized and engaged. This could include assigning a dedicated point of contact or providing extra context about the company culture and role expectations.\nThe sole role of the human recruiter is to make the hiring process feel personal and respectful. This is how experienced talent is attracted, and top talent is retained for the long run. Remember, it is just a warm, human connection that often makes the difference between a candidate accepting an offer or choosing to walk away.\nAI vs. Human: Striking the Right Balance\nAI can handle a lot of the heavy lifting in hiring, but there are areas where human expertise makes all the difference.\nCheck down the table below see how AI and humans contribute to key recruitment tasks:\n| Recruitment Task | AI\u2019s Contribution | Human\u2019s Contribution |\n|---|---|---|\n| Resume Screening | High: Quickly scans and ranks resumes by keywords, skills, and qualifications. | Medium: Looks at unique resumes or career paths that AI might overlook. |\n| Candidate Engagement | Medium: Sends updates, reminders, and answers FAQs instantly. | Medium-High: Builds relationships, handles specific queries and provides personal feedback. |\n| Scheduling Interviews | High: Automates scheduling, syncs calendars, and sends reminders. | Low: Steps in only for special scheduling needs or priority candidates. |\n| Reducing Bias | Medium-High: Anonymizes resumes and applies consistent rules to reduce bias. | High: Audits AI decisions, checks fairness, and addresses nuanced bias issues. |\n| Decision-Making | Medium: Offers data-driven rankings and predicts potential success. | High: Evaluates soft skills cultural fit, and makes the final call using intuition and experience. |\nMake AI Work for You: Finding the Right Job Candidates\nAI can completely change the way you hire, but it\u2019s important to implement it thoughtfully. By enhancing talent acquisition, AI can automate high-impact tasks, allowing recruiters to focus on strategic activities. Instead of letting AI take over everything, allow it to be used where it makes the most sense and let humans handle the rest.\nHere\u2019s how you can make AI work for your hiring process, step by step.\n1. Start Small and Build Gradually\nImplementing AI doesn\u2019t mean overhauling your entire hiring process on day one. The best way to get started is by focusing on high-impact, time-consuming, but easy-to-automate areas. This allows your team to see immediate results without feeling overwhelmed and gradually improve the talent acquisition process. Such as:\n- Resume screening for high-volume roles: Start by automating resume screening, especially for roles that attract hundreds (or thousands) of applications. AI tools can scan resumes in seconds, ranking candidates based on job-specific criteria like skills, experience, and qualifications. This saves recruiters hours of manual effort, allowing them to spend more time interviewing top candidates.\n- Streamlining interview scheduling: Coordinating interviews can be a nightmare, especially when you\u2019re juggling multiple candidates and team members. AI scheduling tools sync calendars, send reminders, and eliminate back-and-forth emails, keeping the process efficient and organized.\nStarting small with tasks like these lets you build confidence in AI, identify what works best for your team, and scale gradually without disrupting your entire workflow.\n2. Use AI as a Partner for Hiring Managers, Not a Replacement\nAI is a powerful tool but works best as an assistant\u2014not the final decision-maker. While AI excels at analyzing data and providing recommendations, it can\u2019t assess a candidate\u2019s personality, creativity, or cultural fit. That\u2019s where recruiters step in during the talent acquisition process:\n- AI shortlists but humans decide: AI can screen and rank candidates based on objective criteria, but recruiters should always have the final say. For example, once AI creates a shortlist, recruiters can dive deeper into interviews to evaluate qualities like communication skills, leadership potential, and adaptability\u2014traits that don\u2019t always show up on paper.\n- Context matters at all costs: AI tools rely on patterns and data but might miss nuances that a human recruiter can catch. For example, a candidate with a non-traditional career path or unconventional resume formatting might be overlooked by AI but could turn out to be a perfect fit after a deeper conversation.\nBy using AI to handle the heavy lifting like screening/ranking, and relying on recruiters to make the final judgment, you get the best of both worlds: efficiency and empathy in the talent acquisition process.\n3. Regularly Monitor and Optimize AI Tools\nAI isn\u2019t a \u201cset it and forget it\u201d solution. To get the most out of your AI tools and enhance the talent acquisition process, you need to monitor their performance, track results, and adjust as needed.\n- Track key hiring metrics: Monitor important metrics like time-to-hire, quality of hires, and candidate satisfaction to measure the impact of AI on your hiring process. For example, has AI helped you hire faster? Are your new hires performing better? Is the candidate\u2019s experience improving?\n- Audit for biases or inefficiencies: Regular audits ensure your AI tools perform as expected. Check for signs of bias in the results (e.g., are certain groups of candidates being consistently filtered out?) and fine-tune the system to align with your company\u2019s goals.\n- Keep improving the process: AI tools evolve, and so should your hiring strategy. As your team gets more comfortable with AI, you can expand its use to other areas, like onboarding or predicting employee performance.\nBy regularly evaluating how AI fits into your workflow, you\u2019ll ensure that it continues to add value and adapt to your company\u2019s changing needs.\nThe Risks of Overpromising AI (and How to Avoid Them)\nAI has the potential to revolutionize hiring, but when companies overpromise its capabilities, it can negatively impact the talent acquisition process, leading to disappointment, missed opportunities, and even harm. To make the most of AI, it\u2019s important to understand its limits and use it thoughtfully. Here are two major risks of relying too heavily on AI in hiring and how to avoid them.\n1. Assuming AI Solves Bias Automatically\nAI is often marketed as a way to eliminate bias in hiring, but this isn\u2019t entirely true. AI can reduce bias, but only if it\u2019s trained on diverse, unbiased datasets. If the data used to train the system contains biases like hiring patterns favoring certain genders, ethnicities, or backgrounds, the AI will learn and repeat those biases. This is a significant concern in the talent acquisition process, where unchecked biases can perpetuate unfair hiring practices.\nFor example, there have been cases where AI tools ranked male candidates higher than females because the historical data favored men in leadership roles. Similarly, candidates from underrepresented groups may be overlooked if the data reflects hiring preferences that exclude them.\nHow to avoid this risk:\n- Audit AI tools regularly to check for biased outcomes. Look for patterns, like whether candidates from certain demographics are consistently being filtered out.\n- Use diverse and inclusive datasets when training AI systems. This ensures that the tool recognizes and values broader skills and experiences.\n- Pair AI with human oversight. Recruiters should regularly review AI-generated decisions to ensure fairness and make adjustments where needed.\nAI is a tool, not a complete solution. So, ensure a complete blend of AI in hiring with humans for fair and inclusive decisions.\n2. Over-Automation Hurts Candidate Experience\nAutomation is great for speeding up repetitive tasks like resume screening and scheduling, but too much of it can make the hiring process feel cold and impersonal. Job applicants want to feel valued and respected during the process. If their entire experience consists of chatbot interactions and automated emails, they might feel like just another number rather than a potential team member.\nFor example, high-priority candidates, like those with niche skills or leadership experience, expect personalized communication. Over-reliance on automation can disengage these job applicants, causing them to lose interest in the role or company.\nHow to avoid this risk:\n- Balance automation with human interaction. Use AI to handle routine tasks but keep human recruiters involved in key touchpoints, like interviews, follow-ups, and feedback.\n- Personalize the process. Even automated messages can feel more thoughtful if they include small personal touches, like using the candidate\u2019s name and referring to specific details about the role or their application.\n- Focus on high-value candidates. For top-tier candidates, go the extra mile by scheduling one-on-one conversations, sharing detailed insights about the company, and addressing their unique questions.\nThe goal is to use AI to enhance the candidate experience, not replace it.\nPeoplebox.ai\u2019s Take on Why AI in Hiring Works Best with Human Expertise\nAI in hiring is changing how companies recruit, making the process faster, smarter, and more efficient. It handles repetitive tasks, improves fairness in decision-making, and enhances the candidate experience. But AI isn\u2019t meant to replace recruiters\u2014it\u2019s a tool to help them do their jobs better.\nBy combining AI\u2019s speed and data-driven insights with human intuition and empathy, you can enhance the talent acquisition process, creating an efficient and personal hiring experience. AI takes care of the heavy lifting, like screening and scheduling, so recruiters can focus on what matters: finding candidates who fit the role and align with the company\u2019s culture and values.\nThe key is to use AI in the talent acquisition process thoughtfully. It\u2019s not a one-size-fits-all solution, but when implemented correctly, it can save time, reduce bias, and improve the entire hiring experience for both recruiters and candidates.\nAre you curious to see how AI can transform your hiring process?\nSchedule a demo with Peoplebox.ai today and take the first step toward smarter, faster, and more human hiring."
    },
    {
      "url": "https://www.ibm.com/think/topics/ai-bias#:~:text=Algorithm%20bias:%20Misinformation%20can%20result,identify%20new%20patterns%20or%20trends.",
      "text": "AI bias, also called machine learning bias or algorithm bias, refers to the occurrence of biased results due to human biases that skew the original training data or AI algorithm\u2014leading to distorted outputs and potentially harmful outcomes.\nWhen AI bias goes unaddressed, it can impact an organization\u2019s success and hinder people\u2019s ability to participate in the economy and society. Bias reduces AI\u2019s accuracy, and therefore its potential.\nBusinesses are less likely to benefit from systems that produce distorted results. And scandals resulting from AI bias could foster mistrust among people of color, women, people with disabilities, the LGBTQ community, or other marginalized groups.\nThe models upon which AI efforts are based absorb the biases of society that can be quietly embedded in the mountains of data they're trained on. Historically biased data collection that reflects societal inequity can result in harm to historically marginalized groups in use cases including hiring, policing, credit scoring and many others. According to The Wall Street Journal, \u201cAs use of artificial intelligence becomes more widespread, businesses are still struggling to address pervasive bias.\u201d1\nIndustry newsletter\nGet curated insights on the most important\u2014and intriguing\u2014AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\nWhen AI makes a mistake due to bias\u2014such as groups of people denied opportunities, misidentified in photos or punished unfairly\u2014the offending organization suffers damage to its brand and reputation. At the same time, the people in those groups and society as a whole can experience harm without even realizing it. Here are a few high-profile examples of disparities and bias in AI and the harm they can cause.\nIn healthcare, underrepresenting data of women or minority groups can skew predictive AI algorithms.2 For example, computer-aided diagnosis (CAD) systems have been found to return lower accuracy results for African-American patients than white patients.\nWhile AI tools can streamline the automation of resume scanning during a search to help identify ideal candidates, the information requested and answers screened out can result in disproportionate outcomes across groups. For example, if a job ad uses the word \u201cninja,\u201d it might attract more men than women, even though that is in no way a job requirement.3\nAs a test of image generation, Bloomberg requested more than 5,000 AI images be created and found that, \u201cThe world according to Stable Diffusion is run by white male CEOs. Women are rarely doctors, lawyers or judges. Men with dark skin commit crimes, while women with dark skin flip burgers.\u201d4 Midjourney conducted a similar study of AI art generation, requesting images of people in specialized professions. The result showed both younger and older people, but the older people were always men, reinforcing gender bias of the role of women in the workplace.5\nAI-powered predictive policing tools used by some organizations in the criminal justice system are supposed to identify areas where crime is likely to occur. However, they often rely on historical arrest data, which can reinforce existing patterns of racial profiling and disproportionate targeting of minority communities.6\nDistorted results can harm organizations and society at large. Here are a few of the more common types of AI bias7.\nOut-group homogeneity bias: This is a case of not knowing what one doesn\u2019t know. There is a tendency for people to have a better understanding of ingroup members\u2014the group one belongs to\u2014and to think they are more diverse than outgroup members. The result can be developers creating algorithms that are less capable of distinguishing between individuals who are not part of the majority group in the training data, leading to racial bias, misclassification and incorrect answers.\nThe first step in avoiding the bias trap is just to step back at the beginning and give an AI effort some thought. As is true with almost any business challenge, problems are much easier to fix up-front rather than waiting for the train wreck and then sorting through the damaged result. But many organizations are in a rush: penny-wise-and-pound-foolish, and it costs them.\nIdentifying and addressing bias in AI requires AI governance, or the ability to direct, manage and monitor the AI activities of an organization. In practice, AI governance creates a set of policies, practices and frameworks to guide the responsible development and use of AI technologies. When done well, AI governance helps to ensure that there is a balance of benefits bestowed upon businesses, customers, employees and society as a whole.\nAI governance often includes methods that aim to assess fairness, equity and inclusion. Approaches such as counterfactual fairness identifies bias in a model\u2019s decision making and ensures equitable results, even when sensitive attributes, such as gender, race or sexual orientation are included.\nBecause of the complexity of AI, an algorithm can be a black box system with little insight into the data used to create it. Transparency practices and technologies help ensure that unbiased data is used to build the system and that results will be fair. Companies that work to protect customers\u2019 information build brand trust and are more likely to create trustworthy AI systems.\nTo provide another layer of quality assurance, institute a \u201chuman-in-the-loop\u201d system to offer options or make recommendations that can then be approved by human decisions.\nHere\u2019s a checklist of six process steps that can keep AI programs free of bias.\n1. Select the correct learning model:\n2. Train with the right data: Machine learning trained on the wrong data will produce wrong results. Whatever data is fed into the AI should be complete and balanced to replicate the actual demographics of the group being considered.\n3. Choose a balanced team: The more varied the AI team\u2014racially, economically, by educational level, by gender and by job description\u2014the more likely it will recognize bias. The talents and viewpoints on a well-rounded AI team should include AI business innovators, AI creators, AI implementers, and a representation of the consumers of this particular AI effort.9\n4. Perform data processing mindfully: Businesses need to be aware of bias at each step when processing data. The risk is not just in data selection: whether during pre-processing, in-processing or post-processing, bias can creep in at any point and be fed into the AI.\n5. Continually monitor: No model is ever complete or permanent. Ongoing monitoring and testing with real-world data from across an organization can help detect and correct bias before it causes harm. To further avoid bias, organizations should consider assessments by an independent team from within the organization or a trusted third-party.\n6. Avoid infrastructural issues: Aside from human and data influences, sometimes infrastructure itself can cause bias. For example, using data collected from mechanical sensors, the equipment itself could inject bias if the sensors are malfunctioning. This kind of bias can be difficult to detect and requires investment in the latest digital and technological infrastructures.\nGovern generative AI models from anywhere and deploy on the cloud or on premises with IBM watsonx.governance.\nSee how AI governance can help increase your employees\u2019 confidence in AI, accelerate adoption and innovation, and improve customer trust.\nPrepare for the EU AI Act and establish a responsible AI governance approach with the help of IBM Consulting."
    }
  ],
  "argos_summary": "AI tools are increasingly used in hiring processes, with a projected 68% of companies adopting them by 2025. However, studies reveal that these tools often exhibit biases, particularly against underrepresented racial groups, leading to unfair filtering of candidates. To mitigate these biases, experts recommend regular audits of AI tools, continuous monitoring, and ensuring human oversight in hiring decisions. Despite the efficiency AI brings, it is crucial to combine its use with human judgment to maintain fairness and inclusivity in recruitment.",
  "argos_id": "QXP7TCPQ3"
}